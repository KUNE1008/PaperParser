{
    "title": "Sparseness meets deepness: 3D human pose estimation from monocular video",
    "id": 4,
    "valid_pdf_number": "298/375",
    "matched_pdf_number": "242/298",
    "matched_rate": 0.8120805369127517,
    "citations": {
        "Learning to reconstruct 3D human pose and shape via model-fitting in the loop": {
            "authors": [
                "Nikos Kolotouros",
                "Georgios Pavlakos",
                "Michael J. Black",
                "Kostas Daniilidis"
            ],
            "url": "http://openaccess.thecvf.com/content_ICCV_2019/papers/Kolotouros_Learning_to_Reconstruct_3D_Human_Pose_and_Shape_via_Model-Fitting_ICCV_2019_paper.pdf",
            "ref_texts": "[45] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3D human pose estimation from monocular video. In CVPR , 2016. 3",
            "ref_ids": [
                "45"
            ],
            "1": "Related work Recent works have made significant advances in the frontier of skeleton-based 3D human pose estimation from single images, with many approaches achieving impressive results [21,23,29,33,35,45]."
        },
        "End-to-end recovery of human shape and pose": {
            "authors": [
                "Angjoo Kanazawa",
                "Michael J. Black",
                "David W. Jacobs",
                "Jitendra Malik"
            ],
            "url": "https://openaccess.thecvf.com/content_cvpr_2018/papers/Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper.pdf",
            "ref_texts": "[54] X. Zhou, M. Zhu, S. Leonardos, K. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3D human pose estimation from monocular video. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR , pages 4966\u2013",
            "ref_ids": [
                "54"
            ],
            "1": "Two stage methods first predict 2D joint locations using 2D pose detectors [30,49,54] or ground truth 2D pose and then predict 3D joint locations from the 2D joints either by regression [26,29] or model fitting, where a common approach exploits a learned dictionary of 3D skeletons [2,34,47,37,53,54].",
            "2": "3\n[54] X."
        },
        "3d human pose estimation in video with temporal convolutions and semi-supervised training": {
            "authors": [
                "Dario Pavllo",
                "Christoph Feichtenhofer",
                "David Grangier",
                "Michael Auli"
            ],
            "url": "https://openaccess.thecvf.com/content_CVPR_2019/papers/Pavllo_3D_Human_Pose_Estimation_in_Video_With_Temporal_Convolutions_and_CVPR_2019_paper.pdf",
            "ref_texts": "[61] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Conference on Computer Vision and Pattern Recognition (CVPR) , 2016. 4,5",
            "ref_ids": [
                "61"
            ],
            "1": "In our experiments, we consider three evaluation protocols: Protocol 1 is the mean per-joint position error (MPJPE) in millimeters which is the mean Euclidean distance between predicted joint positions and ground-truth joint positions and follows [29,53,61,34,41].",
            "2": "Implementation details for 3D pose estimation For consistency with other work [34,29,53,61,34,41], we train and evaluate on 3D poses in camera space by rotating and translating the ground-truth poses according to the camera transformation, and not using the global trajectory (except for the semi-supervised setting, \u00a74).",
            "3": "2,4\n[61] X."
        },
        "A simple yet effective baseline for 3d human pose estimation": {
            "authors": [
                "Julieta Martinez",
                "Rayat Hossain",
                "Javier Romero",
                "James J. Little"
            ],
            "url": "https://openaccess.thecvf.com/content_ICCV_2017/papers/Martinez_A_Simple_yet_ICCV_2017_paper.pdf",
            "ref_texts": "[56] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In CVPR , 2016. 2,3,4,5",
            "ref_ids": [
                "56"
            ],
            "1": "Another way of compiling knowledge about 3d human pose from datasets is by creating overcomplete bases suitable for representing human poses as sparse combinations [2,7,36,49,55,56], lifting the pose to a reproducible kernel Hilbert space (RHKS) [18] or by creating novel priors from specialized datasets of extreme human poses [2].",
            "2": "2d/3d positions Our first design choice is to use 2d and 3d points as inputs and outputs, in contrast to recent work that has used raw images [11,13,24,32,33,45,46,54,56] or 2d probability distributions [33,56] as inputs, and 3d probabilities [33], 3d motion parameters [54] or basis pose coefficients and camera parameter estimation [2,7,36,55,56] as outputs.",
            "3": "2642\n Linear-RELU layers Most deep learning approaches to 3d human pose estimation are based on convolutional neural networks, which learn translation-invariant filters that can be applied to entire images [13,24,32,33,45], or 2dimensional joint-location heatmaps [33,56].",
            "4": "A natural choice of global coordinate frame is the camera frame [11,24,33,46,54,56] since this makes the 2d to 3d problem similar across different cameras, implicitly enabling more training data per camera and preventing overfitting to a particular global coordinate frame.",
            "5": "[56] (MA) 87.",
            "6": "2,3,6\n[56] X."
        },
        "Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1607.08128",
            "ref_texts": "59. Zhou, X., Zhu, M., Leonardos, S., Derpanis, K., Daniilidis, K.: Sparseness meets deepness: 3D human pose estimation from monocular video. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR. pp. 4447{4455 (2016)",
            "ref_ids": [
                "59"
            ],
            "1": "Recent work [59] uses a CNN to estimate 2D joint locations and then fft 3D pose to these using a monocular video sequence.",
            "2": "Following [27,49,59], we report results on sequences of subjects S9 and S11."
        },
        "Vnect: Real-time 3d human pose estimation with a single rgb camera": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1705.01583.pdf%C3%AF%C2%BC%E2%80%B0%C3%A6%CB%86%E2%80%93[%C3%A8%C2%BF%E2%84%A2%C3%A4%C2%B8%E2%82%AC%C3%A4%C2%B8%C2%AA]%C3%AF%C2%BC%CB%86https://arxiv.org/",
            "ref_texts": "1997. Pfinder: real-time tracking of the human body. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) 19, 7 (1997), 780\u2013785. Hashim Yasin, Umar Iqbal, Bj\u00f6rn Kr\u00fcger, Andreas Weber, and Juergen Gall. 2016. A Dual-Source Approach for 3D Pose Estimation from a Single Image. In Conference on Computer Vision and Pattern Recognition (CVPR) . Mao Ye and Ruigang Yang. 2014. Real-time simultaneous pose and shape estimation for articulated objects using a single depth camera. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 2345\u20132352. Yongkang Yu, Feilinand Yonghao, Zhen Yilin, and Weidong Mohan. 2016. Marker-less 3D Human Motion Capture with Monocular Image Sequence and Height-Maps. In European Conference on Computer Vision (ECCV) . Matthew D Zeiler. 2012. ADADELTA: an adaptive learning rate method. arXiv preprint arXiv:1212.5701 (2012). Xiaowei Zhou, Spyridon Leonardos, Xiaoyan Hu, and Kostas Daniilidis. 2015. 3D shape estimation from 2D landmarks: A convex relaxation approach. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . 4447\u20134455. Xingyi Zhou, Xiao Sun, Wei Zhang, Shuang Liang, and Yichen Wei. 2016. Deep Kinematic Pose Regression. ECCV Worktp on Geometry Meets Deep Learning . Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, and Kostas Daniilidis. 2015a. Sparse Representation for 3D Shape Estimation: A Convex Relaxation Approach. arXiv preprint arXiv:1509.04309 (2015). Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Kosta Derpanis, and Kostas Daniilidis. 2015b. Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . Yingying Zhu, Mark Cox, and Simon Lucey. 2011. 3D motion reconstruction for realworld camera motion. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on . IEEE, 1\u20138. Michael Zollh\u00f6fer, Matthias Nie\u00dfner, Shahram Izadi, Christoph Rhemann, Christopher Zach, Matthew Fisher, Chenglei Wu, Andrew Fitzgibbon, Charles Loop, Christian Theobalt, and Marc Stamminger. 2014. Real-time Non-rigid Reconstruction using an RGB-D Camera. ACM Transactions on Graphics (TOG) 33, 4 (2014).",
            "ref_ids": [
                "1997"
            ]
        },
        "Integral human pose regression": {
            "authors": [
                "Xiao Sun",
                "Bin Xiao",
                "Fangyin Wei",
                "Shuang Liang",
                "Yichen Wei"
            ],
            "url": "http://openaccess.thecvf.com/content_ECCV_2018/papers/Xiao_Sun_Integral_Human_Pose_ECCV_2018_paper.pdf",
            "ref_texts": "53. Zhou, X., Zhu, M., Leonardos, S., Derpanis, K.G., Daniilidis , K.: Sparseness meets deepness: 3d human pose estimation from monocular video. In: P roceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp . 4966\u20134975",
            "ref_ids": [
                "53"
            ],
            "1": "For evaluation, many previous works [8,46,32,54,25,31,37,51,41,4,53, 44,56] use the mean per joint position error (MPJPE).",
            "2": "Ours is the best Method Zhou[53] Tekin[44] Xingyi[56] Sun [42] Pavlakos[37] Ours MPJPE 113."
        },
        "Learning from synthetic humans": {
            "authors": [
                "Gul Varol",
                "Javier Romero",
                "Xavier Martin",
                "Naureen Mahmood",
                "Michael J. Black",
                "Ivan Laptev",
                "Cordelia Schmid"
            ],
            "url": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Varol_Learning_From_Synthetic_CVPR_2017_paper.pdf",
            "ref_texts": "[42] X. Zhou, M. Zhu, S. Leonardos, K. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3D human pose estimation from monocular video. CVPR , 2016. 2",
            "ref_ids": [
                "42"
            ],
            "1": "Several works focused on creating synthetic images of human bodies for learning 2D pose estimation [26,29,35], 3D pose estimation [7,9,14,23,34,42], pedestrian detection [21,26,27], and action recognition [30,31].",
            "2": "4\n[42] X."
        },
        "Coarse-to-fine volumetric prediction for single-image 3D human pose": {
            "authors": [
                "Georgios Pavlakos",
                "Xiaowei Zhou",
                "Konstantinos G. Derpanis",
                "Kostas Daniilidis"
            ],
            "url": "http://openaccess.thecvf.com/content_cvpr_2017/papers/Pavlakos_Coarse-To-Fine_Volumetric_Prediction_CVPR_2017_paper.pdf",
            "ref_texts": "[45] X. Zhou, M. Zhu, S. Leonardos, K. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3D human pose estimation from monocular video. In CVPR , 2016. 1,2,5,7,8",
            "ref_ids": [
                "45"
            ],
            "1": ", [45, 6].",
            "2": "[45] utilize a standard 2D pose ConvNet to localize the joints and retrieve the 3D pose using an optimization scheme over a sequence of monocular images.",
            "3": "We follow the same evaluation protocol as prior work [20,45].",
            "4": "The original videos were downsampled from 50fps to 10fps We employed all camera views and trained a single model for all actions , instead of training actionspecific models [20,45].",
            "5": "Note that some previous works [36,45,10] leverage a sequence of frames for pose prediction rather than a single frame as considered by our approach.",
            "6": "[45] 87.",
            "7": "[45] 124.",
            "8": "Note, several approaches use video for prediction rather than a single frame [36,45,10].",
            "9": "2,6,8\n[45] X."
        },
        "Monocular 3d human pose estimation in the wild using improved cnn supervision": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1611.09813.pdf?source=post_page---------------------------",
            "ref_texts": "[83] X. Zhou, M. Zhu, S. Leonardos, K. Derpanis, and K. Daniilidis. Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2015. 1, 2, 7, 10, 11",
            "ref_ids": [
                "83"
            ],
            "1": "Some 3D pose estimation approaches take advantage of this generalizability of 2D pose estimation, and propose to lift the 2D keypoints to 3D [69, 76, 9, 73, 36, 80, 83, 79, 60, 59, 14].",
            "2": "Recent advances in direct CNNbased 3D regression show promise, utilizing different prediction space formulations [65, 35, 81, 44, 40] and incorporating additional constraints [81, 67, 83, 78].",
            "3": "Video input provides temporal cues, which translate to increased accuracy [67, 83].",
            "4": "[83]T,J17,B,10,Act113.",
            "5": "[83], any regression method that works on cropped images could immediately profit from this perspective correction, without computing 2D keypoint detections.",
            "6": "[83]\u0018 34.",
            "7": "2\n[83] X."
        },
        "Hybrik: A hybrid analytical-neural inverse kinematics solution for 3d human pose and shape estimation": {
            "authors": [
                "Jiefeng Li",
                "Chao Xu",
                "Zhicun Chen",
                "Siyuan Bian",
                "Lixin Yang",
                "Cewu Lu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Li_HybrIK_A_Hybrid_Analytical-Neural_Inverse_Kinematics_Solution_for_3D_Human_CVPR_2021_paper.pdf",
            "ref_texts": "[77] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In CVPR , 2016. 2",
            "ref_ids": [
                "77"
            ],
            "1": "Two-stage approaches first estimate 2D pose and then lift them to 3D joint locations by a learned dictionary of 3D skeleton [1,53,64,56,76,77] or regression [46,71,40,11,32,59]."
        },
        "Semantic graph convolutional networks for 3d human pose regression": {
            "authors": [
                "Long Zhao",
                "Xi Peng",
                "Yu Tian",
                "Mubbasir Kapadia",
                "Dimitris N. Metaxas"
            ],
            "url": "https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhao_Semantic_Graph_Convolutional_Networks_for_3D_Human_Pose_Regression_CVPR_2019_paper.pdf",
            "ref_texts": "[77] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3D human pose estimation from monocular video. In CVPR , pages 4966\u20134975, 2016.",
            "ref_ids": [
                "77"
            ],
            "1": "Other methods created over-complete bases which are suitable for representing human poses as sparse combinations [2, 4, 44, 62, 77].",
            "2": "[77] CVPR\u201916 87."
        },
        "Learning to estimate 3D human pose and shape from a single color image": {
            "authors": [
                "Georgios Pavlakos",
                "Luyang Zhu",
                "Xiaowei Zhou",
                "Kostas Daniilidis"
            ],
            "url": "https://openaccess.thecvf.com/content_cvpr_2018/papers/Pavlakos_Learning_to_Estimate_CVPR_2018_paper.pdf",
            "ref_texts": "[57] X. Zhou, M. Zhu, S. Leonardos, K. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3D human pose estimation from monocular video. In CVPR , 2016. 2",
            "ref_ids": [
                "57"
            ],
            "1": "Other approaches commit to the 2D pose estimates provided by state-of-the-art ConvNets and focus on the 3D pose reconstruction [29,57], recover 3D pose exemplars [8], or produce multiple 3D pose candidates consistent with the 2D pose [18].",
            "2": "7\n[57] X."
        },
        "Neural body fitting: Unifying deep learning and model based human pose and shape estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1808.05942",
            "ref_texts": "[69] X. Zhou, M. Zhu, S. Leonardos, K. Derpanis, and K. Daniilidis. Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2015. 3",
            "ref_ids": [
                "69"
            ],
            "1": "Similar to model-based methods, learning approaches have benefited from the advent of robust 2D pose methods \u2013 by matching 2D detections to a 3D pose database [8, 66], by regressing pose from 2D joint distance matrices [35], by exploiting pose and geometric priors for lifting [69, 1, 51, 19, 32, 70, 47]; or simply by training a feed forward network to directly predict 3D pose from 2D joints [30].",
            "2": "7\n[69] X."
        },
        "Learning 3d human dynamics from video": {
            "authors": [
                "Angjoo Kanazawa",
                "Jason Y. Zhang",
                "Panna Felsen",
                "Jitendra Malik"
            ],
            "url": "https://openaccess.thecvf.com/content_CVPR_2019/papers/Kanazawa_Learning_3D_Human_Dynamics_From_Video_CVPR_2019_paper.pdf",
            "ref_texts": "[65] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In CVPR , pages 4966\u2013",
            "ref_ids": [
                "65"
            ],
            "1": "Most approaches take a two-stage approach: first obtaining a singleview 3D reconstruction and then post-processing the result to be smooth via solving a constrained optimization problem [65,57,45,46,26,37,42].",
            "2": "2\n[65] X."
        },
        "Towards 3d human pose estimation in the wild: a weakly-supervised approach": {
            "authors": [
                "Xingyi Zhou",
                "Qixing Huang",
                "Xiao Sun",
                "Xiangyang Xue",
                "Yichen Wei"
            ],
            "url": "http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhou_Towards_3D_Human_ICCV_2017_paper.pdf",
            "ref_texts": "[34] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4966\u20134975, 2016. 2,4,5",
            "ref_ids": [
                "34"
            ],
            "1": "They usually proceed in two sequential steps [34,26,5,3,30,31].",
            "2": "Similar to previous works [34,26,5,3,30,31], our network also consists of a 2D module and a 3D module.",
            "3": "A standard approach to address the domain difference between 3D human pose estimation datasets and images in the wild is to split the task into two separate subtasks [34,26,5,3,30].",
            "4": "Regarding 3D pose estimation from 2D joint locations, [34] use an EM algorithm to compute a 3D skeleton by combining a sparse dictionary induced from the 2D heat-maps; [30,19] use 3D pose data and its 2D projection to train a heatmap-to-3D pose network without the original image; Bogo et al.",
            "5": "A widely used strategy in previous [34,26,5] is to take the 2D joint locations as the only input for depth prediction as in this way the Mocap-only data can be utilized.",
            "6": "Following the standard protocol in [13,34,33], we use5subjects(S1, S5, S6, S7, S8) for training and the rest2subjects(S9, S11) for testing.",
            "7": "1,2,5,6\n[34] X."
        },
        "3d human pose estimation= 2d pose estimation+ matching": {
            "authors": [
                "Hang Chen",
                "Deva Ramanan"
            ],
            "url": "http://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_3D_Human_Pose_CVPR_2017_paper.pdf",
            "ref_texts": "[37] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In CVPR , 2016.",
            "ref_ids": [
                "37"
            ],
            "1": "Previous approaches often make use of a highly sensored environment, including video streams [37, 30], multiview cameras [3, 10], depth images [23, 36, 27].",
            "2": "This is classically treated as a constrained optimization problem who\u2019s objective minimizes the 2D reprojection error of an unknown 3D pose and unknown camera [37, 32, 24, 2].",
            "3": "The optimization problem is often subject to kinematic constraints [34, 29], and sometimes 3D poses are assumed to live a in low-dimensional subspace to better condition the optimization [37].",
            "4": "Protocol 2: Others [37, 30, 17] use five subjects (S1, S5, S6, S7, S8) for training, and two subjects (S9, S11) for testing.",
            "5": "We follow [37]\u2019s setup that downsamples thevideos from 50fps to10fps.",
            "6": "Comparison to state-of-the-art (Protocol 2) Final system: Table 4 provides the comparison to [37] and [30] using Protocol 2.",
            "7": "A qualitative comparison to [37] is also provided in Fig.",
            "8": "Therefore, we investigate the case given ground truth 2D pose, following Zhou\u2019s diagnostic protocol [37]: evaluate MPJPE up to a 3D rigid body transformation including scale, only on the first 30 seconds of the first cam7038\n Images with 2D pose Estimation 3D pose in a novel view Figure 4.",
            "9": "With a shortlist of k=1 0 matches, camera resectioning (5) and exemplar warping (6) produces a slightly lower error than [37]\u2019s approach without a 3D prior.",
            "10": "We posit that a more restricted 3D pose prior (implicitly enforced by a small 7040\n Method Direction Discuss Eat Greet Phone Pose Purchase Sit SitDown Zhou [37] 87.",
            "11": "Median Zhou [37] 107.",
            "12": "Comparison to [37] and [30] by Protocol 2 .",
            "13": "Qualitative comparison of Zhou [37] with our results.",
            "14": "Qualitative comparison of Zhou [37] with our results, given access to the same ground-truth 2D pose .",
            "15": "98 Zhou [37] |gt, single-frame 50.",
            "16": "04 Zhou [37] |gt, multi-frame 49.",
            "17": "[37] X."
        },
        "Lifting from the deep: Convolutional 3d pose estimation from a single image": {
            "authors": [
                "Denis Tome",
                "Chris Russell",
                "Lourdes Agapito"
            ],
            "url": "http://openaccess.thecvf.com/content_cvpr_2017/papers/Tome_Lifting_From_the_CVPR_2017_paper.pdf",
            "ref_texts": "[50] X. Zhou, M. Zhu, S. Leonardos, K. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. arXiv preprint arXiv:1511.09439 , 2015. 3,5,7,8",
            "ref_ids": [
                "50"
            ],
            "1": "[50] tackles the problem of 3D pose estimation for a monocular image sequence integrating 2D, 3D and temporal information to account for uncertainties in the model and the measurements.",
            "2": "\u2019s method [50] does not need synchronized 2D-3D training data, i.",
            "3": "Lifting 2D belief\u00admaps into 3D We follow [50] in assuming a weak perspective model, and first describe the simplest case of estimating the 3D pose of a single frame using a unimodal Gaussian 3D pose model as described in section 4.",
            "4": "[50] 87.",
            "5": "[50] 124.",
            "6": "Note that some approaches [37,50] use video as input instead of a single frame.",
            "7": "[50] to the state-of-the-art 2d architecture [44] (i.",
            "8": "[50] 10.",
            "9": "Note that [50] use video as input and knowledge of the action label.",
            "10": "6M, was followed by [15,22,37,35,36,50,31].",
            "11": "2\n[50] X."
        },
        "3d human pose estimation in the wild by adversarial learning": {
            "authors": [
                "Wei Yang",
                "Wanli Ouyang",
                "Xiaolong Wang",
                "Jimmy Ren",
                "Hongsheng Li",
                "Xiaogang Wang"
            ],
            "url": "http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_3D_Human_Pose_CVPR_2018_paper.pdf",
            "ref_texts": "[58] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In CVPR , 2016. 2[59] J.-Y . Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired imageto-image translation using cycle-consistent adversarial networks. ICCV , 2017. 3,7",
            "ref_ids": [
                "58",
                "59"
            ],
            "1": "Two-stage approaches first estimate 2D poses and then lift 2D poses to 3D poses [58,4,2,51,28,40,25,57,30].",
            "2": "Adversarial learning has been proven effective not only for generative tasks [16,33,46,59,47,10,18,55,21,20,45,23], but also for discriminative tasks [48,50,7,6,36].",
            "3": "Adversarial learning: from scratch or not? The standard practice to train GANs is to learn the generator and the discriminator alternately from scratch [16,33,46,59].",
            "4": "The generator is usually conditioned on noise [33], text [55] or images [59], and lacks of ground-truth for supervised training.",
            "5": "2,3,5,6\n[58] X.",
            "6": "2[59] J."
        },
        "Self-supervised learning of 3d human pose using multi-view geometry": {
            "authors": [
                "Muhammed Kocabas",
                "Salih Karagoz",
                "Emre Akbas"
            ],
            "url": "https://openaccess.thecvf.com/content_CVPR_2019/papers/Kocabas_Self-Supervised_Learning_of_3D_Human_Pose_Using_Multi-View_Geometry_CVPR_2019_paper.pdf",
            "ref_texts": "[46] Xiaowei Zhou, Menglong Zhu, Kosta Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3D human pose estimation from monocular video. In IEEE Conference on Computer Vision and Pattern Recognition , 2016. 2",
            "ref_ids": [
                "46"
            ],
            "1": "Additionally, there are two-stage approaches which decompose the 3D pose inference task into two independent stages: estimating 2D poses, and lifting them into 3D space [8,24,22,11,46,8,40,23]."
        },
        "Modulated graph convolutional network for 3D human pose estimation": {
            "authors": [
                "Zhiming Zou",
                "Wei Tang"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Zou_Modulated_Graph_Convolutional_Network_for_3D_Human_Pose_Estimation_ICCV_2021_paper.pdf",
            "ref_texts": "[62] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 4966\u20134975, 2016.",
            "ref_ids": [
                "62"
            ],
            "1": "[62] leverage a sparsity-driven 3D geometric prior and temporal smoothness to infer 3D poses from uncertain 2D keypoint maps via the EM algorithm."
        },
        "Ordinal depth supervision for 3d human pose estimation": {
            "authors": [
                "Georgios Pavlakos",
                "Xiaowei Zhou",
                "Kostas Daniilidis"
            ],
            "url": "https://openaccess.thecvf.com/content_cvpr_2018/papers/Pavlakos_Ordinal_Depth_Supervision_CVPR_2018_paper.pdf",
            "ref_texts": "[68] X. Zhou, M. Zhu, S. Leonardos, K. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3D human pose estimation from monocular video. In CVPR , 2016. 2,5,7",
            "ref_ids": [
                "68"
            ],
            "1": "[68,69] use 2D heatmaps from a 2D pose ConvNet to reconstruct 3D pose in a video sequence.",
            "2": ", [68]), we train using subjects S1,S5,S6,S7, and S8 and test on subjects S9 and S11.",
            "3": "[68] (CVPR\u201916) 87.",
            "4": "7\n[68] X."
        },
        "6-dof object pose from semantic keypoints": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1703.04670",
            "ref_texts": "[27] X. Zhou, M. Zhu, S. Leonardos, K. Derpanis, and K. Daniilidis, \u201cSparseness meets deepness: 3D human pose estimation from monocular video,\u201d in CVPR , 2015.",
            "ref_ids": [
                "27"
            ],
            "1": ", [26], [27], [28], [29]) has attracted considerable study, while limited attention has been given to their application with generic object categories [30], [24].",
            "2": "[27] X."
        },
        "Single-shot multi-person 3d pose estimation from monocular rgb": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1712.03453",
            "ref_texts": "[69] X. Zhou, M. Zhu, S. Leonardos, K. Derpanis, and K. Daniilidis. Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2015.",
            "ref_ids": [
                "69"
            ],
            "1": "Some works split the problem in two: ffrst estimate 2D joints and then lift them to 3D [58, 54, 9, 66, 35, 69, 1, 52, 20, 32, 6, 26, 38, 57, 2], e.",
            "2": "[69] X."
        },
        "Exploiting temporal information for 3d human pose estimation": {
            "authors": [
                "Mir Rayat",
                "Imtiaz Hossain",
                "Jim Little"
            ],
            "url": "https://openaccess.thecvf.com/content_ECCV_2018/papers/Mir_Rayat_Imtiaz_Hossain_Exploiting_temporal_information_ECCV_2018_paper.pdf",
            "ref_texts": "20. Zhou, X., Zhu, M., Leonardos, S., Derpanis, K.G., Daniilidis , K.: Sparseness meets deepness: 3d human pose estimation from monocular video. In: I EEE Conference on Computer Vision and Pattern Recognition (CVPR). (2016) 49 66\u20134975",
            "ref_ids": [
                "20"
            ],
            "1": "Othe rs model 3D pose as a sparse linear combination of an over-complete dictionary of basis poses [21,20,19].",
            "2": "Another common approach to estimating 3D joint locations given the 2D pose is to separate the camera pose variability from the intrinsic de formation of the human body, the latter of which is modeled by learning an overcomplete dictionary of basis 3D poses from a large database of motion capture data [19, 20,22,21,37].",
            "3": "Using temporal information Since estimating poses for each frame individually leads to incoherent and jittery predictions over a sequence, many approaches tried to exploit temporal information [42,43,20,44,11].",
            "4": "0 Zhou et al [20] (MA) 87.",
            "5": "3 Zhou et al [20]* (MA) 14j 99.",
            "6": "[20] did not find NRSFM techniques to b e effective for 3D human pose estimation."
        },
        "Deep kinematics analysis for monocular 3d human pose estimation": {
            "authors": [
                "Jingwei Xu",
                "Zhenbo Yu",
                "Bingbing Ni",
                "Jiancheng Yang",
                "Xiaokang Yang",
                "Wenjun Zhang"
            ],
            "url": "https://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_Deep_Kinematics_Analysis_for_Monocular_3D_Human_Pose_Estimation_CVPR_2020_paper.pdf",
            "ref_texts": "[55] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In CVPR , pages 4966\u20134975, 2016.",
            "ref_ids": [
                "55"
            ],
            "1": "To avoid collecting 2D-3D paired data, a variety of works [55, 25, 43, 4, 9, 53, 52, 28, 38, 7] decouple the task of 3D pose estimation into two independent stages: (1) firstly predicting 2D joint location in image space using off-the-shelf 2D pose estimation methods; (2) and then learning a mapping to lift them to 3D space."
        },
        "Learning convolutional networks for content-weighted image compression": {
            "authors": [
                "Mu Li",
                "Wangmeng Zuo",
                "Shuhang Gu",
                "Debin Zhao",
                "David Zhang"
            ],
            "url": "https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Learning_Convolutional_Networks_CVPR_2018_paper.pdf",
            "ref_texts": ""
        },
        "Cascaded deep monocular 3d human pose estimation with evolutionary training data": {
            "authors": [
                "Shichao Li",
                "Lei Ke",
                "Kevin Pratama",
                "Wing Tai",
                "Keung Tang",
                "Ting Cheng"
            ],
            "url": "http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Cascaded_Deep_Monocular_3D_Human_Pose_Estimation_With_Evolutionary_Training_CVPR_2020_paper.pdf",
            "ref_texts": "[70] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 4966\u20134975, 2016.",
            "ref_ids": [
                "70"
            ],
            "1": "These approaches represent humans by PCA models [2, 70], graphical models [8, 5] or deformable meshes [4, 30, 7, 42, 24]."
        },
        "3d human pose estimation from a single image via distance matrix regression": {
            "authors": [
                "Francesc Moreno"
            ],
            "url": "http://openaccess.thecvf.com/content_cvpr_2017/papers/Moreno-Noguer_3D_Human_Pose_CVPR_2017_paper.pdf",
            "ref_texts": "[56] X. Zhou, M. Zhu, S. Leonardos, K. Derpanis, and K. Daniilidis. Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video. In Conference on Computer Vision and Pattern Recognition , 2016. 5,6,7",
            "ref_ids": [
                "56"
            ],
            "1": "This is the same evaluation protocol used by the baselines we compare against [3,6,8,10,12,21,30,31,38,39,40,45,47, 50,52,55,56].",
            "2": "This protocol is used in [17,23,34,46,47,56].",
            "3": "97 Zhou CVPR\u201916 [56] 87.",
            "4": "5,6,7\n[56] X."
        },
        "Recent advances of monocular 2d and 3d human pose estimation: A deep learning perspective": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2104.11536",
            "ref_texts": "[221] X.-W. Zhou, M.-L. Zhu, S. Leonardos, K.-G. Derpanis, and K. Daniilidis, \u201cSparseness meets deepness: 3d human pose estimation from monocular video,\u201d in CVPR , 2016.",
            "ref_ids": [
                "221"
            ],
            "1": "[221] CVPR\u201916 113.",
            "2": "[221] X."
        },
        "Denserac: Joint 3d pose and shape estimation by dense render-and-compare": {
            "authors": [
                "Yuanlu Xu",
                "Chun Zhu",
                "Tony Tung"
            ],
            "url": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_DenseRaC_Joint_3D_Pose_and_Shape_Estimation_by_Dense_Render-and-Compare_ICCV_2019_paper.pdf",
            "ref_texts": "[63] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In IEEE Conference on Computer Vision and Pattern Recognition , 2016. 2",
            "ref_ids": [
                "63"
            ],
            "1": "On the other hand, many approaches lift 2D human poses [8,5], used as intermediate representation, and learn a model for 2D-3D pose space mapping [61,63,62,34,9]."
        },
        "Lcr-net++: Multi-person 2d and 3d pose detection in natural images": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1803.00455",
            "ref_texts": "[44] X. Zhou, M. Zhu, S. Leonardos, K. Derpanis, and K. Daniilidis, \u201cSparseness meets deepness: 3D human pose estimation from monocular video,\u201d inCVPR , 2016. 3, 6, 9",
            "ref_ids": [
                "44"
            ],
            "1": "Recently, this has been naturally extended to end-to-end mappings using CNN architectures, either in monocular images [7], [8], [12], [41], [42], [43] or in videos [11], [44].",
            "2": "The second protocol, denoted as P2, is used in [11], [41], [44].",
            "3": "We evaluate only on every 5thframe as in [44], i.",
            "4": "[44] (17 jts) 113.",
            "5": "3, 9, 10 TO APPEAR IN IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, 2019 15\n[44] X."
        },
        "Exploiting temporal context for 3D human pose estimation in the wild": {
            "authors": [
                "Anurag Arnab",
                "Carl Doersch",
                "Andrew Zisserman"
            ],
            "url": "http://openaccess.thecvf.com/content_CVPR_2019/papers/Arnab_Exploiting_Temporal_Context_for_3D_Human_Pose_Estimation_in_the_CVPR_2019_paper.pdf",
            "ref_texts": "[59] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3D human pose estimation from monocular video. In CVPR , 2016. 2",
            "ref_ids": [
                "59"
            ],
            "1": "This has been recently done with neural networks [28,57,31] and previously using a dictionary of 3D skeletons [38,2,59,54] or other priors [47,50,2] to constrain the problem.",
            "2": "2\n[59] X."
        },
        "Learning motion priors for 4d human body capture in 3d scenes": {
            "authors": [
                "Siwei Zhang",
                "Yan Zhang",
                "Federica Bogo",
                "Marc Pollefeys",
                "Siyu Tang"
            ],
            "url": "http://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Learning_Motion_Priors_for_4D_Human_Body_Capture_in_3D_ICCV_2021_paper.pdf",
            "ref_texts": "[73] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 4966\u20134975, 2016. 2,6",
            "ref_ids": [
                "73"
            ],
            "1": "A number of works tackle the problem adopting skeleton/joint-based representations for the body [7,8,11,13,14,29,35,39\u2013\n41,45,46,54,63,65,70,73].",
            "2": "Evaluation of Motion Smoothness Prior We compare our motion smoothness prior (denoted by \u2018Ours-SP\u2019) against three optimization-based baselines: the DCT-based prior from [22]; minimizing velocity magnitude (L2-V) [4,33,57,73]; minimizing acceleration magnitude (L2-A) [33,41,52]."
        },
        "Attention mechanism exploits temporal contexts: Real-time 3d human pose reconstruction": {
            "authors": [
                "Ruixu Liu",
                "Ju Shen",
                "He Wang",
                "Chen Chen",
                "Vijayan Asari"
            ],
            "url": "http://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Attention_Mechanism_Exploits_Temporal_Contexts_Real-Time_3D_Human_Pose_Reconstruction_CVPR_2020_paper.pdf",
            "ref_texts": "[50] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4966\u20134975, 2016.",
            "ref_ids": [
                "50"
            ],
            "1": "Existing works can be generally classified into two categories: direct 3D estimation and2D-to-3D estimation [50,9].",
            "2": "The former explores the possibility of jointly extracting both 2D and 3D poses in a holistic manner [34,42]; while the latter decouples the estimation into two steps: 2D body part detection and 3D correspondence inference [8,5,50].",
            "3": "[50] X."
        },
        "Lcr-net: Localization-classification-regression for human pose": {
            "authors": [
                "Gregory Rogez",
                "Philippe Weinzaepfel",
                "Cordelia Schmid"
            ],
            "url": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Rogez_LCR-Net_Localization-Classification-Regression_for_CVPR_2017_paper.pdf",
            "ref_texts": "[37] X. Zhou, M. Zhu, S. Leonardos, K. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3D human pose estimation from monocular video. In CVPR , 2016. 3,5,6",
            "ref_ids": [
                "37"
            ],
            "1": "Some recent approaches employ CNNs 3434\n for 3D pose estimation in monocular images [5,15,23] or in videos [29,37].",
            "2": "The second protocol, denoted as P2, is used in [15,29,37].",
            "3": "We evaluate only on every 5thframe as in [37] , i.",
            "4": "This is despite the fact that we perform also localization, in contrast to most methods such as [23,37] that assume bounding box annotation of the human.",
            "5": "Some of the competing methods on P2 only evaluate on 6 actions [14,15,16,28], other leverage temporal information [7,29,37].",
            "6": "[37] 87.",
            "7": "[37] 107.",
            "8": "3\n[37] X."
        },
        "Monocular 3d pose and shape estimation of multiple people in natural scenes-the importance of multiple scene constraints": {
            "authors": [
                "Andrei Zanfir",
                "Elisabeta Marinoiu",
                "Cristian Sminchisescu"
            ],
            "url": "https://openaccess.thecvf.com/content_cvpr_2018/papers/Zanfir_Monocular_3D_Pose_CVPR_2018_paper.pdf",
            "ref_texts": "[40] X. Zhou, M. Zhu, K. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In CVPR , 2016.",
            "ref_ids": [
                "40"
            ],
            "1": "Related Work Our work relates to recently developed deep architectures for 2d human pose estimation [4,9,21,35,36], 3d human pose estimation based on fitting volumetric models [2,15], feedforward deep models for 3d prediction [18,22,40], as well as integrated deep models for 2d and 3d reasoning [23,27,34,19].",
            "2": "[40] X."
        },
        "Sfv: Reinforcement learning of physical skills from videos": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1810.03599",
            "ref_texts": "201. X. Zhou, M. Zhu, S. Leonardos, K. Derpanis, and K. Daniilidis. 2015. Sparse Representation for 3D Shape Estimation: A Convex Relaxation Approach. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR . 4447\u20134455. X. Zhou, M. Zhu, S. Leonardos, K. Derpanis, and K. Daniilidis. 2016. Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR . 4966\u20134975. ACM Transactions on Graphics, Vol. 37, No. 6, Article 178. Publication date: November 2018. SFV: Reinforcement Learning of Physical Skills from Videos \u2022178:15 SUPPLEMENTARY MATERIAL Table 4. Summary of Notations Notation Definition \u02c6xt 2D joint prediction at frame t \u02c6qt 3D pose prediction at frame t zt HMR embedding of the image frame at t q(zt) 3D pose as a function of the embedding Fj(\u00b7) Forward kinematics function that computes the 3D position of joint j q\u2217 tFinal 3D reference pose after motion reconstruction st state of the simulated character at timestep t at action rt reward Rt return starting at timestep t,\u00cdT\u2212t l=0\u03b3lrt+l At advantage at timestep t,Rt\u2212V(st)",
            "ref_ids": [
                "201"
            ]
        },
        "Encoder-decoder with multi-level attention for 3d human shape and pose estimation": {
            "authors": [
                "Ziniu Wan",
                "Zhengjia Li",
                "Maoqing Tian",
                "Jianbo Liu",
                "Shuai Yi",
                "Hongsheng Li"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Wan_Encoder-Decoder_With_Multi-Level_Attention_for_3D_Human_Shape_and_Pose_ICCV_2021_paper.pdf",
            "ref_texts": "[45] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 4966\u20134975, 2016. 6",
            "ref_ids": [
                "45"
            ],
            "1": "Here, following [20], we use the 6D rotation representation proposed in [45] for faster convergence."
        },
        "Sim2real transfer learning for 3d human pose estimation: motion to the rescue": {
            "authors": [
                "Carl Doersch",
                "Andrew Zisserman"
            ],
            "url": "https://proceedings.neurips.cc/paper/2019/file/d4a93297083a23cc099f7bd6a8621131-Paper.pdf",
            "ref_texts": "[97] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3D human pose estimation from monocular video. In Proceedings of the IEEE conference on 12 computer vision and pattern recognition , pages 4966\u20134975, 2016.",
            "ref_ids": [
                "97"
            ],
            "1": "Principal among these is work on using datasets of synthetic humans for human pose estimation [11,16,20,51,67,75,84,86,97].",
            "2": "[97] X."
        },
        "Discovery of latent 3d keypoints via end-to-end geometric reasoning": {
            "authors": [
                "Supasorn Suwajanakorn",
                "Noah Snavely",
                "Jonathan J. Tompson",
                "Mohammad Norouzi"
            ],
            "url": "https://proceedings.neurips.cc/paper/2018/file/24146db4eb48c718b84cae0a0799dcfc-Paper.pdf",
            "ref_texts": "[66] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3D human pose estimation from monocular video. CVPR , 2016.",
            "ref_ids": [
                "66"
            ],
            "1": "Other techniques use inferred 2D keypoint detectors and learned 3D priors to perform \u201c2D-to-3D-lifting\u201d [41,7,66,33] or find data-to-model correspondences from depth images [40]."
        },
        "P-stmo: Pre-trained spatial temporal many-to-one model for 3d human pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2203.07628",
            "ref_texts": "56. Zhou, X., Zhu, M., Leonardos, S., Derpanis, K.G., Daniilidis, K.: Sparseness meets deepness: 3d human pose estimation from monocular video. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 4966\u20134975 (2016)",
            "ref_ids": [
                "56"
            ],
            "1": "Some works [33,20,56] also downsample the data, but they reduce the size of the dataset."
        },
        "3d human pose estimation: A review of the literature and analysis of covariates": {
            "authors": [
                "Nikolaos Sarafianos"
            ],
            "url": "https://nsarafianos.github.io/assets/3DHumanPose.pdf",
            "ref_texts": "3D pose estimation from a single image. In: Proc. IEEE Conference on Computer Vision and Pattern Recognition. Las Vegas, NV . Ye, M. , Zhang, Q. , Wang, L. , Zhu, J. , Yang, R. , Gall, J. , 2013. A survey on human motion analysis from depth data. In: Time-of-Flight and Depth Imaging. Sensors, Algorithms, and Applications. Springer, pp. 149\u2013187 . Zhang, Y. , Han, T. , Ren, Z. , Umetani, N. , Tong, X. , Liu, Y. , Shiratori, T. , Cao, X. , 2013. Bodyavatar: creating freeform 3D avatars using first-person body gestures. In: Proc. 26th annual ACM symposium on user interface software and technology. ACM, St. Andrews, Scotland, United Kingdom, pp. 387\u2013396 . Zheng, Y. , Liu, H. , Dorsey, J. , Mitra, N.J. , 2015. Ergonomics-inspired reshaping and exploration of collections of models. IEEE Trans. Visual Comput. Graphics 1\u201314 . Zhou, X., Leonardos, S., Hu, X., Daniilidis, K., 2015b. Source code for 3D shape reconstruction from 2D landmarks: a convex formulation. Available online at: https://fling.seas.upenn.edu/ \u223cxiaowz/dynamic/wordpress/3dshapeestimation/ . Zhou, X. , Leonardos, S. , Hu, X. , Daniilidis, K. , 2015a. 3D shape estimation from 2D landmarks: a convex relaxation approach. In: Proc. IEEE Conference on Computer Vision and Pattern Recognition. Boston, MA, pp. 4 4 47\u20134 455 . Zhou, X., Zhu, M., Leonardos, S., Derpanis, K., Daniilidis, K., 2016a. Source code for Sparseness meets deepness: 3D human pose estimation from monocular video. Available online at: https://fling.seas.upenn.edu/ \u223cxiaowz/dynamic/ wordpress/monocap/ . Zhou, X. , Zhu, M. , Leonardos, S. , Derpanis, K. , Daniilidis, K. , 2016b. Sparseness meets deepness: 3D human pose estimation from monocular video. In: Proc. IEEE Conference on Computer Vision and Pattern Recognition. Las Vegas, NV . Zhou, X. , Zhu, M. , Leonardos, S. , Daniilidis, K. , 2016c. Sparse Representation for 3D shape estimation: A convex relaxation approach. In: IEEE Transactions on Pattern Analysis and Machine Intelligence . Zuffi, S. , Black, M. , 2015. The stitched puppet: A graphical model of 3D human shape and pose. In: Proc. IEEE Conference on Computer Vision and Pattern Recognition. Boston, Massachusetts, pp. 3537\u20133546 . Zuffi, S. , Freifeld, O. , Black, M.J. , 2012. From pictorial structures to deformable structures. In: Proc. IEEE Conference on Computer Vision and Pattern Recognition. Providence, Rhode Island, pp. 3546\u20133553 . "
        },
        "Motion guided 3d pose estimation from videos": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2004.13985",
            "ref_texts": "39. Zhou, X., Zhu, M., Leonardos, S., Derpanis, K.G., Daniilidis, K.: Sparseness meets deepness: 3d human pose estimation from monocular video. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 4966{4975 (2016)",
            "ref_ids": [
                "39"
            ],
            "1": "8 Zhou [39] 87."
        },
        "Propagating lstm: 3d pose estimation based on joint interdependency": {
            "authors": [
                "Kyoungoh Lee",
                "Inwoong Lee",
                "Sanghoon Lee"
            ],
            "url": "https://openaccess.thecvf.com/content_ECCV_2018/papers/Kyoungoh_Lee_Propagating_LSTM_3D_ECCV_2018_paper.pdf",
            "ref_texts": "4. Zhou, X., Zhu, M., Leonardos, S., Derpanis, K.G., Daniilidis, K.: Sparseness meets deepness: 3d human pose estimation from monocular video. In: P roceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (2 016) 4966\u20134975",
            "ref_ids": [
                "4"
            ],
            "1": "Alternatively, the authors in [4,22\u201330] used 2D poses derived from a generalized environment, so their networks h ave shown a superior performance to the direct 3D pose estimation approach.",
            "2": "To efficiently enhance the poor performances, some approaches used 2D pos e as a new invariant feature [4,22\u201327,29,30].",
            "3": "[4] formulated an optimization problem in terms of the relationship betwe en 2D pose and sparsity-driven 3D geometric prior, and predicted 3D pose by u sing an expectation-maximization algorithm.",
            "4": "Thus, the 2D-to-3D pose estimation by means of 2D pose is effectiv e when estimating 3D pose from image [4,22\u201327,29\u201331]."
        },
        "Unsupervised 3d pose estimation with geometric self-supervision": {
            "authors": [
                "Hang Chen",
                "Ambrish Tyagi",
                "Amit Agrawal",
                "Dylan Drover",
                "Rohith M",
                "Stefan Stojanov",
                "James M. Rehg"
            ],
            "url": "https://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Unsupervised_3D_Pose_Estimation_With_Geometric_Self-Supervision_CVPR_2019_paper.pdf",
            "ref_texts": "[54] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G. Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In CVPR , 2016. 1,3,6",
            "ref_ids": [
                "54"
            ],
            "1": "In contrast, this paper addresses the fundamental problem of lifting 2D image coordinates to 3D space without the use of any additional cues such as video [43,54], multi-view cameras [1,16], or depth images [35,40,52].",
            "2": "Weakly Supervised: Approaches such as [3,10,44,53, 54,55] do not explicitly use paired 2D-3D correspondences, but use unpaired 3D data to learn priors on shape (3D basis) or pose (articulation priors).",
            "3": "[54] use a 3D pose dictionary to learn pose priors and Brau et al .",
            "4": "Similar to previous works [9,11,24,26,36,43,54], we report results on subjects S9 and S11."
        },
        "Generating multiple hypotheses for 3d human pose estimation with mixture density network": {
            "authors": [
                "Chen Li",
                "Gim Hee"
            ],
            "url": "http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Generating_Multiple_Hypotheses_for_3D_Human_Pose_Estimation_With_Mixture_CVPR_2019_paper.pdf",
            "ref_texts": "[29] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In IEEE Conference on Computer Vision and Pattern Recognition , pages 4966\u2013",
            "ref_ids": [
                "29"
            ],
            "1": "One of the commonly used and effective deep learning based methods for 3D human pose estimation is the two-stage approach, where the 2D joints are first detected from the image input [18,24] followed by the 3D joint estimations from the detected 2D joints [1,29,4,15,10,6,25,17].",
            "2": "The second category [1,29,4,15,10,6,25,17] decouples 3D pose estimation into the well-studied 2D joint detection [18,24] and 3D pose estimation from the detected 2D joints.",
            "3": "[29] X."
        },
        "Total capture: 3d human pose estimation fusing video and inertial sensors": {
            "authors": [],
            "url": "https://openresearch.surrey.ac.uk/esploro/fulltext/conferencePresentation/Total-Capture-3D-Human-Pose-Estimation/99512708202346?repId=12138918800002346&mId=13140498020002346&institution=44SUR_INST",
            "ref_texts": "[38] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4966\u20134975, 2016.",
            "ref_ids": [
                "38"
            ],
            "1": "While Zhou [38] integrates 2-D, 3-D and temporal information to account for uncertainties in the data."
        },
        "Deep inertial poser: Learning to reconstruct human pose from sparse inertial measurements in real time": {
            "authors": [],
            "url": "https://dl.acm.org/doi/pdf/10.1145/3272127.3275108",
            "ref_texts": "(2017). Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser Sheikh. 2016. Convolutional pose machines. In CVPR . 4724\u20134732. Xiaolin Wei, Peizhao Zhang, and Jinxiang Chai. 2012. Accurate realtime full-body motion capture using a single depth camera. ACM Transactions on Graphics (TOG)31, 6 (2012), 188. Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. 2016. Sparseness meets deepness: 3D human pose estimation from monocular video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 4966\u20134975. Michael Zollh\u00f6fer, Matthias Nie\u00dfner, Shahram Izadi, Christoph Rehmann, Christopher Zach, Matthew Fisher, Chenglei Wu, Andrew Fitzgibbon, Charles Loop, Christian Theobalt, et al .2014. Real-time non-rigid reconstruction using an RGB-D camera. ACM Transactions on Graphics (TOG) 33, 4 (2014), 156. A ADDITIONAL ARCHITECTURES Along with the models discussed in the paper, we experimented with other, non-recurrent structures. Specifically, we implemented a WaveNet architecture [van den Oord et al .2016] and a simple feedforward network (FFN). The FFN is composed of 5 fully-connected layers with 256, 512, 512, 256, and 256 units per layer respectively. At a single time step tthe model is fed a temporal window of 20 past and 5 future frames. Table 4 summarizes the results in terms of mean joint angle error. The FFN performs around 4.4\u25e6worse on TotalCapture than our best BiRNN evaluated on 20 past and 5 future frames. Additionally, the output is greatly corrupted by jerkiness and trembling artifacts. WaveNet performs better both in terms of the joint angle error and visual quality. Although WaveNet is able to considerably reduce the trembling artifacts, they are still apparent, resulting in displeasing visual output. Furthermore, the BiRNN model proposed in this paper offers much greater flexibility. Increasing the input window size in a feedforward network requires both retraining the model and incurs a large growth in trainable parameters. This is not the case for the BiRNN; the window length can be changed \u201con the fly\u201d for the same model and hence does not affect the number of parameters. Table 4. Performance of WaveNet and a feed-forward network (FFN) on TotalCapture and DIP-IMU in terms of the mean joint angle error in degrees. Both models were trained on the synthetic AMASS training set. TotalCapture DIP-IMU"
        },
        "Synthesizing training images for boosting human 3d pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1604.02703",
            "ref_texts": "[58] X. Zhou, M. Zhu, S. Leonardosy, K. G. Derpanisz, and K. Daniilidisy. Sparseness meets deepness: 3d human pose estimation from monocular video. In CVPR , 2016. 4323",
            "ref_ids": [
                "58"
            ],
            "1": "[58] and Tekin et al.",
            "2": "4321, 4322\n[58] X."
        },
        "Mocap-guided data augmentation for 3d pose estimation in the wild": {
            "authors": [
                "Gregory Rogez",
                "Cordelia Schmid"
            ],
            "url": "https://proceedings.neurips.cc/paper/2016/file/35464c848f410e55a13bb9d78e7fddd0-Paper.pdf",
            "ref_texts": "[44] X. Zhou, M. Zhu, S. Leonardos, K. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3D human pose estimation from monocular video. In CVPR , 2016.",
            "ref_ids": [
                "44"
            ],
            "1": "Recent approaches employ CNNs for 3D pose estimation in monocular images [20] or in videos [44].",
            "2": "[44] 113.",
            "3": "We also report results for a second protocol (P2) employed in [20,44,35] where all the frames from subjects S9 and S11 are used for testing and only S1, S5, S6, S7 and S8 are used for training.",
            "4": "[44] report better performance, but they integrate temporal information.",
            "5": "Since 3D pose evaluation is not possible on this dataset, we instead compare 2D pose errors expressed in pixels and measure this error on the normalized 220\u0002220images following [44].",
            "6": "[44] X."
        },
        "Deep kinematic pose regression": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1609.05317",
            "ref_texts": "40. Zhou, X., Zhu, M., Leonardos, S., Derpanis, K.G., Daniilidis, K.: Sparseness meets deepness: 3d human pose estimation from monocular video. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (June 2016)",
            "ref_ids": [
                "40"
            ],
            "1": "The points could further be the physical joints that deffnes the geometry of complex articulated objects, such as human hand [41,21] and human body [17,40,31].",
            "2": "Many other approaches use a low dimensional representation by using dimensionality reduction techniques such as PCA [12,21], sparse coding [34,39,40] or auto-encoder [30].",
            "3": "In this work, we propose to directly incorporate the articulated object model into the deep neutral network learning, which is the dominant approach for object pose estimation nowadays, for hand [32,29,21,22,41,8] or human body[33,35,20,10,17,18,31,30,40].",
            "4": "Such endto-end learning is better than the previous approaches that rely on a separate post-processing step to recover the object geometry [32,40].",
            "5": "Linear Dictionary A widely-used method is to denote the structural points as a linear combination of templates or basis [34,39,40,16].",
            "6": "To adopt these fully-convolutional based heat map regression method for 3D pose estimation, an additional model fftting step is used [40] as a post processing.",
            "7": "We note that it is possible to enforce the geometric constraints by fftting a kinematic model to some estimated joints as a post-processing [32,40].",
            "8": "[40] obtains 3D human joints represented by a sparse dictionary using an EM optimization algorithm.",
            "9": "The training and testing data partition follows previous works [13,17,40].",
            "10": "By contrast, previous methods [13,18,40] use Deep Kinematic Pose Regression 11 Directions Discussion Eating Greeting Phoning Photo Posing Purchases LinKDE [13] 132.",
            "11": "61 Zhou et al [40] 87.",
            "12": "28 Zhou et al [40] 124.",
            "13": "The results for comparison methods [13,17,18,30,30,31,40] are from their published papers."
        },
        "Skeleton based action recognition using translation-scale invariant image mapping and multi-scale deep CNN": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1704.05645",
            "ref_texts": "[15] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis, \u201cSparseness meets deepness: 3d human pose estimation from monocular video,\u201d Computer Science , pp. 4966\u20134975, 2015.",
            "ref_ids": [
                "15"
            ],
            "1": "Very recently, human pose estimation from 2D RGB videos have also been studied with deep CNN method [15]\u2013[17].",
            "2": "In recent years, human pose estimation from 2D RGB videos have been studied with deep CNN method [15]\u2013[17].",
            "3": "[15] X."
        },
        "Learning to fuse 2d and 3d image cues for monocular body pose estimation": {
            "authors": [
                "Bugra Tekin",
                "Pablo Marquez",
                "Mathieu Salzmann",
                "Pascal Fua"
            ],
            "url": "https://openaccess.thecvf.com/content_ICCV_2017/papers/Tekin_Learning_to_Fuse_ICCV_2017_paper.pdf",
            "ref_texts": "[77] X. Zhou, M. Zhu, S. Leonardos, K. Derpanis, and K. Daniilidis. Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video. In CVPR , 2016. 1,2,5,6, 7",
            "ref_ids": [
                "77"
            ],
            "1": "The state-of-the-art approaches can be roughly grouped into those that directly regress 3D pose from images [30,38,64,65] and those that first predict a 2D pose in the form of joint location confidence maps and fit a 3D model to this 2D prediction [9,77].",
            "2": "With the growing availability of large datasets and the advent of Deep Learning, the emphasis has shifted towards using discriminative 2D pose regressors [11,13,15,16,24,27,32,43,47,48,67,71,72] to extract the 2D pose and infer a 3D one from it [9,19,73,77].",
            "3": "A human body representation, such as a skeleton [77], or a more detailed model [9] can then be fitted to these predictions.",
            "4": "6m, we used the same data partition and evaluation protocol as in earlier work [17,38,39,40,45, 46,53,65,64,66,77,76] for a fair comparison.",
            "5": "On HumanEva-I, following the standard evaluation protocol [9,62,65,73,77], we trained our model on the training sequences of subjects S1, S2 and S3 and evaluated on the validation sequences of all subjects.",
            "6": "For completeness, we also compare our approach to the following methods that rely on either multiple consecutive images or impose temporal consistency: regression from short image sequences to 3D poses [65], fitting a sparse 3D pose model to 2D confidence map predictions across frames [77], and fitting a 3D pose sequence to the 2D joints predicted by images and heightmaps that encode the height of each pixel in the image with respect to a reference plane [17].",
            "7": "In particular, we outperform the image-based regression methods of [30,38,39,40,64,45,66,76], as well as the model-fitting strategy of [39,40,77].",
            "8": "[77] Rogez & Schmid [53] Tekin et al.",
            "9": "[77] 87.",
            "10": "[77] 120.",
            "11": "[77] 107.",
            "12": "Even though our algorithm uses only individual images, it also outperforms the methods that rely on sequences [17,65,77].",
            "13": "We adopted the evaluation protocol described in [9,62,73,77] for a fair comparison.",
            "14": "As in [9,62,73,77], we measure 3D pose error as the average joint-to-joint distance after alignment by a rigid transformation.",
            "15": "[77] 34.",
            "16": "2, 5,6\n[77] X."
        },
        "Learning 3d human pose from structure and motion": {
            "authors": [
                "Rishabh Dabral",
                "Anurag Mundhada",
                "Abhishek Sharma"
            ],
            "url": "https://openaccess.thecvf.com/content_ECCV_2018/papers/Rishabh_Dabral_Learning_3D_Human_ECCV_2018_paper.pdf",
            "ref_texts": "43. X. Zhou, M. Zhu, K. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In CVPR , 2016. 1,3,11",
            "ref_ids": [
                "43"
            ],
            "1": "A large body of prior art either directly regresses for 3D joint coordinates [17,18,34] or infers 3D from 2D joint-locations in a two-stage approach [22,24,19,43,41].",
            "2": "ConvNet architectures: Most existing ConvNet based approaches either directly regress 3D poses from the input image [34,17,42,43] or infer 3D from 2D pose in a twostage approach [35,41,23,24,19].",
            "3": "A few approaches use statistical priors [43,1] to lift 2D poses to 3D.",
            "4": "Recent ConvNet based approaches [23,30,41,34,43,27] have reported substantial improvements in real-world setting by pre-training or joint training of their 2D prediction modules, but it still remains an open problem.",
            "5": "[43] introduce a first order smoothing prior in their temporal optimization step.",
            "6": "Method Direction Discuss Eat Greet Phone Pose Purchase Sit Zhou [43] 68.",
            "7": "1 Method SitDown Smoke Photo Wait Walk WalkDog WalkPair Avg Zhou [43] 113.",
            "8": "5 Zhou [43] 47."
        },
        "Monoperfcap: Human performance capture from monocular video": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1708.02136",
            "ref_texts": "2012. Performance capture of interacting characters with handheld kinects. In ECCV , Vol. 7573 LNCS. 828\u2013841. https://doi.org/10.1007/978-3-642-33709-3_59 ACM Transactions on Graphics, Vol. 9, No. 4, Article 39. Publication date: March 2018. 2018-02-26 01:54 page 14 (pp. 1-15) MonoPerfCap: Human Performance Capture from Monocular Video \u202239:15 Rui Yu, Chris Russell, Neill D. F. Campbell, and Lourdes Agapito. 2015. Direct, Dense, and Deformable: Template-Based Non-Rigid 3D Reconstruction From RGB Video. InThe IEEE International Conference on Computer Vision (ICCV) . Qing Zhang, Bo Fu, Mao Ye, and Ruigang Yang. 2014. Quality Dynamic Human Body Modeling Using a Single Low-cost Depth Camera. In CVPR . IEEE, 676\u2013683. Shizhe Zhou, Hongbo Fu, Ligang Liu, Daniel Cohen-Or, and Xiaoguang Han. 2010. Parametric reshaping of human bodies in images. ACM Transactions on Graphics (TOG) 29, 4 (2010), 126. Xiaowei Zhou, Spyridon Leonardos, Xiaoyan Hu, and Kostas Daniilidis. 2015. 3D shape estimation from 2D landmarks: A convex relaxation approach. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . 4447\u20134455. Xingyi Zhou, Xiao Sun, Wei Zhang, Shuang Liang, and Yichen Wei. 2016a. Deep Kinematic Pose Regression. arXiv preprint arXiv:1609.05317 (2016). Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. 2016b. Sparseness meets deepness: 3D human pose estimation from monocular video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 4966\u20134975. Michael Zollh\u00f6fer, Matthias Nie\u00dfner, Shahram Izadi, Christoph Rhemann, Christopher Zach, Matthew Fisher, Chenglei Wu, Andrew Fitzgibbon, Charles Loop, Christian Theobalt, and Marc Stamminger. 2014. Real-time Non-rigid Reconstruction using an RGB-D Camera. ACM Transactions on Graphics (TOG) 33, 4 (2014).",
            "ref_ids": [
                "2012"
            ]
        },
        "Fast and robust multi-person 3d pose estimation from multiple views": {
            "authors": [
                "Junting Dong",
                "Wen Jiang",
                "Qixing Huang",
                "Hujun Bao",
                "Xiaowei Zhou"
            ],
            "url": "http://openaccess.thecvf.com/content_CVPR_2019/papers/Dong_Fast_and_Robust_Multi-Person_3D_Pose_Estimation_From_Multiple_Views_CVPR_2019_paper.pdf",
            "ref_texts": "[47] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In CVPR , 2016. 3",
            "ref_ids": [
                "47"
            ],
            "1": "The advances in learning-based methods also make it possible to recover 3D human pose from a single RGB image, either lifting the detected 2D poses into 3D [28,47, 9,27] or directly regressing 3D poses [40,37,39,45,31] and even 3D body shapes from RGB [4,24,33].",
            "2": "3\n[47] X."
        },
        "Livecap: Real-time human performance capture from monocular video": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1810.02648",
            "ref_texts": "398\u2013407. Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. 2016. Sparseness meets deepness: 3D human pose estimationfrom monocular video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 4966\u20134975. Zoran Zivkovic and Ferdinand van der Heijden. 2006. Efficient Adaptive Density Estimation Per Image Pixel for the Task of Background Subtraction. Pattern Recogn. Lett. 27, 7 (May 2006), 773\u2013780. https://doi.org/10.1016/j.patrec.2005.11.005 Michael Zollh\u00f6fer, Matthias Nie\u00dfner, Shahram Izadi, Christoph Rhemann, Christopher Zach, Matthew Fisher, Chenglei Wu, Andrew Fitzgibbon, Charles Loop, Christian Theobalt, and Marc Stamminger. 2014. Real-time Non-rigid Reconstruction using an RGB-D Camera. ACM Transactions on Graphics (TOG) 33, 4 (2014). Received September 2018; accepted January 2019; final version January 2019"
        },
        "C3dpo: Canonical 3d pose networks for non-rigid structure from motion": {
            "authors": [
                "David Novotny",
                "Nikhila Ravi",
                "Benjamin Graham",
                "Natalia Neverova",
                "Andrea Vedaldi"
            ],
            "url": "http://openaccess.thecvf.com/content_ICCV_2019/papers/Novotny_C3DPO_Canonical_3D_Pose_Networks_for_Non-Rigid_Structure_From_Motion_ICCV_2019_paper.pdf",
            "ref_texts": "[41] Xiaowei Zhou, Menglong Zhu, Kosta Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d hu-man pose estimation from monocular video. In Proc. CVPR , 2016.",
            "ref_ids": [
                "41"
            ],
            "1": "Priors about the shape and the camera motion are employed to improve conditioning of the problem, including the use of low-rank subspaces in the spatial domain [3, 11, 9, 43], temporal domain, for example, fitting 2D keypoint trajectories to a set of predefined DCT basis functions [4, 5], spatio-temporal domain [1, 12, 22, 23], multiple unions of low-rank subspaces [43, 2], learning an overcomplete dictionary of basis shapes from 3D motion capture data and imposing an L1 penalty on basis coefficients [41, 42] and imposing Gaussian priors on the shape coefficients [33].",
            "2": "Besides the fully supervised methods [25, 26], several works have explored multi-view supervision [20, 29, 31], ordinal depth supervision [28], unpaired 2D-3D data [30, 36, 41, 15] or videos [17] to alleviate the need for full 2D-3D annotations."
        },
        "Mutual learning to adapt for joint human parsing and pose estimation": {
            "authors": [
                "Xuecheng Nie",
                "Jiashi Feng",
                "Shuicheng Yan"
            ],
            "url": "https://openaccess.thecvf.com/content_ECCV_2018/papers/Xuecheng_Nie_Mutual_Learning_to_ECCV_2018_paper.pdf",
            "ref_texts": "30. Zhou, X., Zhu, M., Leonardos, S., Derpanis, K.G., Daniilidis , K.: Sparseness meets deepness: 3d human pose estimation from monocular video. In: CVPR (2016) 1",
            "ref_ids": [
                "30"
            ],
            "1": ",humanbehavioranalysis[22,9],person-identification[29,20] and video surveillance [14,30]."
        },
        "Smap: Single-shot multi-person absolute 3d pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2008.11469",
            "ref_texts": "44. Zhou, X., Zhu, M., Leonardos, S., Derpanis, K.G., Daniilidis, K.: Sparseness meets deepness: 3d human pose estimation from monocular video. In: CVPR (2016)",
            "ref_ids": [
                "44"
            ],
            "1": "optimization-based [44,38] and exemplar-based [4] methods, which can benefft from the reliable result of 2D pose estimation."
        },
        "Harvesting multiple views for marker-less 3d human pose annotations": {
            "authors": [
                "Georgios Pavlakos",
                "Xiaowei Zhou",
                "Konstantinos G. Derpanis",
                "Kostas Daniilidis"
            ],
            "url": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Pavlakos_Harvesting_Multiple_Views_CVPR_2017_paper.pdf",
            "ref_texts": "[47] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3D human pose estimation from monocular video. In CVPR , 2016. 3,5,6",
            "ref_ids": [
                "47"
            ],
            "1": "Single view 3D human pose : 3D human pose estimation from a single image has been typically approached by applying more and more powerful discriminative methods on the image and combining them with expressive 3D priors to 6989\n recover the final pose [37,47,7].",
            "2": "Following previous work [25,47], we use two subjects for testing (S9 and S11), and report results based on the average 3D joint error.",
            "3": "[47] 87.",
            "4": "[47] 124.",
            "5": "[47] use video, while our proposed method is multi-view.",
            "6": "[47] use video instead of prediction from a single frame.",
            "7": "Moreover, as a weak multi-view baseline, we averaged the per view 3D estimates from one of the state-of-the-art approaches [47].",
            "8": "6,7\n[47] X."
        },
        "Monocular 3d human pose estimation by generation and ordinal ranking": {
            "authors": [
                "Saurabh Sharma",
                "Pavan Teja",
                "Prashast Bindal",
                "Abhishek Sharma",
                "Arjun Jain"
            ],
            "url": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Sharma_Monocular_3D_Human_Pose_Estimation_by_Generation_and_Ordinal_Ranking_ICCV_2019_paper.pdf",
            "ref_texts": "[46] Xiaowei Zhou, Menglong Zhu, Kosta Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In CVPR , 2016. 1",
            "ref_ids": [
                "46"
            ],
            "1": "Recent advancements in real-world 2D-pose estimation [22,42] has led to several multi-stage architectures, where the 3D-pose is regressed either from both the image features and an intermediate 2D representation [3,8,23,45], or only the estimated 2D-pose [1,19,20,27,46]."
        },
        "Monocap: Monocular human motion capture using a cnn coupled with a geometric prior": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1701.02354",
            "ref_texts": "[67] X. Zhou, M. Zhu, S. Leonardos, K. Derpanis, and K. Daniilidis, \u201cSparseness meets deepness: 3D human pose estimation from monocular video,\u201d in CVPR , 2016. 2",
            "ref_ids": [
                "67"
            ],
            "1": "A preliminary version of this work appeared in CVPR\n2016 [67].",
            "2": "2, 5, 10\n[67] X."
        },
        "Person-in-WiFi: Fine-grained person perception using WiFi": {
            "authors": [
                "Fei Wang",
                "Sanping Zhou",
                "Stanislav Panev",
                "Jinsong Han",
                "Dong Huang"
            ],
            "url": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Person-in-WiFi_Fine-Grained_Person_Perception_Using_WiFi_ICCV_2019_paper.pdf",
            "ref_texts": "[61] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In CVPR , pages 4966\u20134975, 2016. 2",
            "ref_ids": [
                "61"
            ],
            "1": "Deep learning has significantly advanced human pose estimation [48,47,10,14,37,55,55, 9] on images captured by monocular cameras, as well as those with optical flow and motion captures [24,16,36,61]."
        },
        "Long short-term memory kalman filters: Recurrent neural estimators for pose regularization": {
            "authors": [
                "Huseyin Coskun",
                "Felix Achilles",
                "Robert Di",
                "Nassir Navab",
                "Federico Tombari"
            ],
            "url": "https://openaccess.thecvf.com/content_ICCV_2017/papers/Coskun_Long_Short-Term_Memory_ICCV_2017_paper.pdf"
        },
        "Deep network for the integrated 3d sensing of multiple people in natural images": {
            "authors": [
                "Andrei Zanfir",
                "Elisabeta Marinoiu",
                "Mihai Zanfir",
                "Ionut Popa",
                "Cristian Sminchisescu"
            ],
            "url": "https://proceedings.neurips.cc/paper_files/paper/2018/file/6a6610feab86a1f294dbbf5855c74af9-Paper.pdf",
            "ref_texts": "[10] X. Zhou, M. Zhu, K. Derpanis, and K. Daniilidis, \u201cSparseness meets deepness: 3d human pose estimation from monocular video,\u201d in CVPR , 2016.",
            "ref_ids": [
                "10"
            ],
            "1": "[10] X."
        },
        "Online adaptation for consistent mesh reconstruction in the wild": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper_files/paper/2020/file/aba3b6fd5d186d28e06ff97135cade7f-Paper.pdf",
            "ref_texts": "[52] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In CVPR , 2016. 3",
            "ref_ids": [
                "52"
            ],
            "1": "Several works [52,43,34,31] first obtain a singleview 3D reconstruction and then optimize the mesh and skeletal parameters.",
            "2": "6\n[52] X."
        },
        "Can 3d pose be learned from 2d projections alone?": {
            "authors": [
                "Dylan Drover",
                "Rohith M",
                "Hang Chen",
                "Amit Agrawal",
                "Ambrish Tyagi",
                "Cong Phuoc"
            ],
            "url": "http://openaccess.thecvf.com/content_ECCVW_2018/papers/11132/Drover_Can_3D_Pose_be_Learned_from_2D_Projections_Alone_ECCVW_2018_paper.pdf",
            "ref_texts": "47. Zhou, X., Zhu, M., Leonardos, S., Derpanis, K.G., Daniilidis , K.: Sparseness meets deepness: 3d human pose estimation from monocular video. In: CVPR (2016)",
            "ref_ids": [
                "47"
            ],
            "1": "Our work address es the fundamental problem of lifting 2D image coordinates to 3D space without th e use of any additional cues such as video [47,40], multi-view cameras [2,14], or depth images [35,46,38].",
            "2": "Weakly Supervised: Approaches such as [47,41,9,5] use unpaired 3D data to learn a prior, typically as a 3D basis or articulation priors, but do not e xplicitly use paired 2D-3D correspondences.",
            "3": "[47] also use a 3D pose dictionary to learn pose priors.",
            "4": "Protocol 2 reports results for both S9 and S11 as adopted by [26,47,40,10,22].",
            "5": "(2016)[47] 99.",
            "6": "(2016) [47] 109."
        },
        "3d human pose estimation using convolutional neural networks with 2d pose information": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1608.03075",
            "ref_texts": "19. Zhou, X., Zhu, M., Leonardos, S., Derpanis, K.G., Daniil idis, K.: Sparseness meets deepness: 3d human pose estimation from monocular video. In : The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). (Ju ne 2016)",
            "ref_ids": [
                "19"
            ],
            "1": "[19] used the result of 2D pose estimation to reconstruc t a 3D pose.",
            "2": "Similar to [19], we infer the scale using the training data.",
            "3": "[19] 87.",
            "4": "[19] 106.",
            "5": "[19] 114.",
            "6": "Following the previous works on the dataset [5,19], we used 5 subjects (S1, S5, S6, S7, S8) as a training set, and 2 subjects (S9, S11) as a test set.",
            "7": "Note tha t the methods of [20] and [19] makeuse oftemporal informationfrom multiple frames.",
            "8": "Our method is alsob eneficial against [20] and [19] in terms of running time and the simplicity of the alg orithm 10 S."
        },
        "xr-egopose: Egocentric 3d human pose from an hmd camera": {
            "authors": [
                "Denis Tome",
                "Patrick Peluse",
                "Lourdes Agapito",
                "Hernan Badino"
            ],
            "url": "http://openaccess.thecvf.com/content_ICCV_2019/papers/Tome_xR-EgoPose_Egocentric_3D_Human_Pose_From_an_HMD_Camera_ICCV_2019_paper.pdf",
            "ref_texts": "[60] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 4966\u20134975, 2016. 2",
            "ref_ids": [
                "60"
            ],
            "1": "Two main trends have emerged: (i)fully supervised regression of 3D joint locations directly from images [22,31,47,58,32,27] and (ii)pipeline approaches that decouple the problem into the tasks of 2D joint detection followed by 3D lifting [26,29,35,1,59,60,4,43]."
        },
        "Deep multitask architecture for integrated 2d and 3d human sensing": {
            "authors": [
                "Ionut Popa",
                "Mihai Zanfir",
                "Cristian Sminchisescu"
            ],
            "url": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Popa_Deep_Multitask_Architecture_CVPR_2017_paper.pdf",
            "ref_texts": "[51] X. Zhou, M. Zhu, S. Leonardos, K. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In CVPR , 2016. 2",
            "ref_ids": [
                "51"
            ],
            "1": "More recently, deep convolutional architectures have been employed in order to estimate 3d pose directly from images[21,22,34,51] mostly in connection with 3d human motion capture datasets like HumanEva, Human3.",
            "2": "4,7\n[51] X."
        },
        "Image2mesh: A learning framework for single image 3d reconstruction": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1711.10669",
            "ref_texts": "[37] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3D human pose estimation from monocular video. In CVPR , 2016. 1",
            "ref_ids": [
                "37"
            ],
            "1": "This exceedingly difficult and highly ambiguous problem is typically addressed by incorporating prior knowledge about the scene such as shape or scene priors [1,8,12,13,28,31,36, 37].",
            "2": "1\n[37] X."
        },
        "Multi-modal 3d human pose estimation with 2d weak supervision in autonomous driving": {
            "authors": [
                "Jingxiao Zheng",
                "Xinwei Shi",
                "Alexander Gorban",
                "Junhua Mao",
                "Yang Song",
                "Charles R. Qi",
                "Ting Liu",
                "Visesh Chari",
                "Andre Cornman",
                "Yin Zhou",
                "Congcong Li",
                "Dragomir Anguelov"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022W/WAD/papers/Zheng_Multi-Modal_3D_Human_Pose_Estimation_With_2D_Weak_Supervision_in_CVPRW_2022_paper.pdf",
            "ref_texts": "[45] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G. Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4966\u20134975, 2016. 2",
            "ref_ids": [
                "45"
            ],
            "1": "Extending this approach temporally [2, 5, 44, 45] also has been attempted, but still underperforms approaches which use depth information (see [42], [43] table 11)."
        },
        "Eventcap: Monocular 3d capture of high-speed human motions using an event camera": {
            "authors": [
                "Lan Xu",
                "Weipeng Xu",
                "Vladislav Golyanik",
                "Marc Habermann",
                "Lu Fang",
                "Christian Theobalt"
            ],
            "url": "http://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_EventCap_Monocular_3D_Capture_of_High-Speed_Human_Motions_Using_an_CVPR_2020_paper.pdf",
            "ref_texts": "[74] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video. In Computer Vision and Pattern Recognition (CVPR) , 2016. 2",
            "ref_ids": [
                "74"
            ],
            "1": "These methods either regress the root-relative 3D positions of body joints from single images [31,55,73,34,56,40,35], or lift 2D detection to 3D [4,74,10,70,24]."
        },
        "Towards accurate marker-less human shape and pose estimation over time": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1707.07548",
            "ref_texts": "[55] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4966\u2013",
            "ref_ids": [
                "55"
            ],
            "1": "[55] introduce sparsity prior over human pose, and jointly handle the pose and 2D location uncertainty, while Kazemi et al.",
            "2": "[55] X."
        },
        "Human body model fitting by learned gradient descent": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2008.08474",
            "ref_texts": "41.Zhou, X., Zhu, M., Leonardos, S., Derpanis, K.G., Daniilidis, K.: Sparseness meets deepness: 3d human pose estimation from monocular video. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 4966\u20134975 (2016)",
            "ref_ids": [
                "41"
            ],
            "1": "Deep neural networks have significantly advanced skeleton-based 3D human pose estimation from single images [25,26,41,33]."
        },
        "3D human pose machines with self-supervised learning": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1901.03798",
            "ref_texts": "[14] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis, \u201cSparseness meets deepness: 3d human pose estimation from monocular video,\u201d in CVPR , 2016.[15] A. Tekin, Bugra Popaand Rozantsev, V . Lepetit, and P . Fua, \u201cDirect prediction of 3d body poses from motion compensated sequences,\u201d in CVPR , 2016.",
            "ref_ids": [
                "14",
                "15"
            ],
            "1": "Driven by these successes, some 3D pose estimation works [12], [13], [14], [15], [16], [17] attempt to leverage the state-of-the-art 2D pose network architectures (e.",
            "2": "[15] proposed exploiting motion information from consecutive frames and applied a deep learning network to regress the 3D pose.",
            "3": "[14] proposed a 3D pose estimation framework from videos that consists of a novel synthesis among a deep-learningbased 2D part detector, a sparsity-driven 3D reconstruction approach and a 3D temporal smoothness prior.",
            "4": "6M, and it was followed by several works [4], [15], [16], [36].",
            "5": "To be more general and make a fair comparison, our model is trained both on training samples from all 15 actions as previous works [4], [15], [16], [36] and by exploiting individual actions as [14], [36].",
            "6": "We train our model on the training sequences of subjects 1, 2 and 3 and test on the \u2018validation\u2019 sequence under the same protocol as [15], [22], [31], [55], [56], [57], [58], [59].",
            "7": "Following [14], [36], the input image is cropped around the human.",
            "8": "17 Tekin CVPR\u201916 [15]* 102.",
            "9": "97 Zhou CVPR\u201916 [14]* 87.",
            "10": "3 Tekin CVPR\u201916 [15] 37.",
            "11": "[14] Zhou et al.",
            "12": "[14], Pavlakos et al.",
            "13": "[14] et al.",
            "14": "Following [14], [15], [19], [21], [23], [37], we employ the popular 3D pose error metric [55], which calculates the Euclidean errors on all joints and all frames up to translation.",
            "15": "[15], Li et al.",
            "16": "[14], Zhou et al.",
            "17": ", [4], [5], [15], [16], [17], [24], [34], [36], [37], [38], [51]) whose source codes are not publicly available, we directly obtain their results from their published papers.",
            "18": ", [14], [19], [20], [21], [23], [50]), we directly use their official implementations for comparisons.",
            "19": "Clearly, our model outperforms all the competing methods (including those trained from the individual action as in [15], [16], [36] and on all 15 actions) under Protocol #1 .",
            "20": ", [4], [14], [15], [19], [20], [21], [23], [36], [37], also employ deep learning techniques.",
            "21": "Comparison on HumanEva-I: We compare our model against competing methods, including discriminative regressions [57], [58], 2D pose detector-based methods [22], [31], [55], [56], CNN-based approaches [15], [19], [22], [38], [50] and our preliminary version Lin [21], on the HumanEva-I dataset.",
            "22": "In terms of time efficiency, compared with [14] (880 ms per image), [19] (170 ms per image), [23] (311 ms per image), and [20] (444 ms per image), our model model only requires 51 ms per image.",
            "23": "[14] X.",
            "24": "[15] A."
        },
        "3d human body reconstruction from a single image via volumetric regression": {
            "authors": [
                "Aaron S. Jackson",
                "Chris Manafas",
                "Georgios Tzimiropoulos"
            ],
            "url": "http://openaccess.thecvf.com/content_ECCVW_2018/papers/11132/Jackson_3D_Human_Body_Reconstruction_from_a_Single_Image_via_Volumetric_ECCVW_2018_paper.pdf",
            "ref_texts": "11. Zhou, X., Zhu, M., Leonardos, S., Derpanis, K.G., Daniilidis , K.: Sparseness meets deepness: 3d human pose estimation from monocular video. In: P roceedings of the IEEE conference on computer vision and pattern recognition. (2 016) 4966\u20134975",
            "ref_ids": [
                "11"
            ],
            "1": "In [11] they do this from video."
        },
        "Trajectory space factorization for deep video-based 3d human pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1908.08289",
            "ref_texts": "[42] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Conference on Computer Vision and Pattern Recognition , pages 4966\u2013",
            "ref_ids": [
                "42"
            ],
            "1": "[42] formulate an optimization problem to search for the 3d configuration with the highest probability given 2d confidence maps and solve the problem using Expectation-Maximization.",
            "2": "Inspired by matrix factorization methods commonly used in Structure-from-Motion (SfM)\n[36] and non-rigid SfM [5], several works [30, 41, 42] on 3d human pose estimation factorize the sequence of 3d human poses into a linear combination of shape bases.",
            "3": "Unlike approaches [30, 41, 42] that estimate the 3d poses in shape space, we follow [2] to formulate the task in the trajectory space.",
            "4": "Several works [4, 10, 12, 14, 18, 21, 24, 27, 28, 29, 32, 42] also report the error after aligning further with respect to the ground truth pose via Procrustes Analysis.",
            "5": "[42] 87."
        },
        "Ego-pose estimation and forecasting as real-time pd control": {
            "authors": [
                "Ye Yuan",
                "Kris Kitani"
            ],
            "url": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Yuan_Ego-Pose_Estimation_and_Forecasting_As_Real-Time_PD_Control_ICCV_2019_paper.pdf",
            "ref_texts": "[70] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 4966\u20134975, 2016. 3",
            "ref_ids": [
                "70"
            ],
            "1": "Deep learning based approaches [70,35,32, 57] have also succeeded in directly regressing images to 3D joint locations with the help of large-scale MoCap datasets [15]."
        },
        "Mo2Cap2: Real-time Mobile 3D Motion Capture with a Cap-mounted Fisheye Camera": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1803.05959",
            "ref_texts": "[74] X. Zhou, M. Zhu, S. Leonardos, K. Derpanis, and K. Daniilidis. Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video. In CVPR , 2016.",
            "ref_ids": [
                "74"
            ],
            "1": "Methods either operate directly on images [28, 32, 56, 57, 73], lift 2D pose detections to 3D [5, 10, 21, 68, 74], or use motion compensation and optical flow in videos [1, 58].",
            "2": "[74] X."
        },
        "Multi-person 3d human pose estimation from monocular images": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1909.10854",
            "ref_texts": "[41] X. Zhou, M. Zhu, K. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In CVPR , 2016. 1, 3",
            "ref_ids": [
                "41"
            ],
            "1": "Although, there is a vast literature on single-person 3D pose estimation [31, 13, 40, 41, 24, 21, 5, 36, 3, 16, 1, 39, 6, 12], the space of multi-person 3D pose estimation is mostly unexplored with only a handful of prior work [27, 20, 37, 28, 38].",
            "2": "Single-person 3D Pose Estimation: Single person 3D pose estimation works can be broadly divided based on whether they directly regress 3D joints [31, 13, 40, 41] or use a pipelined approach of inferring 3D pose from 2D pose [33, 39, 21, 22, 14].",
            "3": "Many approaches perform a direct 2D-to-3D lifting of poses [41, 17, 22, 5, 36] by either learning the transformation or by a nearest-neighbour lookup in a pose library.",
            "4": "Furthermore, many pipelined approaches [21, 27, 39, 31, 41, 24] have reported significant improvements in in-the-wild performances by using the more diverse 2D pose datasets to pre-train or jointly train their 2D prediction modules.",
            "5": "Several methods in the past have also reported significant improvements by using temporal cues [25, 21, 41, 6, 35, 37] by either learning a motion/refinement model or by using temporal constraints in a constrained optimization framework.",
            "6": "1, 3\n[41] X."
        },
        "Repnet: Weakly supervised training of an adversarial reprojection network for 3d human pose estimation": {
            "authors": [
                "Bastian Wandt",
                "Bodo Rosenhahn"
            ],
            "url": "http://openaccess.thecvf.com/content_CVPR_2019/papers/Wandt_RepNet_Weakly_Supervised_Training_of_an_Adversarial_Reprojection_Network_for_CVPR_2019_paper.pdf",
            "ref_texts": "[50] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 2016. 2,6",
            "ref_ids": [
                "50"
            ],
            "1": "Commonly the best linear combination of bases obtained by a principal component analysis is optimized [7,43,49,50].",
            "2": "[50] 87.",
            "3": "2,7\n[50] X."
        },
        "Marker-less 3D human motion capture with monocular image sequence and height-maps": {
            "authors": [],
            "url": "http://zju-capg.org/heightmap/0697.pdf",
            "ref_texts": "32. Zhou, X., Zhu, M., Leonardos, S., Derpanis, K., Daniilidis, K.: Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video. In: CVPR. (2016)",
            "ref_ids": [
                "32"
            ],
            "1": "In [32], 3D pose estimation is considered as a sparsity-driven reconstruction problem with temporal smoothness prior."
        },
        "Recurrent 3d pose sequence machines": {
            "authors": [
                "Mude Lin",
                "Liang Lin",
                "Xiaodan Liang",
                "Keze Wang",
                "Hui Cheng"
            ],
            "url": "http://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Recurrent_3D_Pose_CVPR_2017_paper.pdf",
            "ref_texts": "[41] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In CVPR , 2016. 2,3,5, 6,7",
            "ref_ids": [
                "41"
            ],
            "1": "There has been some limited attempts on combining the image-based 2D part detectors, 3D geometric pose priors and temporal models for generating 3D poses [2,39,41,30].",
            "2": "They mainly follow two kinds of pipelines: the first [41,20] resorts to the model-based 3D pose reconstruction by using external 3D pose gallery, while the second pipeline [4,40] focuses on elaborately designing human body kinematic constraints with the model training.",
            "3": "[41] proposed a 3D pose estimation framework from videos that consists of a novel synthesis between a deep-learningbased 2D part detector, a sparsity-driven 3D reconstruction approach and a 3D temporal smoothness prior.",
            "4": "In the following experiments, we strictly follow the same data partition protocol as in previous works [41,20,40,30,9,24].",
            "5": "To be more general, our RPSM is trained on training samples from all 15 actions instead of exploiting individual action like [41,20].",
            "6": "[41] 87.",
            "7": "Following [41,9,30], we employ the popular 3D pose error metric [28] , which calculates the Euclidean errors on all joints and all frames up to translation.",
            "8": "Following [41,20], the input image is cropped around the human.",
            "9": "[41] (CNN based), Zhou et al.",
            "10": ", [20,30,9,41,40], also employ deep learning techniques.",
            "11": "[41], Zhou et al.",
            "12": "[41] and Zhou et al.",
            "13": "2,3,5,6,7\n[41] X."
        },
        "Forecasting human dynamics from static images": {
            "authors": [
                "Wei Chao",
                "Jimei Yang",
                "Brian Price",
                "Scott Cohen",
                "Jia Deng"
            ],
            "url": "http://openaccess.thecvf.com/content_cvpr_2017/papers/Chao_Forecasting_Human_Dynamics_CVPR_2017_paper.pdf"
        },
        "Orinet: A fully convolutional network for 3d human pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1811.04989",
            "ref_texts": "[35] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In CVPR , 2016.",
            "ref_ids": [
                "35"
            ],
            "1": "In [35], they treat the 2D estimation as latent variables and use the EM algorithm to update the 2D joints at the same time.",
            "2": "Then the limb length ratio as used in many other works [30, 38] and scale information [15, 26, 35, 38] are used to recover each limb vector.",
            "3": "Following [35], we test our algorithm on all 17 joints defined in [12]."
        },
        "Eventhpe: Event-based 3d human pose and shape estimation": {
            "authors": [
                "Shihao Zou",
                "Chuan Guo",
                "Xinxin Zuo",
                "Sen Wang",
                "Pengyu Wang",
                "Xiaoqin Hu",
                "Shoushun Chen",
                "Minglun Gong",
                "Li Cheng"
            ],
            "url": "http://openaccess.thecvf.com/content/ICCV2021/papers/Zou_EventHPE_Event-Based_3D_Human_Pose_and_Shape_Estimation_ICCV_2021_paper.pdf",
            "ref_texts": "[31] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4966\u20134975, 2016. 2",
            "ref_ids": [
                "31"
            ],
            "1": "The approaches prior to the deep learning era are primary dictionary learningbased [26, 31, 5]."
        },
        "Ego-Body Pose Estimation via Ego-Head Pose Estimation": {
            "authors": [
                "Jiaman Li",
                "Karen Liu",
                "Jiajun Wu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Ego-Body_Pose_Estimation_via_Ego-Head_Pose_Estimation_CVPR_2023_paper.pdf",
            "ref_texts": "[52] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3D human pose estimation from monocular video. In CVPR , 2016. 2",
            "ref_ids": [
                "52"
            ],
            "1": "One is to regress joint positions directly from images and videos [25, 28, 36, 52]."
        },
        "Motion capture from internet videos": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2008.07931",
            "ref_texts": "57. Zhou, X., Zhu, M., Leonardos, S., Derpanis, K.G., Daniilidis, K.: Sparseness meets deepness: 3d human pose estimation from monocular video. In: CVPR (2016)",
            "ref_ids": [
                "57"
            ],
            "1": "To make human MoCap a commodity, many monocular motion capture algorithms [57,24,17,51] have been developed to recover human motion from single RGB videos.",
            "2": "Many works focus on the skeletonbased 3D human pose estimation, either ffrst estimating 2D pose from images and then lifting it to 3D [30,57,7,29,36], or end-to-end regressing to obtain the 3D pose directly [45,42,44,58,33,43]."
        },
        "Deep non-rigid structure from motion": {
            "authors": [
                "Chen Kong",
                "Simon Lucey"
            ],
            "url": "http://openaccess.thecvf.com/content_ICCV_2019/papers/Kong_Deep_Non-Rigid_Structure_From_Motion_ICCV_2019_paper.pdf",
            "ref_texts": "[35] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Kosta Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. arXiv preprint arXiv:1511.09439 , 2015. 2",
            "ref_ids": [
                "35"
            ],
            "1": "Sparse NRS fM:Sparse prior [18,35,19] is more genericthan union-of-subspaces since it is equivalent to the union of all possible local subspaces."
        },
        "Song from PI: A musically plausible network for pop music generation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1611.03477",
            "ref_texts": "Jamshed J. Bharucha and Peter M. Todd. Modeling the perception of tonal structure with neural nets. Computer Music Journal , 13(4):44\u201353, 1989. Nicolas Boulanger-lewandowski, Yoshua Bengio, and Pascal Vincent. Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription. In ICML , 2012. Michael Chan, John Potter, and Emery Schubert. Improving algorithmic music composition with machine learning. In 9th International Conference on Music Perception and Cognition , 2006. Chun-Chi J. Chen and Risto Miikkulainen. Creating melodies with evolving recurrent neural networks. In International Joint Conference on Neural Networks , 2001. Douglas Eck and Juergen Schmidhuber. A first look at music composition using lstm recurrent neural networks. 2002. Steve Engels, Fabian Chan, and Tiffany Tong. Automatic real-time music generation for games. In AIIDE Workshop , 2015. Brendan J Frey and Delbert Dueck. Clustering by passing messages between data points. volume 315, pp. 972\u2013976, 2007. Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge. A neural algorithm of artistic style. In arXiv:1508.06576 , 2015. Allen Huang and Raymond Wu. Deep learning for music. arXiv preprint arXiv:1606.04930 , 2016. Daniel Johnson. Composing music with recurrent neural networks. https://goo.gl/YP9QyR . Semin Kang, Soo-Yol Ok, and Young-Min Kang. Automatic Music Generation and Machine Learning Based Evaluation , pp. 436\u2013443. Springer Berlin Heidelberg, 2012. Andrej Karpathy, Justin Johnson, and Li Fei-Fei. Visualizing and understanding recurrent networks. InICLR 2016 Workshop , 2016. Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Skip-thought vectors. In NIPS , 2015. David Macdonald. Song from \u0019.https://youtu.be/OMq9he-5HUU . Reddit midi man. Midi collection. https://goo.gl/4moEZ3 . Michael C. Mozer. Neural network music composition by prediction: Exploring the benefits of psychoacoustic constraints and multi-scale processing. Connection Science , 6(2-3), 1996. Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hourglass networks for human pose estimation. In ECCV , 2016. Arnold Schoenberg and Dika Newlin. Style and idea. Technical report, Williams and Norgate London, 1951. Edgar Simo-Serra, Sanja Fidler, Francesc Moreno-Noguer, and Raquel Urtasun. Neuroaesthetics in fashion: Modeling the perception of beauty. In CVPR , 2015. Felix Sun. Deephear composing and harmonizing music with neural networks. https://goo. gl/7OTZZL . Elliot Waite, Douglas Eck, Adam Roberts, and Dan Abolafia. Project magenta. https: //magenta.tensorflow.org/ . Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Kosta Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In CVPR , 2016."
        },
        "Uncertainty-aware adaptation for self-supervised 3d human pose estimation": {
            "authors": [
                "Jogendra Nath",
                "Siddharth Seth",
                "Pradyumna Y",
                "Varun Jampani",
                "Anirban Chakraborty",
                "Venkatesh Babu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Kundu_Uncertainty-Aware_Adaptation_for_Self-Supervised_3D_Human_Pose_Estimation_CVPR_2022_paper.pdf",
            "ref_texts": "[102] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3D human pose estimation from monocular video. In CVPR , 2016. 2",
            "ref_ids": [
                "102"
            ],
            "1": "[102] \u2717\u2713\u2717 \u2717 \u2717 \u2713 Rhodin et al.",
            "2": "3\n[102] X."
        },
        "Rethinking pose in 3d: Multi-stage refinement and recovery for markerless motion capture": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1808.01525",
            "ref_texts": "[45] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4966\u20134975, 2016. 3, 6, 7",
            "ref_ids": [
                "45"
            ],
            "1": "The task is then to lift the 2D coordinates into 3D either by model fitting [1, 2, 23, 27, 44, 45] or regression [16, 17].",
            "2": "The evaluation is performed every 5thframe, as in [45], due to the high similarity of subsequent frames.",
            "3": "[45] 87.",
            "4": "3, 7\n[45] X."
        },
        "3d human sensing, action and emotion recognition in robot assisted therapy of children with autism": {
            "authors": [
                "Elisabeta Marinoiu",
                "Mihai Zanfir",
                "Vlad Olaru",
                "Cristian Sminchisescu"
            ],
            "url": "http://openaccess.thecvf.com/content_cvpr_2018/papers/Marinoiu_3D_Human_Sensing_CVPR_2018_paper.pdf",
            "ref_texts": "[38] X. Zhou, M. Zhu, K. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In CVPR , 2016.",
            "ref_ids": [
                "38"
            ],
            "1": "Recent advances in 2d and 3d pose estimation [7,28,21,6,22,26,38] can potentially offer an alternative to depth sensors by providing reliable pose es-timates from only RGB data.",
            "2": "[38] X."
        },
        "Estimating egocentric 3d human pose in global space": {
            "authors": [
                "Jian Wang",
                "Lingjie Liu",
                "Weipeng Xu",
                "Kripasindhu Sarkar",
                "Christian Theobalt"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Estimating_Egocentric_3D_Human_Pose_in_Global_Space_ICCV_2021_paper.pdf",
            "ref_texts": "[52] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G. Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In IEEE Conference on Computer Vision and Pattern Recognition , pages 4966\u20134975, 2016. 3",
            "ref_ids": [
                "52"
            ],
            "1": "[52] introduce EM method to estimate 3D pose from 2D predictions over the entire sequence."
        },
        "Real-time full-body motion capture from video and imus": {
            "authors": [],
            "url": "https://openresearch.surrey.ac.uk/esploro/fulltext/conferencePresentation/Real-time-Full-Body-Motion-Capture-from-Video/99512687002346?repId=12138912520002346&mId=13140472550002346&institution=44SUR_INST",
            "ref_texts": "[22] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3D human pose estimation from monocular video. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4966\u20134975, 2016. 2",
            "ref_ids": [
                "22"
            ],
            "1": "[22] use CNNs for 2D joint detection and offline Expectation-Maximization over an entire sequence for 3D pose.",
            "2": "5\n[22] X."
        },
        "How robust is 3D human pose estimation to occlusion?": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1808.09316",
            "ref_texts": "[28] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In CVPR , 2016.",
            "ref_ids": [
                "28"
            ],
            "1": "Sit SitD Smoke Wait Walk WalkD WalkT Avg Zhou [28] 87.",
            "2": "[28] X."
        },
        "Text2action: Generative adversarial synthesis from language to action": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1710.05298",
            "ref_texts": "[16] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis, \u201cSparseness meets deepness: 3d human pose estimation from monocular video,\u201d in Proc. of the IEEE Conference on Computer Vision and Pattern Recognition , 2016, pp. 4966\u20134975.",
            "ref_ids": [
                "16"
            ],
            "1": "Extracted 2D poses are converted to 3D poses and used as our dataset [16].",
            "2": "Extracted 2D human poses are converted to 3D poses based on the code from [16].",
            "3": "Extracted 2D poses are converted to 3D poses and used as our dataset [16].",
            "4": "Extracted 2D poses are converted to 3D poses and used as our dataset [16].",
            "5": "[16] X."
        },
        "A unified deep framework for joint 3d pose estimation and action recognition from a single rgb camera": {
            "authors": [],
            "url": "https://www.mdpi.com/1424-8220/20/7/1825/pdf",
            "ref_texts": "63. Zhou, X.; Zhu, M.; Leonardos, S.; Derpanis, K.G.; Daniilidis, K. Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV , USA, 26 June\u20131 July 2016; pp. 4966\u20134975.",
            "ref_ids": [
                "63"
            ],
            "1": "[63]?87."
        },
        "Full-body awareness from partial observations": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2008.06046",
            "ref_texts": "60. Zhou, X., Zhu, M., Leonardos, S., Derpanis, K.G., Daniilidis, K.: Sparseness meets deepness: 3D human pose estimation from monocular video. In: CVPR (2016)",
            "ref_ids": [
                "60"
            ],
            "1": "2 Related Work Human Pose Estimation In the Wild: Human pose estimation has improved substantially in recent years due in part to improved methods for 2D [9,17,38,51,56] and 3D [1,28,35,43,45,60] pose, which typically utilize deep networks as opposed to classic approaches such as deformable part models [7,10,12,58]."
        },
        "Estimating 3d motion and forces of person-object interactions from monocular video": {
            "authors": [
                "Zongmian Li",
                "Jiri Sedlar",
                "Justin Carpentier",
                "Ivan Laptev",
                "Nicolas Mansard",
                "Josef Sivic"
            ],
            "url": "http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Estimating_3D_Motion_and_Forces_of_Person-Object_Interactions_From_Monocular_CVPR_2019_paper.pdf",
            "ref_texts": "[71] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In CVPR , 2016.",
            "ref_ids": [
                "71"
            ],
            "1": "Building on the recent progress in 2D human pose estimation [51,50,34,14], two-stage methods have been shown to be very effective [5,71,9,19] and achieve state-of-the-art results [47] on 3D human pose benchmarks [35].",
            "2": "To deal with depth ambiguities, these estimators rely on good pose priors, which are either hand-crafted or learnt from large-scale MoCap data [71,9,37].",
            "3": "[71] X."
        },
        "Not all parts are created equal: 3d pose estimation by modeling bi-directional dependencies of body parts": {
            "authors": [
                "Jue Wang",
                "Shaoli Huang",
                "Xinchao Wang",
                "Dacheng Tao"
            ],
            "url": "http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Not_All_Parts_Are_Created_Equal_3D_Pose_Estimation_by_ICCV_2019_paper.pdf",
            "ref_texts": "[48] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2016.",
            "ref_ids": [
                "48"
            ],
            "1": "These methods comprise a 2D pose detector and a subsequent optimization [48, 47, 49] or regression [4, 3, 17, 30, 36, 14, 19, 7, 12] step to estimate 3D pose .",
            "2": "The most straightforward approach is to represent 3D poses as linear combinations of models learned from training data [48, 47, 49].",
            "3": "[48] 87.",
            "4": "Following [48, 22, 45, 21], we down sampled the original videos from 50fps to 10fps to remove redundancy."
        },
        "Self-learning with rectification strategy for human parsing": {
            "authors": [
                "Tao Li",
                "Zhiyuan Liang",
                "Sanyuan Zhao",
                "Jiahao Gong",
                "Jianbing Shen"
            ],
            "url": "https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Self-Learning_With_Rectification_Strategy_for_Human_Parsing_CVPR_2020_paper.pdf",
            "ref_texts": "[55] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In IEEE CVPR , pages 4966\u20134975, 2016. 1",
            "ref_ids": [
                "55"
            ],
            "1": "It has been widely applied in human-computer interaction [26], human behavior understanding [45,10,53], security monitoring [55,12] etc.",
            "2": "7\n[55] X."
        },
        "To the point: Correspondence-driven monocular 3d category reconstruction": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper/2021/file/40008b9a5380fcacce3976bf7c08af5b-Paper.pdf",
            "ref_texts": "[40] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis, \u201cSparseness meets deepness: 3d human pose estimation from monocular video,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 4966\u20134975, 2016. 3",
            "ref_ids": [
                "40"
            ],
            "1": "The priors are embedded into the methods using low-rank subspaces [37,7,17], spatio-temporal domains [34,38], equivariance constraints [35] or sparse basis coefficients using L1 constraints [36,39,40].",
            "2": "3\n[40] X."
        },
        "Learning latent representations of 3d human pose with deep neural networks": {
            "authors": [],
            "url": "https://hal.science/hal-02509358/file/ijcv18.pdf"
        },
        "Deep autoencoder for combined human pose estimation and body model upscaling": {
            "authors": [
                "Matthew Trumble",
                "Andrew Gilbert",
                "John Collomosse",
                "Adrian Hilton"
            ],
            "url": "http://openaccess.thecvf.com/content_ECCV_2018/papers/Matthew_Trumble_Deep_Autoencoder_for_ECCV_2018_paper.pdf",
            "ref_texts": "30.Zhou, X., Zhu, M., Leonardos, S., Derpanis, K.G., Daniilidis, K. : Sparseness meets deepness: 3d human pose estimation from monocular video. In: P roceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (2 016) 4966\u20134975",
            "ref_ids": [
                "30"
            ],
            "1": "Zhou [30] integrates 2D, 3D and temporal information to account for uncertainties in the data."
        },
        "Generating multiple diverse hypotheses for human 3d pose consistent with 2d joint detections": {
            "authors": [
                "Ehsan Jahangiri",
                "Alan L. Yuille"
            ],
            "url": "http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w16/Jahangiri_Generating_Multiple_Diverse_ICCV_2017_paper.pdf",
            "ref_texts": "[43] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In CVPR , June 2016.",
            "ref_ids": [
                "43"
            ],
            "1": "On the other hand, most of the 3D pose estimation methods use sparse coding based on an overcomplete dictionary of basis poses to represent a 3D pose and fit the 3D pose projection to the 2D joint detections [24,37,1,42,43].",
            "2": "[43] X."
        },
        "Pedx: Benchmark dataset for metric 3-d pose estimation of pedestrians in complex urban intersections": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1809.03605",
            "ref_texts": "[4] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis, \u201cSparseness meets deepness: 3d human pose estimation from monocular video,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2016, pp. 4966\u20134975.",
            "ref_ids": [
                "4"
            ],
            "1": "The recent application of deep neural networks has generated state-of-the-art results for 2D body pose estimation [1], which has inspired extensions to the 3D pose estimation [2], [3], [4], [5], [6].",
            "2": "3D human pose estimation In many papers, 3D human pose estimation has been formulated as a problem of regressing 3D joint locations by directly extracting visual features from an image [9], [10], [2], [3], [4], [11], [5], [12], or by lifting 2D joint detector outputs to 3D joints in a camera relative frame [6], [13], [14].",
            "3": "[4] X."
        },
        "High-order Graph Convolutional Networks for 3D Human Pose Estimation.": {
            "authors": [],
            "url": "https://www.bmvc2020-conference.com/assets/papers/0550.pdf",
            "ref_texts": "[52] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 4966\u20134975, 2016.",
            "ref_ids": [
                "52"
            ],
            "1": "[52] utilize a sparsity-driven 3D geometric prior and temporal smoothness to regress 3D poses from uncertain 2D keypoints maps via the EM algorithm."
        },
        "3D human shape reconstruction from a polarization image": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2007.09268",
            "ref_texts": "51. Zhou, X., Zhu, M., Leonardos, S., Derpanis, K.G., Daniilidis, K.: Sparseness meets deepness: 3d human pose estimation from monocular video. In: Proceedings of the IEEE conference on computer vision and pattern recognition. (2016) 4966{4975",
            "ref_ids": [
                "51"
            ],
            "1": "Many of the studies [51,52,53,54,55,56,57] utilize dictionary-based learning strategies."
        },
        "Configurable 3d scene synthesis and 2d image rendering with per-pixel ground truth using stochastic grammars": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1704.00112",
            "ref_texts": "151. Zhou, X., Zhu, M., Leonardos, S., Derpanis, K.G., Daniilidis, K.: Sparseness meets deepness: 3d human pose estimation from monocular video. In: Conference on Computer Vision and Pattern Recognition (CVPR) (2016)",
            "ref_ids": [
                "151"
            ],
            "1": "1 Related Work Synthetic image datasets have recently been a source of training data for object detection and correspondence matching [26, 32,37,83,91,95,113,117,120,150], single-view reconstruction [58], view-point estimation [84, 119], 2D human pose estimation [93, 96,105], 3D human pose estimation [18, 27,39,104, 109, 111, 126, 139, 151], depth prediction [118], pedestrian detection [49, 81,94,127], action recognition [100, 101, 115], semantic segmentation [103], scene understanding [45, 46,60,97], as well as in benchmark datasets [47]."
        },
        "Reconstructing vehicles from a single image: Shape priors for road scene understanding": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1609.09468"
        },
        "Dual networks based 3d multi-person pose estimation from monocular video": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2205.00748",
            "ref_texts": "[12] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis, \u201cSparseness meets deepness: 3d human pose estimation from monocular video,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition , 2016, pp. 4966\u20134975.",
            "ref_ids": [
                "12"
            ],
            "1": "XXX, MAY YYYY 2 Top-down / Bottom-up2D / 3D Coordinate system Methods Top-down2D human pose estimation2D image coordinate [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11]\n3D human pose estimation3D person-centric[12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23]\n[24], [25], [26], [27], [28], [29], [30], [31]\n3D camera-centric [32], [33], [34], [35], [36], [37], [38] Bottom-up2D human pose estimation2D image coordinate[39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50]\n[51], [52]\n3D human pose estimation3D person-centric N/A\n3D camera-centric [53], [54], [55] TABLE 1 Summary of the top-down and bottom-up 2D/3D human pose estimation methods and the coordinate systems of the obtained human pose results.",
            "2": "6M, 3DHP , HumanEva, Penn Action[12], [13], [14], [15], [16] [17], [18], [19], [20], [21]\n[22], [23], [24], [25], [26] [27], [28], [29], [30], [31]\n3D multi-person poseCamera-centric (absolute coordinate)PCK abs, AProot 25, MPRE [35], F1 value [54]MuPoTS-3D, JTA, 3DPW[32], [33], [34], [35], [36], [37], [53], [54], [56], [57]\n[38], [55] TABLE 2 Summary of the differences between 3D human pose estimation and multi-person pose estimation.",
            "3": "[12] X."
        },
        "Deca: Deep viewpoint-equivariant human pose estimation using capsule autoencoders": {
            "authors": [
                "Nicola Garau",
                "Niccolo Bisagno",
                "Piotr Brodka",
                "Nicola Conci"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Garau_DECA_Deep_Viewpoint-Equivariant_Human_Pose_Estimation_Using_Capsule_Autoencoders_ICCV_2021_paper.pdf",
            "ref_texts": "[38] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4966\u20134975, 2016.",
            "ref_ids": [
                "38"
            ],
            "1": "3D HPE usually leverages on additional cues, such as 2D predictions [32, 34, 30], multiple images [38], pre-trained models [17] and pose dictionaries [27].",
            "2": "[38] X."
        },
        "Lightweight 3d human pose estimation network training using teacher-student learning": {
            "authors": [
                "Hyun Hwang",
                "Suntae Kim",
                "Nicolas Monet",
                "Hideki Koike",
                "Soonmin Bae"
            ],
            "url": "http://openaccess.thecvf.com/content_WACV_2020/papers/Hwang_Lightweight_3D_Human_Pose_Estimation_Network_Training_Using_Teacher-Student_Learning_WACV_2020_paper.pdf",
            "ref_texts": "[51] X. Zhou, M. Zhu, K. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In IEEE CVPR 2016 .",
            "ref_ids": [
                "51"
            ],
            "1": "[51] 87.",
            "2": "[51] X."
        },
        "Using locally corresponding cad models for dense 3d reconstructions from a single image": {
            "authors": [
                "Chen Kong",
                "Hsuan Lin",
                "Simon Lucey"
            ],
            "url": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Kong_Using_Locally_Corresponding_CVPR_2017_paper.pdf",
            "ref_texts": "[21] X. Zhou, M. Zhu, S. Leonardos, K. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. arXiv preprint arXiv:1511.09439 , 2015. 1",
            "ref_ids": [
                "21"
            ],
            "1": "With the remarkable success in Structure from Motion (SfM), which is now capable of reconstructing entire cities using large-scale photo collections [1] and real-time visual SLAM on embedded and mobile devices [14], the computer vision community is starting to explore the possibility of constructing a 3D model of an object from a single image [17,20,11,21,18].",
            "2": "1, 2\n[21] X."
        },
        "Simpose: Effectively learning densepose and surface normals of people from simulated data": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2007.15506",
            "ref_texts": "62. Zhou, X., Zhu, M., Leonardos, S., Derpanis, K.G., Daniilidis, K.: Sparseness meets deepness: 3d human pose estimation from monocular video. In: CVPR (2016)",
            "ref_ids": [
                "62"
            ],
            "1": "3D human pose estimation [32, 62, 63] learns the additional depth dimension for the 2D skeleton, by learning a 3D pose dictionary [62] or adding a 3D bone length constraint [63]."
        },
        "Deformation capture via soft and stretchable sensor arrays": {
            "authors": [],
            "url": "https://dl.acm.org/doi/pdf/10.1145/3311972",
            "ref_texts": "1039/C4TC00392F Daniel Xu, Andreas Tairych, and Iain A. Anderson. 2016. Stretch not flex: Programmable rubber keyboard. Smart Mater. Struct. 25, 1 (2016), 015012. http:// stacks.iop.org/0964-1726/25/i=1/a=015012 Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G. Derpanis, and Kostas Daniilidis. 2016. Sparseness meets deepness: 3D human pose estimation from monocular video. In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition . 4966\u20134975. ThomasG.Zimmerman,JoshuaR.Smith,JosephA.Paradiso,DavidAllport,andNeil Gershenfeld.1995.Applyingelectricfieldsensingtohuman-computerinterfaces. InProceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI\u201995). ACM Press/Addison-Wesley Publishing Co., New York, NY, 280\u2013287. DOI:https://doi.org/10.1145/223904.223940 Michael Zollh\u00f6fer, Matthias Nie\u00dfner, Shahram Izadi, Christoph Rehmann, Christopher Zach, Matthew Fisher, Chenglei Wu, Andrew Fitzgibbon, Charles Loop, ChristianTheobalt,etal.2014.Real-timenon-rigidreconstructionusinganRGBD camera. ACM Trans.Graph. 33, 4 (2014), 156. ReceivedJune 2018;revised November2018;acceptedJanuary 2019 ACMTransactionsonGraphics,Vol.38, No.2, Article16. Publicationdate: March2019."
        },
        "Markerless gait analysis based on a single RGB camera": {
            "authors": [
                "Jessica M"
            ],
            "url": "https://eprints.gla.ac.uk/208065/1/208065.pdf",
            "ref_texts": ""
        },
        "Nose, eyes and ears: Head pose estimation by locating facial keypoints": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1812.00739",
            "ref_texts": "[21] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis, \u201cSparseness meets deepness: 3d human pose estimation from monocular video,\u201d in CVPR , 2016.",
            "ref_ids": [
                "21"
            ],
            "1": "Zhou [21] use heatmap images of 2D joint locations to infer 3D human pose using an Expectation Maximization framework.",
            "2": "Interestingly, both these works [21, 22] use heatmaps over 2D spatial locations to infer 3D structure/pose.",
            "3": "Unlike previous efforts [21, 22] that use heatmaps as an intermediate representation and do not have ground truth data, we have ground truth pose angles available."
        },
        "Synthetic occlusion augmentation with volumetric heatmaps for the 2018 eccv posetrack challenge on 3d human pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1809.04987",
            "ref_texts": "[33] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3D human pose estimation from monocular video. In CVPR , 2016.",
            "ref_ids": [
                "33"
            ],
            "1": "0 Zhou (2016) [33] 87.",
            "2": "[33] X."
        },
        "Geometry-driven self-supervised method for 3d human pose estimation": {
            "authors": [
                "Yang Li",
                "Kan Li",
                "Shuai Jiang",
                "Ziyue Zhang",
                "Congzhentao Huang",
                "Richard Yi",
                "Da Xu"
            ],
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/6808/6662",
            "ref_texts": "1504. Zhou, X.; Zhu, M.; Leonardos, S.; Derpanis, K. G.; and Daniilidis, K. 2016. Sparseness meets deepness: 3d humanpose estimation from monocular video. In CVPR , 4966\u2013",
            "ref_ids": [
                "1504"
            ]
        },
        "Generalizing monocular 3d human pose estimation in the wild": {
            "authors": [
                "Luyang Wang",
                "Yan Chen",
                "Zhenhua Guo",
                "Keyuan Qian",
                "Mude Lin",
                "Hongsheng Li",
                "Jimmy S. Ren"
            ],
            "url": "http://openaccess.thecvf.com/content_ICCVW_2019/papers/GMDL/Wang_Generalizing_Monocular_3D_Human_Pose_Estimation_in_the_Wild_ICCVW_2019_paper.pdf",
            "ref_texts": "[50] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In IEEE Conference on Computer Vision and Pattern Recognition , 2016. 7",
            "ref_ids": [
                "50"
            ],
            "1": "[50] 87.",
            "2": "2\n[50] X."
        },
        "A baseline for cross-database 3d human pose estimation": {
            "authors": [
                "Michal Rapczynski",
                "Philipp Werner",
                "Sebastian Handrich",
                "Ayoub Al"
            ],
            "url": "https://www.mdpi.com/1424-8220/21/11/3769/pdf",
            "ref_texts": "43. Zhou, X.; Zhu, M.; Leonardos, S.; Derpanis, K.; Daniilidis, K. Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Boston, MA, USA, 7\u201312 June 2015; pp. 4966\u20134975.",
            "ref_ids": [
                "43"
            ],
            "1": "[43] 113."
        },
        "Human motion capture using a drone": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1804.06112",
            "ref_texts": "[34] X. Zhou, M. Zhu, S. Leonardos, K. Derpanis, and K. Daniilidis, \u201cSparseness meets deepness: 3D human pose estimation from monocular video,\u201d in CVPR , 2016.",
            "ref_ids": [
                "34"
            ],
            "1": "Our pipeline consists of the following steps: 1)\n2D pose detection in which the subject is detected and the 2D pose is estimated in each frame; 2) single-frame initialization in which the camera viewpoints and 3D human poses are initialized by the single-view pose estimation method [34];\n3)multi-frame bundle adjustment in which the camera viewpoints and 3D poses are refined by minimizing the nuclear norm of shape matrix with an articulation constraint.",
            "2": "To address these difficulties, we propose to use a recent method for single-view 3D pose estimation [34] to initialize the reconstruction.",
            "3": "In [34], a pose dictionary is learned from existing MoCap data and the pose to be reconstructed is assumed as a linear combination of the bases in the dictionary.",
            "4": "An EM algorithm is also developed in [34] to account for uncertainties in CNN based 2D joint localization by incorporating the 3D pose prior learned from MoCap data.",
            "5": "In accordance with the initialization method [34], we optimize Rover the Stiefel manifold, which is implemented using the manifold optimization toolbox [37].",
            "6": "The results of the initial single-view method [34] and the bundle adjustment with (= 1) and without (= 0) the articulation constraint are presented.",
            "7": "The three curves correspond to the single-view initialization (Initial) by [34], the multi-frame bundle adjustment (BA) and BA without the articulation constraint (BA w/o art.",
            "8": "While the initial single-view estimates by [34] have captured global structures, the reconstructions after the multi-frame bundle adjustment are closer to the ground truth recovering more faithful details, e.",
            "9": "The bottom-left figure in Figure 4 shows an example where the original 2D pose estimate is inaccurate but the final reconstruction is correct after handling 2D uncertainties by [34].",
            "10": "7 Initial [34] 74.",
            "11": "[34] X."
        },
        "Reducing footskate in human motion reconstruction with ground contact constraints": {
            "authors": [
                "Yuliang Zou",
                "Jimei Yang",
                "Duygu Ceylan",
                "Jianming Zhang",
                "Federico Perazzi",
                "Bin Huang"
            ],
            "url": "http://openaccess.thecvf.com/content_WACV_2020/papers/Zou_Reducing_Footskate_in_Human_Motion_Reconstruction_with_Ground_Contact_Constraints_WACV_2020_paper.pdf",
            "ref_texts": "[48] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In CVPR , 2016. 2",
            "ref_ids": [
                "48"
            ],
            "1": "Prior works [28,34,36,46,48] formulate a constrained optimization problem to obtain smooth motion trajectories by taking advantage of 2D cues."
        },
        "Optical flow-based 3d human motion estimation from monocular video": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1703.00177",
            "ref_texts": ""
        },
        "Passing a non-verbal turing test: Evaluating gesture animations generated from speech": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2107.00712",
            "ref_texts": "[52] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp. 4966\u20134975, 2016.",
            "ref_ids": [
                "52"
            ],
            "1": "Recent advancements in motion capture estimation from 2D video [6, 11, 52] allow us to process many online videos of lectures and speeches and extract 3D body poses and gestures from them.",
            "2": "[52] X."
        },
        "3d human pose estimation with relational networks": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1805.08961",
            "ref_texts": "[38] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konst antinos G Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pos e estimation from monocular video. In Proceedings of the IEEE conference on computer vision and pa ttern recognition , pages 4966\u20134975, 2016.",
            "ref_ids": [
                "38"
            ],
            "1": "There are a few approaches that exploit temporal informatio n using various methods such asovercomplete dictionaries [38,39], 3D CNNs [7], sequence-to-sequence networks [10], andmultiple-view settings [25]."
        },
        "Procrustean regression networks: Learning 3d structure of non-rigid objects from 2d annotations": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2007.10961",
            "ref_texts": "44. Zhou, X., Zhu, M., Leonardos, S., Derpanis, K.G., Daniilidis, K.: Sparseness meets deepness: 3d human pose estimation from monocular video. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (June 2016) Procrustean Regression Networks: Learning 3D Structure of Non-Rigid Objects from 2D Annotations Supplementary Materials Sungheon Park?1[0000\u00000002\u00007287\u00005661], Minsik Lee?2[0000\u00000003\u00004941\u00004311], and Nojun Kwaky3[0000\u00000002\u00001792\u00000327]",
            "ref_ids": [
                "44"
            ],
            "1": "Prior knowledge can be obtained by dictionary learning [43,44], but neural networks or convolutional neural networks (CNNs) are the most-used methods to learn the 2D-to-3D or image-to-3D mappings [24,30], recently."
        },
        "View invariant 3D human pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1901.10841",
            "ref_texts": "[32] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilid, \u201cSparseness meets deepness: 3D human pose estimation from monocular video.\u201d in IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 4966\u20134975.",
            "ref_ids": [
                "32"
            ],
            "1": "[32] (CVPR\u201916) 87.",
            "2": "[32] (CVPR\u201916) 99.",
            "3": "[32] X."
        },
        "Flycon: Real-time environment-independent multi-view human pose estimation with aerial vehicles": {
            "authors": [],
            "url": "https://autonomousrobots.nl/docs/18_naegelit_siggraph.pdf",
            "ref_texts": "2017. FlyCap: Markerless motion capture using multiple autonomous flying cameras. IEEE transactions on visualization and computer graphics (2017). Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. 2016. Sparseness meets deepness: 3D human pose estimation from monocular video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 4966\u20134975. Michael Zollh\u00f6fer, Matthias Nie\u00dfner, Shahram Izadi, Christoph Rehmann, Christopher Zach, Matthew Fisher, Chenglei Wu, Andrew Fitzgibbon, Charles Loop, Christian Theobalt, et al .2014. Real-time non-rigid reconstruction using an RGB-D camera. ACM Transactions on Graphics (TOG) 33, 4 (2014), 156. A NOTATION The following coordinate frames are used throughout this paper W",
            "ref_ids": [
                "2017"
            ]
        },
        "From image to stability: Learning dynamics from human pose": {
            "authors": [],
            "url": "https://www.cse.psu.edu/~rtc12/Papers/eccv2020_SCOTT_etal.pdf",
            "ref_texts": "74. Zhou, X., Zhu, M., Leonardos, S., Derpanis, K.G., Daniilidis, K.: Sparseness meets deepness: 3D human pose estimation from monocular video. In: IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). pp. 4966{4975 (2016) 3",
            "ref_ids": [
                "74"
            ],
            "1": "Success in 2D human pose estimation also has encouraged researchers to detect 3D skeletons by extending existing 2D human pose detectors [6, 14, 44, 46, 48, 62, 75] or by directly using image features [1, 51, 57, 64, 74]."
        },
        "Towards part-aware monocular 3d human pose estimation: An architecture search approach": {
            "authors": [],
            "url": "https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123480715.pdf",
            "ref_texts": "63. Zhou, X., Zhu, M., Leonardos, S., Derpanis, K.G., Daniilidis, K.: Sparseness meets deepness: 3d human pose estimation from monocular video. In: CVPR (2016)",
            "ref_ids": [
                "63"
            ],
            "1": "Another common approach [63, 3] is to learn an over-complete dictionary of basis 3D poses from a large database of motion capture data."
        },
        "Mosculp: Interactive visualization of shape and time": {
            "authors": [
                "First Author",
                "Second Author",
                "Third Author",
                "Fourth Author",
                "Fifth Author",
                "Sixth Author"
            ],
            "url": "https://dl.acm.org/doi/pdf/10.1145/3242587.3242592",
            "ref_texts": "50. Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G. Derpanis, and Kostas Daniilidis. 2016. Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video. In IEEE Conference on Computer Vision and Pattern Recognition.",
            "ref_ids": [
                "50"
            ],
            "1": "Various methods have been proposed to estimate 3D pose from a single image [6, 26, 40, 41, 39, 12, 48], or from a video [21, 22, 50, 36, 1]."
        },
        "Paul: Procrustean autoencoder for unsupervised lifting": {
            "authors": [
                "Chaoyang Wang",
                "Simon Lucey"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_PAUL_Procrustean_Autoencoder_for_Unsupervised_Lifting_CVPR_2021_paper.pdf",
            "ref_texts": "[43] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 4966\u20134975, 2016. 2[44] Yingying Zhu, Dong Huang, Fernando De La Torre, and Simon Lucey. Complex non-rigid motion 3d reconstruction by union of subspaces. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 1542\u2013",
            "ref_ids": [
                "43",
                "44"
            ],
            "1": "Non-Rigid Structure from Motion (NRS fM) methods approach the problem by introducing additional priors \u2013 of particular note in this regards are low rank [10,6,3] and union of subspaces [25,44] methods.",
            "2": "It is also well understood [10,21,44,25] that such low rank priors have poor performance when applied to more complex 3D shape variations.",
            "3": "(ii)union-of-subspaces is inspired by the intuition that complex non-rigid deformations could be clustered into a sequence of simple motions [44].",
            "4": "(iii)sparsity [22,20,43], is a more generic prior compared to union-of-subspaces.",
            "5": "2[44] Yingying Zhu, Dong Huang, Fernando De La Torre, and Simon Lucey."
        },
        "A dual-source approach for 3D human pose estimation from single images": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1705.02883",
            "ref_texts": "[66] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In IEEE Conference on Computer Vision and Pattern Recognition , 2016. SUBMITTED TO COMPUTER VISION AND IMAGE UNDERSTANDING. 13 Umar Iqbal is a PhD candidate with the Computer Vision Research Group at University of Bonn, Germany. He received his MS degree in Signal Processing from Tampere University of Technology, Finland, in 2013. He worked as computer vision research assistant at Computer Vision Group, COMSATS Institute of Information Technology, Pakistan (2010-2011), Nokia Research Center, Finland (2011-2013), and Tampere University of Technology, Finland (2013). His research interests include human pose estimation, activity recognition, multi-target tracking, and person reidentification. Andreas Doering received his B.Sc. degree in computer science from the University of Bonn, Germany, in 2015. Currently, he is a master\u2019s student and a research assistant at the Computer Vision Group at the University of Bonn. His research interests include: human pose estimation, object detection, scene understanding and machine learning. Hashim Yasin received his MS and PhD degrees from the Department of Computer Science, University of Bonn, Germany, in 2012 and 2016 respectively. Currently, he is an assistant professor at the National University of Computer & Emerging Sciences, Pakistan. His research interests include vision-based 3D motion retrieval and reconstruction, 3D pose estimation, motion synthesis and analysis etc. Bj\u00f6rn Kr\u00fcger studied computer science, mathematics and physics at Bonn university. He received his MS in computer science (Dipl.Inform.) in 2006 and his PhD (Dr. rer. nat.) in computer science in 2012. From 2012 to 2015 he worked as postdoc at Bonn university. Since 2015 he joined the Gokhale Method Institute (Stanford, CA) as senior researcher. His research interests include: computer animation, computer graphics, machine learning, and motion capture. Andreas Weber studied mathematics and computer science at the Universities of Tubingen, Germany and Boulder, Colorado, U.S.A. From the University of Tubingen he received his MS in Mathematics (Dipl.-Math) in 1990 and his PhD",
            "ref_ids": [
                "66"
            ],
            "1": "5 provides a detailed comparison of the proposed approach with the state-of-the-art methods [23], [29], [54], [52], [66], [64], [45], [55], [32], [13].",
            "2": "[66]* 87.",
            "3": "[66]* 199.",
            "4": "[66] X."
        },
        "CIMI4D: A Large Multimodal Climbing Motion Dataset under Human-scene Interactions": {
            "authors": [
                "Ming Yan",
                "Xin Wang",
                "Yudi Dai",
                "Siqi Shen",
                "Chenglu Wen",
                "Lan Xu",
                "Yuexin Ma",
                "Cheng Wang"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Yan_CIMI4D_A_Large_Multimodal_Climbing_Motion_Dataset_Under_Human-Scene_Interactions_CVPR_2023_paper.pdf",
            "ref_texts": "[75] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video. In Computer Vision and Pattern Recognition (CVPR) , 2016. 1",
            "ref_ids": [
                "75"
            ],
            "1": "However, it is a challenging and long-standing problem [1, 7,35,37,75] due to the diversity of human poses and complex interactive environment."
        },
        "Joint attention in driver-pedestrian interaction: from theory to practice": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1802.02522",
            "ref_texts": "[545] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis, \\Sparseness meets deepness: 3D human pose estimation from monocular video,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2016, pp. 4966{4975.",
            "ref_ids": [
                "545"
            ],
            "1": "These models achieve state-of-the-art performance in both 2D [544] and 3D [545] pose estimation.",
            "2": "Forest Full FashionPose ESM [543] 2013 HOG SVM Full Parse MBP [542] 2014 HOG,Optical Flow SVM UT VideoPose,MPII,PiW MDL [541] 2014 HOG SVM, Neural Net Full LSP, UIUC,PARSE DeepPose [546] 2014 Conv Neural Net Full FLIC, LSP DS-CNN [547] 2015 Conv Neural Net Full FLIC, LSP Flow-CNN [548] 2015 Conv Neural Net Full FLIC, ChaLearn, PiW, BBC CPM [544] 2016 Conv Neural Net Full MPII, LSP, FLIC SMD [545] 2016 Conv Neural Net Full Human3.",
            "3": "[545] X."
        },
        "RGB-based 3D hand pose estimation via privileged learning with depth images": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1811.07376",
            "ref_texts": "[50] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3D human pose estimation from monocular video. In CVPR , 2016.",
            "ref_ids": [
                "50"
            ],
            "1": "2D key points can be reliably estimated using CNNs and 3D pose is estimated using structured learning or a kinematic model [35, 37, 26, 50].",
            "2": "[50] X."
        },
        "Image-based synthesis for deep 3D human pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1802.04216",
            "ref_texts": "71.X. Zhou, M. Zhu, S. Leonardos, K. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3D human pose estimation from monocular video. In CVPR , 2016.",
            "ref_ids": [
                "71"
            ],
            "1": "Recent approaches employ CNNs for 3D pose estimation in monocular images [8,28,36] or in videos [71].",
            "2": "This allows us to also evaluate on a second protocol (P2) employed in [28,57,71] where only these 5 subjects are used for training.",
            "3": "[71] 87.",
            "4": "[71] 107.",
            "5": "[71] who integrate temporal information.",
            "6": "Since 3D pose evaluation is not possible on this dataset, we instead compare 2D pose errors expressed in pixels and measure this error on the normalized 220\u0002220images following [71]."
        },
        "Self-supervised 3d human pose estimation with multiple-view geometry": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2108.07777",
            "ref_texts": "[50] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 4966\u20134975, 2016.",
            "ref_ids": [
                "50"
            ],
            "1": "7 Weakly supervised Zhou [50] H36M 69.",
            "2": "[50] X."
        },
        "The earth ain't flat: Monocular reconstruction of vehicles on steep and graded roads from a moving camera": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1803.02057",
            "ref_texts": "[11] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis, \u201cSparseness meets deepness: 3d human pose estimation from monocular video,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition , 2016, pp. 4966\u20134975.",
            "ref_ids": [
                "11"
            ],
            "1": "Approaches such as [1], [11] follow a 3D-2D pipeline that involves modeling the 3D shape offline and then solving for the 3D deformations in that shape using localized 2D keypoints in RGB image as evidence, thus overcoming the need to explicitly estimate the 3D keypoints.",
            "2": "[11] X."
        },
        "Poselifter: Absolute 3d human pose lifting network from a single noisy 2d human pose": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1910.12029",
            "ref_texts": "[46] X. Zhou, M. Zhu, S. Leonardos, K. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR) , 2016. 8",
            "ref_ids": [
                "46"
            ],
            "1": "04 Zhou [46] 49.",
            "2": "The proposed method outperforms optimization-based methods [5, 31, 45, 46] and the more recent regression-based methods [21, 26] in terms of rootrelative 3D pose estimation while allowing the acquisition of absolute location information, which the other methods are incapable of doing.",
            "3": "3, 8\n[46] X."
        },
        "End-to-end 6-dof object pose estimation through differentiable rasterization": {
            "authors": [
                "Andrea Palazzi",
                "Luca Bergamini",
                "Simone Calderara",
                "Rita Cucchiara"
            ],
            "url": "http://openaccess.thecvf.com/content_ECCVW_2018/papers/11131/Palazzi_End-to-end_6-DoF_Object_Pose_Estimation_through_Differentiable_Rasterization_ECCVW_2018_paper.pdf",
            "ref_texts": "49. Zhou, X., Zhu, M., Leonardos, S., Derpanis, K.G., Daniilidis , K.: Sparseness meets deepness: 3d human pose estimation from monocular video. In: P roceedings of the IEEE conference on computer vision and pattern recognition. pp. 4966\u20134975 (2016)",
            "ref_ids": [
                "49"
            ],
            "1": "With respect to descriptor-based methods [8,9,25], modern methods relying on CNNs [23,37,40] can solve ambiguities and handle occlude d keypoints thanks to their high representational power and composite fi eld of view, and have shown impressive results in specific tasks such as the one of human pose estimation [27,43,39,49]."
        },
        "3d human pose estimation on a configurable bed from a pressure image": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1804.07873",
            "ref_texts": "[19] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis, \u201cSparseness meets deepness: 3d human pose estimation from monocular video,\u201d in CVPR . IEEE, 2016, pp. 4966\u20134975.",
            "ref_ids": [
                "19"
            ],
            "1": "[19] and Bogo et al.",
            "2": "[19] X."
        },
        "Offsetnet: Deep learning for localization in the lung using rendered images": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1809.05645",
            "ref_texts": "[16] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis, \u201cSparseness Meets Deepness: 3D Human Pose Estimation From Monocular Video,\u201d in The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , jun 2016.",
            "ref_ids": [
                "16"
            ],
            "1": "Using convolutional neural networks (CNN) to estimate the position and orientation of objects has been shown in many contexts, including for human posture and objects in a hand [16], [17].",
            "2": "[16] X."
        },
        "Accurate, low-latency visual perception for autonomous racing: Challenges, mechanisms, and practical solutions": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2007.13971",
            "ref_texts": "[51] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis, \u201cSparseness meets deepness: 3D human pose estimation from monocular video,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition , 2016.",
            "ref_ids": [
                "51"
            ],
            "1": "Other prior works related to computer vision tasks have explored more narrow solutions to monocular depth estimation [17, 20, 39, 49], stereo depth and pose estimation [6, 10, 26, 40], fused monocular/stereo pose estimation [13], 2D object detection [23, 31, 37, 38, 46], obstacle detection [29, 30, 32], and instance segmentation [11, 36, 41, 42] among others [18, 50, 51].",
            "2": "[51] X."
        },
        "Deep learning methods for 3D human pose estimation under different supervision paradigms: a survey": {
            "authors": [
                "Dejun Zhang",
                "Yiqi Wu",
                "Mingyue Guo",
                "Yilin Chen"
            ],
            "url": "https://www.mdpi.com/2079-9292/10/18/2267/pdf"
        },
        "Self-supervised 3D Human Pose Estimation from a Single Image": {
            "authors": [
                "Jose Sosa",
                "David Hogg"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023W/L3D-IVU/papers/Sosa_Self-Supervised_3D_Human_Pose_Estimation_From_a_Single_Image_CVPRW_2023_paper.pdf",
            "ref_texts": "[45] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 4966\u20134975, 2016. 2",
            "ref_ids": [
                "45"
            ],
            "1": "[45] use sequences of images and their corresponding 2D pose to guide their 3D\n4788\n Rotation Inverse rotationProjectionProjectionImage to pose mappingFigure 2."
        },
        "A kinematic chain space for monocular motion capture": {
            "authors": [
                "Bastian Wandt",
                "Hanno Ackermann",
                "Bodo Rosenhahn"
            ],
            "url": "http://openaccess.thecvf.com/content_ECCVW_2018/papers/11132/Wandt_A_Kinematic_Chain_Space_for_Monocular_Motion_Capture_ECCVW_2018_paper.pdf",
            "ref_texts": "36. Zhou, X., Zhu, M., Leonardos, S., Derpanis, K.G., Daniilidis , K.: Sparseness meets deepness: 3d human pose estimation from monocular video. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). (Jun e 2016)",
            "ref_ids": [
                "36"
            ],
            "1": "[36] combined a deep neural network that estimates 2D landmarks with 3D reconstruction of the human pose."
        },
        "Human pose and shape estimation from single polarization images": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2108.06834",
            "ref_texts": "[38] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis, \u201cSparseness meets deepness: 3d human pose estimation from monocular video,\u201d in CVPR , 2016.",
            "ref_ids": [
                "38"
            ],
            "1": "Many early efforts [38]\u2013[40] utilize dictionary-based learning strategies to capture prior knowledge from large motion-capture dataset.",
            "2": "[38] X."
        },
        "Robust Monocular 3D Human Motion With Lasso-Based Differential Kinematics": {
            "authors": [
                "Abed Malti"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023W/DynaVis/papers/Malti_Robust_Monocular_3D_Human_Motion_With_Lasso-Based_Differential_Kinematics_CVPRW_2023_paper.pdf",
            "ref_texts": "[60] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4966\u20134975, June 2016. 1",
            "ref_ids": [
                "60"
            ],
            "1": "3D humans from a single image are usually extended to 3D humans from image sequence by constraining smooth variations over the coefficients of the basis shapes and the camera motion [53, 60].",
            "2": "2\n[60] X."
        },
        "CapsulePose: A variational CapsNet for real-time end-to-end 3D human pose estimation": {
            "authors": [
                "Nicola Garau",
                "Nicola Conci"
            ],
            "url": "https://iris.unitn.it/bitstream/11572/379230/1/_Journal__Neurocomputing__CapsulePose-2.pdf",
            "ref_texts": "[22] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, K. Daniilidis, Sparseness meets deepness: 3d human pose estimation from monocular video, in: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 440",
            "ref_ids": [
                "22"
            ],
            "1": "3D HPE usually relies on additional cues, such as 2D predictions [19, 20, 21], multiple images [22], pre-trained autoencoders [23] and pose dictionaries [24].",
            "2": "19 No Procrustes Procrustes ActivityZhou *\n[22]Tekin *\n[21]Tome, I *\n[19]Ram\u0012 \u0010rez, I\n[36]Tome, II *\n[19]Ram\u0012 \u0010rez, II\n[36]Ram\u0012 \u0010rez, III\n[36]Ours, ISanzari *\n[24]Bogo *\n[29]Ram\u0012 \u0010rez, IV\n[36]Ours, II Directions 87.",
            "3": "[22] X."
        },
        "DensePose 3D: Lifting canonical surface maps of articulated objects to the third dimension": {
            "authors": [
                "Roman Shapovalov",
                "David Novotny",
                "Benjamin Graham",
                "Patrick Labatut",
                "Andrea Vedaldi"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Shapovalov_DensePose_3D_Lifting_Canonical_Surface_Maps_of_Articulated_Objects_to_ICCV_2021_paper.pdf",
            "ref_texts": "[65] Xiaowei Zhou, Menglong Zhu, Kosta Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In IEEE Conf. Comput. Vis. Pattern Recog. , 2016. 3",
            "ref_ids": [
                "65"
            ],
            "1": "The seminal work ofBregler [13], which proposed to express the possible deformations of the 3D shape as a linear combination of a small number of basis shapes, has since inspired many follow-up works [3,18,16,67,4,5,1,19,34,35,67,2,65,66,57]."
        },
        "Deep full-body HPE for activity recognition from RGB frames only": {
            "authors": [
                "Sameh Neili",
                "Najoua Essoukri",
                "Ben Amara"
            ],
            "url": "https://www.mdpi.com/2227-9709/8/1/2/pdf",
            "ref_texts": "28. Zhou, X.; Zhu, M.; Leonardos, S.; Derpanis, K.G.; Daniilidis, K. Sparseness meets deepness: 3D human pose estimation from monocular video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV , USA, 27\u201330 June 2016; pp. 4966\u20134975.",
            "ref_ids": [
                "28"
            ],
            "1": "Another CNN was trained in [28] to infer 3D human poses from uncertainty maps of 2D joint estimates."
        },
        "Human pose estimation in space and time using 3d cnn": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1609.00036",
            "ref_texts": "19. Zhou, X., Zhu, M., Leonardos, S., Derpanis, K., Daniilidis, K.: Sparseness meets deepness: 3D human pose estimation from monocular video. arXiv preprint arXiv:1511.09439 (2015)",
            "ref_ids": [
                "19"
            ],
            "1": "Additionally, many approaches incorporate 2D pose estimations or features to retrieve 3D poses [18, 19].",
            "2": "6M dataset include a discriminative approach to 3D human pose estimation using spatiotemporal features (HOG-KDE) [13], as well as a 2D CNN based 3D pose estimation framework (2DCNN-EM) [19].",
            "3": "6M dataset by [13, 19] fail to report their scores on the official test sets, thereby making it very hard to compare out works.",
            "4": "However, they do report average MPJPE scores of 124\n([19]) and 113 ([13]) on two male subjects (S9 and S11, which are in our training set)."
        },
        "Silhouette body measurement benchmarks": {
            "authors": [],
            "url": "https://trepo.tuni.fi/bitstream/handle/10024/143122/Silhouette_Body_Measurement_Benchmarks.pdf?sequence=1",
            "ref_texts": "[5] X. Zhou, M. Zhu, K. Derpanis, and K. Daniilidis, \u201cSparseness meets deepness: 3d human pose estimation from monocular video,\u201d in CVPR , 2016.",
            "ref_ids": [
                "5"
            ],
            "1": "Recently research on 3D pose estimation has been active [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], but shape recovery has received less attention.",
            "2": ": Human 3D pose recovery \u201cin the wild\u201d has recently gained momentum [5], [6], [7], [8], [9], [10], [11], [12], [13], [14].",
            "3": "B ENCHMARK DATASETS The data used in the existing works can be divided to generated body shapes [5], [6], [7], [8], [9], [10], [11], [12], [13], [14] and fitted body shapes [19], [20], [21], [22].",
            "4": "[5] X."
        },
        "Semantic keypoint-based pose estimation from single RGB frames": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2204.05864",
            "ref_texts": "551. Lee, C.-Y., Xie, S., Gallagher, P., Zhang, Z., and Tu, Z. (2015). Deeply-supervised nets. In AISTATS , volume 2, page 6. Lepetit, V., Moreno-Noguer, F., and Fua, P. (2009). EP nP: An accurate O(n) solution to the P nP problem. IJCV , 81(2):155{166. Li, Y., Wang, G., Ji, X., Xiang, Y., and Fox, D. (2018). Deepim: Deep iterative matching for 6d pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV) , pages 683{698. Li, Z., Wang, G., and Ji, X. (2019). Cdpn: Coordinates-based disentangled pose network for real-time rgb-based 6-dof object pose estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 7678{7687. Liu, J., Zou, Z., Ye, X., Tan, X., Ding, E., Xu, F., and Yu, X. (2020). Leaping from 2d detection to eflcient 6dof object pose estimation. In Computer Vision { ECCV 2020 Workshops , pages 707{714. Long, J., Zhang, N., and Darrell, T. (2014). Do convnets learn correspondence? In NIPS , pages 1601{1609. Lowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. IJCV , 60(2):91{110. Manuelli, L., Gao, W., Florence, P., and Tedrake, R. (2019). Kpam: Keypoint afiordances for category-level robotic manipulation. International Symposium on Robotics Research (ISRR) . Marion, P., Florence, P. R., Manuelli, L., and Tedrake, R. (2018). Label fusion: A pipeline for generating ground truth labels for real rgbd data of cluttered scenes. In ICRA . Massa, F., Aubry, M., and Marlet, R. (2014). Convolutional neural networks for joint object detection and pose estimation: A comparative study. CoRR , abs/1412.7190. Michel, F., Kirillov, A., Brachmann, E., Krull, A., Gumhold, S., Savchynskyy, B., and Rother, C. (2017). Global hypothesis generation for 6D object pose estimation. In CVPR . Montserrat, D. M., Chen, J., Lin, Q., Allebach, J. P., and Delp, E. J. (2019). Multi-view matching network for 6d pose estimation. arXiv preprint arXiv:1911.12330 . Mousavian, A., Anguelov, D., Flynn, J., and Kosecka, J. (2017). 3d bounding box estimation using deep learning and geometry. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 7074{7082. Muja, M., Rusu, R. B., Bradski, G. R., and Lowe, D. G. (2011). REIN A fast, robust, scalable recognition infrastructure. In ICRA , pages 2939{2946. Murthy, J. K., Krishna, G., Chhaya, F., and Krishna, K. M. (2017). Reconstructing vechicles from a single image: Shape priors for road scene understanding. ICRA . Narayanan, P., Yeh, B., Holmes, E., Martucci, S., Schmeckpeper, K., Mertz, C., Osteen, P., and Wigness, M. (2020). An integrated perception pipeline for robot mission execution in unstructured environments. InArtiffcial Intelligence and Machine Learning for Multi-Domain Operations Applications II , volume 11413, page 1141318. International Society for Optics and Photonics. Newell, A., Yang, K., and Deng, J. (2016). Stacked hourglass networks for human pose estimation. In ECCV . Osteen, P. R., Owens, J. L., and Kaukeinen, B. (2019). Reducing the cost of visual DL datasets. In Pham, T., editor, Artiffcial Intelligence and Machine Learning for Multi-Domain Operations Applications , volume 11006, pages 121 { 139. International Society for Optics and Photonics, SPIE. Park, K., Patten, T., and Vincze, M. (2019). Pix2pose: Pixel-wise coordinate regression of objects for 6d pose estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 7668{7677. Pavlakos, G., Zhou, X., Chan, A., Derpanis, K. G., and Daniilidis, K. (2017). 6-DoF object pose from semantic keypoints. ICRA , pages 2011{2018. Peng, S., Liu, Y., Huang, Q., Zhou, X., and Bao, H. (2019). Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , pages 4561{4570. Pepik, B., Stark, M., Gehler, P. V., and Schiele, B. (2012). Teaching 3D geometry to deformable part models. InCVPR , pages 3362{3369. Qin, Z., Fang, K., Zhu, Y., Fei-Fei, L., and Savarese, S. (2019). Keto: Learning keypoint representations for tool manipulation. International Conference on Robotics and Automation (ICRA) . Rad, M. and Lepetit, V. (2017). Bb8: A scalable, accurate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth. In Proceedings of the IEEE International Conference on Computer Vision , pages 3828{3836. Ramakrishna, V., Kanade, T., and Sheikh, Y. (2012). Reconstructing 3D human pose from 2D image landmarks. In ECCV , pages 573{586. Ren, S., He, K., Girshick, R., and Sun, J. (2015). Faster R-CNN: Towards real-time object detection with region proposal networks. In NIPS , pages 91{99. Rios-Cabrera, R. and Tuytelaars, T. (2013). Discriminatively trained templates for 3D object detection: A real time scalable approach. In ICCV , pages 2048{2055. Rusu, R. B., Blodow, N., and Beetz, M. (2009). Fast point feature histograms (FPFH) for 3D registration. InICRA , pages 3212{3217. Salti, S., Tombari, F., and di Stefano, L. (2014). Shot: Unique signatures of histograms for surface and texture description. CVIU , 125:251{264. Sohn, K., Berthelot, D., Li, C.-L., Zhang, Z., Carlini, N., Cubuk, E. D., Kurakin, A., Zhang, H., and Rafiel, C. (2020). Fixmatch: Simplifying semi-supervised learning with consistency and conffdence. Advances in Neural Information Processing Systems (NeurIPS) . Song, C., Song, J., and Huang, Q. (2020). Hybridpose: 6d object pose estimation under hybrid representations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 431{440. Su, H., Qi, C. R., Li, Y., and Guibas, L. J. (2015a). Render for CNN: viewpoint estimation in images using CNNs trained with rendered 3D model views. In ICCV , pages 2686{2694. Su, H., Qi, C. R., Li, Y., and Guibas, L. J. (2015b). Render for cnn: Viewpoint estimation in images using cnns trained with rendered 3d model views. In Proceedings of the IEEE International Conference on Computer Vision , pages 2686{2694. Sundermeyer, M., Marton, Z.-C., Durner, M., and Triebel, R. (2020). Augmented autoencoders: Implicit 3d orientation learning for 6d object detection. International Journal of Computer Vision , 128(3):714{729. Tekin, B., Sinha, S. N., and Fua, P. (2018). Real-time seamless single shot 6d object pose prediction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 292{301. Tieleman, T. and Hinton, G. (2012). Lecture 6.5-rmsprop, coursera: Neural networks for machine learning. University of Toronto, Technical Report . Toshev, A. and Szegedy, C. (2014). DeepPose: Human pose estimation via deep neural networks. In CVPR , pages 1653{1660. Tremblay, J., To, T., Sundaralingam, B., Xiang, Y., Fox, D., and Birchffeld, S. (2018). Deep object pose estimation for semantic robotic grasping of household objects. Conference on Robot Learning (CoRL) . Tulsiani, S. and Malik, J. (2015). Viewpoints and keypoints. In CVPR , pages 1510{1519. Vasilopoulos, V. and Koditschek, D. E. (2018). Reactive navigation in partially known non-convex environments. In International Workshop on the Algorithmic Foundations of Robotics , pages 406{421. Springer. Vasilopoulos, V., Pavlakos, G., Bowman, S. L., Caporale, J. D., Daniilidis, K., Pappas, G. J., and Koditschek, D. E. (2020a). Reactive Semantic Planning in Unexplored Semantic Environments Using Deep Perceptual Feedback. RAL , 5(3):4455{4462. Vasilopoulos, V., Pavlakos, G., Schmeckpeper, K., Daniilidis, K., and Koditschek, D. E. (2020b). Reactive navigation in partially familiar planar environments using semantic perceptual feedback. arXiv preprint arXiv:2002.08946 . Wang, C., Mart\u0013 \u0010n-Mart\u0013 \u0010n, R., Xu, D., Lv, J., Lu, C., Fei-Fei, L., Savarese, S., and Zhu, Y. (2019). 6-pack: Category-level 6D pose tracker with anchor-based keypoints. International Conference on Robotics and Automation (ICRA) . Wang, G., Manhardt, F., Shao, J., Ji, X., Navab, N., and Tombari, F. (2020). Self6d: Self-supervised monocular 6d object pose estimation. In European Conference on Computer Vision , pages 108{125. Springer. Wei, S.-E., Ramakrishna, V., Kanade, T., and Sheikh, Y. (2016). Convolutional pose machines. In CVPR . Whelan, T., Kaess, M., Johannsson, H., Fallon, M., Leonard, J. J., and McDonald, J. (2015). Real-time large-scale dense rgb-d slam with volumetric fusion. The International Journal of Robotics Research , 34(4-5):598{626. Whelan, T., Salas-Moreno, R. F., Glocker, B., Davison, A. J., and Leutenegger, S. (2016). Elasticfusion: Real-time dense slam and light source estimation. The International Journal of Robotics Research , 35(14):1697{1716. Xiang, Y., Mottaghi, R., and Savarese, S. (2014). Beyond PASCAL: A benchmark for 3D object detection in the wild. In WACV , pages 75{82. Xiang, Y., Schmidt, T., Narayanan, V., and Fox, D. (2017). Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes. Robotics: Science and Systems (RSS) . Xie, Q., Dai, Z., Hovy, E., Luong, M.-T., and Le, Q. V. (2019). Unsupervised data augmentation for consistency training. Advances in Neural Information Processing Systems (NeurIPS) . Xie, Z., Singh, A., Uang, J., Narayan, K. S., and Abbeel, P. (2013). Multimodal blending for high-accuracy instance recognition. In IROS , pages 2214{2221. Zeng, A., Yu, K.-T., Song, S., Suo, D., Walker, E., Rodriguez, A., and Xiao, J. (2017). Multi-view selfsupervised deep learning for 6d pose estimation in the amazon picking challenge. In 2017 IEEE international conference on robotics and automation (ICRA) , pages 1386{1383. IEEE. Zhou, X., Karpur, A., Luo, L., and Huang, Q. (2018). Starmap for category-agnostic keypoint and viewpoint estimation. In ECCV , pages 318{334. Zhou, X., Leonardos, S., Hu, X., and Daniilidis, K. (2015a). 3D shape estimation from 2D landmarks: A convex relaxation approach. In CVPR , pages 4447{4455. Zhou, X., Zhu, M., Leonardos, S., Derpanis, K., and Daniilidis, K. (2015b). Sparseness meets deepness: 3D human pose estimation from monocular video. In CVPR . Zhu, M., Derpanis, K. G., Yang, Y., Brahmbhatt, S., Zhang, M., Phillips, C., Lecce, M., and Daniilidis, K.",
            "ref_ids": [
                "551"
            ]
        },
        "Canonical 3D Deformer Maps: Unifying parametric and non-parametric methods for dense weakly-supervised category reconstruction": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper/2020/file/efe34c4e2190e97d1adc625902822b13-Paper.pdf",
            "ref_texts": "[73] Xiaowei Zhou, Menglong Zhu, Kosta Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proc. CVPR , 2016.",
            "ref_ids": [
                "73"
            ],
            "1": "They acheive it by constraining the space of deformations in one of the following ways: assume that shapes span a low-rank subspace [3,17,16,76] or that 3D trajectories are smooth in time [4,5], or combine both types of constraints [1,19,38,37], or use multiple subspaces [76,2], 2 sparsity [73,74] or Gaussian priors [59]."
        },
        "Gsir: Generalizable 3d shape interpretation and reconstruction": {
            "authors": [],
            "url": "https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123580494.pdf",
            "ref_texts": "65.Zhou, X., Zhu, M., Leonardos, S., Derpanis, K.G., Daniilidis, K.: Sparseness meets deepness: 3d human pose estimation from monocular video. In: CVPR (2016)",
            "ref_ids": [
                "65"
            ],
            "1": "Among all ways to abstract object structures, a 3d skeleton is most common in use because of its simplicity, especially in human pose estimation [1,6,65,42]."
        },
        "3D Human Pose Lifting with Grid Convolution": {
            "authors": [
                "Yangyuxuan Kang",
                "Yuyang Liu",
                "Anbang Yao",
                "Shandong Wang",
                "Enhua Wu"
            ],
            "url": "https://arxiv.org/pdf/2302.08760",
            "ref_texts": "2021. Learning skeletal graph neural networks for hard 3d pose estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision . Zhao, L.; Peng, X.; Tian, Y .; Kapadia, M.; and Metaxas, D. N. 2019. Semantic graph convolutional networks for 3d human pose regression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . Zhou, K.; Han, X.; Jiang, N.; Jia, K.; and Lu, J. 2021. HEMlets PoSh: Learning Part-Centric Heatmap Triplets for 3D Human Pose and Shape Estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence . Zhou, X.; Huang, Q.; Sun, X.; Xue, X.; and Wei, Y . 2017. Towards 3d human pose estimation in the wild: a weaklysupervised approach. In Proceedings of the IEEE/CVF International Conference on Computer Vision . Zhou, X.; Zhu, M.; Leonardos, S.; Derpanis, K. G.; and Daniilidis, K. 2016. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . Zou, Z.; Liu, K.; 0003, L. W.; and Tang, W. 2020. Highorder Graph Convolutional Networks for 3D Human Pose Estimation. In Proceedings of the British Machine Vision Conference . A More Details about Implementation A.1 Inverse SGT Process In the main paper, we did not describe inverse SGT process in detail due to limited space. Here we present its formulation as follows: ~ST i=ST iPHP j=1ST ij(11)",
            "ref_ids": [
                "2021"
            ]
        },
        "MocapNET: Ensemble of SNN Encoders for 3D Human Pose Estimation in RGB Images.": {
            "authors": [],
            "url": "https://bmvc2019.org/wp-content/uploads/papers/0710-paper.pdf",
            "ref_texts": "[82] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 4966\u20134975, 2016.",
            "ref_ids": [
                "82"
            ],
            "1": "Some methods use a purely convolutional approach to extract 3D pose like [72] that models 3D volume loss, while some others rely on separate algorithms [82] to perform optimization.",
            "2": "6M (H36M) [25] dataset which it is the de facto standard [6, 12, 18, 26, 29, 39, 40, 47, 54, 57, 58, 63, 64, QAMMAZ, ARGYROS: MOCAPNET, AN ENSEMBLE OF SNN ENCODERS 9 Comparison of methods tested on H36M Protocol 1 (Method / MPJPE)\n[25] Our* [18] [29] [6] [82] [79] [83] [64] [57] [66] [26] [54] [12] [40] [47] [61]\n162 136 119 118 116 113 108 107 101 93 88 88 88 82 80 72 40 Table 3: Comparison of the proposed method to others (errors in mm)."
        },
        "Towards single 2D image-level self-supervision for 3D human pose and shape estimation": {
            "authors": [
                "Junuk Cha",
                "Muhammad Saqlain",
                "Changhwa Lee",
                "Seongyeong Lee",
                "Seungeun Lee",
                "Donguk Kim",
                "Hee Park",
                "Seungryul Baek"
            ],
            "url": "https://www.mdpi.com/2076-3417/11/20/9724/pdf",
            "ref_texts": "3. Zhou, X.; Zhu, M.; Leonardos, S.; Derpanis, K.G.; Daniilidis, K. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV , USA, 27\u201330 June 2016.",
            "ref_ids": [
                "3"
            ],
            "1": "[3] proposed a method using both a 3D geometric prior and temporal smoothness prior to treat considerable uncertainties in 2D joint locations.",
            "2": "2021 ,11, 9724 4 of 19 from the 2D joints or features by performing regression [4,25\u201327] or model fitting [3,28\u201332]."
        },
        "Event-based human pose tracking by spiking spatiotemporal transformer": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2303.09681",
            "ref_texts": "[6] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis, \u201cSparseness meets deepness: 3d human pose estimation from monocular video,\u201d in CVPR , 2016, pp. 4966\u20134975.",
            "ref_ids": [
                "6"
            ],
            "1": "Prior to the deep learning era, research efforts are mainly based on random forest [1], [4] or dictionary learning [5], [6].",
            "2": "[6] X."
        },
        "Compact model representation for 3D reconstruction": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1707.07360",
            "ref_texts": "[29] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3D human pose estimation from monocular video. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4966\u20134975, 2016. 1",
            "ref_ids": [
                "29"
            ],
            "1": "Vision researchers have recently started to consider using 3D prior knowledge to make the problem of 3D reconstruction from a single image less ambiguous [24, 28, 29, 25, 15, 2, 9, 13].",
            "2": "1, 2\n[29] X."
        },
        "Overview of 3d human pose estimation": {
            "authors": [],
            "url": "https://cdn.techscience.cn/ueditor/files/cmes/134-3/TSP_CMES_20857/TSP_CMES_20857.pdf",
            "ref_texts": "105. Zhou, X., Zhu, M., Leonardos, S., Derpanis, K. G., Daniilidis, K. (2016). Sparseness meets deepness: 3D human pose estimation from monocular video. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp. 4966\u20134975. Las Vegas, NV, USA.",
            "ref_ids": [
                "105"
            ],
            "1": "CVPR\u2019\n16 [105]87."
        },
        "Estimating 3D motion and forces of human\u2013Object interactions from internet videos": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2111.01591",
            "ref_texts": "2157 2 Zhou X, Zhu M, Leonardos S, Derpanis KG, Daniilidis K (2016) Sparseness meets deepness: 3d human pose estimation from monocular video. In: CVPR 2"
        },
        "Unsupervised cross-dataset adaptation via probabilistic amodal 3D human pose completion": {
            "authors": [
                "Jogendra Nath",
                "Rahul M",
                "Jay Patravali",
                "Venkatesh Babu"
            ],
            "url": "https://openaccess.thecvf.com/content_WACV_2020/papers/Kundu_Unsupervised_Cross-Dataset_Adaptation_via_Probabilistic_Amodal_3D_Human_Pose_Completion_WACV_2020_paper.pdf",
            "ref_texts": "[56] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In IEEE Conference on Computer Vision and Pattern Recognition , pages 4966\u2013",
            "ref_ids": [
                "56"
            ],
            "1": "a) direct estimation of 3D joint locations [25,46] and b) 2D heat-map projection followed by 3D pose estimation [28, 31,26,56].",
            "2": "Several approaches have successfully exploited structural [1,29,31] and temporal regularity [29,56] as as an additional cue to further improve pose estimation performance [26].",
            "3": "Some approaches [29,56] utilize temporal smoothness in estimated 3D pose for consecutive video frames as an additional information to regularize sequential 3D pose estimation.",
            "4": "[56] X."
        },
        "Structure from articulated motion: accurate and stable monocular 3D reconstruction without training data": {
            "authors": [],
            "url": "https://www.mdpi.com/1424-8220/19/20/4603/pdf",
            "ref_texts": "3. Zhou, X.; Zhu, M.; Derpanis, K.; Daniilidis, K. Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV , USA, 27\u201330 June 2016. Sensors 2019 ,19, 4603 16 of 19",
            "ref_ids": [
                "3"
            ],
            "1": "[3] * 106."
        },
        "Skeleton transformer networks: 3d human pose and skinned mesh from single rgb image": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1812.11328",
            "ref_texts": "30. X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In CVPR , pages 4966{4975, 2016. Skeleton Transformer Networks 17 Fig. 6. More results.",
            "ref_ids": [
                "30"
            ],
            "1": "[30] Bogo et al."
        },
        "Consensus-based optimization for 3D human pose estimation in camera coordinates": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1911.09245",
            "ref_texts": "[61] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness Meets Deepness: 3D Human Pose Estimation From Monocular Video. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 2016. 1, 3",
            "ref_ids": [
                "61"
            ],
            "1": "Despite the recent works on 3D human pose estimation, most of the methods in the literature are limited to the problem of relative pose prediction [8, 49, 61, 4, 2],where the root body joint is centered at the origin and the remaining joints are estimated relative to the center.",
            "2": "A\n2 simple approach is to infer the distance to the camera considering a normalized or constant body size [61, 30], which is an information that may not be available and difficult to be estimated [12].",
            "3": "2\n[61] X."
        },
        "Decanus to Legatus: Synthetic training for 2D-3D human pose lifting": {
            "authors": [
                "Yue Zhu",
                "David Picard"
            ],
            "url": "https://openaccess.thecvf.com/content/ACCV2022/papers/Zhu_Decanus_to_Legatus_Synthetic_training_for_2D-3D_human_pose_lifting_ACCV_2022_paper.pdf",
            "ref_texts": "52. Zhou, X., Zhu, M., Leonardos, S., Derpanis, K.G., Daniilidis, K.: Sparseness meets deepness: 3d human pose estimation from monocular video. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016) 3",
            "ref_ids": [
                "52"
            ],
            "1": "Discriminative models can also be categorized into one stage models which predict directly 3D poses from images [14, 25, 29, 34] and two stage methods which first learn a 2D pose estimator, then lift the obtained 2D poses to 3D [18, 28, 45, 48, 49, 52]."
        },
        "Local feature based online mode detection with recurrent neural networks": {
            "authors": [],
            "url": "http://www.iapr-tc11.org/archive/icfhr2012/paper086.pdf"
        },
        "Invariant teacher and equivariant student for unsupervised 3d human pose estimation": {
            "authors": [
                "Chenxin Xu",
                "Siheng Chen",
                "Maosen Li",
                "Ya Zhang"
            ],
            "url": "https://ojs.aaai.org/index.php/AAAI/article/view/16409/16216",
            "ref_texts": "2017a. Rocket launching: A universal and efficient framework for training well-performing light net. arXiv preprint arXiv:1708.04106 . Zhou, X.; Huang, Q.; Sun, X.; Xue, X.; and Wei, Y . 2017b. Towards 3d human pose estimation in the wild: a weaklysupervised approach. In ICCV. Zhou, X.; Zhu, M.; Leonardos, S.; Derpanis, K. G.; and Daniilidis, K. 2016. Sparseness meets deepness: 3d human pose estimation from monocular video. In CVPR. Zhu, Y .; Huang, D.; De La Torre, F.; and Lucey, S. 2014. Complex non-rigid motion 3d reconstruction by union of subspaces. In CVPR."
        },
        "Multi-scale networks for 3d human pose estimation with inference stage optimization": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2010.06844",
            "ref_texts": "[19] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis, \u201cSparseness meets deepness: 3d human pose estimation from monocular video,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition , 2016, pp. 4966\u20134975.",
            "ref_ids": [
                "19"
            ],
            "1": "2 R ELATED WORKS Within the last few years, human pose estimation has been undergoing rapid development with deep learning techniques [6], [7], [8], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35].",
            "2": "[19] X."
        },
        "Monocular 3D human pose estimation by predicting depth on joints": {
            "authors": [
                "Bruce Xiaohan",
                "Ping Wei",
                "Chun Zhu"
            ],
            "url": "http://openaccess.thecvf.com/content_ICCV_2017/papers/Nie_Monocular_3D_Human_ICCV_2017_paper.pdf",
            "ref_texts": "[41] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In CVPR , 2016.",
            "ref_ids": [
                "41"
            ],
            "1": "With the success of deep networks on a wide range of computer vision tasks and especially 2D human pose estimation, the 3D pose estimation from monocular image using deep networks [14, 15, 23, 39, 41] have received lots of attentions recently.",
            "2": "Some methods [39, 41] use two different data sources for training 2D pose estimator and 3D pose predictor.",
            "3": "[41] predict 3D poses from a video sequence by using temporal information.",
            "4": "3 Zhou[41] 87.",
            "5": "We also report results for protocol 2 (P2) which is employed in [41, 30, 23].",
            "6": "Our method clearly outperforms the second best result [41] by 13.",
            "7": "[41] X."
        },
        "Deep nrsfm++: Towards unsupervised 2d-3d lifting in the wild": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2001.10090",
            "ref_texts": "[73] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4966\u20134975, 2016. 2",
            "ref_ids": [
                "73"
            ],
            "1": "These priors include the assumption of shape/trajectory matrices being (i) low-rank [15, 8, 4, 19, 38], (ii) being compressible [35, 33, 73], or (iii) lying in a union of subspaces [39, 74, 3].",
            "2": "2, 3\n[73] X."
        },
        "Multi-person 3D pose estimation from 3D cloud data using 3D convolutional neural networks": {
            "authors": [],
            "url": "https://spiral.imperial.ac.uk/bitstream/10044/1/70442/2/Vasileiadis_CVIU2019.pdf",
            "ref_texts": "14 Zhou, X., Zhu, M., Leonardos, S., Derpanis, K.G., Daniilidis, K., 2016b. Sparseness meets deepness: 3d human pose estimation from monocular video, in: IEEE Conference on Computer Vision and Pattern Recognition, pp. 4966\u20134975."
        },
        "3D hand pose estimation from RGB using privileged learning with depth data": {
            "authors": [
                "Shanxin Yuan",
                "Bjorn Stenger",
                "Kyun Kim"
            ],
            "url": "http://openaccess.thecvf.com/content_ICCVW_2019/papers/HANDS/Yuan_3D_Hand_Pose_Estimation_from_RGB_Using_Privileged_Learning_with_ICCVW_2019_paper.pdf"
        },
        "Deep 3D human pose estimation under partial body presence": {
            "authors": [],
            "url": "https://spectrum.library.concordia.ca/id/eprint/984661/1/cuthesis.pdf",
            "ref_texts": "[11] X. Zhou, M. Zhu, S. Leonardos, K.G. Derpanis, and K. Daniilidis, \\Sparseness meets deepness: 3D human pose estimation from monocular video,\" in Proc. IEEE Conf. Computer Vision Pattern Recognition, 2016, pp. 4966{4975.",
            "ref_ids": [
                "11"
            ],
            "1": "Assuming Jthe number of main body joints, the 2D human pose matrix W2R2\u0002Jis a 2-dimensional matrix in which each column represents the 2D Cartesian coordinates (x and y) for one of the Jmain body joints [6, 10, 11].",
            "2": "1\n(see [11]).",
            "3": "3D pose estimation is an active research ffeld [1, 2, 6, 11, 13] but remains challenging due to the ambiguity caused by the loss of depth information in the usual intensity images but also due to image related issues such as blur, noise, occlusion, etc.",
            "4": "They use various techniques such as sparse representation[6, 11], factorization [13], and neural networks[53] to estimate the 3D poses from 2D poses.",
            "5": "in [6, 11] propose to use a convex approach while using sparse representation for the 3D human pose estimation from 2D landmarks.",
            "6": "The method in [11] presents a 2D joints' uncertainty map predictor to handle the cases when 2D joints' information is not available.",
            "7": "[15, 55]) as opposed to the ones performing on a sequence of images (video) [11, 14].",
            "8": "[11] use an expectationMaximization algorithm on the whole image sequence.",
            "9": "Numerous papers have utilized deep learning concepts to address the problem of 3D human pose estimation[1, 2, 11, 15, 16, 55, 58].",
            "10": "Among these methods, some perform an initial 2D pose estimation stage and then use that information to estimate the 3D pose[11].",
            "11": "[11] X."
        },
        "Occluded joints recovery in 3d human pose estimation based on distance matrix": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1807.11147",
            "ref_texts": "[22] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis and K. Daniilidis. Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video, in IEEE Conference on Computer Vision and Pattern Recognition , 2016, pp. 4966-4975.",
            "ref_ids": [
                "22"
            ],
            "1": "Inspired from a model of sparse representation for 3D shape estimation [21], [22], every 91 dimensional vector Tcould be represented as a linear combination of each basis Bof a pre-learned over-complete dictionary D: T=kX i=1ciBi; (5) where Tis the transformed version of a EDM, Biis one of the basis 91\u00021vector in Dandciis the corresponding weight.",
            "2": "This sparse representation owns the ability to model large variates of human pose, which is EDMs in this case [1], [22], [21].",
            "3": "[22] X."
        },
        "A review of 3D human pose estimation from 2D images": {
            "authors": [],
            "url": "http://www.3dbodyscanning.org/cap/papers/2020/2029bartol.pdf",
            "ref_texts": "[79] Xiaowei Zhou et al. \u201cSparseness Meets Deepness: 3DHuman Pose Estimation from Monocular Video\u201d. In:2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)(2016), pp. 4966\u20134975. ",
            "ref_ids": [
                "79"
            ],
            "1": "Some deep learning models are able to learn the priors without the explicit clues [30, 79].",
            "2": "Sparseness-meetsdeepness [79] is the first method that uses a variation of standard sparse pose representation (Fig."
        },
        "Single-shot 3D multi-person pose estimation in complex images": {
            "authors": [
                "Abdallah Benzine",
                "Bertrand Luvison",
                "Quoc Cuong",
                "Catherine Achard"
            ],
            "url": "https://arxiv.org/pdf/1911.03391",
            "ref_texts": "[43] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, K. Daniilidis, Sparseness meets deepness: 3d human pose estimation from monocular video, CVPR (2016).",
            "ref_ids": [
                "43"
            ],
            "1": "Recent methods make this prediction directly from monocular images [35, 36, 37, 38, 39, 40, 41] or from sequences of images [42, 43] using Convolutional Neural Networks.",
            "2": "[43] X."
        },
        "Autonomous driving in the lung using deep learning for localization": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1907.08136",
            "ref_texts": "[17] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis, \u201cSparseness Meets Deepness: 3D Human Pose Estimation From Monocular Video,\u201d in The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , jun 2016.",
            "ref_ids": [
                "17"
            ],
            "1": "Using convolutional neural networks (CNN) to estimate the position and orientation of objects has been shown in many contexts, including for human posture and objects in a robotic hand [17], [18].",
            "2": "[17] X."
        },
        "On the exact recovery conditions of 3D human motion from 2D landmark motion with sparse articulated motion": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1907.03967",
            "ref_texts": "[64] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness Meets Deepness: 3d Human Pose Estimation from Monocular Video. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4966\u20134975, June 2016.",
            "ref_ids": [
                "64"
            ],
            "1": "These approaches are extended to 3D reconstruction from image sequence by imposing smoothness over the coefficients of the basis shapes and the camera poses [64, 56].",
            "2": "[64] extend the former approach to account for temporal smoothness by imposing regularization terms on the coefficients and on the camera to human pose orientation in an image sequence.",
            "3": "[64] is an image sequence based approach using also basis shapes.",
            "4": "[64] 86.",
            "5": "[64] 56.",
            "6": "[64] 56.",
            "7": "[64] X."
        },
        "From kinematics to dynamics: Estimating center of pressure and base of support from video frames of human motion": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2001.00657",
            "ref_texts": "[54] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4966\u20134975, 2016.",
            "ref_ids": [
                "54"
            ],
            "1": "Success in 2D human pose estimation also has encouraged researchers to detect 3D skeletons by extending existing 2D human pose detectors [6, 10, 28, 30, 33, 43, 53] or by directly using image features [1, 36, 40, 44, 54].",
            "2": "[54] X."
        },
        "2d\u20133d pose consistency-based conditional random fields for 3d human pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1704.03986",
            "ref_texts": "[28] X. Zhou, M. Zhu, S. Leonardos, K. Derpanis, K. Daniilidis, Sparseness meets deepness: 3d human pose estimation from monocular video, in: Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2016.",
            "ref_ids": [
                "28"
            ],
            "1": "The optimization-based approaches [23, 24, 25, 26, 27, 28, 29] attempt to minimize an energy function including the prior terms that are usually based on 3D pose statistics.",
            "2": "In the optimization-based approaches [23, 24, 25, 26, 27, 28, 29], an energy function is built using the prior term and the (intermediate) results of the 2D pose estimation methods, and the 3D human pose is obtained by minimizing the energy function.",
            "3": "In [28], from an input monocular RGB image sequence, the 3D human poses for all frames are recovered based on an expectation-maximization (EM) algorithm that combines image-based 2D body joint detection, 3D geometric pose priors using sparse representation, and temporal model.",
            "4": "6M [18] and HumanEva [39] provide the bounding box information using background subtraction, and many recent 3D human pose estimation approaches [21, 26, 28] adopt that assumption.",
            "5": "We do not use spatial fusion and temporal pooling as in [28].",
            "6": "To calculate the accuracy of the estimated 3D pose, we use the mean per-joint position error (MPJPE) applied in many studies [18, 20, 21, 26, 28], which is as follows: JMPJPE =1 MMX i=1k(Xi\u0000Xr)\u0000(X\u0003 i\u0000X\u0003 r)k2;(13) whererdenotes the index of the root joint.",
            "7": "[28] 49.",
            "8": "Implementation Details According to existing studies [18, 21, 26, 28, 29], the number of joints Mis set to 17 and 14 for the Human3.",
            "9": "Then, according to the protocol of [28], the frames within 30 seconds belonging to the sequences of S9 and S11 from the ffrst camera are used for the evaluation.",
            "10": "The recently proposed convex relaxation-based method [36] achieves impressive 3D human pose estimation performance, and the method is combined with the video input in [28] for improved results.",
            "11": "To perform this task, according to the protocol of [28], we use the sequences of S9 and S11 from all cameras belonging to the Human3.",
            "12": "[28] 87.",
            "13": "[28] 124.",
            "14": "The method in [28] shows good performance (113.",
            "15": "using the unary term Usthat is trained in the same way as in [28] for a fair comparison.",
            "16": "Note that our approach is based on a single image unlike the methods in [21] and [28] that utilize information from multiple frames.",
            "17": "[28] X."
        },
        "High Fidelity 3D Reconstructions with Limited Physical Views": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2110.11599",
            "ref_texts": "[52] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 4966\u20134975, 2016. 3",
            "ref_ids": [
                "52"
            ],
            "1": "Notable priors include the assumption of Sbeing(i)low rank [8, 6, 2, 9, 31], (ii)lying in a union-ofsubspaces [32, 53, 1] (iii)or compressible [26, 52, 28]."
        },
        "HybrIK-X: Hybrid Analytical-Neural Inverse Kinematics for Whole-body Mesh Recovery": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2304.05690",
            "ref_texts": "[35] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis, \u201cSparseness meets deepness: 3d human pose estimation from monocular video,\u201d in CVPR , 2016. 2",
            "ref_ids": [
                "35"
            ],
            "1": "Two-stage approaches first estimate 2D pose and then lift them to 3D joint locations by a learned dictionary of 3D skeleton [30], [31], [32], [33], [34], [35] or regression [36], [37], [38], [39], [40], [41].",
            "2": "2\n[35] X."
        },
        "Motion-DVAE: Unsupervised learning for fast human motion denoising": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2306.05846",
            "ref_texts": "[35] Xiaowei Zhou, Menglong Zhu, Kosta Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Computer Vision and Pattern Recognition (CVPR) , 2016.",
            "ref_ids": [
                "35"
            ],
            "1": "While earlier methods only focused on 3D joint locations [33,34,35,36,37], more and more works are model-based."
        },
        "Shape and Pose Estimation for Closely Interacting Persons Using Multi\u2010view Images": {
            "authors": [
                "Kun Li"
            ],
            "url": "https://www.yangangwang.com/papers/LI-SPE-2018-08.pdf"
        },
        "PONet: Robust 3D Human Pose Estimation via Learning Orientations Only": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2112.11153",
            "ref_texts": "[53] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G JOURNAL OF L ATEXe CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 11 Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In CVPR , 2016.",
            "ref_ids": [
                "53"
            ],
            "1": "State-of-the-art 3D human pose estimation methods [6], [14], [19], [20], [24], [27], [53] rely on first detecting several 2D keypoints, like the body joints, from the image, followed by mapping the 2D keypoint locations back to the 3D world."
        },
        "Segmentation and reconstruction of 3D models from a point cloud with deep neural networks": {
            "authors": [],
            "url": "http://eprints.fri.uni-lj.si/4299/1/Published_version_Korea18.pdf",
            "ref_texts": "[38] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3D human pose estimation from monocular video. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4966\u2013",
            "ref_ids": [
                "38"
            ],
            "1": "3D human pose estimation using CNNs can be done from monocular video [38]."
        },
        "Lifting 2d human pose to 3d: A weakly supervised approach": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1905.01047",
            "ref_texts": "[12] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis, \u201cSparseness meets deepness: 3d human pose estimation from monocular video,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition , 2016, pp. 4966\u2013",
            "ref_ids": [
                "12"
            ],
            "1": "Most of these works focus on end-to-end 3d pose estimation from single images [3], [5]\u2013[9], [12], [13], [20], while some utilize temporal sequences for estimating 3d pose from video [10], [11]."
        },
        "RobCap: A mobile motion capture system mounted on a robotic arm": {
            "authors": [],
            "url": "https://hal.science/hal-03539339/document",
            "ref_texts": "[15] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis, \u201cSparseness meets deepness: 3d human pose estimation from monocular video,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 4966\u20134975, 2016.",
            "ref_ids": [
                "15"
            ],
            "1": "[15] X."
        },
        "Explicit spatiotemporal joint relation learning for tracking human pose": {
            "authors": [
                "Xiao Sun",
                "Chuankang Li",
                "Stephen Lin"
            ],
            "url": "http://openaccess.thecvf.com/content_ICCVW_2019/papers/HANDS/Sun_Explicit_Spatiotemporal_Joint_Relation_Learning_for_Tracking_Human_Pose_ICCVW_2019_paper.pdf",
            "ref_texts": "[75] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ,pages 4966\u20134975, 2016. 1,2,6,8",
            "ref_ids": [
                "75"
            ],
            "1": "Among them are techniques that use dense optical flow to propagate joint estimates from previous frames [31,48,52], local model fitting with the predicted pose of the preceding frame as initialization [11,2,38,49,45], and temporal smoothness priors as a pose estimation constraint [75,41].",
            "2": "Some works follow the tracking by detection paradigm, which performs detection in each frame, estimates the pose of each detection result with an off-the-shelf technique, then enforces spatio-temporal smoothness for tracking [75,41].",
            "3": "For this benchmark, we employ the most widely used evaluation protocol in the literature [6,59,40,76,30, 37,47,70,50,2,75,58,73,56,51].",
            "4": "Method Zhou [75]Tekin [58]Xingyi [73]Sun [54]Pavlakos [47]Sun [56]Lin [33]\u2217Coskun [10]\u2217Ours\u2217 MPJPE 113.",
            "5": "2\n[75] X."
        },
        "Unsupervised 3D Animal Canonical Pose Estimation with Geometric Self-Supervision": {
            "authors": [],
            "url": "http://www.scubrl.org/files/picture/20221122/172/1669107994396.pdf",
            "ref_texts": "[32] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4966\u20134975, 2016.",
            "ref_ids": [
                "32"
            ],
            "1": "2) Weakly Supervised: Weakly supervised approaches use unpaired 2D-3D data to learn 3D pose priors [2], [15], [30], [32], [33].",
            "2": "Methods in [4], [32], [33] use a 3D pose database/dictionary to represent prior knowledge.",
            "3": "[32] X."
        },
        "Evaluating current state of monocular 3D pose models for golf": {
            "authors": [],
            "url": "https://septentrio.uit.no/index.php/nldl/article/download/6793/7024",
            "ref_texts": "[45] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4966\u20134975, 2016. doi: 10.",
            "ref_ids": [
                "45"
            ],
            "1": "One category predicts the 3D joint locations directly or regresses it from a 2D pose [45, 27, 31, 2, 34], while methods in the other category fit a parametric body model to the image [5].",
            "2": "The PA-MPJPE and MPJPE are evaluated following the common protocols [45, 27, 31, 2, 34, 18, 22, 10].",
            "3": "[45] X."
        },
        "Permutation-invariant relational network for multi-person 3d pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2204.04913",
            "ref_texts": "[43] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 4966\u20134975, 2016.",
            "ref_ids": [
                "43"
            ],
            "1": "[43] X."
        },
        "Capsules as viewpoint learners for human pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2302.06194",
            "ref_texts": "[22] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4966\u20134975, 2016.",
            "ref_ids": [
                "22"
            ],
            "1": "3D HPE usually leverages on additional cues, such as 2D predictions [6,20,21], multiple images [22], pre-trained models [23] and pose dictionaries [24].",
            "2": "We achieve the lowest average MPJPE on both the 11 Nicola Garau, Nicola Conci No Procrustes Procrustes ActivityZhou *\n[22]Tekin *\n[21]Tome, I *\n[6]Ram\u00ecrez, I\n[32]Tome, II *\n[6]Ram\u00ecrez, II\n[32]Ram\u00ecrez, III\n[32]DECA-H4, ISanzari *\n[24]Bogo *\n[40]Ram\u00ecrez, IV\n[32]DECA-H4, II Directions 87.",
            "3": "[22] X."
        },
        "Deepskeleton: Skeleton map for 3d human pose regression": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1711.10796",
            "ref_texts": "[60] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4966\u20134975, 2016. 2, 5, 7",
            "ref_ids": [
                "60"
            ],
            "1": "[60] combine heatmap and 3D geometric prior in EM algorithm to reconstruct 3D skeleton from 2D joints.",
            "2": "Following standard practice in [57, 22, 58, 60], five subjects(S1, S5, S6, S7, S8) are used in training.",
            "3": "1 Zhou[60] 87.",
            "4": "2 Zhou[60] 199.",
            "5": "1, 6\n[60] X."
        },
        "GLA-GCN: Global-local Adaptive Graph Convolutional Network for 3D Human": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2307.05853",
            "ref_texts": "[84] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 4966\u20134975, 2016.",
            "ref_ids": [
                "84"
            ],
            "1": ", pelvis) and the ground truth 3D pose joints collected via motion capture, which follows [84, 60, 50]."
        },
        "Improving office workers' workspace using a self-adjusting computer screen": {
            "authors": [],
            "url": "https://scholar.archive.org/work/o2tqghqry5cmlijibpswvwh74a/access/wayback/https://dl.acm.org/doi/pdf/10.1145/3545993"
        },
        "Fbi-pose: Towards bridging the gap between 2d images and 3d human poses using forward-or-backward information": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1806.09241",
            "ref_texts": "[34] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4966\u20134975, 2016.",
            "ref_ids": [
                "34"
            ],
            "1": "Inferring 3D Pose by 2D Joints Estimation To avoid collecting 2D-3D paired data in the wild, a large portion of recent works (such as [16, 13, 9, 34, 4, 2, 15, 25, 18]) decomposed the task of 3D pose inference into two independent stages: generating 2D poses firstly and then lifting them into 3D space.",
            "2": "[34] X."
        },
        "Cross-view person identification by matching human poses estimated with confidence on each body joint": {
            "authors": [
                "Guoqiang Liang",
                "Xuguang Lan",
                "Kang Zheng",
                "Song Wang",
                "Nanning Zheng"
            ],
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/12236/12095",
            "ref_texts": "4306-4312. Zhang, P.; Wang, J.; Farhadi, A.; Hebert, M.; and Parikh, D. 2014. Predicting failures of vision systems. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 35663573. Zheng, K.; Guo, H.; Fan, X.; Yu, H.; and Wang, S. 2016. Identifying Same Persons from Temporally Synchronized Videos Taken by Multiple Wearable Cameras. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops , 105-113. Zheng, K.; Fan, X.; Lin, Y.; Guo, H.; Yu, H.; Guo, D.; and Wang, S. 2017. Learning View-Invariant Features for Person Identification in Temporally Synchronized Videos Taken by Wearable Cameras. In Proceedings of the IEEE International Conference on Computer Vision , 2858-2866. Zhou, X.; Zhu, M.; Leonardos, S.; Derpanis, K. G.; and Daniilidis, K. 2016. Sparseness meets deepness: 3D human pose estimation from monocular video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 4966-4975. Zhou, X.; Huang, Q.; Sun, X.; Xue, X., and Wei, Y. 2017. Towards 3D Human Pose Estimation in the Wild: a Weaklysupervised Approach. In arXiv preprint arxiv :1704.02447. "
        },
        "System for estimating a three dimensional pose of one or more persons in a scene": {
            "authors": [],
            "url": "https://patentimages.storage.googleapis.com/87/60/fe/2e9c24e672d8ef/US11521373.pdf",
            "ref_texts": " J. Wu , T. Xue , J. J. Lim , Y. Tian , J. B. Tenenbaum , A. Torralba , and W. T. Freeman . Single image 3d interpreter network . In European Conference on Computer Vision (ECCV ) . (Apr. 2016 ) pp . 1-17 . B. Xiaohan Nie , P. Wei , and S.-C. Zhu . Monocular 3d human pose estimation by predicting depth on joints . In International Confer ence on Computer Vision . (Oct. 2017 ) pp . 3467-3475 . X. Zhou , M. Zhu , K. Derpanis , and K. Daniilidis . Sparseness meets deepness : 3D human pose estimation from monocular video . In IEEE Conference on Computer Vision and Pattern Recognition . (Apr. 2016 ) pp . 1-10 . Pavlakos , Georgios , Xiaowei Zhou , and Kostas Daniilidis . \u201c Ordinal depth supervision for 3d human pose estimation . \u201d Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2018. (Year : 2018 ) . First office action on the merits (Non Final Rejection ) in U.S. Appl . No. 16 / 826,200 , dated Apr. 27 , 2020 . Notice of Allowance in U.S. Appl . No. 16 / 826,200 , dated Aug. 3 , 2020 . First office action on the merits (Non Final Rejection ) in U.S. Appl . No. 17 / 107,845 , dated Jan. 22 , 2021 . Second office action on the merits (Final Rejection ) in U.S. Appl . No. 17 / 107,845 , dated Apr. 1 , 2021 . Notice of Allowance in U.S. Appl . No. 17 / 107,845 , dated Jul . 16 , 2021 . "
        },
        "An articulated structure-aware network for 3D human pose estimation": {
            "authors": [],
            "url": "http://proceedings.mlr.press/v101/tang19a/tang19a.pdf",
            "ref_texts": "62 Short Title Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, and Xiaoou Tang. Residual attention network for image classiffcation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 3156{3164, 2017. Guanghui Wang and QM Jonathan Wu. Simpliffed camera projection models. In Guide to Three Dimensional Structure and Motion Factorization , pages 29{41. 2011. Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser Sheikh. Convolutional pose machines. In IEEE Conference on Computer Vision and Pattern Recognition , pages 4724{4732, 2016. Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy. Hierarchical attention networks for document classiffcation. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 1480{1489, 2016. Hashim Yasin, Umar Iqbal, Bjorn Kruger, Andreas Weber, and Juergen Gall. A dualsource approach for 3d pose estimation from a single image. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4948{4956, 2016. Wenpeng Yin, Hinrich Sch\u007f utze, Bing Xiang, and Bowen Zhou. Abcnn: Attention-based convolutional neural network for modeling sentence pairs. Transactions of the Association for Computational Linguistics , 4:259{272, 2016. Bo Zhao, Xiao Wu, Jiashi Feng, Qiang Peng, and Shuicheng Yan. Diversiffed visual attention networks for ffne-grained object classiffcation. IEEE Transactions on Multimedia , 19(6): 1245{1256, 2017. Xiaowei Zhou, Spyridon Leonardos, Xiaoyan Hu, Kostas Daniilidis, et al. 3d shape estimation from 2d landmarks: A convex relaxation approach. In IEEE Conference on Computer Vision and Pattern Recognition , volume 2, pages 4447{4455, 2015. Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. InIEEE Conference on Computer Vision and Pattern Recognition , pages 4966{4975, 2016. Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, and Kostas Daniilidis. Sparse representation for 3d shape estimation: A convex relaxation approach. IEEE Transactions on Pattern Analysis and Machine Intelligence , 39(8):1648{1661, 2017. Xiaowei Zhou, Menglong Zhu, Georgios Pavlakos, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Monocap: Monocular human motion capture using a cnn coupled with a geometric prior. IEEE Transactions on Pattern Analysis and Machine Intelligence , 2018."
        },
        "Walking on thin air: Environment-free physics-based markerless motion capture": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1812.01203",
            "ref_texts": "[13] X. Zhou, M. Zhu, K. Derpanis, and K. Daniilidis, \u201cSparseness meets deepness: 3d human pose estimation from monocular video,\u201d in CVPR , 2016.",
            "ref_ids": [
                "13"
            ],
            "1": "[13] X."
        },
        "Pointless Pose: Part Affinity Field-Based 3D Pose Estimation without Detecting Keypoints": {
            "authors": [
                "Jue Wang",
                "Zhigang Luo"
            ],
            "url": "https://www.mdpi.com/2079-9292/10/8/929/pdf",
            "ref_texts": "13. Zhou, X.; Zhu, M.; Leonardos, S.; Derpanis, K.G.; Daniilidis, K. Sparseness meets deepness: 3D human pose estimation from monocular video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV , USA, 26 June\u20131 July 2016.",
            "ref_ids": [
                "13"
            ],
            "1": "Following [4,13,32,33], we down-sampled the original videos from 50 fps to 10 fps to remove redundancy in both training and evaluation."
        },
        "End-to-End Learning of Multi-category 3D Pose and Shape Estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2112.10196",
            "ref_texts": "57. Zhou, X., Zhu, M., Leonardos, S., Derpanis, K.G., Daniilidis, K.: Sparseness meets deepness: 3d human pose estimation from monocular video. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016. pp. 4966\u20134975. IEEE Computer Society (2016). https://doi.org/10.1109/CVPR.2016.537, https://doi.org/10.1109/CVPR.2016.",
            "ref_ids": [
                "57"
            ],
            "1": "Significant research has been carried out in NrSfM to improve the performance through sparse dictionary learning [23,57], low-rank constraints [13], union of local subspaces [59], diffeomorphism [33], and coarse-to-fine low-rank reconstruction [2]."
        },
        "Learning Depth-aware Heatmaps for 3D Human Pose Estimation in the Wild.": {
            "authors": [],
            "url": "https://bmvc2019.org/wp-content/uploads/papers/1217-paper.pdf",
            "ref_texts": "[27] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2016.",
            "ref_ids": [
                "27"
            ],
            "1": "It is used in [2, 3, 7, 11, 12, 16, 23, 24, 27, 28, 29]."
        },
        "Error bounds of projection models in weakly supervised 3D human pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2010.12317",
            "ref_texts": "[27] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4966\u20134975, 2015. 2, 3",
            "ref_ids": [
                "27"
            ],
            "1": "[7, 11, 12, 19, 27]), which we do not consider here.",
            "2": "Identical or similar evaluation protocols are utilized in [14, 18, 24, 27, 28].",
            "3": "6\n[27] X."
        },
        "A Synchronized Reprojection-based Model for 3D Human Pose Estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2106.04274",
            "ref_texts": "46.Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 4966\u20134975, 2016.",
            "ref_ids": [
                "46"
            ],
            "1": "[46] 87."
        },
        "View Consistency Aware Holistic Triangulation for 3D Human Pose Estimation": {
            "authors": [
                "Xiaoyue Wan",
                "Zhuo Chen",
                "Xu Zhao"
            ],
            "url": "https://arxiv.org/pdf/2302.11301",
            "ref_texts": "3425\u20133435. Zhou, X., Zhu, M., Leonardos, S., Derpanis, K.G., Daniilidis, K., 2016. Sparseness meets deepness: 3d human pose estimation from monocular video, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4966\u20134975."
        },
        "Augment Yourself: Mixed Reality Self-Augmentation Using Optical See-through Head-mounted Displays and Physical Mirrors": {
            "authors": [
                "Mathias Unberath",
                "Kevin Yu",
                "Roghayeh Barmaki",
                "Alex Johnson",
                "Nassir Navab"
            ],
            "url": "https://arxiv.org/pdf/2007.02884",
            "ref_texts": "[57] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp. 4966\u20134975, 2016.",
            "ref_ids": [
                "57"
            ],
            "1": "In this case, more sophisticated tracking solutions are required that employ dense or sparse methods, such as deformable SLAM [24, 40] or human skeleton tracking [56, 57].",
            "2": "Considering that the HMD used here only allows access to the RGB camera feed, a skeleton tracking method based on monocular RGB images [30,57] would be ideal, as it would allow a fully integrated prototype.",
            "3": "The authors believe that, as previously mentioned in the manuscript, human pose estimation methods that operate on monocular RGB images [30, 57] would lend themselves well for the considered use-case.",
            "4": "[57] X."
        },
        "Parametric human shape reconstruction via bidirectional silhouette guidance": {
            "authors": [
                "Shuang Sun",
                "Chen Li",
                "Zhenhua Guo",
                "Yuwing Tai"
            ],
            "url": "http://openaccess.thecvf.com/content_ICCVW_2019/papers/GMDL/Sun_Parametric_Human_Shape_Reconstruction_via_Bidirectional_Silhouette_Guidance_ICCVW_2019_paper.pdf",
            "ref_texts": "[50] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proc. of Computer Vision and Pattern Recognition , 2016. 2",
            "ref_ids": [
                "50"
            ],
            "1": "Existing approaches can be categorized into the two-stage methods [19,22,48,40,6,23,50] and the direct end-to-end methods [31,7,43,33,25,36].",
            "2": "Two-stage methods first predict the 2D projection of 3D joints in image spaces and then estimate the corresponding depth values with various constraints, such as pose prior [6], skeleton prior [23], and geometric prior [50].",
            "3": "2\n[50] X."
        },
        "Sports movements modification based on 2D joint position using YOLO to 3D skeletal model adaptation": {
            "authors": [],
            "url": "https://jad.shahroodut.ac.ir/article_2578_746a9149db4eaa7cec31690d56fb406f.pdf",
            "ref_texts": "[16] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis, \u201c Sparseness meets deepness: 3D human pose estimation from monocular video ,\u201d in Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, Dec. ",
            "ref_ids": [
                "16"
            ],
            "1": "In [16], an image sequence is used to three -dimensionally estimate the complete position of the human body.",
            "2": "[16] X."
        },
        "Learning dynamics from kinematics: Estimating 2d foot pressure maps from video frames": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1811.12607",
            "ref_texts": "[57] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4966\u20134975, 2016.",
            "ref_ids": [
                "57"
            ],
            "1": "Success in 2D human pose estimation has encouraged researchers to detect 3D skeletons from image/video by extending existing 2D human pose detectors [7, 11, 32, 33, 36, 45] or by directly using image features [1, 39, 44, 46, 57].",
            "2": "[57] X."
        },
        "2.5 D human pose estimation for shadow puppet animation.": {
            "authors": [],
            "url": "https://itiis.org/digital-library/manuscript/file/22075/TIISVol13No4-17.pdf",
            "ref_texts": "[30] X Zhou, M Zhu, S Leonardos, et al. \u201cSparseness meets deepness: 3D human pose estimation from monocular video, \u201d in Proc. of the IEEE Conference on Computer Vision and P attern Recognition ",
            "ref_ids": [
                "30"
            ],
            "1": "It is means that researcher need utility 2D poses to predict 3D poses [30] [31] [32] [33] [34].",
            "2": "[30] X Zhou, M Zhu, S Leonardos, et al."
        },
        "Learning Transferable Kinematic Dictionary for 3D Human Pose and Shape Reconstruction": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2104.00953",
            "ref_texts": "3D shape estimation from 2D landmarks: A convex relaxation approach. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . Zhou, X.; Sun, X.; Zhang, W.; Liang, S.; and Wei, Y . 2016a. Deep kinematic pose regression. In European Conference on Computer Vision . Springer. Zhou, X.; Zhu, M.; Leonardos, S.; Derpanis, K. G.; and Daniilidis, K. 2016b. Sparseness meets deepness: 3D human pose estimation from monocular video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . Zhou, Y .; Barnes, C.; Lu, J.; Yang, J.; and Li, H. 2019. On the continuity of rotation representations in neural networks. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition ."
        },
        "Unsupervised 3D human pose estimation in multi-view-multi-pose video": {
            "authors": [],
            "url": "http://klab2.cvg.ait.kyushu-u.ac.jp/papers/2020/ICPR_Cheng_Sun.pdf",
            "ref_texts": "[14] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis, \u201cSparseness meets deepness: 3d human pose estimation from monocular video,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition , 2016, pp. 4966\u20134975.",
            "ref_ids": [
                "14"
            ],
            "1": "[14] use a 3D pose dictionary and capture how poses appear from different camera views.",
            "2": "[14] X."
        },
        "Deep pose consensus networks": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1803.08190",
            "ref_texts": "10. Zhou, X., Zhu, M., Leonardos, S., Derpanis, K.G., Daniilidis, K.: Sparseness meets deepness: 3D human pose estimation from monocular video. In: Proc. of the IEEE Computer Vision and Pattern Recognition. (2016) 4966\u20134975",
            "ref_ids": [
                "10"
            ],
            "1": "Here, we consider two cases, following the general practice in the literature [10, 14]: (i) estimating 3D poses when the ground truth 2D poses are given (\u201cCase 1\u201d), and (ii) estimating 3D poses when only RGB images are given without any ground truth 2D poses (\u201cCase 2\u201d).",
            "2": "0 Zhou et al [10] 87."
        },
        "A revision control system for image editing in collaborative multimedia design": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1806.00263"
        },
        "3D Human Pose Estimation with a Catadioptric Sensor in Unconstrained Environments Using an Annealed Particle Filter": {
            "authors": [],
            "url": "https://www.mdpi.com/1424-8220/20/23/6985/pdf",
            "ref_texts": "7. Zhou, X.; Zhu, M.; Leonardos, S.; Derpanis, K.G.; Daniilidis, K. Sparseness meets deepness: 3D human pose estimation from monocular video. In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV , USA, 27\u201330 June 2016; pp. 4966\u20134975.",
            "ref_ids": [
                "7"
            ],
            "1": "[7] expressed the optimization problem as the relationship between 2D pose and 3D geometric features, and predicted the 3D pose using an expectation maximization algorithm."
        },
        "Spatio-temporal Self-Attention for Egocentric 3D Pose Estimation": {
            "authors": [],
            "url": "https://openreview.net/pdf?id=F_P8Dtg43vF",
            "ref_texts": "[41] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. InProceedings of the IEEE conference on computer vision and pattern recognition, pages 4966\u20134975, 2016.",
            "ref_ids": [
                "41"
            ]
        },
        "Human pose estimation from sparse 3D Data on low power systems": {
            "authors": [],
            "url": "https://spiral.imperial.ac.uk/bitstream/10044/1/83254/1/Vasileiadis-M-2020-PhD-Thesis.pdf",
            "ref_texts": ""
        },
        "Automated identification of trampoline skills using computer vision extracted pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1709.03399",
            "ref_texts": "[Zhou et al., 2016] Zhou, X., Zhu, M., Leonardos, S., Derpanis, K. G., and Daniilidis, K. (2016). Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4966\u20134975.",
            "ref_ids": [
                "Zhou et al\\., 2016"
            ]
        },
        "Regress 3D human pose from 2D skeleton with kinematics knowledge": {
            "authors": [
                "Longkui Jiang"
            ],
            "url": "http://www.aimspress.com/aimspress-data/era/2023/3/PDF/era-31-03-075.pdf",
            "ref_texts": "4. X. Zhou, M. Zhu, S. Leonardos, K. Derpanis, K. Da niilidis, Sparseness me ets deepness: 3D human pose estimation from monocular video. in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , (2016), 4966\u20134975. https://doi.org/10.1109/CVPR.2016.537 ",
            "ref_ids": [
                "4"
            ]
        },
        "Machine Learning for Human Action Recognition and Pose Estimation based on 3D Information": {
            "authors": [],
            "url": "https://theses.hal.science/tel-02492463/file/72722_CARBONERA%20LUVIZON_2019_archivage.pdf",
            "ref_texts": "[172] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 2016. 9",
            "ref_ids": [
                "172"
            ],
            "1": "3 Monocular 3D Human Pose Estimation Estimating the human body joints in 3D coordinates from monocular RGB images is a very challenging problem with a vast bibliography available in the literature [56, 107, 38, 134, 74, 73, 55, 172].",
            "2": "[172] X."
        },
        "Computer Vision and Abnormal Patient Gait Assessment a Comparison of Machine Learning Models": {
            "authors": [
                "Aisvarya Chandramohan"
            ],
            "url": "https://arxiv.org/pdf/2004.02810",
            "ref_texts": ""
        },
        "Articulated motion and deformable objects": {
            "authors": [],
            "url": "https://diposit.ub.edu/dspace/bitstream/2445/132150/1/679928.pdf",
            "ref_texts": "[40]X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, K. Daniilidis, Sparseness meets deepness: 3d human pose estimation from monocular video, in: Proceedings of the IEEE Conference on Computer Vision and Pattern 735 Recognition, 2016, pp. 4966\u20134975.",
            "ref_ids": [
                "40"
            ],
            "1": "[40] propose a sparseness meets deepness (SMP) algorithm to 165 address the challenge of 3D full-body human pose estimation from a monocular image sequence.",
            "2": "44 SMP [40], 2016 3.",
            "3": "[40]X."
        },
        "Deep Structured Layers for Instance-Level Optimization in 2D and 3D Vision": {
            "authors": [],
            "url": "https://discovery.ucl.ac.uk/id/eprint/10164015/1/Kokkinos_Main.pdf",
            "ref_texts": "[214] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 4966\u20134975, 2016.",
            "ref_ids": [
                "214"
            ],
            "1": "The priors are embedded into the methods using low-rank subspaces [212, 169, 203], spatio-temporal domains [171, 178], equivariance constraints [210] or sparse basis coefficients using L1 constraints [211, 213, 214]."
        },
        "Multi-task and multi-level detection neural network based real-time 3d pose estimation": {
            "authors": [],
            "url": "http://www.apsipa.org/proceedings/2019/pdfs/293.pdf",
            "ref_texts": "[24] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis, \u201cSparseness meets deepness: 3d human pose estimation from monocular video,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4966\u20134975, 2016.",
            "ref_ids": [
                "24"
            ],
            "1": "So many works [24] choose video as input.",
            "2": "[24] Video 64.",
            "3": "[24] X."
        },
        "Human Motion Analysis Using 3D Skeleton Representation in The Context of Real-World Applications: From Home-Based Rehabilitation to Sensing In The Wild": {
            "authors": [],
            "url": "https://orbilu.uni.lu/bitstream/10993/45950/1/RenatoBaptista_PhD_thesis.pdf",
            "ref_texts": "[91] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis, \u201cSparseness meets deepness: 3d human pose estimation from monocular video,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition , 2016, pp. 4966\u20134975.",
            "ref_ids": [
                "91"
            ],
            "1": "Recently, and thanks to the advances in deep learning techniques, 3D human pose estimation has become more accessible [33]\u2013\n[38], [91]\u2013[93].",
            "2": "[91] X."
        },
        "From Dense 2D to Sparse 3D Trajectories for Human Action Detection and Recognition": {
            "authors": [],
            "url": "https://orbilu.uni.lu/bitstream/10993/43167/1/phd_thesis.pdf",
            "ref_texts": "[51] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis, \u201cSparseness meets deepness: 3d human pose estimation from monocular video,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition , 2016, pp. 4966\u20134975.",
            "ref_ids": [
                "51"
            ],
            "1": "Recently, thanks to the advances in deep learning, a wide range of more reliable approaches have started to emerge [51]\u2013[56].",
            "2": "[51] X."
        },
        "Back to the future: Joint aware temporal deep learning 3D human pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2002.11251",
            "ref_texts": "33. Zhou, X., Zhu, M., Leonardos, S., Derpanis, K.G., Daniilidis, K.: Sparseness meets deepness: 3d human pose estimation from monocular video. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 4966{4975 (2016)",
            "ref_ids": [
                "33"
            ],
            "1": "[22] notes dilated convolutions [7] success in a number of temporal domains [31] [33] [11] by preserving long term dependencies and maintaining eflciency.",
            "2": "Protocol 1is the mean per-joint position error (MPJPE) in millimeters calculated as the Euclidean distance between predicted joint positions and ground-truth joint positions and follows [16] [29] [33] [18] [20] [22]."
        },
        "On the Design of 2D Human Pose Estimation Networks using Accelerated Neuroevolution and Novel Keypoint Representations": {
            "authors": [],
            "url": "https://uwspace.uwaterloo.ca/bitstream/handle/10012/18215/McNally_William.pdf?sequence=3&isAllowed=y",
            "ref_texts": "[65] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis, \u201cSparseness meets deepness: 3d human pose estimation from monocular video,\u201d in CVPR , 2016.",
            "ref_ids": [
                "65"
            ],
            "1": "6M [61], HumanEva [62], MPI-INF-3DHP [63]) to support the development of 3D human pose estimation algorithms [64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86].",
            "2": "[65] X."
        },
        "Not all parts are created equal: 3d human pose estimation by modeling bi-directional dependencies of body parts": {
            "authors": [
                "Greg Hampshire"
            ],
            "url": "https://opus.lib.uts.edu.au/bitstream/10453/137277/3/Binder1.pdf",
            "ref_texts": "[48] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2016.",
            "ref_ids": [
                "48"
            ],
            "1": "These methods comprise a 2D pose detector and a subsequent optimization [48, 47, 49] or regression [4, 3, 17, 30, 36, 14, 19, 7, 12] step to estimate 3D pose .",
            "2": "The most straightforward approach is to represent 3D poses as linear combinations of models learned from training data [48, 47, 49].",
            "3": "[48] 87.",
            "4": "Following [48, 22, 45, 21], we down sampled the original videos from 50fps to 10fps to remove redundancy."
        },
        "Estimating 3D Motion and Forces from Monocular Videos": {
            "authors": [],
            "url": "https://hal.science/tel-04141548/document",
            "ref_texts": "92 BIBLIOGRAPHY. Triggs, B., McLauchlan, P. F., Hartley, R. I., and Fitzgibbon, A. W. Bundle adjustment\u2014a modern synthesis. In International workshop on vision algorithms , 1999. 53 Wei, X. and Chai, J. Videomocap: Modeling physically realistic human motion from monocular video sequences. ACM Trans. Graph. , 29(4):42:1\u201342:10, Jul 2010. ISSN 0730-0301. doi: 10.1145/1778765.1778779. URL http://doi.acm. org/10.1145/1778765.1778779 . 18, 19 Westervelt, E. R., Grizzle, J. W., and Koditschek, D. E. Hybrid zero dynamics of planar biped walkers. IEEE Transactions on Automatic Control , 48(1):42\u201356, 2003. doi: 10.1109/TAC.2002.806653. 20 Winkler, A. W., Bellicoso, C. D., Hutter, M., and Buchli, J. Gait and trajectory optimization for legged systems through phase-based end-effector parameterization. IEEE Robotics and Automation Letters , 3(3):1560\u20131567, 2018. 20 Xiang, D., Joo, H., and Sheikh, Y. Monocular total capture: Posing face, body, and hands in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 10965\u201310974, 2019. 16, 17 Xiang, Y., Schmidt, T., Narayanan, V., andFox, D. Posecnn: Aconvolutionalneural network for 6d object pose estimation in cluttered scenes. CoRR, abs/1711.00199, 2017. URL http://arxiv.org/abs/1711.00199 . 19 Yao, B. and Fei-Fei, L. Recognizing human-object interactions in still images by modeling the mutual context of objects and human poses. PAMI, 34(9): 1691\u20131703, 2012. 18 Zanfir, A., Marinoiu, E., and Sminchisescu, C. Monocular 3d pose and shape estimation of multiple people in natural scenes-the importance of multiple scene constraints. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 2148\u20132157, 2018. 18 Zhou, X., Zhu, M., Leonardos, S., Derpanis, K. G., and Daniilidis, K. Sparseness meets deepness: 3d human pose estimation from monocular video. In CVPR, 2016. 16, 18 Zorina, K., Carpentier, J., Sivic, J., and Petr\u00edk, V. Learning to manipulate tools by aligning simulation to video demonstration. IEEE Robotics and Automation Letters, 7(1):438\u2013445, 2021. 7, 83"
        },
        "A Real-Time Photogrammetric System for Acquisition and Monitoring of Three-Dimensional Human Body Kinematics": {
            "authors": [
                "Long Chen",
                "Bo Wu",
                "Yao Zhao",
                "Yuan Li"
            ],
            "url": "https://www.ingentaconnect.com/contentone/asprs/pers/2021/00000087/00000005/art00014?crawler=true&mimetype=application/pdf",
            "ref_texts": ""
        },
        "Continuous Learning of Inverse Problems with Applications to Structure from Motion": {
            "authors": [],
            "url": "https://openresearch.surrey.ac.uk/view/delivery/44SUR_INST/12152228770002346/13152228760002346",
            "ref_texts": "[144] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3D human pose estimation from monocular video. Conference on Computer Vision and Pattern Recognitions (CVPR) , 2016.",
            "ref_ids": [
                "144"
            ],
            "1": "The lifting stage is then performed by model fitting [4, 15, 88, 97, 143, 144] or regression [68, 73].",
            "2": "Once obtained the 2D pose prediction, it can be lifted into 3D either by model fitting [4,15,88,97,143,144] or regression [68,73].",
            "3": "In light of the high similarity of subsequent frames, we evaluate only every fifth frame, following the example of [144];\n58 CHAPTER 3.",
            "4": "[144] 87.",
            "5": "[144] X."
        },
        "cvpaper. challenge in 2016: futuristic computer vision through 1,600 papers survey": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1707.06436",
            "ref_texts": "[888] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Kosta Derpanis, Kostas Daniilidis, Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video, in CVPR, 2016.",
            "ref_ids": [
                "888"
            ],
            "1": "ICCV/ECCV/NIPS/PAMI/IJCV) in addition to the CVPR [685], [686], [687], [688], [689], [690], [691], [692] , [693], [694], [695], [696], [697], [698], [699], [700], [70 1], [702], [704], [705], [706], [707], [708], [709], [710], [711], [71 2], [713],[714], [715], [716], [717], [718], [719], [720], [721], [72 2], [723], [724], [725], [726], [727], [728], [729], [730], [731], [73 2], [733], [734], [735], [736], [737], [738], [739], [740], [741], [74 2], [743], [744], [745], [746], [747], [748], [749], [750], [751], [75 2], [753], [754], [755], [756], [757], [758], [759], [760], [761], [76 2], [763], [764], [765], [766], [767], [768], [769], [770], [771], [77 2], [773], [774], [775], [776], [777], [778], [779], [780], [781], [78 2], [783], [784], [785], [786], [787], [788], [789], [790], [791], [79 2], [793], [794], [795], [796], [797], [798], [799], [800], [801], [80 2], [803], [804], [805], [806], [807], [808], [809], [810], [811], [81 2], [813], [814], [815], [816], [817], [818], [819], [820], [821], [82 2], [823], [824], [826], [827], [828], [829], [830], [831], [832], [83 3], [834], [835], [836], [837], [838], [839], [840], [841], [842], [84 3], [844], [845], [846], [847], [848], [849], [850], [851], [852], [85 3], [854], [855], [856], [857], [858], [859], [860], [861], [862], [86 3], [864], [865], [866], [867], [868], [869], [870], [871], [872], [87 3], [874], [875], [876], [877], [878], [879], [880], [881], [882], [88 3], [884], [885], [886], [887], [888], [889], [890], [891], [892], [89 3], [894], [895], [896], [897], [898], [899], [900], [901], [902], [90 3], [904], [905], [906], [907], [908], [909], [910], [911], [912], [91 3], [914], [915], [916], [917], [918], [919], [920], [921], [922], [92 3], [924], [925], [926], [927], [928], [929], [930], [931], [932], [93 3], [934], [935], [936], [937], [938], [939], [940], [941], [942], [94 3], [944], [945], [946], [947], [948], [949], [950], [951], [952], [95 3], [954], [955], [956], [957], [958], [959], [960], [961], [962], [96 3], [964], [965], [966], [967], [968], [969], [970], [971], [972], [97 3], [974], [975], [976], [977], [978], [979], [980], [981], [982], [98 3], [984], [985], [986], [987], [988], [989], [990], [991], [992], [99 3], [994], [995], [996], [997], [998], [999], [1000], [1001], [1002], [1003], [1004], [1005], [1006], [1007], [1008], [1009], [1010], [1 011], [1012], [1013], [1014], [1015], [1016], [1017], [1018], [1 019], [1020], [1021], [1022], [1023], [1024], [1025], [1026], [1 027], [1028], [1029], [1030], [1031], [1032], [1033], [1034], [1 035], [1036], [1037], [1038], [1039], [1040], [1041], [1042], [1 043], [1044], [1045], [1046], [1047], [1048], [1049], [1050], [1 051], [1052], [1053], [1054], [1055], [1056], [1057], [1058], [1 059], [1060], [1061], [1062], [1063], [1064], [1065], [1066], [1 067], [1068], [1069], [1070], [1071], [1072], [1073], [1074], [1 075], [1076], [1077], [1078], [1079], [1080], [1081], [1082], [1 083], [1084], [1085], [1086], [1087], [1088], [1089], [1090], [1 091], [1092], [1093], [1094], [1095], [1096], [1097], [1098], [1 099], [1100], [1101], [1102], [1103], [1104], [1105], [1106], [1 107], [1108], [1109], [1110], [1111], [1112], [1113], [1114], [1 115], [1116], [1117], [1118], [1119], [1120], [1121], [1122], [1 123], [1124], [1125], [1126], [1127], [1128], [1129], [1130], [1 131], [1132], [1133], [1134], [1135], [1136], [1137], [1138], [1 139], [1140], [1141], [1142], [1143], [1144], [1145], [1146], [1 147], [1148], [1149], [1150], [1151], [1152], [1153], [1154], [1 155], [1156], [1157], [1158], [1159], [1160], [1161], [1162], [1 163], [1164], [1165], [1166], [1167], [1168], [1169], [1170], [1 171], [1172], [1173], [1174], [1175], [1176], [1177], [1178], [1 179], [1180], [1181], [1182], [1183], [1184], [1185], [1186], [1 187], [1188], [1189], [1190], [1191], [1192], [1193], [1194], [1 195], [1196], [1197], [1198], [1199], [1200], [1201], [1202], [1 203], [1204], [1205], [1206], [1207], [1208], [1209], [1210], [1 211], [1212], [1213], [1214], [1215], [1216], [1217], [1218], [1 219], [1220], [1221], [1222], [1223], [1224], [1225], [1226], [1 227], [1228], [1229], [1230], [1231], [1232], [1233], [1234], [1 235], [1236], [1237], [1238], [1239], [1240], [1241], [1242], [1 243], [1244], [1245], [1246], [1248], [1249], [1250], [1251], [1 252], [1253], [1254], [1255], [1256], [1257], [1258], [1259], [1 260], CVPAPER."
        },
        "Towards Efficient and Reliable Skeleton-Based Human Pose Modeling": {
            "authors": [],
            "url": "https://rucore.libraries.rutgers.edu/rutgers-lib/66983/PDF/1/play/",
            "ref_texts": "[66] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis, \u201cSparseness meets deepness: 3D human pose estimation from monocular video,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2016, pp. 4966\u20134975.",
            "ref_ids": [
                "66"
            ],
            "1": "Other methods created over-complete bases which are suitable for representing human poses as sparse combinations [62, 63, 64, 65, 66].",
            "2": "[66] 87.",
            "3": "101\n[66] X."
        },
        "Deep learning based RGB-D vision tasks": {
            "authors": [],
            "url": "https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/112866/2/Cao2018_PhD.pdf",
            "ref_texts": "3dmatch: Learning local geometric descriptors from rgb-d reconstructions. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn. (cited on page 14) Zhang , J.; L azebnik , S.;and Schmid , C., 2007. Local features and kernels for classification of texture and object categories: a comprehensive study. Int. J. Comp. Vis. , 73 (2007), 2007. (cited on pages 3 and 20) Zhang , K.; L u, J.;and Lafruit , G., 2009. Cross-based local stereo matching using orthogonal integral images. IEEE Trans. Circuits Syst. Video Technol. , 19, 7 (2009), 1073\u20131079. (cited on page 59) Zhang , L.; V \u00e1zquez , C.; and Knorr , S., 2011. 3d-tv content creation: Automatic 2d-to-3d video conversion. IEEE Trans. Broad. , 57, 2 (2011). (cited on page 13) Zhang , Y.; S ohn , K.; V illegas , R.; P an, G.; and Lee, H., 2015. Improving object detection with deep convolutional networks via bayesian optimization and structured prediction. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn. (cited on page 20) Zhou, B.; L apedriza , A.; K hosla , A.; O liva , A.; and Torralba , A., 2017. Places: A 10 million image database for scene recognition. IEEE Trans. Pattern Anal. Mach. Intell. , (2017). (cited on page 65) Zhou, B.; L apedriza , A.; X iao, J.; T orralba , A.; and Oliva , A., 2014. Learning deep features for scene recognition using places database. In Proc. Adv. Neural Inf. Process. Syst. (cited on page 35) Zhou, X.; Z hu, M.; L eonardos , S.; D erpanis , K. G.; and Daniilidis , K., 2016. Sparseness meets deepness: 3d human pose estimation from monocular video. InProc. IEEE Conf. Comp. Vis. Patt. Recogn. (cited on page 14) Zoran , D.; I sola , P .; K rishnan , D.; and Freeman , W. T., 2015. Learning ordinal relationships for mid-level vision. In Proc. IEEE Int. Conf. Comp. Vis. (cited on pages 59, 61, and 68)"
        },
        "Adversarially parameterized optimization for 3d human pose estimation": {
            "authors": [],
            "url": "https://eprints.qut.edu.au/115073/1/adversarial_param_opt.pdf",
            "ref_texts": "[49] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4966\u20134975, 2016. 6",
            "ref_ids": [
                "49"
            ],
            "1": "Some are undoubtably more accurate [23] [43] [32] [25], while others use slightly different metrics, training data and/or report different metrics [4] [24]\n[49] [50].",
            "2": "2\n[49] X."
        },
        "Improving 3D human pose estimation in-the-wild": {
            "authors": [],
            "url": "https://www.ideals.illinois.edu/items/124614/bitstreams/409598/object?dl=1",
            "ref_texts": "[8] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis, \u201cSparseness meets deepness: 3d human pose estimation from monocular video,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition , 2016, pp. 4966\u20134975.",
            "ref_ids": [
                "8"
            ],
            "1": "One approach is a two-stage detector which first estimates the 2D pose then then predicts 3D pose from 2D [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20].",
            "2": "[8] X."
        },
        "2d/3d human pose estimation using deep convolutional neural nets": {
            "authors": [],
            "url": "https://open.metu.edu.tr/bitstream/handle/11511/27978/index.pdf",
            "ref_texts": "[76] X. Zhou, M. Zhu, K. Derpanis, and K. Daniilidis, \u201cSparseness meets deepness: 3D human pose estimation from monocular video,\u201d IEEE Conference on Computer Vision and Pattern Recognition , 2016.",
            "ref_ids": [
                "76"
            ],
            "1": "Additionally, there are two-stage approaches which decompose the 3D pose inference task into two independent stages: estimating 2D poses, and lifting them into 3D space [72, 73, 74, 75, 76, 72, 67, 21].",
            "2": "[76] X."
        },
        "Generic video-based motion capture data retrieval": {
            "authors": [],
            "url": "http://www.apsipa.org/proceedings/2019/pdfs/272.pdf",
            "ref_texts": "[12] E. Jahangiri and A. L. Yuille, \u201cGenerating multiple diverse hypotheses for human 3d pose consistent with 2d joint detections,\u201d in Proceedings of the IEEE International Conference on Computer Vision, pp. 805\u2013814, 2017.[13] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis, \u201cSparseness meets deepness: 3d human pose estimation from monocular video,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4966\u20134975, 2016.",
            "ref_ids": [
                "12",
                "13"
            ],
            "1": "A lot of research estimates corresponding 3D poses on the basis of the state-of-the-art 2D pose estimations [11], [12].",
            "2": "[13], [14] use a CNN to extract 2D heatmaps from 2D poses to reconstruct a 3D pose sequence from a video clip.",
            "3": "[12] E.",
            "4": "[13] X."
        },
        "Multi-core computation of transfer matrices for strip lattices in the potts model": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1305.6325"
        },
        "Data reduction using cluster sampling": {
            "authors": [],
            "url": "http://www.apsipa.org/proceedings/2020/pdfs/0001274.pdf"
        },
        "Acquiring Motor Skills Through Motion Imitation and Reinforcement Learning": {
            "authors": [],
            "url": "https://digitalassets.lib.berkeley.edu/techreports/ucb/incoming/EECS-2021-267.pdf",
            "ref_texts": "[316] X. Zhou et al. \u201cSparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video\u201d. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR. 2016, pp. 4966\u20134975.",
            "ref_ids": [
                "316"
            ],
            "1": "The introduction of large-scale mocap datasets [113] with ground truth 3D joint locations allowed for the development of deep learning based methods that directly estimate 3D joint locations from images [316, 173, 199].",
            "2": "[316] X."
        },
        "Multi-modal analysis for the automatic evaluation of epilepsy": {
            "authors": [
                "David Esteban",
                "Ahmedt Aristizabal"
            ],
            "url": "https://eprints.qut.edu.au/132537/1/David_Ahmedt%20Aristizabal_Thesis.pdf",
            "ref_texts": "[Zhou et al., 2016d] Zhou, X., Zhu, M., Leonardos, S., Derpanis, K. G., and Daniilidis, K. (2016d). Sparseness meets deepness: 3D human pose estimation from monocular video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4966\u20134975.",
            "ref_ids": [
                "Zhou et al\\., 2016d"
            ]
        },
        "Monocular 3D Pose Recovery via Nonconvex Sparsity with Theoretical Analysis": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1812.11295",
            "ref_texts": "[46] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4966\u20134975, 2016. 1, 8",
            "ref_ids": [
                "46"
            ],
            "1": "The recent progress in deep learning based algorithms to 3D object pose recovery could be roughly divided into two lines: 1), a two-stage pipeline that first recovers the 2D poses using specifically designed deep networks for special objects like human [26][31] [38][10], and then estimate the 3D poses by solving a geometric inference problem that the 2D pose is captured through a projection, rotation, and translation of the object in the 3D world [28]; [46] encourages the consistency of the combination coefficients and rotation matrices between two consecutive frames in a video; Bogo et al.",
            "2": "We follow the expectation-maximization framework of [46], on the probabilistic model described in Eq.",
            "3": "[46] has used 1000 iterations to perform inference, and each iteration costs about the same time with ours.",
            "4": "2, 4, 6\n[46] X."
        },
        "Human head pose and emotion analysis": {
            "authors": [],
            "url": "http://cvit.iiit.ac.in/images/Thesis/MS/aryaman_gupta/aryaman_gupta_thesis.pdf",
            "ref_texts": "[93] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In CVPR , 2016.",
            "ref_ids": [
                "93"
            ],
            "1": "Zhou [93] use heatmap images of 2D joint locations to infer 3D human pose using an Expectation Maximization framework.",
            "2": "Interestingly, both these works [93, 80] use heatmaps over 2D spatial locations to infer 3D structure/pose.",
            "3": "Unlike previous efforts [93, 80] that use heatmaps as an intermediate representation and do not have ground truth data, we have ground truth pose angles available.",
            "4": "[93] X."
        },
        "3D Single Person Pose Estimation Method Based on Deep Learning.": {
            "authors": [],
            "url": "https://scholar.archive.org/work/wz25ffq5vramxfcys5tsero2g4/access/wayback/https://ebooks.iospress.nl/pdf/doi/10.3233/FAIA200726",
            "ref_texts": "[14] Zhou X, Zhu M, Leonardos S, et al. Sparseness meets deepness: 3d human pose estimation from monocular video[C]//Proceedings of the IEEE conf erence on computer visio n and pattern recognition. ",
            "ref_ids": [
                "14",
                "C"
            ],
            "1": "[14] com bined a 2D part regressor based on deep learning and a sparsedriven 3D reconstruction method to design a 3D human pose estimation framework.",
            "2": "Occlusion-aware networks for 3d human pose estimation in video[C]//Proceedings of the IEEE International Conf erence on Computer Vision.",
            "3": "Lightweight 3D Human Pose Estimation Network Training Using Teacher-Student Learning[C]//The IEEE Winter Co nference on Applications of Computer Vision.",
            "4": "DeepFuse: An IMU-Aware Network for Real-Time 3D Human Pose Estimation from Multi-View Image[C]//The IEEE Wi nter Conference on Applications of Computer Vision.",
            "5": "Adversarial posenet: A structure-awa re convolutional network for human pose estimation[C]//Proceedings of the IE EE International Confe rence on Computer Vision.",
            "6": "Self adversarial training for human pose estimation[C]//2018 AsiaPacific Signal and Information Processing Associat ion Annual Summit an d Conference (APSIPA ASC).",
            "7": "Deep high -resolution representation learning for human pose estimation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition.",
            "8": "Deep kinematic pose regression[C]//European Conference on Computer Vision.",
            "9": "Fully convolutional networks for semantic segmentation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition.",
            "10": "[14] Zhou X, Zhu M, Leonardos S, et al.",
            "11": "Sparseness meets deepness: 3d human pose estimation from monocular video[C]//Proceedings of the IEEE conf erence on computer visio n and pattern recognition.",
            "12": "Lif ting from the deep: Convolutional 3d pose estimation from a single image[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.",
            "13": "3d human pose estimation= 2d pose estimation+ matching[C]//Proceedings of the IEEE Conference on Computer Visio n and Pattern Recognition.",
            "14": "3d human pose estimation fr om a single image via distance matrix regression[C]//Proceedings of the IE EE Conference on Computer Vision and Pattern Recognition.",
            "15": "A simple yet effective baseline for 3d human pose estimation[C]//Proceedings of the IEEE International Conference on Computer Vision.",
            "16": "Coarse-to-fine volumetric prediction for single-image 3D human pose[C]//Proceedings of the IEEE Co nference on Computer Vision and Pattern Recognition.",
            "17": "To wards 3d human po se estimation in the wild: a weakly -supervised approach[C]//Proceedings of the IE EE International Conference on Comp uter Vision.",
            "18": "Ordinal depth supervision for 3d human pose estimation[C]//Proceedings of the IE EE Conference on Computer V ision and Pattern Recognition.",
            "19": "3d human pose estimation in the wild by adversarial learning[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.",
            "20": "3d human pose estimation in video with temporal convolutions and semi-supervised training[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.",
            "21": "Generating multiple hypotheses for 3d human pose estimation with mixture density network[C]//Proceedings of the IE EE Conference on Computer Vision and Pattern Recognition.",
            "22": "Panoptic studio: A massively multiview system for social motion capture[C]//Proceedings of the IEEE International Conference on Com puter Vision.",
            "23": "Monocular 3d hum an pose estimation in the wild using improved cnn supervision[C]//2017 international conference on 3D vision (3DV)."
        },
        "Machine Learning for Human Performance Capture from Multi-Viewpoint Video": {
            "authors": [],
            "url": "https://openresearch.surrey.ac.uk/esploro/fulltext/doctoral/Machine-learning-for-human-performance-capture/99514385602346?repId=12139438260002346&mId=13140527520002346&institution=44SUR_INST",
            "ref_texts": "[96] Zhou, X., Zhu, M., Leonardos, S., Derpanis, K. G., and Daniilidis, K. (2016). Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4966\u20134975.",
            "ref_ids": [
                "96"
            ],
            "1": "[96] integrates 2D, 3D and temporal information to account for uncertainties in the data.",
            "2": "[96] Zhou, X."
        },
        "Visual commonsense reasoning: Functionality, physics, causality, and utility": {
            "authors": [],
            "url": "https://escholarship.org/content/qt7sm0389z/qt7sm0389z.pdf",
            "ref_texts": "[ZZL16] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. \\Sparseness meets deepness: 3D human pose estimation from monocular video.\" In Conference on Computer Vision and Pattern Recognition (CVPR), 2016.",
            "ref_ids": [
                "ZZL16"
            ],
            "1": "1 Related Work Synthetic image datasets have recently been a source of training data for object detection and correspondence matching [SGS10, SS14, SX14, FKI14, DFI15, PSA15, ZKA16, GWC16, MKS16, QSN16], single-view reconstruction [HWK15], view-point estimation [MSB14, SQL15], 2D human pose estimation [PJA12, RLB15, Qiu16], 3D human pose estimation [SSK13, SVD03, YIK16, DWL16, GKS16, RS16, ZZL16, CWL16, VRM17], depth prediction [SHM14], pedestrian detection [MVG10, PJW11, VLM14, HNK15], action recognition [RM15, RM16, SGC17], semantic segmentation[RVR16], scene understanding [HPS16, KIX16, QY16, HPB16], and in benchmark data sets [HWM14]."
        },
        "Synthetic occlusion augmentation for 3D human pose estimation with volumetric heatmaps": {
            "authors": [],
            "url": "https://www.vision.rwth-aachen.de/media/papers/synthetic_occl_augm/posetrack3d-sarandi.pdf",
            "ref_texts": "28. Zhou, X., Zhu, M., Leonardos, S., Derpanis, K.G., Daniilidis, K.: Sparseness meets deepness: 3d human pose estimation from monocular video. In: CVPR (2016)",
            "ref_ids": [
                "28"
            ],
            "1": "0 { Zhou (CVPR'16) [28] 113."
        },
        "Vision-based human gestures recognition for human-robot interaction": {
            "authors": [],
            "url": "https://theses.hal.science/tel-02310606/file/MAZHAR_2019_archivage.pdf",
            "ref_texts": "[128] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 4966\u20134975, 2016.",
            "ref_ids": [
                "128"
            ],
            "1": "3D human pose estimation methods can also be categorized into two groups; one-stage approaches [114,126,127] which directly regress 3D pose from images and two-stage methods [122,128\u2013130] that first estimate 2D pose in the form of joint location confidence maps and then lift this 2D prediction to 3D pose either by a constraint deep regression strategy [131,132] or by matching the predictions with 2D projections of existing 3D poses from a database [129] or by fitting a 3D model on this 2D prediction [122,128]."
        },
        "Understanding the sources of error for 3D human pose estimation from monocular images and videos": {
            "authors": [
                "Johnny Canuck"
            ],
            "url": "https://open.library.ubc.ca/media/download/pdf/24/1.0361162/4",
            "ref_texts": "[134] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4966\u20134975, 2016. !pages 2, 17, 18, 24, 25, 44, 47, 49, 50, 65, 76, 77",
            "ref_ids": [
                "134"
            ],
            "1": "The 3D human pose is represented as a sparse combination of a set of basis poses which is learned separately [2, 92, 134].",
            "2": "Another group of work tries to exploit temporal consistency over multiple frames [4, 65, 81, 117, 134].",
            "3": "The 3D pose is then computed as a sparse linear combination of this over-complete dictionary [2, 132, 134].",
            "4": "Some work uses the 2D human pose from image and learns to back-project these 2D joint locations into 3D [2, 16, 62, 76, 90, 92, 122, 132, 134].",
            "5": "Others have tried to predict 3D pose from a sequence of images trying to exploit the temporal information from the sequence [4, 29, 74, 117, 134].",
            "6": "A common approach to estimating 3D joint locations given 2D pose is to separate the camera 23 pose variability from the intrinsic deformation of human body, the latter of which is modeled by learning an overcomplete dictionary of basis 3D poses from a large database of 3D human pose [2, 16, 92, 122, 132, 134].",
            "7": "The method is extended by same authors [134], where they imposed temporal smoothness constraint during optimization.",
            "8": "Some decoupled approaches [134] have used 2D probability distributions or 2D joint heatmaps from 2D pose estimators as inputs as inputs.",
            "9": "3D probabilities or volumetric heatmap of joints [87], 3D motion parameters [133] or coefficients of basis pose [2, 16, 92, 132, 134].",
            "10": "There are a number of works that have predicted 3D pose in camera coordinate frame [29, 64, 87, 117, 133, 134].",
            "11": "0 Zhou et al [134] (MA) 87.",
            "12": "3 Zhou et al [134]* (MA) 14j 99.",
            "13": "0 \u2013 Zhou et al [134]* 113.",
            "14": "0 Zhou et al [134] (MA) 87.",
            "15": "3 Zhou et al [134]* (MA) 14j 99.",
            "16": "!pages 2, 16, 18, 22, 44, 47, 49, 65, 76\n[134] X."
        },
        "Identifying people using temporal and spatial changes in local movements measured from body sway": {
            "authors": [],
            "url": "https://pattern.eecs.tottori-u.ac.jp/pdf/2017-ACPR-body-sway.pdf"
        },
        "Architectures d'apprentissage profond pour la reconnaissance d'actions humaines dans des s\u00e9quences vid\u00e9o RGB-D monoculaires: application \u00e0 la surveillance \u2026": {
            "authors": [],
            "url": "https://theses.hal.science/tel-02879316/document",
            "ref_texts": "833. Zhang, H. et al. (2014). \u201cReal-time action recognition based on a modified deep belief network model\u201d. In: IEEE International Conference on Information and Automation (ICIA), pp. 225\u2013228. Zhang, Jing et al. (2016). \u201cRGB-D-based action recognition datasets: A survey\u201d. In: Pattern Recognition 60, pp. 86\u2013105. Zhang, P . et al. (2019). \u201cView adaptive neural networks for high performance skeletonbased human action recognition\u201d. In: IEEE Transactions on Pattern Analysis and Machine Intelligence (TP AMI) 1, pp. 1\u20131. Zhang, Songyang, Xiaoming Liu, and Jun Xiao (2017). \u201cOn geometric features for skeleton-based action recognition using multilayer lstm networks\u201d. In: IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 148\u2013157. BIBLIOGRAPHY. 153 Zhang, Zhengyou (2012). \u201cMicrosoft Kinect sensor and its effect\u201d. In: IEEE Multimedia19, pp. 4\u201310. Zhao, R., H. Ali, and P . van der Smagt (2017). \u201cTwo-stream RNN/CNN for action recognition in 3D videos\u201d. In: 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 4260\u20134267. DOI:10.1109/IROS.2017.8206288. Zhou, X. et al. (2016). \u201cSparseness meets deepness: 3D human pose estimation from monocular video\u201d. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4966\u20134975. Zhu, Guangming et al. (2016a). \u201cAn online continuous human action recognition algorithm based on the Kinect sensor\u201d. In: Sensors 16.2, p. 161. Zhu, H., R. Vial, and S. Lu (2017). \u201cTORNADO: A spatio-temporal convolutional regression network for video action proposal\u201d. In: IEEE International Conference on Computer Vision (ICCV), pp. 5813\u20135821. Zhu, Wentao et al. (2016b). \u201cCo-occurrence feature learning for skeleton based action recognition using regularized deep LSTM networks\u201d. In: Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence. AAAI\u201916. Phoenix, Arizona: AAAI Press, pp. 3697\u20133703. URL:http://dl.acm.org/citation.cfm?id=3016387.",
            "ref_ids": [
                "833"
            ]
        },
        "Using unlabeled 3D motion examples for human activity understanding": {
            "authors": [
                "Ankur Gupta"
            ],
            "url": "https://open.library.ubc.ca/media/download/pdf/24/1.0305862/4",
            "ref_texts": "[ZZL+16] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Kosta Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In CVPR , 2016.! pages 90",
            "ref_ids": [
                "ZZL\\+16"
            ],
            "1": "Some of the past approaches deal with this problem by optimizing for 3d pose and updating the underlying 2d pose estimate in an alternating fashion [SSQTMN13, ZZL+16]."
        },
        "POSE-AWARE EMBEDDING NETWORKS AND MULTI-MODAL IMAGE-LANGUAGE RETRIEVAL": {
            "authors": [],
            "url": "https://scholar.archive.org/work/xyay4txzyjbldhdwvu4lwr4uam/access/wayback/https://s3.ca-central-1.amazonaws.com/pstorage-ryerson-5010877717/28129728/Curro_Domenico.pdf",
            "ref_texts": "[106] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceed104 ings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4966{4975, 2016.",
            "ref_ids": [
                "106"
            ],
            "1": "Geometric representations of human pose usually come in the form of human joint locations in 2D pixel coordinates [86, 9, 96, 64] or 3D metric coordinates [49, 50, 106].",
            "2": "The two most successful approaches to estimate 3D joint locations are: direct 3D joint inference [49, 50] from raw pixel data, and 3D joint inference from 2D joint annotations, referred to as \\lifting\" [106, 85].",
            "3": "Taking advantage of the previously generated 2D joint annotations, and an ofi-the-shelf state-of-the-art lifting model [106], an equivalent 3D joint annotation dataset is generated.",
            "4": "One major distinction from the 2D joint annotation generation process is that [106] operates spatiotemporally, and thus inherently takes advantage of the temporal component of the pose data.",
            "5": "6 MoCap (motion capture) [37] annotation dataset as an over-complete set of basis poses, [106] computes a linear combination to produce a new pose.",
            "6": "[106] X."
        },
        "Using Privileged Learning with Depth Data": {
            "authors": [
                "Shanxin Yuan",
                "Bjorn Stenger",
                "Kyun Kim"
            ],
            "url": "https://openaccess.thecvf.com/content_ICCVW_2019/papers/HANDS/Yuan_3D_Hand_Pose_Estimation_from_RGB_Using_Privileged_Learning_with_ICCVW_2019_paper.pdf"
        },
        "Context-Aware Robot Behavior Learning for Practical Human-Robot Interaction": {
            "authors": [],
            "url": "https://s-space.snu.ac.kr/bitstream/10371/168042/1/000000160611.pdf",
            "ref_texts": "[56] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis, \\Sparseness meets deepness: 3d human pose estimation from monocular video,\" in Proc. of the IEEE Conference on Computer Vision and Pattern Recognition , 2016, pp. 4966{4975.",
            "ref_ids": [
                "56"
            ],
            "1": "Extracted 2D poses are converted to 3D poses and used as our dataset [56].",
            "2": "Extracted 2D poses are converted to 3D poses and used as our dataset [56].",
            "3": "[56] X."
        },
        "Deep Learning based Human Pose Estimation": {
            "authors": [
                "Yang Li"
            ],
            "url": "https://opus.lib.uts.edu.au/bitstream/10453/149022/2/02Whole.pdf",
            "ref_texts": "[110] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis,\u201cSparseness meets deepness: 3d human pose estimation from monocular 104 video,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,2 0 1 6 ,p p .4 9 6 6 \u2013 4 9 7 5 .",
            "ref_ids": [
                "110"
            ],
            "1": "Besides, there are also works [65, 3, 110, 109] that considers temporalinformation from frame sequence to produce more robust predictions.",
            "2": "Besides,there are also works [65, 3, 110, 109] that considers temporal information from framesequence to produce more robust predictions.",
            "3": "Besides, there are also works [65, 3, 110, 109] that exploit temporal information invideos to generate more smooth results.",
            "4": "[110] X."
        },
        "Estimating Desk Work Status from Video Stream Using a Deep Neural Network": {
            "authors": [],
            "url": "https://scholar.archive.org/work/ii5pegazpffclhtr52ab7frlei/access/wayback/http://www.icicelb.org/ellb/contents/2020/7/elb-11-07-04.pdf",
            "ref_texts": "[9]X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis and K. Daniilidis, Sparseness meets deepness: 3D human pose estimation from monocular video, Proc. of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition , pp.4966-4975, 2016.",
            "ref_ids": [
                "9"
            ],
            "1": "[9]X."
        },
        "\u57fa\u4e8e L 1/2 \u6b63\u5219\u5316\u7684\u4e09\u7ef4\u4eba\u4f53\u59ff\u6001\u91cd\u6784": {
            "authors": [],
            "url": "http://www.aas.net.cn/fileZDHXB/journal/article/zdhxb/2018/6/PDF/zdhxb-44-6-1086.pdf",
            "ref_texts": "39Zhou X W, Zhu M L, Leonardos S, Derpanis K G, Daniilidis K. Sparseness meets deepness: 3D human pose estimation from monocular video. In: Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition. Las Vegas, NV, USA: IEEE, 2016. 4966 \u00a14975"
        },
        "Robust Video Object Tracking via Camera Self-calibration": {
            "authors": [],
            "url": "https://digital.lib.washington.edu/researchworks/bitstream/handle/1773/43951/Tang_washington_0250E_20501.pdf?sequence=1&isAllowed=y",
            "ref_texts": "[144] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G. Derpanis, and Kostas Daniilidis. Sparseness meets deepness: 3D human pose estimation from monocular video. In Proc. CVPR , pages 4966{4975, 2016.",
            "ref_ids": [
                "144"
            ],
            "1": "Two-stage approaches ffrst estimate 2D poses and then lift 2D poses to 3D poses[144, 12, 6, 131, 80, 120, 72, 145, 85].",
            "2": "[144] 114."
        },
        "Application of computers to circuit design for UNIVAC LARC": {
            "authors": [],
            "url": "https://dl.acm.org/doi/pdf/10.1145/1460690.1460710"
        },
        "Deep Neural Networks for Human Motion Analysis in Biomechanics Applications": {
            "authors": [],
            "url": "https://rucore.libraries.rutgers.edu/rutgers-lib/60894/PDF/1/play/",
            "ref_texts": "[136] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis, \"Sparseness meets deepness: 3D human pose estimation from monocular video,\" in Proceedings of the IEEE conference on computer vision and pattern recognition , 2016, pp. ",
            "ref_ids": [
                "136"
            ],
            "1": "9 \n[136] 87.",
            "2": "3 \n[136] 124.",
            "3": "[136] X."
        },
        "MONOCULAR RECONSTRUCTION OF DYNAMIC VEHICLES ON ARBITRARY ROAD PROFILES FROM A MOVING CAMERA": {
            "authors": [],
            "url": "https://junaidcs032.github.io/data/MyThesis_compressed.pdf",
            "ref_texts": "[53] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In CVPR , pages 4966\u20134975, 2016.",
            "ref_ids": [
                "53"
            ],
            "1": "[30, 31, 34, 19, 4, 53] use shape priors to estimate the shape and pose of the object.",
            "2": "Similar concept was used in [53] for 3D human pose estimation using a sequence of monocular images; here 3D pose is represented as a linear combination of predefined basis poses.",
            "3": "Shape priors have been widely used in many works [30, 31, 34, 19, 4, 53] to tackle the ill-posedness of the problem.",
            "4": "[53] X."
        },
        "Fast 3D Post Estimation of Human Based on Optical Flow and Particle Filter": {
            "authors": [],
            "url": "http://www.jsoftware.us/vol14/400-JSW15407.pdf",
            "ref_texts": "[20] Zhou, X., Zhu, M., Leonardos, S., Derpanis, K. G. , & Daniilidis, K. (2016). Sparseness meets deepness : 3D human pose estimation from monocular video . Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern R ecognition . ",
            "ref_ids": [
                "20"
            ],
            "1": "[20] 87.",
            "2": "[20] 124.",
            "3": "[20] 12 Martinez et al.",
            "4": "Note, several approa ches use vid eo for prediction rather than a single frame [20].",
            "5": "[20] 83 Martinez et al.",
            "6": "[20] Martinez et al.",
            "7": "edu/ \\#abs/2015arXiv151106692T \n[20] Zhou, X."
        },
        "Extraction de comportements reproductibles en avatar virtuel": {
            "authors": [
                "Kodjine Dare"
            ],
            "url": "https://papyrus.bib.umontreal.ca/xmlui/bitstream/handle/1866/26525/Dare_Kodjine_2021_Memoire.pdf?sequence=4",
            "ref_texts": ""
        },
        "Joint Representation of Multiple Geometric Priors via a Shape Decomposition Model for Single Monocular 3D Pose Estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1905.13466",
            "ref_texts": "[12] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, K. Daniilidis, Sparseness meets deepness: 3D human pose estimation from monocular video, in: Computer Vision and Pattern Recognition, 2016, pp. 4966{4975.",
            "ref_ids": [
                "12"
            ],
            "1": "Recently, the 3D pose estimation problem has been addressed by a hybrid approaches [12, 13].",
            "2": "For the coding of global structure part, based on the assumption that the 3D pose lies in a small subspace [10, 11, 12, 20], we use k\u0001k 1oncuto enforce the solution sparsity.",
            "3": "This is the standard evaluation protocol which is applied in most approaches [12, 8, 29, 19].",
            "4": "[12] 87.",
            "5": "[12] X."
        },
        "Supplementary Material for VRGym: A Virtual Testbed for Physical and Interactive AI": {
            "authors": [],
            "url": "https://xuxie1031.github.io/projects/VRGym/VRGymResource/sigai19xie_sppl.pdf",
            "ref_texts": "[60] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. 2016. Sparseness meets deepness: 3D human pose estimation from monocular video. In CVPR .",
            "ref_ids": [
                "60"
            ],
            "1": "Synthetic image datasets have recently been a source of training data for object detection and correspondence matching [8,11, 12,29,30,34,59], single-view reconstruction [17], view-point estimation [49], human pose estimation [10,31,41,45,46,52,57,60], depth prediction [48], pedestrian detection [16,28,32,53], action recognition [37,38,40], semantic segmentation[39], scene understanding [5,13,14,47], and in benchmark data sets [15,18]."
        },
        "Three-dimensional object reconstruction from a video": {
            "authors": [],
            "url": "https://patentimages.storage.googleapis.com/98/68/78/dee40b55114374/US11354847.pdf",
            "ref_texts": " Wen , C. , et al . , \u201c Pixel2mesh ++ : Multi view 3d mesh generation via deformation , \" In ICCV , 2019 . Wiles , O. , et al . , \u201c Silnet : Single and multi view reconstruction by learning from silhouettes , \" arXiv preprint arXiv : 1711.07888 , 2017 . Wu , S. , et al . , \u201c Unsupervised learning of probably symmetric deformable 3d objects from images in the wild , \u201d In CVPR , 2020 . Wu , Y. , et al . , \u201c Group normalization , \u201d In ECCV , 2018 . Yan , X. , et al . , Perspective transformer nets : Learning single view 3d object reconstruction without 3d supervision , In NeurlPS , 2016 . Zhou , X. , et al . , \u201c Sparseness meets deepness : 3d human pose estimation from monocular video , \u201d In CVPR , 2016 . Zhu , R. , et al . , \" Object centric photometric bundle adjustment with deep shape prior , \u201d In WACV , 2018 . Zhu , Y. , et al . , \u201c Complex non rigid motion 3d reconstruction by union of subspaces , \u201d In CVPR , 2014 . Zhang , J. Y. , et al . \u201c Predicting 3d human dynamics from video . \" ICCV . 2019 . Kanazawa , A. , et al . \u201c Learning 3d human dynamics from video . \" CVPR . 2019 . Loper , M. , et al . \u201c SMPL : A skinned multi person linear model . \u201d ACM transactions on graphics (TOG ) 2015 . Lin , Chen Hsuan , et al . \u201c Photometric mesh optimization for video aligned 3D object reconstruction . \u201d CVPR 2019 . Von Marcard , T. , et al . , \u201c Recovering accurate 3d human pose in the wild using imus and a moving camera , \u201d In ECCV , 2018 . Biggs , B. , et al . \u201c Creatures great and SMAL : Recovering the shape and motion of animals from video , \u201d Asian Conference on Computer Vision . Springer , Cham , 2018 . He , K. , et al . , \u201c Deep residual learning for image recognition , \" In "
        },
        "Variational and deep learning methods in computer vision": {
            "authors": [],
            "url": "https://burjcdigital.urjc.es/bitstream/handle/10115/16336/Tesis_Ivan_Online.pdf?sequence=1&isAllowed=y",
            "ref_texts": "[Zhou et al., 2016] Zhou, X., Zhu, M., Leonardos, S., Derpanis, K. G., and Daniilidis, K. (2016). Sparseness meets deepness: 3d human pose estimation from monocular video. In Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition , pages 4966\u20134975.",
            "ref_ids": [
                "Zhou et al\\., 2016"
            ]
        },
        "3D Reconstruction, Weakly-Supervised Learning, and Supervised Learning Methods for 3D Human Pose Estimation": {
            "authors": [],
            "url": "https://s-space.snu.ac.kr/bitstream/10371/152572/1/000000155042.pdf",
            "ref_texts": "[131] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 4966\u20134975, 2016.",
            "ref_ids": [
                "131"
            ],
            "1": "There are a few approaches that exploit temporal information using various methods such as overcomplete dictionaries [131, 132], 3D CNNs [41], sequence-to-sequence networks [88], and multiple-view settings [86].",
            "2": "[131] X."
        },
        "3D Human pose estimation on Taiji sequence": {
            "authors": [],
            "url": "https://etda.libraries.psu.edu/files/final_submissions/17625",
            "ref_texts": "[28]Zhou, X. ,M. Zhu,S. Leonardos ,K. G. Derpanis , and K. Daniilidis (2016) \u201cSparseness meets deepness: 3D human pose estimation from monocular video,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp. 4966\u20134975.",
            "ref_ids": [
                "28"
            ],
            "1": "Estimate 2D joint locations form the image first, and then estimate the 3D joint locations from the 2D joint locations [5,28,29].",
            "2": "[28] combine a sparse dictionary induced from the 2D heatmaps to compute a 3D skeleton by a EM algorithm; [25,30] use 3D pose data and its 2D projection to train a heat-map-to-3D pose network without the original image; Bogoet al.",
            "3": "[28]Zhou, X."
        },
        "The Design of Lil'Flo, an Affordable Socially Assistive Robot for Telepresence Rehabilitation": {
            "authors": [],
            "url": "https://www.resna.org/sites/default/files/conference/2018/pdf_versions/emerging_tech/Sobrepera.pdf",
            "ref_texts": "[1] Cans, C. (2000). Surveillance of cerebral palsy in Europe: a collaboration of cerebral palsy surveys and registers. Developmental Medicine & Child Neurology, 42(12), 816-824. [2] Feil-Seifer, D., & Mataric, M. J. (2005, June). Defining socially assistive robotics. In Rehabilitation Robotics, 2005. ICORR 2005. 9th International Conference on (pp. 465-468). IEEE. [3] Calderita, L. V., Manso, L. J., Bustos, P., Su\u00e1rez-Mej\u00edas, C., Fern\u00e1ndez, F., & Bandera, A. (2014). THERAPIST: Towards an autonomous socially interactive robot for motor and neurorehabilitation therapies for children. Journal of Medical Internet Research, 16(10), e1. https://doi.org/10.2196/rehab.3151 [4] Calderita, L. V., Bustos, P., Su\u00e1rez-Mej\u00edas, C., Fern\u00e1ndez, F., & Bandera, A. (2013). THERAPIST: Towards an autonomous socially interactive robot for motor and neurorehabilitation therapies for children. In Pervasive Computing Technologies for Healthcare (PervasiveHealth), 2013 7th International Conference on (pp. 374-377). IEEE. [5] Mej\u00edas, C. S., Echevarr\u00eda, C., Nu\u00f1ez, P., Manso, L., Bustos, P., Leal, S., & Parra, C. (2013). Ursus: A robotic assistant for training of children with motor impairments. Converging Clinical and Engineering Research on Neurorehabilitation, 1, 249-253. [6] Fridin, M., & Belokopytov, M. (2014). Robotics agent coacher for CP motor function (RAC CP Fun). Robotica, 32(8), 1265-1279. [7] M. Fridin, S. Bar-Haim, M. B. (2011). Robotics Agent Coacher for CP motor Function (RAC CP Fun). Workshop on Robotics for Neurology and Rehabilitation. [8] Wilk, R., & Johnson, M. J. (2014, August). Usability feedback of patients and therapists on a conceptual mobile service robot for inpatient and home-based stroke rehabilitation. In Biomedical Robotics and Biomechatronics (2014 5th IEEE RAS & EMBS International Conference on (pp. 438-443). IEEE. [9] Hogan, N. (1984). An organizing principle for a class of voluntary movements. Journal of Neuroscience, 4(11), 2745-2754. [10] Nelson, W. L. (1983). Physical principles for economies of skilled movements. Biological cybernetics, 46(2), 135-147. [11] Matthew, R. P., Kurillo, G., Han, J. J., & Bajcsy, R. (2014, September). Calculating Reachable Workspace Volume for Use in Quantitative Medicine. In ECCV Workshops (3) (pp. 570-583). [12] Rammer, J. R., Krzak, J. J., Riedel, S. A., & Harris, G. F. (2014, August). Evaluation of upper extremity movement characteristics during standardized pediatric functional assessment with a Kinect\u00ae-based markerless motion analysis system. In Engineering in Medicine and Biology Society (EMBC), 2014 36th Annual International Conference of the IEEE (pp. 2525-2528). IEEE. [13] Lott, C., & Johnson, M. J. (2016, August). Upper limb kinematics of adults with cerebral palsy on bilateral functional tasks. In 2016 IEEE 38th Annual International Conference of the Engineering in Medicine and Biology Society (EMBC) (pp. 5676-5679). IEEE. [14] Newell, A., Yang, K., & Deng, J. (2016, October). Stacked hourglass networks for human pose estimation. In European Conference on Computer Vision (pp. 483-499). Springer International Publishing. [15] Ren, S., He, K., Girshick, R., & Sun, J. (2017). Faster r-cnn: Towards real-time object detection with region proposal networks. IEEE transactions on pattern analysis and machine intelligence, 39(6), 1137-1149. [16] Cao, Z., Simon, T., Wei, S. E., & Sheikh, Y. (2016). Realtime multi-person 2d pose estimation using part affinity fields. arXiv preprint arXiv:1611.08050. [17] Zhou, X., Zhu, M., Leonardos, S., Derpanis, K. G., & Daniilidis, K. (2016). Sparseness meets deepness: 3D human pose estimation from monocular video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 4966-4975). ",
            "ref_ids": [
                "1",
                "2",
                "3",
                "4",
                "5",
                "6",
                "7",
                "8",
                "9",
                "10",
                "11",
                "12",
                "13",
                "14",
                "15",
                "16",
                "17"
            ],
            "1": "Sobrepera1,2, Michelle J.",
            "2": "Johnson1,3 1University of Pennsylvania, 2Department of Mechanical Engineering, 3Department of Rehabilitation Medicine INTRODUCTION Cerebral Palsy (CP) occurs in 2 to 3 per every 1000 live births, making it the most common motor disorder in young children [1].",
            "3": "To begin to address the need for an affordable quantitative diagnostic tool in pediatric rehabilitation which can be used both in-person and remotely, the UPenn Rehab Robotics lab is developing a low cost socially assistive robot (SAR) [2] (Lil\u2019Flo) to aid in remote, semi-autonomous, and fully autonomous assessment and treatment of upper extremity impaired pediatric patients.",
            "4": "There has been previous work in this space, most notably by the NAOTherapist (formerly Therapist, Ursus) project [3-5] and the RAC CP Fun project [6-7].",
            "5": "1) [8].",
            "6": "This idea is summarized in figure 2.",
            "7": "HARDWARE DESIGN Although robotic systems exist which could be used to achieve some of our goals, they fail to meet all our design requirements: 1) low cost for maximum impact 2) expressive face for social connectivity 3) easily modifiable hardware 4) mobile for remote deployments.",
            "8": "We currently have a mobile robot from VGo with a removable Aldebaran Nao Robot torso mounted on the robot, which has been used for initial validation work [8].",
            "9": "2 our low cost robotic platform is complete, it will replace the Nao/VGo system at an expected cost ratio of 1:5.",
            "10": "To enable remote telepresence and autonomous robot interactions with patients, as shown in figure 2, and to improve the objectivity of the diagnostic space in general, it is necessary to develop computational techniques for analyzing patient function with high repeatability on low cost hardware.",
            "11": "It is known that neurological damage alters UEx motion, making it less smooth, with lower maximum velocities, smaller ranges of motion, etc [9-10].",
            "12": "Normal Patient Clinician Interaction In PersonClinicianPatientEmotionInstructionDiagnostic InformationMotivationPatient \u2013 Clinician Robot InteractionInstructionDiagnosticInformationLil\u2019Flo (Robot)CommandsInstructionEmotionVideoMachinePerceptionClinicianPatientEmotionalInformationMotivation Figure 2: A comparison of interactions between patients and clinicians in person and patients, clinicians, and the robot in telepresence interactions.",
            "13": "Video General Clinical Measures: -Joint ROM: jx1 -Overall Score: 1x1 -Arm by Arm Score: 2x1 -etc\u2026 Engineered Measures Learned Measures Joint Data: jxt -Left Wrist Position: tx1 -Right Shoulder Position: tx1 -etc\u2026 Derived Data: -Joint Velocity: txj -Wrist Velocity: tx2 -Joint Angle: txj Figure 3: Block diagram of the perception system showing the progression from video input to general clinical measures being output to the clinician.",
            "14": "3 The advent of the Microsoft Kinect and related low cost RGBD sensors led to technology demonstrations capturing subject motion to determine kinematic measures such as velocity and range of motion [11] and even as grading tools for existing tests such as the SHUEE [12].",
            "15": "Our lab has performed work looking at precision contact measurement of wrist kinematics for diagnostics in adult stroke and CP patients [13].",
            "16": "More recent novel network architectures such as stacked hourglass networks [14], Faster R-CNN [15], and part affinity fields [16] have made pose estimation from video realizable.",
            "17": "Building on this work, progress has been made towards reconstruction 3D joint positions from 2D joint positions [17-18].",
            "18": "We are currently working to develop the pipeline for performing objective evaluations of upper extremity impaired pediatric patients as overviewed in figure 4.",
            "19": "The first step of the pipeline will be to extract 3D time series joint positions (TSJP) from RGB video.",
            "20": "In initial testing, we have used part affinity fields [16] and stacked hourglass networks [14] to extract the 2D joint locations of subjects doing a series of range of motion activities.",
            "21": "We have then leveraged [17] to estimate the full 3D pose of the subject.",
            "22": "The results can be seen in Figure 4.",
            "23": "To perform the necessary computations for the perception system a small computer (Intel NUC 7 i5 BNK) has been placed on the robot.",
            "24": "Figure 4: Data collected during a pilot test with a healthy subject, with the subject touching the back of their head.",
            "25": "Data was collected using a standard un calibrated cell phone camera and processed using a stacked hourglass network [14] followed by Zhou\u2019s method for 3D reconstruction [17] and cleaned with dropout rejection.",
            "26": "5 unit change) and z (.",
            "27": "6 unit change), show smooth movements with velocities approximating the bell curve predicted by Hogan [9].",
            "28": "The x axes which underwent much \n 4 REFERENCES.",
            "29": "[1] Cans, C.",
            "30": "(2000).",
            "31": "Developmental Medicine & Child Neurology, 42(12), 816-824.",
            "32": "[2] Feil-Seifer, D.",
            "33": "(2005, June).",
            "34": "In Rehabilitation Robotics, 2005.",
            "35": "ICORR 2005.",
            "36": "9th International Conference on (pp.",
            "37": "465-468).",
            "38": "[3] Calderita, L.",
            "39": "(2014).",
            "40": "Journal of Medical Internet Research, 16(10), e1.",
            "41": "org/10.",
            "42": "2196/rehab.",
            "43": "3151 [4] Calderita, L.",
            "44": "(2013).",
            "45": "In Pervasive Computing Technologies for Healthcare (PervasiveHealth), 2013 7th International Conference on (pp.",
            "46": "374-377).",
            "47": "[5] Mej\u00edas, C.",
            "48": "(2013).",
            "49": "Converging Clinical and Engineering Research on Neurorehabilitation, 1, 249-253.",
            "50": "[6] Fridin, M.",
            "51": "(2014).",
            "52": "Robotica, 32(8), 1265-1279.",
            "53": "[7] M.",
            "54": "(2011).",
            "55": "[8] Wilk, R.",
            "56": "(2014, August).",
            "57": "In Biomedical Robotics and Biomechatronics (2014 5th IEEE RAS & EMBS International Conference on (pp.",
            "58": "438-443).",
            "59": "[9] Hogan, N.",
            "60": "(1984).",
            "61": "Journal of Neuroscience, 4(11), 2745-2754.",
            "62": "[10] Nelson, W.",
            "63": "(1983).",
            "64": "Biological cybernetics, 46(2), 135-147.",
            "65": "[11] Matthew, R.",
            "66": "(2014, September).",
            "67": "In ECCV Workshops (3) (pp.",
            "68": "570-583).",
            "69": "[12] Rammer, J.",
            "70": "(2014, August).",
            "71": "In Engineering in Medicine and Biology Society (EMBC), 2014 36th Annual International Conference of the IEEE (pp.",
            "72": "2525-2528).",
            "73": "[13] Lott, C.",
            "74": "(2016, August).",
            "75": "In 2016 IEEE 38th Annual International Conference of the Engineering in Medicine and Biology Society (EMBC) (pp.",
            "76": "5676-5679).",
            "77": "[14] Newell, A.",
            "78": "(2016, October).",
            "79": "483-499).",
            "80": "[15] Ren, S.",
            "81": "(2017).",
            "82": "IEEE transactions on pattern analysis and machine intelligence, 39(6), 1137-1149.",
            "83": "[16] Cao, Z.",
            "84": "(2016).",
            "85": "Realtime multi-person 2d pose estimation using part affinity fields.",
            "86": "arXiv preprint arXiv:1611.",
            "87": "08050.",
            "88": "[17] Zhou, X.",
            "89": "(2016).",
            "90": "Sparseness meets deepness: 3D human pose estimation from monocular video.",
            "91": "4966-4975).",
            "92": "5 [18] Bogo, F.",
            "93": "(2016, October).",
            "94": "Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image.",
            "95": "561-578)."
        },
        "3D human pose estimation using part affinity field": {
            "authors": [],
            "url": "https://core.ac.uk/download/pdf/158324790.pdf",
            "ref_texts": "[27] Zhou, Xiaowei, et al. \"Sparseness meets deepness: 3D human pose estimation from monocular video.\" Proceedings of the IEEE conference on computer vision and pattern recognition . 2016. ",
            "ref_ids": [
                "27"
            ],
            "1": "[27] 87.",
            "2": "[27] 124.",
            "3": "[27] Zhou, Xiaowei, et al."
        },
        "Field Studies with Multimedia Big Data: Opportunities and Challenges (Extended Version)": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1712.09915",
            "ref_texts": "[147] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G. Derpanis, and Kostas Daniilidis. 2016. Sparseness Meets Deepness: 3D Human Pose Estimation From Monocular Video. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . M. M. Krell, J. Bernd, et al. Field Studies with Multimedia Big Data // ICSI TR-17-002 23",
            "ref_ids": [
                "147"
            ],
            "1": "If so, this work can be extended to video [147], in combination with work on motion trajectories already being done with YFCC100M videos [18].",
            "2": "Within our framework, 3D human pose estimation would have to be applied to single keyframe images from a video [11, 24, 143, 147]."
        },
        "3D hand pose estimation using convolutional neural networks": {
            "authors": [],
            "url": "https://spiral.imperial.ac.uk/bitstream/10044/1/68489/1/Ye-Q-2019-PhD-thesis.pdf",
            "ref_texts": "128 BIBLIOGRAPHY. X. Zhou, Q. Wan, W. Zhang, X. Xue, and Y. Wei. Model-based deep hand pose estimation. In International Joint Conferences on Artiffcial Intelligence, 2016b. X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In Computer Vision and Pattern Recognition, 2016c. J.-Y. Zhu, R. Zhang, D. Pathak, T. Darrell, A. A. Efros, O. Wang, and E. Shechtman. Toward multimodal image-to-image translation. In Advances in Neural Information Processing Systems, 2017. S. Zhu, C. Li, C. Change Loy, and X. Tang. Face alignment by coarse-to-ffne shape searching. In Computer Vision and Pattern Recognition, 2015a. X. Zhu and D. Ramanan. Face detection, pose estimation, and landmark localization in the wild. In Computer Vision and Pattern Recognition, 2012. X. Zhu, Z. Lei, J. Yan, D. Yi, and S. Z. Li. High-ffdelity pose and expression normalization for face recognition in the wild. In Computer Vision and Pattern Recognition, 2015b. Y. Zhu, B. Dariush, and K. Fujimura. Controlled human pose estimation from depth image streams. In Computer Vision and Pattern Recognition Workshops, 2008."
        },
        "Fine-Grained Object Recognition Under Limited Training Data": {
            "authors": [],
            "url": "https://ir.library.oregonstate.edu/downloads/pg15bm18b",
            "ref_texts": "[179] Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis. Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4966\u20134975, 2016.",
            "ref_ids": [
                "179"
            ],
            "1": "2 Zhou 2016 [179] 113.",
            "2": "6M by Protocol 2 (MPJPE) Method Direction Discuss Eat Greet Phone Pose Purchase Sit Zhou 2016 [179] 87.",
            "3": "22 Method SitDown Smoke Photo Wait Walk WalkDog WalkPair Avg Zhou 2016 [179] 199.",
            "4": "In [179, 146, 91], the entire dataset is partitioned into five training subjects (S1, S5, S6, S7, S8) and two test subjects (S9, S11).",
            "5": "The setup of [179] downsamples the videos from 50 fps to 10 fps.",
            "6": "6M, we follow the set up of [179, 146, 91], where the entire dataset is partitioned into five training subjects (S1, S5, S6, S7, S8) and two test subjects (S9, S11).",
            "7": "The setup of [179] also downsamples the videos from 50 fps to 10 fps.",
            "8": "79 Zhou 2016 [179] 87.",
            "9": "39 Zhou 2016 [179] 199."
        },
        "Image-based Human Pose Estimation": {
            "authors": [],
            "url": "https://core.ac.uk/download/pdf/195277865.pdf"
        },
        "Objective Vision-Based Assessment of Parkinsonism and Levodopa-Induced Dyskinesia in Persons with Parkinson's Disease": {
            "authors": [],
            "url": "https://dam-oclc.bac-lac.gc.ca/download?is_thesis=1&oclc_number=1334672476&id=81456bad-afee-4474-9cbd-92d7ddf80da7&fileName=Li_Michael_H_201706_MAS_thesis.pdf",
            "ref_texts": "[113] X. Zhou, M. Zhu, S. Leonardos, K. Derpanis, and K. Daniilidis, \u201cSparseness M eets Deepness: 3D Human Pose Estimation from Monocular Video,\u201d ArXiv151109439 Cs , Nov. 2015. ",
            "ref_ids": [
                "113"
            ],
            "1": "There have also been recent papers on producing a 3D skeleton from a 2D video [112], [113] .",
            "2": "[113] X."
        },
        "Spatial-Temporal Hierarchical Model for Joint Learning and Inference of Human Action and Pose": {
            "authors": [],
            "url": "https://escholarship.org/content/qt5vr30269/qt5vr30269.pdf",
            "ref_texts": "[ZZL16] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. \\Sparseness meets deepness: 3d human pose estimation from monocular video.\" In CVPR , 2016.",
            "ref_ids": [
                "ZZL16"
            ],
            "1": "2D human pose estimation, the 3D pose estimation from monocular image using deep networks [LC14, LZC15, RS16, YIK16, ZZL16] have received lots of attentions recently.",
            "2": "Some methods [YIK16, ZZL16] use two difierent data sources for training 2D pose estimator and 3D pose predictor.",
            "3": "[ZZL16] predict 3D poses from a video sequence by using temporal information.",
            "4": "We also report results for protocol 2 (P2) which is employed in [ZZL16, TRL16, RS16].",
            "5": "Our method clearly outperforms the second best result [ZZL16] by 13.",
            "6": "3 Zhou[ZZL16] 87.",
            "7": "[ZZL16] X."
        },
        "StayCentered-Methodenbasis eines Assistenzsystems f\u00fcr Centerlotsen (MACeLot): Schlussbericht": {
            "authors": [],
            "url": "https://core.ac.uk/download/pdf/322851384.pdf",
            "ref_texts": "[176] Zhou, X., Zhu, M., Leonardos, S., Derpanis, K., Daniilidis, K., 2015. Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video. arXiv:1511.09439 [cs]. ",
            "ref_ids": [
                "176",
                "cs"
            ],
            "1": "Zur Erkennung der K\u00f6rperpose sind mit tlerweile vielversprechende Algorithmen verf\u00fcgbar [154] [176] .",
            "2": "06550 [cs] .",
            "3": "01497 [cs] .",
            "4": "2007 \n[176] Zhou, X.",
            "5": "09439 [cs]."
        },
        "RGB-D \uc815\ubcf4\ub97c \uc774\uc6a9\ud55c 2 \ucc28\uc6d0 \ud0a4\ud3ec\uc778\ud2b8 \ud0d0\uc9c0 \uae30\ubc18 3 \ucc28\uc6d0 \uc778\uac04 \uc790\uc138 \ucd94\uc815 \ubc29\ubc95.": {
            "authors": [],
            "url": "https://www.jics.or.kr/digital-library/manuscript/file/15446/05-%EB%B0%95%EC%84%9C%ED%9D%AC.pdf",
            "ref_texts": ""
        },
        "3D \u5361\u901a\u89d2\u8272\u7684\u5feb\u901f\u9aa8\u67b6\u63d0\u53d6\u7b97\u6cd5": {
            "authors": [],
            "url": "http://tsg.humenfz.com/upfile/201904/2019040157703053.pdf"
        },
        "\u4eba\u4f53\u59ff\u52e2\u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3\u56f0\u96e3\u306a\u6620\u50cf\u306b\u304a\u3051\u308b\u985e\u4f3c\u59ff\u52e2\u5b66\u7fd2\u306e\u6709\u7528\u6027": {
            "authors": [],
            "url": "https://ipsj.ixsq.nii.ac.jp/ej/?action=repository_action_common_download&item_id=187475&item_no=1&attribute_id=1&file_no=1",
            "ref_texts": "[19] Zhou, X., Zhu, M., Leonardos, S., Derpanis, K. G. and Daniilidis, K. Sparseness meets deepness: 3D human pose estimation from monocular video, CVPR (2016).",
            "ref_ids": [
                "19"
            ],
            "1": "\u0680\u075a\n\u03b5\u03d9\u0294\u03c0\u038d\u04a9\u0f8d\u0ee8\u0c24\u0377\u04e1\u0c88\u0572\u0a73\u027c\u03ae\u0294\u03dc\u0373\u0372\u0377\u03a4\u03ef\u03bb\u0294\n\u0362\u0394\n\u035e\u0395\u036f\n\u0378\u027c\u0cd6\u0f97\u0371\u0373\u0394\u0568\u0afe\u0377\n\u091b\u0fa8\u0374\u0391\u036c\u036f 2\u091b\u0fa8\u0374\u0dfc\u035a\u0394\u035c\u0371\u0355\u0370\u0356\u0394\u027d RGBD\u0568\u0afe\n\u0371\u027cRGB\u0568\u0afe\u0370\u034b\u0394\u027d RGBD\u0568\u0afe\u0370\u0378\u027c RGB\u0568\u0afe\u0374\u0d7a\u0382\u027c \u0a02\u0c53\u09d8\u0e43 \u02a2 Depth\u02a3 \u039b\n\u0f62\u0f3b\u0370\u0356\u0394\u0dfc\u027c \u03e6\u039e\u03e7\u03bb\u03a0\u03dc\u0377 3\u0a4e\u0a2a\u0c06\u0ef0\u0b4a\u039b\u0b6f\u096d\u053d\n\u0370\u0356\u0394 [1]\u027d\u0360\u0354\u0360\u027c\u0a02\u0c53\u09d8\u0e43\u039b\u0c98\u0394\u0374\u0378 Kinect[2], [3], [4]\n\u0374\u0b45\u0daf\u035e\u0395\u0394 RGBD\u0a4e\u0a2a\n\u0c06\u03de\u03c3\u03e7\u039b\u0c26\u0520\u0362\u0394\u0b30\u09c5\u038b RGBD\u0392\u0395\u036f\u0360\u0387\u034f\u027d\n\u036d RGBD\u0568\u0afe\u0371\u0d7a\u0382\u027c RGB\u0568\u0afe\u0354\u0392\u0377\n\u0b18\u0377\u0b47\u0f94\u0377\u0568\u0afe\u03c3\u0294\u03bb\u02a2 Youtube, Netix \u0b34\u035e\u0395\n\u036f\u034d\u0394\u027dRGB\u0568\u0afe\u0354\u0392\u0377 2\u0374\u0378\u027c\u03ac\u03e5\n\u0a4e\u0a2a\u0c06\u0916\u0e4f [5], [6], [7] \u0355\n\u0b04\u0371\u0373\u0394\n\u0f70\u0373\u0a2a\u0c06\u0355\u0544\u0cf3\u0370\u034b\u036c\u0368\u027d\u0360\u0354\u0360\u027c\u0fa1\u0a80\u0360\u036f\u034d\n\u0370\u0356\u036f\u034d\u0373\u0354\u036c\u0368\u027d\u035c\u0377\u0ef0\n\u048a\u035e\u0395\u036f\u0356\u0368\u027d\u0fab\u0351\n\u0b04\u039b\u0914\u036c\u0368\u03de\u03c3\u03e7 [8]\u0f70\n\u0f97\n\u0377\u03a4\u03bf\u03b4\u039b\u0aff\u038d\u0362\u035c\u0371\u0374\n\u0b04\u0377\u038b\u0377\u0355\u034b\u0394 [9]\u027d\u035c\u0377\u03de\n\u0370\u0356\u0394\u038b\u0377\u0377\u027c\u03ac\n\u0351\u0363\u027c\u0366\n\u054c\u0370\u034b\u0394\u027d\n\u035c\u0377\u0391\u034f\u0374\u027c\u03ac\u03e5\u03d1\u039f\u03a7\u03e7\u03de\u03c3\u03e7\u03d5\u0294\u03b5\u0377\u0916\u0e4f\u0374\u0378\u0fa1\u0a80\n\u054c\n\u0360\u036f\u034d\u0394 2\u0a4e\u0a2a\u0c06\u0916\u0e4f\u0377\u0384\u0371\u039c\u0372\u0355\u027c CNN\u03d5\u0294\n\u03b5\u0377\u0916\u0e4f\u0370\u034b\u0394\u027d CNN\u038b\n\u0377\u0914\u0393\u0aca\u0388 [10]\u0370\u0378\u027c CNN\u0a4e\u039b\n\u0a2a\u0c06\u0360\u036f\u034d\u0394\u027d CNN\u0374\u0378\u03ac\u03e5\u03d1\u039f\u03a7\n\u03e7\u03de\u03c3\u03e7\u0371 CNN\u0398\u0364\u0368\u038b\u0377\u038b\u034b\u0393\u027c\u0de6\u0dfc\u0c24\u0373\u03a9\u0294\n\u03d9\u03a0\u03ef\u03c4\u0377\u0568\u0afe\u039b\u027c\u0fa1\u0a80\u0362\u0394\u03a9\u0294\u03d9\u03a0\u03ef\u03c4\u0490\u0b94\u0377\u0a2a\u0c06\u0374\u0efe\n\u0f71\u036f\u036f\u034d\u0394 [11]\u07f1\n\u03772\u039b\u0f29\u0351\u0368\u0377\u0355\u027c\u03ce\u0294\u03c4\n\u03da\u03bf\u03d3\u0371 CNN\u0398\u0364\u0368\u0916\u0e4f\u0370\u034b\u0394 [12], [13], [14] \u027d\n\u0b45\u0daf\u0c24\u0373\u038b\u0377\u0378\u027c\u0b1f\u0b88\u058a\u0377\u0916\u0e4f [15]\u0374CNN\u039b\u0f62\u0f3b\u0362\u0394\u035c\n\u039b\u0b0a\u0351\u0394\u035c\u0371\u039b\u0544\u0cf3\u0374\u0360\n\u0368[14]\u027d\u035c\u0377\u0391\u034f\u0374\u027c CNN\u039b\u0f62\u0f3b\u0360\u0368 2\u0a4e\u0a2a\u0c06\u0916\u0e4f\n\u09fa\u0378\u02ae\u0df3\u0a3a\u0a13\u0374\u0b30\u0362\u0394 2\u0769\u08cd\n\u0355\u036c\u036f\u034d\u0394 [16], [17], [18] \u027d RGB\u0568\u0afe\u0354\u0392\u0377 3\u0a4e\u0a2a\u0c06\u0370\u038b CNN\u039b\u0f3b\u034d\u0368\u0916\u0e4f\n\u09cd\u035e\u0364\u036f\u034d\u0394 [19], [20], [21], [22] \u027d\n\u0360\u036f\n\u034d\u0394\u038b\u0377 [23], [24] \u038d\u027c\u03e6\u039e\u03e7\u03bb\u03a0\u03dc 3\u0a4e\u0a2a\u0c06 [25]\n\u0398\u0395\u036f\u034d\u0394\u027d\u04b0\u0e4d\u0370\u027c 3\u0a4e\u0a2a\u0c06\u0378 2\u0769\u08cd\n\u0a4e\u0a2a\u0c06\u0371\u0d7a\u0382\u0394\u0371\u034d\u0387\u0369\u0374\u0cc9\u0360\u034d\u03bb\u03b5\u03ab\u0370\u034b\u0393\u0b13\u035a\u036f\u034d\n\u0394\u027d2\u0a4e\u0355\u0c98\u0392\u0395\u0368\u0371\u0360\u036f\u038b\u027c 3\u0c24\u0374\n\u0362\u0394\u0354\u0392\u0370\n\u0936\u03c3\u0294\u03bb\u0355\u0947\u0dfc\u0370\u0373\u034d\u035c\u0371\u038b\u027c 3\u089f\u0769\u08cd\n\u0a4e\u0a2a\u0c06\u039b\u0cc9\u0360\u034d\u03bb\u03b5\u03ab\u0374\u0360\u036f\u034d\u0394\u027d 2\u0a4e\u0a2a\u0c06\u0374\u0353\u035a\n\u0377\u0b47\u0356\u0373\u0f41\u04bc\u0377\u04b0\u036d\u0378\u0b47\u0f94\u0377\u03c3\u0294\u03bb\u03b7\u03bf\n\u03c4[26], [27]\u0e2d\u0370\u0356\u0368\u035c\u0371\u0370\u034b\u0393\u027c\u0f94\u0371\u091b\u0fa8\u0378 3\u089f\u0769\u08cd\n\u0a4e\u0a2a\u0c06\u0f3b\u0377\u038b\u0377 [28], [29] \u0371\u0d7a\u0382\u0394\u0371\u0d87\u09d7\u0374\u0b1f\u034d\u027d CG\u095b\n\u0394\u0c88\u0356 [30], [31] \u038b\u034b\u0394\u0355\u027c\u035c\u0395\n\u0370\n\u039c\n\u0377\u0a13\u0dfa\u0377\u0568\u0afe\u03c3\u0294\u03bb\u03b7\u03bf\u03c4 [32]\u0355\u094f\u0f41\u0374\u0373\u0394\u027d c\u20dd2018 Information Processing Society of Japan 2Vol.",
            "2": "[19] Zhou, X."
        },
        "Estimation de pose humaine et reconnaissance d'action par un syst\u00e8me multi-robots": {
            "authors": [],
            "url": "https://theses.hal.science/tel-01921842/file/these.pdf",
            "ref_texts": "[357] Zhou, X., Zhu, M., Leonardos, S., Derpanis, K., and Daniilidis, K. (2015). Sparseness meets deepness: 3d human pose estimation from monocular video. arXiv preprint arXiv:1511.09439.",
            "ref_ids": [
                "357"
            ],
            "1": "Another convolutional neural networkis trained to infer 3D human pose from uncertainty maps of 2D joint estimates [357].",
            "2": "Alternative ways of inferring 3D pose from a collection of 2D poses are investigatedfrequently, using kinematic jump processes [273], convolutional neural networks [357], implicit mixture of Conditional Restricted Boltzmann Machines [288] or supervised spectral embedding [347].",
            "3": "[357] Zhou, X."
        },
        "Apprentissage automatique pour la reconnaissance d'action humaine et l'estimation de pose \u00e0 partir de l'information 3D": {
            "authors": [
                "Diogo Luvizon"
            ],
            "url": "https://www.theses.fr/2019CERG1015.pdf",
            "ref_texts": "[172] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and K. Daniilidis. Sparseness meets deepness: 3d human pose estimation from monocular video. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 2016. 9",
            "ref_ids": [
                "172"
            ],
            "1": "3 Monocular 3D Human Pose Estimation Estimating the human body joints in 3D coordinates from monocular RGB images is a very challenging problem with a vast bibliography available in the literature [56, 107, 38, 134, 74, 73, 55, 172].",
            "2": "[172] X."
        }
    }
}