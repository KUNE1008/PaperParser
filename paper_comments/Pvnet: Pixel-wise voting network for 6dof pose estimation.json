{
    "title": "Pvnet: Pixel-wise voting network for 6dof pose estimation",
    "id": 1,
    "valid_pdf_number": "446/549",
    "matched_pdf_number": "372/446",
    "matched_rate": 0.8340807174887892,
    "citations": {
        "Vision-based robotic grasping from object localization, object pose estimation to grasp estimation for parallel grippers: a review": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1905.06658",
            "ref_texts": "[Peng et al. , 2019 ]Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "Peng et al\\. , 2019 "
            ]
        },
        "A review on object pose recovery: From 3D bounding box detectors to full 6D pose estimators": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2001.10609",
            "ref_texts": "[160] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation , In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 4561-4570), 2019.",
            "ref_ids": [
                "160"
            ],
            "1": "6D object pose estimators [40], [38], [163], [167], [168], [159], [160], [31], [32], [4], [35], [161] extract features from the input images, and using the trained regressor, estimate objects\u2019 6D pose.",
            "2": "Several methods further refine the output of the trained regressors [101], [83], [79], [82], [108], [40], [38], [163], [167], [168], [159], [160], [31], [32], [4], [35], [161] (refinement block), and finally hypothesize the object pose after filtering.",
            "3": "[159] RGB 7 R & Sfpproj 3Dig8 i=1 L1 CNN EPnP 7 instance PVNet [160] RGB 7 R & Sfpproj 3Dig8 i=1 L1s CNN EPnP & LMA 7 instance IHF[31] Depth 7 S x;\u0002 offset & pose entr HF co-tra 7 instance Sahin et al.",
            "4": "PVNet [160] estimates full 6D poses of the objects of interest under severe occlusion or truncation.",
            "5": "[163], [167], [168], [159], [160] use both real and synthetic images to train.",
            "6": "The 2D-driven 3D methods and the 3D BB detectors work at the level of categories, and the6D methods [40], [38], [163], [167], [168], [159], [160], [31], [32], [4], [35], [161] work at instance-level.",
            "7": "[160] S."
        },
        "Cosypose: Consistent multi-view multi-object 6d pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2008.08465",
            "ref_texts": "5. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (2019) 4561{4570",
            "ref_ids": [
                "5"
            ],
            "1": "A convolutional neural network (CNN) can be used to detect object features in 2D [4,6,18,21,22] or to directly ffnd 2D-to-3D correspondences [5,7,8,23].",
            "2": "1 PVNet [5] 73.",
            "3": "Following [5,10,18], we evaluate on a subset of 2949 keyframes from videos of the 12 testing scenes.",
            "4": "When testing, we follow previous works [5,10,18] and evaluate on a subset of 2949 keyframes."
        },
        "Gdr-net: Geometry-guided direct regression network for monocular 6d object pose estimation": {
            "authors": [
                "Gu Wang",
                "Fabian Manhardt",
                "Federico Tombari",
                "Xiangyang Ji"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_GDR-Net_Geometry-Guided_Direct_Regression_Network_for_Monocular_6D_Object_Pose_CVPR_2021_paper.pdf",
            "ref_texts": "[40] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise V oting Network for 6DoF Pose Estimation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4561\u20134570, 2019. 2,5, 7,8",
            "ref_ids": [
                "40"
            ],
            "1": "To enhance the robustness, [20] and [40] additionally conduct segmentation coupled with voting for each correspondence.",
            "2": "Similarly, we render 10k synthetic images (syn) for each object as in [40].",
            "3": "4During training, we 4We follow the most commonly used evaluation protocol for LM-O and YCB-V , which has also been employed by another learned P nP [19] and many other works such as [60,40,47,62,27,25].",
            "4": "16617\n Methodw/o Refinement w/ Refinement PoseCNN [60] PVNet [40] Single-Stage [19] HybridPose [47] GDR-Net (Ours ) DPOD [62] DeepIM [27] P.",
            "5": "0 PVNet [40] N 73."
        },
        "Pvn3d: A deep point-wise 3d keypoints voting network for 6dof pose estimation": {
            "authors": [
                "Yisheng He",
                "Wei Sun",
                "Haibin Huang",
                "Jianran Liu",
                "Haoqiang Fan",
                "Jian Sun"
            ],
            "url": "https://openaccess.thecvf.com/content_CVPR_2020/papers/He_PVN3D_A_Deep_Point-Wise_3D_Keypoints_Voting_Network_for_6DoF_CVPR_2020_paper.pdf",
            "ref_texts": "[37] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019. 1,2,4,5,6,7",
            "ref_ids": [
                "37"
            ],
            "1": "However, these methods usually had poor generalization due to the nonlinearity of the rotation space explained by [37].",
            "2": "Instead, recent works utilized DNNs to detect 2D keypoints of an object, and computed 6D pose parameters with Perspectiven-Point (PnP) algorithms [37,36,41,47].",
            "3": "To better deal with truncated and occluded scenes, [37] proposes a pixel-wise voting network to vote for the 2D keypoints location.",
            "4": "PVNet [37] uses per-pixel voting for 2D Keypoints to combine the advantages of Dense methods and keypoint-based methods.",
            "5": "Therefore, we follow [37] and use the farthest point sampling (FPS) algorithm to select keypoints on the mesh.",
            "6": "Also, we follow [37] and add synthesis images into our training set.",
            "7": "4%\n11636\n RGB RGBD PoseCNN DeepIM\n[26,52]PVNet [37]CDPN\n[27]Implicit ICP[45]SSD-6D ICP[22]PointFusion[50]DF(perpixel)[50]DF(iterative)[50]PVN3D ape 77.",
            "8": "DF(RT)[50] DF(3D KP)[50] Ours(RT) Ours(2D KPC) Ours(2D KP) PVNet[37] Ours(Corr) Ours(3D KP) ADD-S 92.",
            "9": "Note that other existing 2D keypoints detection approaches, such as heatmap [33,24,34] and vector voting [37] models may also suffer from overlapped keypoints.",
            "10": "1\n[37] S."
        },
        "Ffb6d: A full flow bidirectional fusion network for 6d pose estimation": {
            "authors": [
                "Yisheng He",
                "Haibin Huang",
                "Haoqiang Fan",
                "Qifeng Chen",
                "Jian Sun"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2021/papers/He_FFB6D_A_Full_Flow_Bidirectional_Fusion_Network_for_6D_Pose_CVPR_2021_paper.pdf",
            "ref_texts": "[46] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019. 1, 2,5,7",
            "ref_ids": [
                "46"
            ],
            "1": "Recently, the dramatic growth of deep learning techniques motivates several works to tackle this problem using convolution neural networks (CNNs) on RGB images [68,46,70,34].",
            "2": "Instead, 2Dkeypoint-based [53,52,42,29,43,46,72,38] detect 2D keypoints of objects to build the 2D-3D correspondence for pose estimation.",
            "3": "Keypoint selection Previous works [46,17] select keypoints from the target object surface using the Farthest Point Sampling (FPS) algorithm.",
            "4": "In this way, the selected keypoints spread on the object surface and stabilize the following pose estimation procedure [46,17].",
            "5": "We split the training and testing set following previous works [68,46] and generate synthesis images for training following [46,17].",
            "6": "1d) as in [20,46].",
            "7": "Qualitative results are reported in 3008\n RGB RGB-D PoseCNN DeepIM\n[68,33]PVNet[46] CDPN[34] DPOD[70] PointFusion[69]DenseFusion[65]G2LNet[7]PVN3D[17] Our FFB6D MEAN 88.",
            "8": "[26]Pix2Pose [45]PVNet [46] ADD-0."
        },
        "Grnet: Gridding residual network for dense point cloud completion": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2006.03761",
            "ref_texts": "24. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: CVPR 2019. (2019) 3",
            "ref_ids": [
                "24"
            ],
            "1": "However, the projection requires extrinsic camera parameters, which are challenging to estimate in most scenarios [24]."
        },
        "Deep snake for real-time instance segmentation": {
            "authors": [
                "Sida Peng",
                "Wen Jiang",
                "Huaijin Pi",
                "Xiuli Li",
                "Hujun Bao",
                "Xiaowei Zhou"
            ],
            "url": "https://openaccess.thecvf.com/content_CVPR_2020/papers/Peng_Deep_Snake_for_Real-Time_Instance_Segmentation_CVPR_2020_paper.pdf",
            "ref_texts": "[33] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 3",
            "ref_ids": [
                "33"
            ],
            "1": "An alternative method is to use standard CNNs to regress a pixel-wise vector field from the input image to guide the evolution of the initial contour [37,33,40]."
        },
        "Pointdsc: Robust point cloud registration using deep spatial consistency": {
            "authors": [
                "Xuyang Bai",
                "Zixin Luo",
                "Lei Zhou",
                "Hongkai Chen",
                "Lei Li",
                "Zeyu Hu",
                "Hongbo Fu",
                "Lan Tai"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2021/papers/Bai_PointDSC_Robust_Point_Cloud_Registration_Using_Deep_Spatial_Consistency_CVPR_2021_paper.pdf",
            "ref_texts": ""
        },
        "Dpod: 6d pose object detector and refiner": {
            "authors": [
                "Sergey Zakharov",
                "Ivan Shugurov",
                "Slobodan Ilic"
            ],
            "url": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zakharov_DPOD_6D_Pose_Object_Detector_and_Refiner_ICCV_2019_paper.pdf",
            "ref_texts": "[25] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "25"
            ],
            "1": "Recent deep learning-based approaches, such as SSD6D [15], YOLO6D [33], AAE [31], PoseCNN [34] and PVNet [25], are the current top performers for this task in RGB images.",
            "2": "The majority is trained on real data [33, 34, 25, 14] while only SSD6D [15] and AAE [31] are trained on syn1941\n thetic renderings.",
            "3": "Here we review the following ones: SSD6D [15], YOLO6D [33], BB8 [26], iPose [14], AAE [31], PoseCNN [34] and PVNet [25].",
            "4": "Among the methods that are specifically designed to be robust to occlusions we would like to highlight iPose [14], 1942\n PoseCNN [34], and PVNet [25].",
            "5": "PVNet [25] takes a different approach and designs a network which for every pixel in the image regresses an offset to some predefined keypoints.",
            "6": ", BB8 [26], YOLO6D [33], PVNet [25], use the training split of the real dataset.",
            "7": "Train data Synthetic + Refinement Real + Refinement Object SSD6D [15] AAE [31] Ours SSD6D [22] Ours YOLO6D [33] PoseCNN [34] PVNet [25] Ours DeepIM [18] Ours Ape 2.",
            "8": "Analogously to other related papers [33, 15, 25, 34], we measure the accuracy of pose estimation using theADD score [12].",
            "9": "If trained on real data, our method is the second best after [25].",
            "10": "MethodYOLO6D\n[33]PoseCNN\n[34]SSD6D\n+ Ref [22]HMap [24]PVNet [25]Ours Ours +Ref Mean 6.",
            "11": "We demonstrated that for both, real and synthetic training data, our detector outperforms other related works, such as [33, 34], by a large margin and performs similarly to [25]."
        },
        "DexYCB: A benchmark for capturing hand grasping of objects": {
            "authors": [
                "Wei Chao",
                "Wei Yang",
                "Yu Xiang",
                "Pavlo Molchanov",
                "Ankur Handa",
                "Jonathan Tremblay",
                "Yashraj S. Narang",
                "Karl Van",
                "Umar Iqbal",
                "Stan Birchfield",
                "Jan Kautz",
                "Dieter Fox"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Chao_DexYCB_A_Benchmark_for_Capturing_Hand_Grasping_of_Objects_CVPR_2021_paper.pdf",
            "ref_texts": "[27] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. In CVPR , 2019. 1",
            "ref_ids": [
                "27"
            ],
            "1": "State-of-the-art approaches for both 3D object pose [37, 20,34,27,40,26,19] and 3D hand pose estimation [49, 23,18,2,9,14,31] rely on deep learning and thus require large datasets with labeled hand or object poses for training."
        },
        "Onepose: One-shot object pose estimation without cad models": {
            "authors": [
                "Jiaming Sun",
                "Zihao Wang",
                "Siyu Zhang",
                "Xingyi He",
                "Hongcheng Zhao",
                "Guofeng Zhang",
                "Xiaowei Zhou"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Sun_OnePose_One-Shot_Object_Pose_Estimation_Without_CAD_Models_CVPR_2022_paper.pdf",
            "ref_texts": "[26] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 1, 2, 6, 7",
            "ref_ids": [
                "26"
            ],
            "1": "Most established works in object pose estimation [16, 26, 46] assume that the CAD model of the object is known a priori .",
            "2": "In contrast, the latter type of methods first find correspondences between image pixels and 3D object coordinates either by regression [22, 24, 25] or by voting [26, 27], and then compute the pose with Perspective-n-Points (PnP).",
            "3": "2) Instance-level method PVNet [26, 27].",
            "4": "Our method is compared with PVNet [26] on selected objects from the OnePose dataset with the 5cm-5deg metric.",
            "5": "The proposed method is compared with PVNet [26] with 5cm-5deg on selected objects from our OnePose dataset and the results are as presented in Tab."
        },
        "Hybridpose: 6d object pose estimation under hybrid representations": {
            "authors": [
                "Chen Song",
                "Jiaru Song",
                "Qixing Huang"
            ],
            "url": "http://openaccess.thecvf.com/content_CVPR_2020/papers/Song_HybridPose_6D_Object_Pose_Estimation_Under_Hybrid_Representations_CVPR_2020_paper.pdf",
            "ref_texts": "[34] Sida Peng, Yuan Liu, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Pvnet: Pixel-wise voting network for 6dof pose estimation. CoRR , abs/1812.11788, 2018. 1,2,3,4,6,7,8",
            "ref_ids": [
                "34"
            ],
            "1": "While early works typically formulate pose estimation as end-to-end pose classification [39] or pose regression [16,42], recent pose estimation methods usually leverage keypoints as an intermediate representation [38,34], and align predicted 2D keypoints with ground-truth 3D keypoints.",
            "2": "To express the geometric information in an RGB image, a prevalent intermediate representation is keypoints, which achieves stateof-the-art performance [34,32,36].",
            "3": "Alternative keypoint representations include vector-fields [34] and patches [14].",
            "4": "To mitigate pose error, several works assign different weights to different predicted elements in the 2D-3D alignment stage [34,32].",
            "5": "The keypoint network fK\n\u03b8employs an off-the-shelf prediction network [34].",
            "6": "In our experiments, HybridPose incorporates an off-the-shelf architecture called PVNet [34], which is the state-of-the-art keypoint-based pose estimator that employs a voting scheme to predict both visible and invisible keypoints.",
            "7": "Our keypoint annotation strategy follows that of [34], i.",
            "8": "A voting-based keypoint localization scheme [34] is applied to extract the coordinates of 2D keypoints from this 2|K|-channel tensor and the segmentation mask M.",
            "9": "[38], BB8 [36], Pix2Pose [30], PVNet [34], CDPN [20], and DPOD [44].",
            "10": "HybridPose outperforms PVNet [34], the backbone model we use to predict keypoints.",
            "11": "[14], PVNet [34], and DPOD [44].",
            "12": "In terms of ADD(-S), our approach improves PVNet [34] from 40."
        },
        "Epos: Estimating 6d pose of objects with symmetries": {
            "authors": [
                "Tomas Hodan",
                "Daniel Barath",
                "Jiri Matas"
            ],
            "url": "https://openaccess.thecvf.com/content_CVPR_2020/papers/Hodan_EPOS_Estimating_6D_Pose_of_Objects_With_Symmetries_CVPR_2020_paper.pdf",
            "ref_texts": "[50] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. CVPR , 2019. 1, 2, 3",
            "ref_ids": [
                "50"
            ],
            "1": "Recent methods, which are mostly based on convolutional neural networks, produce dense correspondences [4, 48, 69] or predict 2D image locations of pre-selected 3D keypoints [52, 61, 50].",
            "2": "A popular approach is to establish 2D-3D correspondences by predicting the 2D projections of a fixed set of 3D keypoints, which are pre-selected for each object model, and solve for the object pose using P nP-RANSAC [52, 49, 47, 61, 65, 15, 29, 50].",
            "3": "On the other hand, regression-based methods [61, 69, 50] need to compromise among the possible corresponding locations and tend to return the average, which is often not a valid solution."
        },
        "Honnotate: A method for 3d annotation of hand and object poses": {
            "authors": [
                "Shreyas Hampali",
                "Mahdi Rad",
                "Markus Oberweger",
                "Vincent Lepetit"
            ],
            "url": "https://openaccess.thecvf.com/content_CVPR_2020/papers/Hampali_HOnnotate_A_Method_for_3D_Annotation_of_Hand_and_Object_CVPR_2020_paper.pdf",
            "ref_texts": ""
        },
        "Fs-net: Fast shape-based network for category-level 6d object pose estimation with decoupled rotation mechanism": {
            "authors": [
                "Wei Chen",
                "Xi Jia",
                "Hyung Jin",
                "Jinming Duan",
                "Linlin Shen",
                "Ales Leonardis"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2021/papers/Chen_FS-Net_Fast_Shape-Based_Network_for_Category-Level_6D_Object_Pose_Estimation_CVPR_2021_paper.pdf",
            "ref_texts": "[23] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise V oting Network for 6DoF Pose Estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "23"
            ],
            "1": "Correspondences-based methods trained their model to establish 2D-3D correspondences [28,29,23] or 3D-3D correspondences [6,5].",
            "2": "Method Input ADD-(S) Speed(FPS) PVNet [23] RGB 86."
        },
        "Epro-pnp: Generalized end-to-end probabilistic perspective-n-points for monocular object pose estimation": {
            "authors": [
                "Hansheng Chen",
                "Pichao Wang",
                "Fan Wang",
                "Wei Tian",
                "Lu Xiong",
                "Hao Li"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Chen_EPro-PnP_Generalized_End-to-End_Probabilistic_Perspective-N-Points_for_Monocular_Object_Pose_Estimation_CVPR_2022_paper.pdf",
            "ref_texts": "[31] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 1, 2, 4, 7",
            "ref_ids": [
                "31"
            ],
            "1": "BB8 [32] and RTM3D [23] locate the corners of the 3D bounding box as keypoints, while PVNet [31] defines the keypoints by farthest point sampling and Deep MANTA [9] by handcrafted templates.",
            "2": "Existing work [11,31] on learning uncertainty-aware correspondences only considers the former, hence lacking the discriminative ability.",
            "3": "Comparison to the State of the Art As shown in Table 2, despite modified from the lower baseline, EPro-PnP easily reaches comparable performance to the top pose refiner RePOSE [20], which adds extra overhead to the PnP-based initial estimator PVNet [31]."
        },
        "Tracking objects as pixel-wise distributions": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2207.05518",
            "ref_texts": "44. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
            "ref_ids": [
                "44"
            ],
            "1": "First, pixel-wise information may help overcome occlusion based on low-level clues [57,44,45].",
            "2": "Dense fusion [57] and pixel-wise voting network [44,18] are proposed to overcome occlusions in the object pose estimation [19]."
        },
        "H3dnet: 3d object detection using hybrid geometric primitives": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2006.05682",
            "ref_texts": "26. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019. pp. 4561{4570",
            "ref_ids": [
                "26"
            ],
            "1": "This regression methodology, which is motivated from the recent success of keypoint-based pose regression for 6D object pose estimation [19, 25, 11, 21, 26, 36], displays two appealing advantages for 3D object detection."
        },
        "Geometry-based distance decomposition for monocular 3d object detection": {
            "authors": [
                "Xuepeng Shi",
                "Qi Ye",
                "Xiaozhi Chen",
                "Chuangrong Chen",
                "Zhixiang Chen",
                "Kyun Kim"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Shi_Geometry-Based_Distance_Decomposition_for_Monocular_3D_Object_Detection_ICCV_2021_paper.pdf",
            "ref_texts": "[35] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019.",
            "ref_ids": [
                "35"
            ],
            "1": "In 6D object pose estimation, PVNet [35] and SegDriven [17] regress the 2D keypoints of objects.",
            "2": "These works [35, 17, 23, 44] recover the pose or distance by several factors, such as 2D keypoints, 2D bounding boxes, and object physical size, which achieves interpretable and robust pose or distance estimation.",
            "3": "In 6D object pose estimation, PVNet [35] and SegDriven [17] regress 2D keypoints of objects, then optimize the estimation of the 6D pose by solving a Perspective-n-Point (PnP) problem."
        },
        "Semi-supervised 3d hand-object poses estimation with interactions in time": {
            "authors": [
                "Shaowei Liu",
                "Hanwen Jiang",
                "Jiarui Xu",
                "Sifei Liu",
                "Xiaolong Wang"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Semi-Supervised_3D_Hand-Object_Poses_Estimation_With_Interactions_in_Time_CVPR_2021_paper.pdf",
            "ref_texts": "[43] Sida Peng, Yuan Liu, Qi-Xing Huang, Hujun Bao, and Xiaowei Zhou. Pvnet: Pixel-wise voting network for 6dof pose estimation. CVPR , pages 4556\u20134565, 2019.",
            "ref_ids": [
                "43"
            ],
            "1": "There are also two main paradigms to perform object 6-Dof pose estimation, with one directly regressing the pose as network outputs [28,67] and another regressing the projected 3D object control points location in the image and recovering the pose with 2D-to-3D correspondence [45,60,43,24]."
        },
        "So-pose: Exploiting self-occlusion for direct 6d pose estimation": {
            "authors": [
                "Yan Di",
                "Fabian Manhardt",
                "Gu Wang",
                "Xiangyang Ji",
                "Nassir Navab",
                "Federico Tombari"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Di_SO-Pose_Exploiting_Self-Occlusion_for_Direct_6D_Pose_Estimation_ICCV_2021_paper.pdf",
            "ref_texts": "[28] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof poseestimation. In CVPR , 2019. 2,6,7",
            "ref_ids": [
                "28"
            ],
            "1": "[28] demonstrate that keypoints away from the object surface induce largererrors and, therefore, instead sample several keypoints onthe object model based on farthest point sampling.",
            "2": "HybridPose [36] follows and develops [28] by introducing hybrid representations.",
            "3": "0 PVNet [28] M 73.",
            "4": "Stage HybridPose GDR-Net Ours DPOD DeepIM\n[45] [28] [12] [36] [43] (SO-Pose) [47] [19] P.",
            "5": "621 PVNet [28] M 0."
        },
        "Single-stage 6d object pose estimation": {
            "authors": [
                "Yinlin Hu",
                "Pascal Fua",
                "Wei Wang",
                "Mathieu Salzmann"
            ],
            "url": "http://openaccess.thecvf.com/content_CVPR_2020/papers/Hu_Single-Stage_6D_Object_Pose_Estimation_CVPR_2020_paper.pdf",
            "ref_texts": "[36] Sida Peng, Yuan Liu, Qixing Huang, Hujun Bao, and Xiaowei Zhou. PVNet: Pixel-Wise V oting Network for 6DoF Pose Estimation. In Conference on Computer Vision and Pattern Recognition , 2019. 1,2,3,4,5,6,7,8",
            "ref_ids": [
                "36"
            ],
            "1": "State-of-the-art approaches [39,41,32,16,13,36, 54,34,24] follow a two-stage paradigm: First use a deep network to establish correspondences between 3D object points and their 2D image projections, then use a RANSACbased Perspective-n-Point (PnP) algorithm to compute the 6 pose parameters [9,20,40,47,21,18,7,46].",
            "2": "We then demonstrate the generality of this network by combining it with two state-of-the-art correspondenceextraction frameworks [13,36].",
            "3": "We show that these single-stage frameworks systematically outperform the original twostage ones [13,36], in terms of both accuracy and runtime.",
            "4": "However, this tends to be less accurate than first establishing 3D-to-2D correspondences [39,41,32,16,13,36,54,34,24] and then running a RANSAC-based Perspective-n-Point (PnP) algorithm [9] to estimate the object position and orientation given the camera intrinsic parameters.",
            "5": "2(b), our formalism can handle 3D point to 2D vector correspondences, which have been shown to be better-suited to use in conjunction with a deep network [36].",
            "6": "After establishing 3D-to-2D correspondences by some segmentation-driven CNN for 6D pose [13,36], we use three main modules to infer the pose from these correspondence clusters directly: a local feature extraction module with shared network parameters, a feature aggregation module operating within the different clusters, and a global inference module consisting of simple fully-connected layers to estimate the final pose as a quaternion and a translation.",
            "7": "To implement f, we use the recent encoder-decoder architecture of either [13]or [36].",
            "8": "When using the network of [36] instead of that of [13] to find the correspondences, we use the same input format but normalize the dxanddyso that they represent an orientation.",
            "9": "We takeLsto be the Focal Loss of [25], andLkto be the regression term of either [13] or [36] depending on which of the two architectures we use.",
            "10": "Experiments We compare our single-stage approach to more traditional but state-of-the-art two-stage frameworks [13,36], first on synthetic data and then on real data from the challenging Occluded-LINEMOD [19] and YCB-Video [50] datasets.",
            "11": "Since one of the best current techniques [36] uses directions instead and infers poses from those using a voting-based PnP scheme, we feed the same 3D point to 2D vector correspondences to our own network.",
            "12": "3 noise level \u03c3(outliers=30%)pose error Voting-based PnP Ours Figure 8: Comparison with PVNet\u2019s voting-based PnP [36].",
            "13": "For Occluded-LINEMOD, as in [41, 13,36], we first use the Cut-and-Paste synthetic technique [6] to generate 20K images from LINEMOD data and random background data [51], with 4 to 10 different instances for each image.",
            "14": "Then, we generate 10K rendering images for each object type from the textured 3D mesh, as in [36].",
            "15": "The last column shows two failure cases, where the target egg box is occluded too much and the target glue exhibits subtle symmetry ambiguities, making it not easy for the correspondence-extraction network [36] to establish stable correspondences.",
            "16": "[13] [13] +Ours [36] [36] +Ours Ape 12.",
            "17": "We evaluate two state-ofthe-art correspondence-extraction networks: SegDriven [13] and PVNet [36], by replacing their original RANSAC-based post processing with our small network.",
            "18": "For both datasets, we use an input image resolution of 640 \u00d7480 for both training and testing, as in [36].",
            "19": "1 Occluded-LINEMOD Results As discussed before, to demonstrate that our method is generic, we test it in conjunction with two correspondenceextraction networks SegDriven [13] and PVNet [36].",
            "20": "We compare our results with those of PoseCNN [50], SegDriven [13], and PVNet [36] in terms of both ADD-0.",
            "21": "We compare the running times (in milliseconds) of PoseCNN [50], SegDriven [13], PVNet [36] and our method on a modern GPU (GTX1080 Ti).",
            "22": "We compare our results with those of PoseCNN [50], SegDriven [13], and PVNet [36] in terms of ADD-0.",
            "23": "In Table 2, we shown that our single-stage network outperform the state-of-the-art methods, PoseCNN [50], SegDriven [13] and PVNet [36].",
            "24": "2 YCB-Video Results Table 4summarizes the results comparing against PoseCNN [50], SegDriven [13], and PVNet [36].",
            "25": "Limitations While our method is accurate and fast when used in conjunction with state-of-the-art correspondence-extraction networks [13,36], the network that estimates the poses from the correspondences is still not as accurate as traditional geometry-based PnP algorithms when very precise correspondences can be obtained by other means, as shown in Fig."
        },
        "Sgpa: Structure-guided prior adaptation for category-level 6d object pose estimation": {
            "authors": [
                "Kai Chen",
                "Qi Dou"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_SGPA_Structure-Guided_Prior_Adaptation_for_Category-Level_6D_Object_Pose_Estimation_ICCV_2021_paper.pdf",
            "ref_texts": "[20] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof poseestimation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4561\u20134570, 2019. 1,2",
            "ref_ids": [
                "20"
            ],
            "1": "Dif-ferent from conventional instance-level [12,20,30,35] object pose estimation, which gives instance CAD models andpredicts poses for the instances that have been seen during training, category-level task requires capturing the general properties while accounting for the large variation of differPrior Point CloudCamera Instance I Camera Instance IIw/o Prior adaptationw/ Prior adaptation Figure 1.",
            "2": "Methods [20,25,2,13,17,16] mainly focus on learning a robust embedding that is con-ditioned on the object pose.",
            "3": "The second group of methods [20,12,25,13] assume the object 3D CAD model is available."
        },
        "3d-future: 3d furniture shape with texture": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2009.09633",
            "ref_texts": "(2020) Latentfusion: End-to-end difierentiable reconstruction and rendering for unseen object pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 10710{10719 Peng S, Liu Y, Huang Q, Zhou X, Bao H (2019) Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp 4561{4570 Qi CR, Su H, Mo K, Guibas LJ (2017a) Pointnet: Deep learning on point sets for 3d classiffcation and segmentation. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 652{660 Qi CR, Yi L, Su H, Guibas LJ (2017b) Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In: Advances in neural information processing systems, pp 5099{5108 Rad M, Lepetit V (2017) Bb8: A scalable, accurate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth. In: Proceedings of the IEEE International Conference on Computer Vision, pp 3828{3836 Raj A, Ham C, Barnes C, Kim V, Lu J, Hays J"
        },
        "Dexmv: Imitation learning for dexterous manipulation from human videos": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2108.05877",
            "ref_texts": "57. Peng, S., Liu, Y., Huang, Q.X., Bao, H., Zhou, X.: Pvnet: Pixel-wise voting network for 6dof pose estimation. CVPR (2019) 4",
            "ref_ids": [
                "57"
            ],
            "1": "One line of work focus on object pose estimation [40,63,90,84,57,34,30] and hand pose estimation [93,35,82,26,6,12,29,42,47]."
        },
        "Shape prior deformation for categorical 6d object pose and size estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2007.08454",
            "ref_texts": "18. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4561{4570 (2019)",
            "ref_ids": [
                "18"
            ],
            "1": "Unfortunately, these methods [18, 32, 42] cannot be directly generalized to category-level 6D object pose estimation on new object instances with unknown 3D models.",
            "2": "[18,23,28] detect the keypoints of the object on image and then solve a Perspective-n-Point problem."
        },
        "Satellite pose estimation challenge: Dataset, competition design, and results": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1911.02050",
            "ref_texts": "[40] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d in The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Oral , 2019.",
            "ref_ids": [
                "40"
            ],
            "1": "While various DNN-based approaches have been proposed to perform pose estimation [30]\u2013[40], current state-of-the-art methods employ Convolutional Neural Networks (CNN) that either directly predict the 6D pose or an intermediate information that can be used to compute the 6D pose, notably a set of keypoints defined a priori .",
            "2": "Most recently, architectures like KPD [39] and PVNet [40] have been proposed to predict the locations of the 2D keypoints on the target\u2019s surface.",
            "3": "[40] S."
        },
        "Zebrapose: Coarse to fine surface encoding for 6dof object pose estimation": {
            "authors": [
                "Yongzhi Su",
                "Mahdi Saleh",
                "Torben Fetzer",
                "Jason Rambach",
                "Nassir Navab",
                "Benjamin Busam",
                "Didier Stricker",
                "Federico Tombari"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Su_ZebraPose_Coarse_To_Fine_Surface_Encoding_for_6DoF_Object_Pose_CVPR_2022_paper.pdf",
            "ref_texts": "[49] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "49"
            ],
            "1": "In contrast to previous works where there is no guaranteed putative correspondence [48,49,67], our encoding promotes direct pixel-to-surface matching just by means of a look-up table.",
            "2": "BB8 [52] firstly defines the 3D object bounding box corners as the keypoints and PVNet [49] reaches high recall rate in LM [27] dataset by predicting the keypoints with a dense pixel-wise voting for sampled keypoints on the object.",
            "3": "Since the LM-O dataset includes only a limited number of training images, [34, 49] additionally render a large number of synthetic images for training."
        },
        "Shapo: Implicit representations for multi-object shape, appearance, and pose optimization": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2207.13691",
            "ref_texts": "41. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: CVPR (2019)",
            "ref_ids": [
                "41"
            ],
            "1": "Such methods [41,52,55], while achieving impressive results, rely on provided 3D reconstructions or prior CAD models for successful detection and pose estimation."
        },
        "Learning canonical shape space for category-level 6d object pose and size estimation": {
            "authors": [
                "Dengsheng Chen",
                "Jun Li",
                "Zheng Wang",
                "Kai Xu"
            ],
            "url": "http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Learning_Canonical_Shape_Space_for_Category-Level_6D_Object_Pose_and_CVPR_2020_paper.pdf",
            "ref_texts": "[17] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6dof pose estimation. In Proc. CVPR , pages 4561\u20134570, 2019. 1,2",
            "ref_ids": [
                "17"
            ],
            "1": "Most existing works have so far been addressing instance-level 6D pose estimation where each target object has a corresponding CAD model with exact shape and size [17].",
            "2": "This circumvents the difficulty in estimating dense correspondence between two representations as in other methods [17,29].",
            "3": "PVNet [17] is a unique approach of feature point detection using CNNs: A vector field is estimated for the input RGB image based on which the feature points are voted."
        },
        "H2o: Two hands manipulating objects for first person interaction recognition": {
            "authors": [
                "Taein Kwon",
                "Bugra Tekin",
                "Jan Stuhmer",
                "Federica Bogo",
                "Marc Pollefeys"
            ],
            "url": "http://openaccess.thecvf.com/content/ICCV2021/papers/Kwon_H2O_Two_Hands_Manipulating_Objects_for_First_Person_Interaction_Recognition_ICCV_2021_paper.pdf",
            "ref_texts": "[59] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 3",
            "ref_ids": [
                "59"
            ],
            "1": "While a significant amount of research has focused on predicting the pose of hands [27, 54, 56, 58, 70, 93, 94, 98] or objects [5, 48, 59, 78, 83, 90] in isolation, joint understanding of handobject interactions has received far less attention."
        },
        "Surfemb: Dense and continuous correspondence distributions for object pose estimation with learnt surface embeddings": {
            "authors": [
                "Rasmus Laurvig",
                "Anders Glent"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Haugaard_SurfEmb_Dense_and_Continuous_Correspondence_Distributions_for_Object_Pose_Estimation_CVPR_2022_paper.pdf",
            "ref_texts": "[26] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "26"
            ],
            "1": "Many establish 2D-3D correspondences [12, 21, 25, 26, 28, 31] followed by PnP-RANSAC [9], and mainly differ in how they establish correspondences.",
            "2": "Some methods establish correspondences for a fixed set of object keypoints [26, 28] while others establish dense (pixel-wise) query model key model contrastive loss kq k+Figure 1.",
            "3": "Other learning based approaches are based on establishing 2D-3D correspondences [5, 12, 21, 25, 26, 28, 31] followed by a variant of PnP-RANSAC.",
            "4": "PVNet [26] regresses vector fields toward the 2D projections of a set of fixed 3D key points and handles symmetries like BB8.",
            "5": "725 PVNet [26] RGB \u2713 0."
        },
        "Variable compliance control for robotic peg-in-hole assembly: A deep-reinforcement-learning approach": {
            "authors": [
                "Cristian Camilo",
                "Damien Petit",
                "Ixchel Georgina",
                "Kensuke Harada"
            ],
            "url": "https://www.mdpi.com/2076-3417/10/19/6923/pdf",
            "ref_texts": "29. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 16\u201320 June 2019; pp. 4561\u20134570.",
            "ref_ids": [
                "29"
            ],
            "1": "We considered the second assumption fair given the advances in vision-recognition techniques, wherein the 6D poses of objects can be estimated from single RGB images [28,29] or RGB images with depth maps (RGB-D) [30,31]."
        },
        "Self6d: Self-supervised monocular 6d object pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2004.06468",
            "ref_texts": "44. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4561{4570 (2019)",
            "ref_ids": [
                "44"
            ],
            "1": "the 3D translation and rotation), a huge amount of training data is required [30,44,53].",
            "2": "Similarly, [17,44] also regress 2D projections of associated sparse 3D keypoints, however, both employ segmentation paired with voting to improve the reliability.",
            "3": "On the other hand, as for training with real pose labels, we are again on par with other recently published methods such as PVNet [44] and CDPN [30] reporting a mean average recall of 86 :9%.",
            "4": "Train data w/o Real Pose Labels with Real Pose Labels Object AAE[52] MHP[38] DPOD[61] Self6D Tekin[53] DPOD[61] PVNet[44] CDPN[30] Ape 4."
        },
        "Fs6d: Few-shot 6d pose estimation of novel objects": {
            "authors": [
                "Yisheng He",
                "Yao Wang",
                "Haoqiang Fan",
                "Jian Sun",
                "Qifeng Chen"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/He_FS6D_Few-Shot_6D_Pose_Estimation_of_Novel_Objects_CVPR_2022_paper.pdf",
            "ref_texts": "[38] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019. 2, 6[39] Gabriel Peyr \u00b4e, Marco Cuturi, et al. Computational optimal transport: With applications to data science. Foundations and Trends\u00ae in Machine Learning , 11(5-6):355\u2013607, 2019.",
            "ref_ids": [
                "38",
                "39"
            ],
            "1": "Learning-based approaches includes direct pose regression [53,58], dense correspondence exploration [29] and recent keypoint-based approaches [15, 16, 38], which improve the performance by large margins.",
            "2": "The Sinkhorn Algorithm [39] is applied for differentiable optimization as well.",
            "3": "1d) as in [19, 38].",
            "4": "2, 6[39] Gabriel Peyr \u00b4e, Marco Cuturi, et al."
        },
        "Rnnpose: Recurrent 6-dof object pose refinement with robust correspondence field estimation and pose optimization": {
            "authors": [
                "Yan Xu",
                "Yee Lin",
                "Guofeng Zhang",
                "Xiaogang Wang",
                "Hongsheng Li"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Xu_RNNPose_Recurrent_6-DoF_Object_Pose_Refinement_With_Robust_Correspondence_Field_CVPR_2022_paper.pdf",
            "ref_texts": "[34] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "34"
            ],
            "1": "These methods may estimate the object\u2019s bounding box corners [35,45], predict dense 2D-3D correspondence maps [33] or vote the keypoints by all object pixels [34].",
            "2": "At the beginning of the first rendering cycle, a reference image Iref is rendered with the object\u2019s CAD model according to its initial pose Pinit(estimated by any direct methods [34,52]).",
            "3": "Here, the initial poses for pose refinement are originally from PVNet [34] but added with significant disturbances for robustness testing.",
            "4": "We follow similar conventions in data processing and synthetic data generation as the previous works [20,34].",
            "5": "For the initial poses, we mainly rely on PoseCNN [52] and PVNet [34], two typical direct estimation methods, following [23] and [20].",
            "6": "Robustness comparison with RePOSE by degrading the initial poses (from PVNet [34]) with Gaussian noise on LINEMOD dataset.",
            "7": "The comparison of estimation accuracy with competitive direct methods (PoseCNN [52], PVNet [34] and HybridPose [38]) and refinement methods (DPOD [58], DeepIM [23] and RePOSE\n[20]) on LINEMOD dataset in terms of the ADD(-S) metric.",
            "8": "Object PoseCNN [52] PVNet [34] HybridPose [38] GDR-Net [51] DPOD [58] RePOSE [20] Ours Ape 9.",
            "9": "For the LINEMOD dataset, we compare with the recent pose refinement methods RePOSE [20], DPOD [58] and DeepIM [23] as well as some direct estimation baselines [34, 38, 52].",
            "10": "4 the PVNet [34], although the pose accuracy of PVNet is much better as exhibited in Table 3."
        },
        "Towards unbiased label distribution learning for facial pose estimation using anisotropic spherical gaussian": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2208.09122",
            "ref_texts": "34. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: CVPR (2019)",
            "ref_ids": [
                "34"
            ],
            "1": "The first type such as [34,37,47] first capture instance information and keypoints from images to determine locations of 6 Z."
        },
        "Dualposenet: Category-level 6d object pose and size estimation using dual pose network with refined learning of pose consistency": {
            "authors": [
                "Jiehong Lin",
                "Zewei Wei",
                "Zhihao Li",
                "Songcen Xu",
                "Kui Jia",
                "Yuanqing Li"
            ],
            "url": "http://openaccess.thecvf.com/content/ICCV2021/papers/Lin_DualPoseNet_Category-Level_6D_Object_Pose_and_Size_Estimation_Using_Dual_ICCV_2021_paper.pdf",
            "ref_texts": "[20] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 1, 2",
            "ref_ids": [
                "20"
            ],
            "1": ", the above 7DoF setting) and instance-level 6D object pose estimation [12, 8, 14, 16, 32, 26, 20, 17, 29, 18].",
            "2": "More recent solutions build on the power of deep networks and can directly estimate object poses from RGB images alone [16, 32, 26, 20] or RGB-D ones [17, 29]."
        },
        "Neural correspondence field for object pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2208.00113",
            "ref_texts": "61. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: PVNet: Pixel-wise voting network for 6DoF pose estimation. CVPR (2019) 2, 3",
            "ref_ids": [
                "61"
            ],
            "1": "Classical methods for 6DoF object pose estimation [9, 5, 59, 83, 63, 73, 61, 23] rely on 2D-3D correspondences established between pixels of the input image and the 3D object model, and estimate the pose by the P nP-RANSAC algorithm [39].",
            "2": ", by predicting the 2D projections of a fixed set of 3D keypoints pre-selected for each object model, have also been proposed [63, 60, 54, 73, 76, 27, 61]."
        },
        "Category level object pose estimation via neural analysis-by-synthesis": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2008.08145",
            "ref_texts": "39. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) (2019)",
            "ref_ids": [
                "39"
            ],
            "1": "[29,49,33,17,39] or dense correspondence maps [25,38,55].",
            "2": "Template matching techniques align 3D CAD models to observed 3D point clouds [4,56], images [24,29], learned keypoints [29,49,33,17,39] or correspondence features [25,38,55]."
        },
        "Gpv-pose: Category-level object pose estimation via geometry-guided point-wise voting": {
            "authors": [
                "Yan Di",
                "Ruida Zhang",
                "Zhiqiang Lou",
                "Fabian Manhardt",
                "Xiangyang Ji",
                "Nassir Navab",
                "Federico Tombari"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Di_GPV-Pose_Category-Level_Object_Pose_Estimation_via_Geometry-Guided_Point-Wise_Voting_CVPR_2022_paper.pdf"
        },
        "Satellite pose estimation with deep landmark regression and nonlinear pose refinement": {
            "authors": [
                "Bo Chen",
                "Jiewei Cao",
                "Alvaro Parra",
                "Jun Chin"
            ],
            "url": "http://openaccess.thecvf.com/content_ICCVW_2019/papers/R6D/Chen_Satellite_Pose_Estimation_with_Deep_Landmark_Regression_and_Nonlinear_Pose_ICCVW_2019_paper.pdf",
            "ref_texts": "[26] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 2,3",
            "ref_ids": [
                "26"
            ],
            "1": "Inspired by works that combine the strength of deep neural networks and geometric optimisation [26,25, 35], our approach contains three main components: 1.",
            "2": "While the keypoint matching problem can be solved using machine learning, deep CNN-based feature learning methods typically fix the 2D-3D keypoint associations and learn to predict the image locations of each corresponding 3D keypoint such as [26,25,35].",
            "3": "2,3\n[26] S."
        },
        "Coupled iterative refinement for 6d multi-object pose estimation": {
            "authors": [
                "Lahav Lipson",
                "Zachary Teed",
                "Ankit Goyal",
                "Jia Deng"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Lipson_Coupled_Iterative_Refinement_for_6D_Multi-Object_Pose_Estimation_CVPR_2022_paper.pdf",
            "ref_texts": "[28] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "28"
            ]
        },
        "Wide-depth-range 6d object pose estimation in space": {
            "authors": [
                "Yinlin Hu",
                "Sebastien Speierer",
                "Wenzel Jakob",
                "Pascal Fua",
                "Mathieu Salzmann"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Hu_Wide-Depth-Range_6D_Object_Pose_Estimation_in_Space_CVPR_2021_paper.pdf",
            "ref_texts": "[31] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-Wise V oting Network for 6DoF Pose Estimation. In Conference on Computer Vision and Pattern Recognition , 2019.",
            "ref_ids": [
                "31"
            ],
            "1": "This network is usually trained to predict the image location of the 3D object bounding box corners, either in a single global fashion [18,33,37,42], or by aggregating multiple local predictions to improve robustness to occlusions [29,16,11,31,43,23].",
            "2": "We compare our results with those of PVNet [31], SimplePnP [10] and Hybrid [36]."
        },
        "Onepose++: Keypoint-free one-shot object pose estimation without CAD models": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/e43f900f571de6c96a70d5724a0fb565-Paper-Conference.pdf",
            "ref_texts": "[39] Sida Peng, Xiaowei Zhou, Yuan Liu, Haotong Lin, Qixing Huang, and Hujun Bao. PVNet: pixel-wise voting network for 6dof object pose estimation. T-PAMI , 2020. 1, 2, 3, 8, 9",
            "ref_ids": [
                "39"
            ],
            "1": "However, most existing methods [39,29,38,55,2,4,37] either rely on high-fidelity object CAD models or require training a separate network for each object category.",
            "2": "The experiments show that our method outperforms all existing one-shot pose estimation methods [48,33] by a large margin and even achieves comparable results with instance-level methods [39,29] which are trained for each object instance with a CAD model.",
            "3": "Instance-level methods estimate object poses either by directly regressing poses from images [58,20,29] or construct 2D-3D correspondences and then solve poses with PnP [39,59].",
            "4": "Our method is compared with PVNet[39] on objects with CAD models in the OnePose-LowTexture dataset using the ADD(S)-0.",
            "5": "2) Instance-level baselines [39,29] that require CAD-models and need to be trained separately for each object.",
            "6": "For the comparison with PVNet [39], we follow its original training setting, which first samples 8keypoints on the object surface and then trains a network using 5000 synthetic images for each object.",
            "7": "On the OnePose-LowTexture dataset, the proposed method is compared with PVNet [39] on the subset objects with scanned models.",
            "8": "4 Results on LINEMOD We compare the proposed method with OnePose [48] and Gen6D [33] which are under the One-shot setting, and Instance-level methods PVNet [39] and CDPN [29] onADD(S)-0.",
            "9": "Our method has lower or comparable performance with instance-level methods [39,29], which are trained to fit each object instance, and thus perform well naturally, at the expense of the tedious training for each object."
        },
        "Gen6D: Generalizable model-free 6-DoF object pose estimation from RGB images": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2204.10776",
            "ref_texts": "42. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6-dof pose estimation. In: CVPR (2019) Gen6D Pose Estimator 17",
            "ref_ids": [
                "42"
            ],
            "1": "In general, an object pose can be estimated by directly predicting rotation/translation by regression [70,27,59], solving a Perspective-nPoints (PnP) problem [42,47] or matching images with known poses [58,69,57].",
            "2": "Experiments show that without training on these objects, our method still outperforms instance-speciffc estimator PVNet [42] on the GenMOP dataset and another model-free MOPED [41] dataset.",
            "3": "1 Speciffc object pose estimator Most object pose estimators [70,58,42,28,24,68,14,63,34,54,27,45,26,25,46,55] are instance-speciffc, which cannot generalize to unseen objects and usually require a 3D model of the object to render extensive images for training.",
            "4": "3 Results on GenMOP For comparison, we choose the generalizable image-matching based ObjDesc [69] and two instance-speciffc estimators PVNet [42] and RLLG [6] as baseline methods.",
            "5": "For instance-speciffc estimators PVNet [42] and RLLG [6], we have to train difierent models for difierent objects separately.",
            "6": "1dPVNet [42] 7 49.",
            "7": "39 Prj-5PVNet [42] 7 15.",
            "8": "For baselines, we include the instancespeciffc pose estimators [58,62,74,6,42,70,29] and a generalizable estimator PoseFrom-Shape (PFS) [72].",
            "9": "The instance-speciffc estimators are either trained on the synthetic data of the object (\\synthetic training\") [58,62,74] or trained on both the synthetic and real data of the object (\\real training\") [42,70,74].",
            "10": "18 PVNet [42] 7 No 79.",
            "11": "3) However, Gen6D performs worse than instance-speciffc estimators [42,70,74] with real training.",
            "12": "4) With ground-truth bounding box, Gen6D achieves comparable results as the instance-speciffc estimators [42,70,74] with real training because such ground-truth bounding boxes provide correct depths.",
            "13": "5 Results on MOPED [41] On the MOPED dataset, we compare Gen6D with Latent-Fusion [41] and PVNet [42].",
            "14": "14 PVNet [42] 7 RGB 49.",
            "15": "For training PVNet [42], we apply the same strategy as used on the GenMOP dataset.",
            "16": "On the LINEMOD [23] dataset, we use the training set of previous instance-speciffc estimators [59,42] as reference images and the other images are selected as query images.",
            "17": "Note that reference images are used in inference of Gen6D but not in training the Gen6D estimator while instance-speciffc estimators like PVNet [42] actually use these reference images to train their models.",
            "18": "Note PVNet [42] is trained on the speciffc test object with both synthetic and real images while our Gen6D is not trained on the test object.",
            "19": "1d PVNet [42] Ours PVNet [42] Ours Eggbox 99."
        },
        "Ifor: Iterative flow minimization for robotic object rearrangement": {
            "authors": [
                "Ankit Goyal",
                "Arsalan Mousavian",
                "Chris Paxton",
                "Wei Chao",
                "Brian Okorn",
                "Jia Deng",
                "Dieter Fox"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Goyal_IFOR_Iterative_Flow_Minimization_for_Robotic_Object_Rearrangement_CVPR_2022_paper.pdf",
            "ref_texts": "[50] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. In CVPR , 2019. 2",
            "ref_ids": [
                "50"
            ],
            "1": "This makes explicit object pose estimation [33, 35, 50, 61, 66] a necessary part of the pipeline, and the full system susceptible to pose estimation error from real vision systems.",
            "2": ", detecting and segmenting objects [4, 6, 22, 38, 70] and estimating their 6D poses [33, 35, 50, 61, 66]."
        },
        "A vector-based representation to enhance head pose estimation": {
            "authors": [
                "Zongcheng Chu",
                "Dongfang Liu",
                "Yingjie Chen",
                "Zhiwen Cao"
            ],
            "url": "http://openaccess.thecvf.com/content/WACV2021/papers/Chu_A_Vector-Based_Representation_to_Enhance_Head_Pose_Estimation_WACV_2021_paper.pdf",
            "ref_texts": "[20] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "20"
            ],
            "1": "The approaches can be divided into two categories: [20, 33, 27] first estimate the object mask to determine its location in the image, then build the correspondence between the image pixels and the available 3D models."
        },
        "Repose: Fast 6d object pose refinement via deep texture rendering": {
            "authors": [
                "Shun Iwase",
                "Xingyu Liu",
                "Rawal Khirodkar",
                "Rio Yokota",
                "Kris M. Kitani"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Iwase_RePOSE_Fast_6D_Object_Pose_Refinement_via_Deep_Texture_Rendering_ICCV_2021_paper.pdf",
            "ref_texts": ""
        },
        "Osop: A multi-stage one shot object pose estimation framework": {
            "authors": [
                "Ivan Shugurov",
                "Fu Li",
                "Benjamin Busam",
                "Slobodan Ilic"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Shugurov_OSOP_A_Multi-Stage_One_Shot_Object_Pose_Estimation_Framework_CVPR_2022_paper.pdf",
            "ref_texts": "[37] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 1, 2",
            "ref_ids": [
                "37"
            ],
            "1": "According to the BOP challenge [14], which combines publicly available 6 DoF pose estimation datasets and offers standardized evaluation and comparison procedures, the field is dominated by deep learning methods [2, 12, 16, 19, 21, 22, 22, 24\u201327, 36, 37, 47, 49\u201352, 59].",
            "2": "In particular, IPose [16], YOLO6D [53], PVNet [37], HybridPose [50] and [19] predict a sparse set of the pre-defined keypoints."
        },
        "Templates for 3d object pose estimation revisited: Generalization to new objects and robustness to occlusions": {
            "authors": [
                "Van Nguyen",
                "Yinlin Hu",
                "Yang Xiao",
                "Mathieu Salzmann",
                "Vincent Lepetit"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Nguyen_Templates_for_3D_Object_Pose_Estimation_Revisited_Generalization_to_New_CVPR_2022_paper.pdf",
            "ref_texts": "[27] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-Wise V oting Network for 6DoF Pose Estimation. In Conference on Computer Vision and Pattern Recognition (CVPR) , 2019. 1, 2",
            "ref_ids": [
                "27"
            ],
            "1": "In particular, the robustness to partial occlusions has greatly increased [27, 16, 23], and the need for large amounts of real annotated training images has been relaxed thanks to domain transfer [1], domain randomization [35, 18, 30], and self-supervised learning [32] techniques that leverage synthetic images for training.",
            "2": "Some also show remarkable robustness to partial occlusions of the objects [23, 27, 16]."
        },
        "RBP-Pose: Residual bounding box projection for category-level pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2208.00237",
            "ref_texts": "31. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: CVPR (2019) 3",
            "ref_ids": [
                "31"
            ],
            "1": "The first group of methods [47,27,26,20,18,16] regresses the pose directly, whereas the second group instead establishes 2D-3D correspondences via keypoint detection or dense pixelwise prediction of 3D coordinates [21,33,31,49,12,30]."
        },
        "Autolabeling 3d objects with differentiable rendering of sdf shape priors": {
            "authors": [
                "Sergey Zakharov",
                "Wadim Kehl",
                "Arjun Bhargava",
                "Adrien Gaidon"
            ],
            "url": "https://openaccess.thecvf.com/content_CVPR_2020/papers/Zakharov_Autolabeling_3D_Objects_With_Differentiable_Rendering_of_SDF_Shape_Priors_CVPR_2020_paper.pdf",
            "ref_texts": "[33] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 2",
            "ref_ids": [
                "33"
            ],
            "1": "The authors in [43,31,24,17,33] apply such representations for monocular pose estimation of known CAD models."
        },
        "Deep learning on monocular object pose detection and tracking: A comprehensive overview": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2105.14291",
            "ref_texts": "[118] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. 2019. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 4561\u20134570.",
            "ref_ids": [
                "118"
            ],
            "1": "To solve this problem, PVNet [118] adopts the strategy of voting-based keypoint localization.",
            "2": "In addition, many methods [107] attempt to generate more realistic renderings [107, 153, 160] or use data augmentation strategies [10, 118].",
            "3": "4 PVNet(2019) [118] Keypoints RGB No 99.",
            "4": "0 PVNet(2019) [118] Keypoints RGB No 61.",
            "5": "0 PVNet(2019) [118] Keypoints RGB No 47.",
            "6": "Beyond leveraging image-level consistency for self-supervised learning, inspired by recent keypoint-based methods [118,137], DSC-PoseNet [168] develops a weakly supervised and a self-supervised learning-based pose estimation framework that enforces dual-scale keypoint consistency without using pose annotations."
        },
        "End-to-end learnable geometric vision by backpropagating pnp optimization": {
            "authors": [
                "Bo Chen",
                "Alvaro Parra",
                "Jiewei Cao",
                "Nan Li",
                "Jun Chin"
            ],
            "url": "https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_End-to-End_Learnable_Geometric_Vision_by_Backpropagating_PnP_Optimization_CVPR_2020_paper.pdf",
            "ref_texts": "[36] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 2,7",
            "ref_ids": [
                "36"
            ],
            "1": "Other pose estimation approaches that combine deep learning with geometric optimization (PnP solver) [35,37, 47,36,10] adopt a two-stage strategy: first learn to predict the 2D landmarks or fiducial points from the input image, then perform pose estimation by solving PnP on the 2D-3D correspondences.",
            "2": "For each object we\u2022obtain a 3D model representation consisting of 15 landmarks by using the Farthest Point Sampling (FPS) [36] over the original object mesh, \u2022randomly reserve 400images as the test set and set the remaining (about 800, depending on the object) as the training set, and \u2022train a model to predict the 6DOF object pose from the input image.",
            "3": "We provide the result of the current state-of-the-art PVNet [36] as a reference."
        },
        "Gapartnet: Cross-category domain-generalizable object perception and manipulation via generalizable and actionable parts": {
            "authors": [
                "Haoran Geng",
                "Helin Xu",
                "Chengyang Zhao",
                "Chao Xu",
                "Li Yi",
                "Siyuan Huang",
                "He Wang"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Geng_GAPartNet_Cross-Category_Domain-Generalizable_Object_Perception_and_Manipulation_via_Generalizable_and_CVPR_2023_paper.pdf",
            "ref_texts": "[41] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "41"
            ],
            "1": "Instance-level object pose estimation works [18, 22, 30,41,47,52,62] assume known CAD models and thus have their limitations."
        },
        "Rgb matters: Learning 7-dof grasp poses on monocular rgbd images": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2103.02184"
        },
        "G2l-net: Global to local network for real-time 6d pose estimation with embedding vector features": {
            "authors": [
                "Wei Chen",
                "Xi Jia",
                "Hyung Jin",
                "Jinming Duan",
                "Ales Leonardis"
            ],
            "url": "https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_G2L-Net_Global_to_Local_Network_for_Real-Time_6D_Pose_Estimation_CVPR_2020_paper.pdf",
            "ref_texts": "[29] Sida Peng, Yuan Liu, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Pvnet: Pixel-wise voting network for 6dof pose estimation. arXiv preprint arXiv:1812.11788 , 2018. 1,2,4, 5,6,7",
            "ref_ids": [
                "29"
            ],
            "1": "Introduction Real-time performance is important in many computer vision tasks, such as, object detection [35,23], semantic segmentation [36,10], object tracking [5,11], and pose estimation [29,38,16].",
            "2": "While there exist some real-time deep learning methods [29,34,38,45]\n(>20fps), they use only RGB information from an image.",
            "3": "These methods use handcrafted features that are not robust to background clutter and image variations [44,37,29].",
            "4": "Learning-based methods [33,29,34,28,15,38] alleviate this problem by training their model to predict 2D keypoints and compute the object pose by the PnP algorithm [9,20].",
            "5": "Another way is, as proposed in [29], to use the farthest point sampling (FPS) algorithm to sample the keypoints in each object model.",
            "6": "Different from other state-of-the-art methods [29,42,4], we adopt a multilayer perceptron (MLP) that takes pointwise embedding vector features as input and outputs the rotation of object as shown in Figure 5.",
            "7": "In experiments, we have found that our proposed method can make faster and more accurate predictions than the methods [29,42,4].",
            "8": "(3) When evaluating on YCB-Video dataset, same as [42, 29,21], we use the ADD-S AUC metric proposed in [42],Table 1.",
            "9": "Method PVNet [29]PoseCNN + DeepIM [42,21]DPOD [45] Frustum-P [30]Hinterstoisser [13]DenseFusion [40] Ours Input RGB RGB RGB RGB+Depth Depth RGB+Depth RGB+Depth Refinement \u00d7 /check /check(\u00d7) \u00d7 /check /check(\u00d7) \u00d7 Ape 43."
        },
        "Latentfusion: End-to-end differentiable reconstruction and rendering for unseen object pose estimation": {
            "authors": [
                "Keunhong Park",
                "Arsalan Mousavian",
                "Yu Xiang",
                "Dieter Fox"
            ],
            "url": "http://openaccess.thecvf.com/content_CVPR_2020/papers/Park_LatentFusion_End-to-End_Differentiable_Reconstruction_and_Rendering_for_Unseen_Object_Pose_CVPR_2020_paper.pdf",
            "ref_texts": "[33] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4561\u20134570, 2019. 2",
            "ref_ids": [
                "33"
            ],
            "1": "The second category formulates the pose estimation by predicting a set of 2D image features, such as the projection of 3D box corners [42,45,14,33] and direction of the center of the object [49], then recovering the pose of the object using the predictions."
        },
        "Catre: Iterative point clouds alignment for category-level object pose refinement": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2207.08082",
            "ref_texts": "38. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4561\u20134570 (2019)",
            "ref_ids": [
                "38"
            ],
            "1": "The vast majority of previous works [30,64,45,62,53,54,31,52,38] study with instance-level object pose estimation, which can be decomposed by two procedures: initial pose estimation and pose refinement."
        },
        "Symmetry and uncertainty-aware object slam for 6dof object pose estimation": {
            "authors": [
                "Nathaniel Merrill",
                "Yuliang Guo",
                "Xingxing Zuo",
                "Xinyu Huang",
                "Stefan Leutenegger",
                "Xi Peng",
                "Liu Ren",
                "Guoquan Huang"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Merrill_Symmetry_and_Uncertainty-Aware_Object_SLAM_for_6DoF_Object_Pose_Estimation_CVPR_2022_paper.pdf",
            "ref_texts": "[26] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose 14909",
            "ref_ids": [
                "26"
            ],
            "1": ", autonomous driving, robotic navigation, manipulation, and augmented reality), and has been extensively studied in computer vision and robotics communities [5,14,17,22,26,28,33].",
            "2": "Another trend is to either estimate the 2D projected locations of sparse 3D semantic points from the CAD model [25,26,28], or to regress the 3D coordinates from the dense 2D pixels within object masks [22,33], and then solves a perspective npoint (PnP) problem to estimate object poses.",
            "3": "Some works have retrieved a weight directly from the output of the keypoint network [25,26] to be used in PnP as a scalar measure of certainty [25] or Gaussian covariance matrix [26], while [30] adapted the Bayesian method of [10] to estimate a covariance matrix for the keypoints by sampling over a randomized batch."
        },
        "Stereobj-1m: Large-scale stereo image dataset for 6d object pose estimation": {
            "authors": [
                "Xingyu Liu",
                "Shun Iwase",
                "Kris M. Kitani"
            ],
            "url": "http://openaccess.thecvf.com/content/ICCV2021/papers/Liu_StereOBJ-1M_Large-Scale_Stereo_Image_Dataset_for_6D_Object_Pose_Estimation_ICCV_2021_paper.pdf",
            "ref_texts": "[27] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 1, 2, 6, 7, 8",
            "ref_ids": [
                "27"
            ],
            "1": "To increase data size for training large-scale neural networks, previous works have explored leveraging synthetically rendered [33, 27, 10] or augmented images [35] with 3D mesh models.",
            "2": "We implement two state-of-the-art methods [27, 17] as the baseline comparisons for 6D pose estimation using stereo on the StereOBJ-1M dataset.",
            "3": "Specifically, we implement PVNet [27] and KeyPose [17], two classic keypoint-based 6D pose estimation frameworks that have achieved state-of-the-art performance on various datasets.",
            "4": "PVNet [27] is a single-RGB keypoint-based method.",
            "5": "25 Table 4: The results of PVNet [27] on single-object pose estimation in terms of ADD(-S) AUC andADD(-S) accuracy on StereOBJ-1M dataset.",
            "6": "The monocular method PVNet [27] is adapted to its stereo variant where keypoints in both stereo images are predicted individually."
        },
        "Category-level 6D object pose estimation via cascaded relation and recurrent reconstruction networks": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2108.08755",
            "ref_texts": "[2] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR , 2019, pp. 4561\u2013",
            "ref_ids": [
                "2"
            ],
            "1": "I NTRODUCTION Accurate 6D pose estimation has increasingly been an important yet challenging research topic in computer vision, which aims to predict the location and orientation of 3D objects [2], [3], [4].",
            "2": "Instead of explicitly detecting and matching object keypoints, some methods [8] take corner points of 3D object bounding boxes as keypoints, or implicitly represent keypoints by a dense voting field [2].",
            "3": "Although impressive, these RGB-based methods face problems in complex environments such as cluttering or occlusion [24], [2].",
            "4": "[2] S."
        },
        "Hot-net: Non-autoregressive transformer for 3d hand-object pose estimation": {
            "authors": [],
            "url": "https://cse.buffalo.edu/~jsyuan/papers/2020/lin_mm20.pdf",
            "ref_texts": "[37] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. 2019. PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition .",
            "ref_ids": [
                "37"
            ],
            "1": "In the last decade, we have witnessed a rapid advance towards both 3D hand pose estimation [4,5,11,13,14,24,30,33,41,42,47, 47,51,56,57,60] and object pose estimation [25,26,37,38,46,52, 53,55] in isolation."
        },
        "6D pose estimation of objects: Recent technologies and challenges": {
            "authors": [],
            "url": "https://www.mdpi.com/2076-3417/11/1/228/pdf",
            "ref_texts": "32. Peng, S.; Liu, Y.; Huang, Q.; Bao, H.; Zhou, X. PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 18\u201323 June 2018.",
            "ref_ids": [
                "32"
            ],
            "1": "To solve this problem, Hu [32] et al."
        },
        "Camera-to-robot pose estimation from a single image": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1911.09231",
            "ref_texts": "[10] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d in CVPR , 2019.",
            "ref_ids": [
                "10"
            ],
            "1": "Network Architecture Inspired by recent work on object pose estimation [9], [10], [11], we use an auto-encoder network to detect the keypoints.",
            "2": "Pose Estimation Given the 2D keypoint coordinates, robot joint configuration with forward kinematics, and camera intrinsics, P nP [7] is used to retrieve the pose of the robot, similar to [14], [15], [9], [10], [11].",
            "3": "Even so, there is growing interest in the problem of markerless object pose estimation in both the robotics and computer vision communities [14], [24], [30], [9], [25], [11], [10], [31], [15], building upon work in keypoint detection for human pose estimation [32], [33], [13], [34], [35].",
            "4": "Recent leading methods are similar to the approach proposed here: A network is trained to predict object keypoints in the 2D image, followed by P nP [7] to estimate the pose of the object in the camera coordinate frame [14], [10], [36], [11], [15], [37], or alternatively, a deformable shape model is fit to the detect keypoints [38].",
            "5": "[10], who showed that regressing to keypoints on the object is better than regressing to vertices of an enveloping cuboid.",
            "6": "[10] S."
        },
        "Photo-realistic neural domain randomization": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2210.12682",
            "ref_texts": "48. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: CVPR. pp. 4561\u20134570 (2019)",
            "ref_ids": [
                "48"
            ],
            "1": "Correspondence-based methods [69,37,27,23,46,48] tend to show superior generalization performance in terms of adapting to different pose distributions."
        },
        "Keypoint-graph-driven learning framework for object pose estimation": {
            "authors": [
                "Shaobo Zhang",
                "Wanqing Zhao",
                "Ziyu Guan",
                "Xianlin Peng",
                "Jinye Peng"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Keypoint-Graph-Driven_Learning_Framework_for_Object_Pose_Estimation_CVPR_2021_paper.pdf",
            "ref_texts": "[26] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 1,2,5,6,7,8",
            "ref_ids": [
                "26"
            ],
            "1": "Recently, deep learning approaches [13,39,4,29,34,10,26,24,25,18,3] have shown impressive results of pose estimation in RGB\n*Corresponding authorimages.",
            "2": "Some keypoint-based approaches [27,25,34,10,26,31,42] build the correspondence using sparse 2D keypoints on objects as an intermediate representation for pose estimation.",
            "3": "Synthetic data generation Given 3D models of the objects, first, we define the keypoints on the surface of them as proposed in PVnet [26] whereKkeypoints are selected using the farthest point sampling (FPS) algorithm.",
            "4": "We use blender [26] to render these 3D models from different camera viewpoints to sufficiently cover the objects and project the keypoints to images under the viewpoints.",
            "5": "To evaluate the accuracy of the estimated pose, we use two standard metrics for LINEMOD used in other related paper [36,41,26] which are ADD and ADD-S (for symmetric objects).",
            "6": "labels w/o manual pose labels w/ manual pose labels Training data Syn Syn+Real Real Method AAE [33]1MHP [20]1DPOD [41]Self6D [36]1Ours YOLO6D [34] DPOD PVNet [26] CDPN [18] Ape 4.",
            "7": "We compare our method with state-of-the-art 6D pose estimation methods (AAE [33], MHP [20], DPOD [41] Self6D\n[36]) that use the synthetic images generated by 3D CAD models and the methods (YOLD6D [34], DPOD [41], PVNet [26], CDPN[18]) using real images with manual 3D annotations for training.",
            "8": "labels w/o manual pose labels w/ manual pose labels Training data Syn Syn+Real Real Method DPOD [41] CDPN [18] Self6D [36] Ours YOLO6D [34] HMap [23] PVNet [26] Mean 6.",
            "9": "We use the model trained on the synthetic images for testing on the Occlusion dataset and compare our method with the three methods (DPOD [41], CDPN [18] and Self6D [36]) that do not require manual pose labels for training and three methods (YOLO6D [34], HMap [23] and PVNet [26]) using manual pose labels for training.",
            "10": "1,2\n[26] S."
        },
        "Occlusion-aware self-supervised monocular 6D object pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2203.10339",
            "ref_texts": "[14] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , 2019, pp. 4561\u20134570.",
            "ref_ids": [
                "14"
            ],
            "1": "The most common way is to simply render a huge number of synthetic training images with tools such as OpenGL [12], [13] or Blender [14].",
            "2": "Similarly, SegDriven [47] and PVNet [14] also regress 2D projections of associated sparse 3D keypoints, however, both employ segmentation paired with voting to improve reliability.",
            "3": "Thus, we cannot resort to methods based on establishing 2D-3D correspondences [14], [50], [52], despite those currently dominating the field.",
            "4": "Following standard procedure [14], [45], [48], we train the pose estimator and refiner separately for each object.",
            "5": "On the other hand, as for training with real pose labels, we outperform all other recently published methods including PVNet [14] and CDPN [50] reporting a mean average recall of 91:0%w.",
            "6": "6 PVNet [14] 13:0(p)x13 Blender 43.",
            "7": "In TABLE 7 we compare our method against state of the art [14], [32], [33], [38] on YCBVideo w.",
            "8": "3 PVNet [14] 13:0(p)x21 Blender 73.",
            "9": "[14] S."
        },
        "Robust category-level 6d pose estimation with coarse-to-fine rendering of neural features": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2209.05624",
            "ref_texts": "23. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (June 2019)",
            "ref_ids": [
                "23"
            ],
            "1": "However, most prior work on 6D pose estimation focused on the \u201cinstance-level\u201d task, where exact CAD models of the object instances are available [35,23,19,10,12]."
        },
        "iCaps: Iterative category-level object pose and shape estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2201.00059",
            "ref_texts": "[5] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d in Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. , 2019, pp. 4556\u20134565.",
            "ref_ids": [
                "5"
            ],
            "1": "[5] S."
        },
        "UDA-COPE: unsupervised domain adaptation for category-level object pose estimation": {
            "authors": [
                "Taeyeop Lee",
                "Uk Lee",
                "Inkyu Shin",
                "Jaesung Choe",
                "Ukcheol Shin",
                "In So",
                "Jin Yoon"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Lee_UDA-COPE_Unsupervised_Domain_Adaptation_for_Category-Level_Object_Pose_Estimation_CVPR_2022_paper.pdf",
            "ref_texts": "[27] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4561\u20134570, 2019. 1",
            "ref_ids": [
                "27"
            ],
            "1": "Previous 6D object pose estimation methods follow the instance-level pose estimation schemes [12, 13, 25, 27, 31, 34,38] that rely on given 3D CAD model information (e."
        },
        "Dsc-posenet: Learning 6dof object pose estimation via dual-scale consistency": {
            "authors": [
                "Zongxin Yang",
                "Xin Yu",
                "Yi Yang"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2021/papers/Yang_DSC-PoseNet_Learning_6DoF_Object_Pose_Estimation_via_Dual-Scale_Consistency_CVPR_2021_paper.pdf",
            "ref_texts": "[29] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , pages 4561\u20134570, 2019. 2, 4, 7",
            "ref_ids": [
                "29"
            ],
            "1": "When an object undergoes occlusions or drastic illumination changes, those methods might fail to estimate object poses accurately [29].",
            "2": "Fully-supervised deep model based methods: Deep learning based methods have demonstrated promising pose estimation performance [29, 43, 47, 46, 19, 22].",
            "3": "Instead of treating pose estimation as a classification task, recent approaches directly regress 3D boundingboxes [30, 38], local features [43, 29] or coordinate maps [47, 42, 18, 51] of objects, and then predict object poses via PnP.",
            "4": "CPDN [49], DPOD [47] and Pix2Pose [18] output the 2D UV coordinates or 3D coordinates of 3D object models from images, while PoseCNN [43] and PVNet [29] employ Hough voting to localize object keypoints from estimated vector fields.",
            "5": "Self-supervised DSC-PoseNet Inspired by recent keypoint based estimation methods [29, 34], we use the intermediate object representation, i.",
            "6": "PVNet [29] predicts a vector field for each keypoint and employs voting to determine keypoint locations.",
            "7": ", AAE [37], MHP [25], DPOD [47] and PVNet [29], as well as RGBD based methods, i."
        },
        "Uni6d: A unified cnn framework without projection breakdown for 6d pose estimation": {
            "authors": [
                "Xiaoke Jiang",
                "Donghai Li",
                "Hao Chen",
                "Ye Zheng",
                "Rui Zhao",
                "Liwei Wu"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Jiang_Uni6D_A_Unified_CNN_Framework_Without_Projection_Breakdown_for_6D_CVPR_2022_paper.pdf",
            "ref_texts": "[30] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "30"
            ],
            "1": "We follow previous work [30,53] to split the training and testing sets, and we also obtain synthesis images for the training set as the same with [6, 53].",
            "2": "For LineMOD dataset, we follow [16,30] to report the accuracy of distance less than 10% of the objects\u2019 diameter (ADD-0."
        },
        "Single-view robot pose and joint angle estimation via render & compare": {
            "authors": [
                "Yann Labbe",
                "Justin Carpentier",
                "Mathieu Aubry",
                "Josef Sivic"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Labbe_Single-View_Robot_Pose_and_Joint_Angle_Estimation_via_Render__CVPR_2021_paper.pdf",
            "ref_texts": "[44] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 2",
            "ref_ids": [
                "44"
            ],
            "1": "For rigid objects, however, methods based on 2D keypoints [34,3,7,6,45,52,23,50,44,43,18] have been recently outperformed by render & compare methods that forgo explicit detection of 2D keypoints but instead use the entire shape of the object by comparing the rendered view of the 3D model to the input image and iteratively refining the object\u2019s 6D pose [59,31,25].",
            "2": "A set of sparse [45,52,23,50,44,43,18] or dense [56,41,49,59] features is detected on the object in the image using a CNN and theresulting 2D-to-3D correspondences are used to recover the camera pose using PnP [29]."
        },
        "Towards robust learning-based pose estimation of noncooperative spacecraft": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1909.00392",
            "ref_texts": "[21] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise V oting Network for 6DoF Pose Estimation,\u201d The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Oral , 2019.",
            "ref_ids": [
                "21"
            ],
            "1": "[21] S."
        },
        "Object pose estimation with statistical guarantees: Conformal keypoint detection and geometric uncertainty propagation": {
            "authors": [
                "Heng Yang",
                "Marco Pavone"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Object_Pose_Estimation_With_Statistical_Guarantees_Conformal_Keypoint_Detection_and_CVPR_2023_paper.pdf",
            "ref_texts": "[72] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dofpose estimation. IEEE Trans. Pattern Anal. Machine Intell. , 2022. 1,2,8",
            "ref_ids": [
                "72"
            ],
            "1": "One of the most popular paradigms for object pose estimation is a two-stage pipeline [20,71,72,79,81,85,89,101], where the first stage detects (semantic) keypoints of the objects on the image, and the second stage computes the object pose by solving an optimization known as Perspectiven-Points (PnP) that minimizes reprojection errors of the detected keypoints.",
            "2": ", PVNet [72]) and show that the average pose achieves better or similar accuracy.",
            "3": "Sparse methods define a handful of keypoints and predict locations of the keypoints via direct regression [74,89], probabilistic heatmap [67,71], or voting [72].",
            "4": "Baselines (results adapted from [72]) Conformalized heatmap Tekin PoseCNN Oberweger PVNet gt-ball gt-ellipse frcnn-ball frcnn-ellipse objects [89][95][67][72] \u270f=0."
        },
        "6-DoF pose estimation of household objects for robotic manipulation: An accessible dataset and benchmark": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2203.05701",
            "ref_texts": "[6] S. Peng, Y . Liu, Q. Huang, H. Bao, and X. Zhou, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d in CVPR , 2019. 1",
            "ref_ids": [
                "6"
            ],
            "1": "Recent progress on this 6-DoF (\u201cdegrees of freedom\u201d) pose estimation problem has been significant [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13].",
            "2": "1\n[6] S."
        },
        "Vs-net: Voting with segmentation for visual localization": {
            "authors": [
                "Zhaoyang Huang",
                "Han Zhou",
                "Yijin Li",
                "Bangbang Yang",
                "Yan Xu",
                "Xiaowei Zhou",
                "Hujun Bao",
                "Guofeng Zhang",
                "Hongsheng Li"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Huang_VS-Net_Voting_With_Segmentation_for_Visual_Localization_CVPR_2021_paper.pdf",
            "ref_texts": "[38] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "38"
            ],
            "1": "Keypoint is widely utilized as an intermediate representation in object pose estimation [38,20,34,37,50].",
            "2": "Recently, PVNet [38] significantly improves robustness and accuracy of object pose estimation by detecting keypoints with pixel-wise votes, inspired by which, we propose to detect scene-specific landmarks with pixelwise votes.",
            "3": "The initial estimation of the 2D location \u02c6ljof the landmark jis computed from RANSAC with a vote intersection model [38], which generates multiple landmark location hypotheses by computing intersections of two randomly sampled directional votes and choosing the hypothesis having the most inlier votes."
        },
        "Guided uncertainty-aware policy optimization: Combining learning and model-based strategies for sample-efficient policy learning": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2005.10872",
            "ref_texts": "[44] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR , 2019.",
            "ref_ids": [
                "44"
            ],
            "1": "The problem of known object pose estimation is a vibrant subject within the robotics and computer vision communities [19], [39], [40], [41], [42], [43], [44], [45], [46].",
            "2": "[44] also explored the problem of using uncertainty by leveraging a ransac voting algorithm to find regions where a keypoint could be detected.",
            "3": "[44] S."
        },
        "Centersnap: Single-shot multi-object 3d shape reconstruction and categorical 6d pose and size estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2203.01929",
            "ref_texts": "[17] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp.",
            "ref_ids": [
                "17"
            ],
            "1": "Unfortunately, these methods [16, 17, 18] do not generalize well to realistic-settings on novel object instances with unknown 3D models in the same category, often referred to as category-level settings .",
            "2": "[17] S."
        },
        "Self-supervised geometric perception": {
            "authors": [
                "Heng Yang",
                "Wei Dong",
                "Luca Carlone",
                "Vladlen Koltun"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Self-Supervised_Geometric_Perception_CVPR_2021_paper.pdf",
            "ref_texts": "[58] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , pages 4561\u20134570, 2019. 1,6",
            "ref_ids": [
                "58"
            ],
            "1": "Learned feature descriptors have been shown to consistently and significantly outperform their hand-crafted counterparts across applications such as relative camera pose estimation [69,61], 3D point cloud registration [21,32], and object detection and pose estimation [58,86,64,72].",
            "2": "For example, ground-truth relative camera poses are needed for training image keypoint descriptors [69,54,27], pairwise rigid transformations are required for training point cloud descriptors [21,32,74,85,70], and object poses are used to train image keypoint predictors [58,86].",
            "3": "For example, we also present the formulation for object detection and pose estimation [64,58,86,15], and discuss the application of SGP in the Supplementary Material."
        },
        "Category-level 6d object pose estimation in the wild: A semi-supervised learning approach and a new dataset": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/afe99e55be23b3523818da1fefa33494-Paper-Conference.pdf",
            "ref_texts": "[34] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019. 1",
            "ref_ids": [
                "34"
            ],
            "1": "One is performing instance-level 6D pose estimation, where a model is trained to estimate the pose of one exact instance with an existing 3D model [13,34,22,50,32,5,14]."
        },
        "Crt-6d: Fast 6d object pose estimation with cascaded refinement transformers": {
            "authors": [
                "Pedro Castro",
                "Kyun Kim"
            ],
            "url": "https://openaccess.thecvf.com/content/WACV2023/papers/Castro_CRT-6D_Fast_6D_Object_Pose_Estimation_With_Cascaded_Refinement_Transformers_WACV_2023_paper.pdf",
            "ref_texts": "[34] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019.",
            "ref_ids": [
                "34"
            ],
            "1": "ject pose estimators [41, 6, 34, 39, 35, 47, 9, 24, 40, 32, 3].",
            "2": ") These 5746\n features are transformed into an intermediate representation [34, 32, 47, 35, 15] which are then used to extract pose (using PnP [25] or other variations [40, 15]) or pose is extracted directly [7, 47, 9, 24].",
            "3": "NOCS, keypoint heatmaps [48, 35, 34, 51].",
            "4": "This shortfall was noticed by PVNet [34], which suggests the use of the surface region to find suitable keypoints.",
            "5": ", K} chosen for OSKFs are generated using the farthest point sampling algorithm [34], where Kis a hyperparameter of the number of used keypoints.",
            "6": "1 8 1 8 1 1 8 8 Method PVNet [34] GDR [47] GDR [47] SO-Pose [9] ZebraPose [40] RePose [20] DeepIM [28] CRT-6D Ape 15.",
            "7": "Repose [20] proposed a faster refinement method at 18ms with 5 iterations however they require a good initialization (they use PVNet [34] which itself takes over 25ms) and it only support a single object per model."
        },
        "Generative category-level shape and pose estimation with semantic primitives": {
            "authors": [
                "Anonymous Submission"
            ],
            "url": "https://proceedings.mlr.press/v205/li23d/li23d.pdf",
            "ref_texts": "[9]S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "9"
            ],
            "1": "Early works for this task are focused on instance-level pose estimation [7,8,9,10,11], which aligns the observed object with the given CAD model.",
            "2": "The second are correspondence-based methods [9,10,11,23].",
            "3": "For example, PVNet [9] predicts the 3D keypoints on the RGB image by a voting scheme.",
            "4": "[9]S."
        },
        "Perspective flow aggregation for data-limited 6d object pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2203.09836",
            "ref_texts": "32. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation. In: Conference on Computer Vision and Pattern Recognition (2019)",
            "ref_ids": [
                "32"
            ],
            "1": "When ample amounts of annotated real images are available, deep learning-based methods now deliver excellent results [7,32,31,46,38].",
            "2": "2 Related Work 6D pose estimation is currently dominated by neural network-based methods [11,32,38,37,14,2].",
            "3": "1 Data-Limited Pose Initialization Most pose reffnement methods [25,51,23] assume that rough pose estimates are provided by another approach trained on a combination of real and synthetic data [49], often augmented in some manner [32,10,14].",
            "4": "Most methods train their models for LINEMOD and Occluded-LINEMOD separately [11,26], sometimes even one model per object [32,47], which yields better accuracy but is less exible and does not scale well.",
            "5": "Furthermore, to compare with other methods on YCB-V, we also report the AUC metric as in [32,23,47], which varies the threshold with a maximum of 10cm and accumulates the area under the accuracy curve.",
            "6": "1 Comparison with the State of the Art We now compare our method to the state-of-the-art ones, PoseCNN [49], SegDriven [11], PVNet [32], GDR-Net [47], DeepIM [25], and CosyPose [23], where DeepIM and CosyPose are two reffnement methods based on an iterative strategy."
        },
        "Sar-net: Shape alignment and recovery network for category-level 6d object pose and size estimation": {
            "authors": [
                "Haitao Lin",
                "Zichang Liu",
                "Chilam Cheang",
                "Yanwei Fu",
                "Guodong Guo",
                "Xiangyang Xue"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Lin_SAR-Net_Shape_Alignment_and_Recovery_Network_for_Category-Level_6D_Object_CVPR_2022_paper.pdf",
            "ref_texts": "[39] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019. 1, 3, 4, 5, 6, 7",
            "ref_ids": [
                "39"
            ],
            "1": "However, most 6D pose estimation works [8,9,15,19,28,37, 39,55,57] assume exact 3D CAD object models at instancelevel, which unfortunately greatly limits their practical applicability in real-world applications.",
            "2": "Another line of works [29,36,38,39,42,50,65] first regress object coordinates or keypoints in 2D images and then recover poses by Perspective-n-Point algorithm [25], e.",
            "3": ", PVNet [39].",
            "4": "In contrast to these keypoint voting methods [15,16,39], our approach focuses on a more practical setting without relying on exact object 3D models.",
            "5": "We further sample the category-level template shape into a sparse 3D point cloudKc\u2208R3\u00d7Nkby using Farthest Point Sampling (FPS) algorithm [39], where Nkis the number of points.",
            "6": "Inspired by previous 2D [39, 63] and 3D [15, 16, 40] keypoint voting methods, we treat the object center as a specific keypoint.",
            "7": "Compared with RGB(-D) methods [15, 39] or depth-only method [12, 13], our SAR-Net achieves comparable results in terms of ADD(-S) metric as in Tab.",
            "8": "Training data Methods ape can cat driller eggbox glue RGB(S+R) PVNet [39] 43."
        },
        "Translating a visual lego manual to a machine-executable plan": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2207.12572",
            "ref_texts": "32.Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: CVPR (2019)",
            "ref_ids": [
                "32"
            ],
            "1": "Other works adopt a two-stage approach where 2D keypoints of objects are first extracted and then poses are inferred from them [33,31,32]."
        },
        "Dpodv2: Dense correspondence-based 6 dof pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2207.02805",
            "ref_texts": "[34] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR , 2019.",
            "ref_ids": [
                "34"
            ],
            "1": "Some of the most recent representatives include iPose [6], PVNet [34], DPOD [35], Pix2Pose [36], CDPN [37], and SDFlabel [38].",
            "2": "PVNet [34] takes a different approach and designs a network which for every pixel in the image regresses an offset to the predefined keypoints located on the object itself.",
            "3": "Train Data Real Method Pix2Pose [36] DPOD [35] PVNet [34] CDPN [37] HybridPose [42] Ours BB8 [30] PoseCNN [76] DPOD [35] Ours Ours Refinement DL [30] DeepIM [15] DL [35] 2 calib.",
            "4": "387 PVNet [34] ICP 0.",
            "5": "274 PVNet [34] 0.",
            "6": "[34] S."
        },
        "Ove6d: Object viewpoint encoding for depth-based 6d object pose estimation": {
            "authors": [
                "Dingding Cai",
                "Janne Heikkila",
                "Esa Rahtu"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Cai_OVE6D_Object_Viewpoint_Encoding_for_Depth-Based_6D_Object_Pose_Estimation_CVPR_2022_paper.pdf",
            "ref_texts": "[36] Sida Peng, Xiaowei Zhou, Yuan Liu, Haotong Lin, Qixing Huang, and Hujun Bao. Pvnet: pixel-wise voting network for 6dof object pose estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence , 2020. 1, 2, 6, 7, 8",
            "ref_ids": [
                "36"
            ],
            "1": "In recent works, the object pose estimation problem is commonly approached by either establishing local correspondences between the object 3D model and the observed data [16, 17, 36], or via direct regression [6, 39].",
            "2": "Related Work Pose estimation from RGB data Most RGB-based object 6D pose estimation methods [1,20,33,35,36,38,44,57] attempt to establish sparse or dense 2D-3D correspondences between the 2D coordinates in the RGB image and the 3D coordinates on the object 3D model surface.",
            "3": "5 PVNet [36] RGB 40.",
            "4": ")PVNet [36] RGB 42.",
            "5": "2 PVNet [36] RGBD \u2713 79.",
            "6": "0% recall on LMO with PVNet [36])."
        },
        "Nerf-pose: A first-reconstruct-then-regress approach for weakly-supervised 6d object pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2203.04802",
            "ref_texts": "[40] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019. 1, 2, 7, 8",
            "ref_ids": [
                "40"
            ],
            "1": "Most of the recent approaches [25, 31, 32, 40, 41, 51, 53, 61, 65, 27, 56, 7] require 6D pose labels as supervision signals.",
            "2": "As widely used in the recent methods [25, 31, 32, 40, 41, 51, 53, 61, 65, 27, 56, 7], segmentation mask can be obtained either manually or automatically with the off-the-shelf object segmentation approaches [14, 6], few-shot segmentation[59, 37], depth driven segmentation or background substaction[12], while relative camera poses can be obtained with Structure from Motion (SfM) [46], Simultaneous Localization And Mapping (SLAM) [1], Inertial Visual Odometry [30], or simply a marker board [24].",
            "3": "Object Tekin[53] DPOD[65] PVNet[40] CDPN[32] GDR[56] SO-Pose[7] LieNet[8] Cai.",
            "4": "Our-pose Our-pose Our-weak Our-weak [61] [40] [22] [50] [56] [7] [56] [7] [5] w/o NeRF w/ NeRF w/o NeRF w/ NeRF CAD w/ CAD w/ CAD w/o CAD w/o CAD training real+syn real+pbr real real Ape 9."
        },
        "Monocinis: Camera independent monocular 3d object detection using instance segmentation": {
            "authors": [
                "Jonas Heylen",
                "Mark De",
                "Bruno Dawagne",
                "Marc Proesmans",
                "Luc Van",
                "Wim Abbeloos",
                "Hazem Abdelkawy",
                "Daniel Olmeda"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021W/3DODI/papers/Heylen_MonoCInIS_Camera_Independent_Monocular_3D_Object_Detection_Using_Instance_Segmentation_ICCVW_2021_paper.pdf",
            "ref_texts": "[65] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp. 4561\u20134570, 2019.[66] Y . Hu, J. Hugonot, P. Fua, and M. Salzmann, \u201cSegmentationdriven 6d object pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp. 3385\u20133394, 2019.",
            "ref_ids": [
                "65",
                "66"
            ],
            "1": "Several existing methods can be used to map the 2D predicted RPs to a 3D object pose [60, 65, 66, 61, 62].",
            "2": "[65] S.",
            "3": "[66] Y ."
        },
        "Single-stage keypoint-based category-level object pose estimation from an RGB image": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2109.06161",
            "ref_texts": "[28] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixelwise voting network for 6DoF pose estimation,\u201d in CVPR , 2019, pp.",
            "ref_ids": [
                "28"
            ],
            "1": "Other works have explored different ways to better represent objects, including dense coordinate maps [1], keypoints [28], and symmetry correspondences [29].",
            "2": "[28] S."
        },
        "Parallel inversion of neural radiance fields for robust pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2210.10108.pdf?trk=public_post_comment-text",
            "ref_texts": "[9] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2019.",
            "ref_ids": [
                "9"
            ],
            "1": "[9] S."
        },
        "Gsnet: Joint vehicle pose and shape reconstruction with geometrical and scene-aware supervision": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2007.13124",
            "ref_texts": "39. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: CVPR (2019)",
            "ref_ids": [
                "39"
            ],
            "1": "Traditionally, 6D object pose estimation is handled by creating correspondences between the objects known 3D model and 2D pixel locations, followed by Perspective-n-Point (PnP) algorithm [45,54,39]."
        },
        "Stablepose: Learning 6d object poses from geometrically stable patches": {
            "authors": [
                "Yifei Shi",
                "Junwen Huang",
                "Xin Xu",
                "Yifan Zhang",
                "Kai Xu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Shi_StablePose_Learning_6D_Object_Poses_From_Geometrically_Stable_Patches_CVPR_2021_paper.pdf",
            "ref_texts": "[32] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019. 2",
            "ref_ids": [
                "32"
            ],
            "1": ", [36,43,32,51,39] and a survey [10])."
        },
        "Megapose: 6d pose estimation of novel objects via render & compare": {
            "authors": [
                "Anonymous Submission"
            ],
            "url": "https://arxiv.org/pdf/2212.06870",
            "ref_texts": "[44] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. InCVPR , 2019.",
            "ref_ids": [
                "44"
            ],
            "1": "These have been replaced by learning-based methods with convolutional neural networks that directly regress sets of sparse [41,42,27,43,44,45,46] or dense [47,48,49,50,3,44] features.",
            "2": "10\n[44] S."
        },
        "Focal length and object pose estimation via render and compare": {
            "authors": [
                "Georgy Ponimatkin",
                "Yann Labbe",
                "Bryan Russell",
                "Mathieu Aubry",
                "Josef Sivic"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Ponimatkin_Focal_Length_and_Object_Pose_Estimation_via_Render_and_Compare_CVPR_2022_paper.pdf",
            "ref_texts": "[34] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. In CVPR , pages 4561\u20134570, 2019. 2",
            "ref_ids": [
                "34"
            ],
            "1": "Previous approaches for this task primarily rely on establishing local 2D-3D correspondences between an image 3825\n and a 3D model using either hand-crafted [2,3,7,8,17,27] or CNN features [12,19,20,31,32,34,35,38,41,42,47,48], followed by robust camera pose estimation using PnP [23].",
            "2": "Both of these strategies rely on shallow hand-designed image features and have been revisited with learnable deep convolutional neural networks (CNNs)\n[19,20,31,32,34,35,38,41,42,47,48]."
        },
        "Category-level metric scale object shape and pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2109.00326",
            "ref_texts": "[19] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "19"
            ],
            "1": "Recent RGB-based approaches [19], [25] have Manuscript received: April 30, 2021; Revised July 27, 2021; Accepted August, 23, 2021.",
            "2": "[19], [20], [25], [27] detected the keypoints in the projected 3D points on the image and then solved a non-linear problem using the Perspective-n-Point (PnP) algorithm [12].",
            "3": "1) Normalized Object Mesh Estimation (Mesh header): Instead of training a single network for each category [19], [20],[25], [27], we aim to cover all object categories with a single network [26], [30]."
        },
        "Self-supervised category-level 6D object pose estimation with deep implicit shape representation": {
            "authors": [
                "Wanli Peng",
                "Jianhang Yan",
                "Hongtao Wen",
                "Yi Sun"
            ],
            "url": "https://ojs.aaai.org/index.php/AAAI/article/view/20104/19863",
            "ref_texts": "2016. 3d-r2n2: A unified approach for single and multiview 3d object reconstruction. In European conference on computer vision, 628\u2013644. Springer. Ester, M.; Kriegel, H.-P.; Sander, J.; and Xu, X. 1996. Density-based spatial clustering of applications with noise. InInt. Conf. Knowledge Discovery and Data Mining, volume 240, 6. Fan, H.; Su, H.; and Guibas, L. J. 2017. A point set generation network for 3d object reconstruction from a single image. In Proceedings of the IEEE conference on computer vision and pattern recognition, 605\u2013613. Groueix, T.; Fisher, M.; Kim, V . G.; Russell, B.; and Aubry, M. 2018. AtlasNet: A Papier-M \u02c6ach\u00b4e Approach to Learning 3D Surface Generation. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). He, K.; Gkioxari, G.; Doll \u00b4ar, P.; and Girshick, R. 2017. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, 2961\u20132969. He, Y .; Sun, W.; Huang, H.; Liu, J.; Fan, H.; and Sun, J. 2020. Pvn3d: A deep point-wise 3d keypoints voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 11632\u201311641. Kingma, D. P.; and Ba, J. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. Lin, J.; Wei, Z.; Li, Z.; Xu, S.; Jia, K.; and Li, Y . 2021. DualPoseNet: Category-level 6D Object Pose and Size Estimation using Dual Pose Network with Refined Learning of Pose Consistency. arXiv preprint arXiv:2103.06526. Liu, S.; Zhang, Y .; Peng, S.; Shi, B.; Pollefeys, M.; and Cui, Z. 2020. Dist: Rendering deep implicit signed distance function with differentiable sphere tracing. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019\u20132028. Lorensen, W. E.; and Cline, H. E. 1987. Marching cubes: A high resolution 3D surface construction algorithm. ACM siggraph computer graphics, 21(4): 163\u2013169. Manhardt, F.; Wang, G.; Busam, B.; Nickel, M.; Meier, S.; Minciullo, L.; Ji, X.; and Navab, N. 2020. CPS++: Improving Class-level 6D Pose and Shape Estimation From Monocular Images With Self-Supervised Learning. arXiv preprint arXiv:2003.05848. Mescheder, L.; Oechsle, M.; Niemeyer, M.; Nowozin, S.; and Geiger, A. 2019. Occupancy networks: Learning 3d reconstruction in function space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 4460\u20134470. Park, J. J.; Florence, P.; Straub, J.; Newcombe, R.; and Lovegrove, S. 2019. Deepsdf: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 165\u2013174. Park, K.; Mousavian, A.; Xiang, Y .; and Fox, D. 2020. Latentfusion: End-to-end differentiable reconstruction and rendering for unseen object pose estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 10710\u201310719. Peng, S.; Liu, Y .; Huang, Q.; Zhou, X.; and Bao, H. 2019. Pvnet: Pixel-wise voting network for 6dof pose estimation. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 4561\u20134570. Pitteri, G.; Ramamonjisoa, M.; Ilic, S.; and Lepetit, V . 2019. On object symmetries and 6d pose estimation from images. In2019 International Conference on 3D Vision (3DV), 614\u2013",
            "ref_ids": [
                "2016"
            ]
        },
        "Sparse steerable convolutions: An efficient learning of se (3)-equivariant features for estimation and tracking of object poses in 3d space": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/8c1b6fa97c4288a4514365198566c6fa-Paper.pdf",
            "ref_texts": "[17] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "17"
            ],
            "1": "These works can be broadly categorized into three types: i) template matching [12] by constructing templates to search for the best matched poses; ii) 2D-3D correspondence methods [1,14,16,19,17], which establish 2D-3D correspondence via 2D keypoint detection [19,17] or dense 3D coordinate predictions [1,14,16], followed by a PnP algorithm to obtain the target pose; iii) direct pose regression [26,13,23] via deep networks."
        },
        "Neural free-viewpoint performance rendering under complex human-object interactions": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2108.00362",
            "ref_texts": "[46] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. 2019. PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation. In CVPR .",
            "ref_ids": [
                "46"
            ],
            "1": "We believe the coarse-to-fine strategy [57] and the end-to-end 6DoF estimation [46] can accelerate human reconstruction and object tracking respectively."
        },
        "Pr-gcn: A deep graph convolutional network with point refinement for 6d pose estimation": {
            "authors": [
                "Guangyuan Zhou",
                "Huiqun Wang",
                "Jiaxin Chen",
                "Di Huang"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_PR-GCN_A_Deep_Graph_Convolutional_Network_With_Point_Refinement_for_ICCV_2021_paper.pdf",
            "ref_texts": "[29] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "29"
            ],
            "1": "PVNet [29] proposes a deep offset prediction model to alleviate negative impacts of occlusions.",
            "2": "Similar to [9, 29, 38], we select the candidate with the highest confidence score as the estimated pose, formulated as: (\u02c6Ro,\u02c6to) = argmaxn (\u02c6R(k) o,\u02c6t(k) o)|k=1,\u00b7\u00b7\u00b7,Kos(k) o.",
            "3": "RGB based methods RGB-D based methods Object PoseCNN* PVNet CDPN DPOD DPVL PF* SSD6D\u2020DF* PVN3D G2L* Ours* Ours [40, 18] [29] [19] [44] [42] [41] [14] [38] [9] [4] ape 77.",
            "4": "Object PoseCNN [40] DeepHeat [26] SS [12] Pix2pose [28] PVNet [29] HybridPose [34] PVN3D [9] Ours Ape 9.",
            "5": "We first compare PR-GCN to the state-of-the-art methods on Linemod, including the RGB based models: PoseCNN (+DeepIM) [40, 18], PVNet [29], CDPN [19], DPOD [44] and DPVL [42] and the RGB-D based ones: Point Fusion [41], SSD6D (+ICP) [14], Dense Fusion [38], PVN3D [9] and G2L[4].",
            "6": "1 ness of PR-GCN to inter-object occlusions, we display detailed results on Occlusion Linemod, in comparison with PoseCNN [40], DeepHeat [26], SS [12], Pix2Pose [28], PVNet [29], HybridPose [34] and PVN3D [9]."
        },
        "Robust 6d object pose estimation by learning rgb-d features": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2003.00188",
            "ref_texts": "[7] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp.",
            "ref_ids": [
                "7"
            ],
            "1": "To address the occlusion problem, dense prediction methods [5], [7]\u2013[10] are proposed.",
            "2": "[5] and [7] predict a unit vector at each pixel pointing towards keypoints.",
            "3": "Among them, [7] and [8] achieve top performances.",
            "4": "Translation In order to make use of the complementary depth information, we extend a RANSAC-based voting method [5], [7] from 2D to 3D space.",
            "5": "RGB w/o refinement RGB w/ refinement RGB-D w/o refinement RGB-D w/ refinement Method PoseCNN [5] PVNet [7] DPOD [8] DeepIM [41] DPOD+ [8] Per-Pixel DF [13] Ours Iterative DF [13] ape 21.",
            "6": "Comparison on LINEMOD We compare our method with the state-of-the-art object pose detectors [5], [7], [8], [13] on the LINEMOD dataset.",
            "7": "Of all those methods without pose refinement, PVNet [7] and Per-Pixel DF [13] are the state of the art on LINEMOD dataset.",
            "8": "[7] S."
        },
        "Complementary bi-directional feature compression for indoor 360deg semantic segmentation with self-distillation": {
            "authors": [
                "Zishuo Zheng",
                "Chunyu Lin",
                "Lang Nie",
                "Kang Liao",
                "Zhijie Shen",
                "Yao Zhao"
            ],
            "url": "https://openaccess.thecvf.com/content/WACV2023/papers/Zheng_Complementary_Bi-Directional_Feature_Compression_for_Indoor_360deg_Semantic_Segmentation_With_WACV_2023_paper.pdf",
            "ref_texts": "[26] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "26"
            ],
            "1": "mantic segmentation aims to assign each pixel in the image a category label and is critical for various applications such as pose estimation [26], autonomous vehicles [31], augmented reality [2]."
        },
        "Hpnet: Deep primitive segmentation using hybrid representations": {
            "authors": [
                "Siming Yan",
                "Zhenpei Yang",
                "Chongyang Ma",
                "Haibin Huang",
                "Etienne Vouga",
                "Qixing Huang"
            ],
            "url": "http://openaccess.thecvf.com/content/ICCV2021/papers/Yan_HPNet_Deep_Primitive_Segmentation_Using_Hybrid_Representations_ICCV_2021_paper.pdf",
            "ref_texts": "[18] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019 , pages 4561\u20134570. Computer Vision Foundation / IEEE, 2019. 3",
            "ref_ids": [
                "18"
            ],
            "1": "The main idea, which has proven to be successful for keypoint-based 6D object pose estimation [18], is to use a large-scale training set and a small-scale validation set."
        },
        "6dof object pose estimation via differentiable proxy voting loss": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2002.03923",
            "ref_texts": "[21] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "21"
            ],
            "1": "Recently, deep learning based object pose estimation methods have been proposed recently [13, 21, 30].",
            "2": "To increase the robustness to occlusions, voting strategies are employed to localize feature keypoints or coordinates [12, 21, 30].",
            "3": "Deep model based methods: Due to the powerful feature representation ability of deep neural networks, deep learning based methods have demonstrated impressive results on object pose estimation [13, 21, 30].",
            "4": "Motivated by the state-of-the-art image detection methods [18, 23], pose estimation approaches are designed to localize objects while predicting their viewpoints based on the estimation of 3D bounding-boxes [13, 22, 27], features of interest [21, 30] or coordinate maps [14, 29, 31].",
            "5": "Rather than only estimating a centroid, PVNet [21] votes several features of interest, while the work [12] votes the corners of a 3D boundingbox from each segmentation grid.",
            "6": "Since voting based methods [21, 30] have demonstrated their robustness to occlusions and view changes, we therefore follow the voting based pose estimation pipeline.",
            "7": "Motivated by the works [21, 30], our network simultaneously outputs a segmentation mask for an object and keypoint vector-fields, as shown in Fig.",
            "8": "3 Network architecture and training strategy To demonstrate the effectiveness of our proposed loss, we adopt the same architecture as PVNet [21], as illustrated in Fig.",
            "9": "Following [21], we render 10,000 images and synthesize 10,000 images by \u201cCut and Paste\" for each object.",
            "10": "Comparisons on LINEMOD: We compare our algorithm with BB8 [22], SSD6D [13], YOLO6D [27], DPOD [31], Pix2Pose [14], CDPN [32], PoseCNN [30], and PVNet [21] on ADD(-S) scores (in Table 1) and 2D projection errors (in Table 2)."
        },
        "TTA-COPE: Test-Time Adaptation for Category-Level Object Pose Estimation": {
            "authors": [
                "Taeyeop Lee",
                "Jonathan Tremblay",
                "Valts Blukis",
                "Bowen Wen",
                "Uk Lee",
                "Inkyu Shin",
                "Stan Birchfield",
                "In So",
                "Jin Yoon"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_TTA-COPE_Test-Time_Adaptation_for_Category-Level_Object_Pose_Estimation_CVPR_2023_paper.pdf",
            "ref_texts": "[28] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4561\u20134570, 2019. 1",
            "ref_ids": [
                "28"
            ],
            "1": "Advanced methods that focus on diverse variations of object 6D pose estimation have been introduced, such as known 3D objects (instancelevel) [28, 38], category-level [18, 36, 43], few-shot [52], and zero-shot pose estimation [13, 47]."
        },
        "On object symmetries and 6d pose estimation from images": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1908.07640",
            "ref_texts": "[22] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. PVNet: Pixel-Wise V oting Network for 6DoF Pose Estimation. In Conference on Computer Vision and Pattern Recognition , 2019. 1, 2, 6",
            "ref_ids": [
                "22"
            ],
            "1": "Introduction 3D object detection and pose estimation are of primary importance for tasks such as robotic manipulation, virtual and augmented reality and they have been the focus of intense research in recent years, mostly due to the advent of Deep Learning based approaches and the possibility of using large datasets for training such methods [12, 7, 17, 23, 27, 16, 29, 22].",
            "2": "[21, 22] predict the 2D projections of 3D points from image patches or local features, to avoid the effects of occluders when performing the prediction.",
            "3": "We chose to predict the objects\u2019 6D poses in the form of the 2D reprojections of the 8 corners of the 3D bounding boxes, as in [23, 27, 28, 22] for simplicity.",
            "4": "2\n[22] S."
        },
        "3d object detection and pose estimation of unseen objects in color images with local surface embeddings": {
            "authors": [
                "Giorgia Pitteri",
                "Aurelie Bugeau",
                "Slobodan Ilic",
                "Vincent Lepetit"
            ],
            "url": "http://openaccess.thecvf.com/content/ACCV2020/papers/Pitteri_3D_Object_Detection_and_Pose_Estimation_of_Unseen_Objects_in_ACCV_2020_paper.pdf",
            "ref_texts": "8. Peng,S.,Liu,Y.,Huang,Q.,Bao,H.,Zhou,X.: PVNet:Pixe l-WiseVotingNetwork for 6DoF Pose Estimation. CoRR abs/1812.11788 (2018)",
            "ref_ids": [
                "8"
            ],
            "1": "1 Introduction Deep Learning (DL) provides powerful techniques to estimate the 6D pose of an object from color images, and impressive results have been achieve d over the last years [1,2,3,4,5,6], including in the presence of occlusions [7,8,9], in the absence of texture, and for objects with symmetries (which create p ose ambiguities) [10,11]."
        },
        "A survey on deep learning based methods and datasets for monocular 3D object detection": {
            "authors": [],
            "url": "https://www.mdpi.com/2079-9292/10/4/517/pdf",
            "ref_texts": "72. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. PVNet: Pixel-wise Voting Network for 6DOF Pose Estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 16\u201320 June 2019; pp. 4561\u20134570.",
            "ref_ids": [
                "72"
            ],
            "1": "For example, a pixel-wise voting network (PVNet) [72] predicts pixel-level indicators corresponding to the key points so that they can handle truncation or occlusion of object parts.",
            "2": "PVNet [72] also uses a denser key point prediction method, as shown in Figure 11.",
            "3": "Overview of the keypoint localization in PVNet [72].",
            "4": "The most recent trend in monocular 3D object detection is learning deep neural networks to directly regress the 6D pose from a single image [25\u201327,68,75] or to estimate the 2D positions of 3D key points and solve the PnP algorithm [28\u201330,72,76,78,79]."
        },
        "CPS++: Improving class-level 6D pose and shape estimation from monocular images with self-supervised learning": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2003.05848",
            "ref_texts": "3515 Park JJ, Florence P, Straub J, Newcombe R, Lovegrove S (2019a) Deepsdf: Learning continuous signed distance functions for shape representation. In: CVPR Park K, Patten T, Vincze M (2019b) Pix2pose: Pixel-wise coordinate regression of objects for 6d pose estimation. In: ICCV Park K, Mousavian A, Xiang Y, Fox D (2020) Latentfusion: End-to-end difierentiable reconstruction and rendering for unseen object pose estimation. In: CVPR, pp 10710{10719 Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, Killeen T, Lin Z, Gimelshein N, Antiga L, et al. (2019) Pytorch: An imperative style, high-performance deep learning library. In: NeurIPS, pp 8026{8037 Peng S, Liu Y, Huang Q, Zhou X, Bao H (2019) Pvnet: Pixelwise voting network for 6dof pose estimation. In: CVPR Qi CR, Su H, Mo K, Guibas LJ (2017) Pointnet: Deep learning on point sets for 3d classiffcation and segmentation. In: CVPR Rad M, Lepetit V (2017) BB8: A scalable, accurate, robust to partial occlusion method for predicting the 3D poses of challenging objects without using depth. In: ICCV, pp 3828{3836 Ren S, He K, Girshick R, Sun J (2015) Faster r-cnn: Towards real-time object detection with region proposal networks. In: NeurIPS Romea AC, Torres MM, Srinivasa S (2011) The moped framework: Object recognition and pose estimation for manipulation. International Journal of Robotics Research 30(10):1284 { 1306 Simonelli A, Rota Bulo S, Porzi L, Lopez-Antequera M, Kontschieder P (2019) Disentangling monocular 3d object detection. In: ICCV Song S, Xiao J (2016) Deep sliding shapes for amodal 3d object detection in rgb-d images. In: CVPR Sundermeyer M, Marton ZC, Durner M, Brucker M, Triebel R (2018a) Implicit 3d orientation learning for 6d object detection from rgb images. In: ECCV Sundermeyer M, Marton ZC, Durner M, Brucker M, Triebel R (2018b) Implicit 3d orientation learning for 6d object detection from rgb images. In: ECCV, pp 699{715 Sundermeyer M, Durner M, Puang EY, Marton ZC, Vaskevicius N, Arras KO, Triebel R (2020) Multi-path learning for object pose estimation across domains. In: CVPR, pp 13916{13925 Tekin B, Sinha SN, Fua P (2018) Real-time seamless single shot 6d object pose prediction. In: CVPR Tian Z, Shen C, Chen H, He T (2019) FCOS: Fully convolutional one-stage object detection. In: ICCV, pp 9627{9636 Tremblay J, To T, Birchffeld S (2018) Falling things: A synthetic dataset for 3d object detection and pose estimation. In: CVPRW, pp 2038{2041 Umeyama S (1991) Least-squares estimation of transformation parameters between two point patterns. IEEE Transactions on Pattern Analysis & Machine Intelligence pp 18 F. Manhardt et al."
        },
        "Mobilepose: Real-time pose estimation for unseen objects with weak shape supervision": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2003.03522",
            "ref_texts": "20. Peng, S., Liu, Y., Huang, Q., Bao, H., Zhou, X.: Pvnet: Pixel-wise voting network for 6DoF pose estimation. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019)",
            "ref_ids": [
                "20"
            ],
            "1": "in pose estimation has been made by leveraging 2D-3D correspondence at inference time [14,32,20,15,19].",
            "2": "This is often overlooked in previous methods [20,32,15,30], where expensive algorithms are widely used, e.",
            "3": "PVNet [20] ffnds 2D-3D correspondence of object features, and formulates pose estimation as a PnP problem.",
            "4": "segmentation [21,8], 3D features [20], parameterization map [32], coordinate map [30,15,19], etc."
        },
        "Nemo: Neural mesh models of contrastive features for robust 3d pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2101.12378",
            "ref_texts": "6-dof object pose from semantic keypoints. In 2017 IEEE international conference on robotics and automation (ICRA) , pp. 2011\u20132018. IEEE, 2017. 2, 3 Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp. 4561\u20134570, 2019. 3 Nikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia Gkioxari. Accelerating 3d deep learning with pytorch3d. arXiv:2007.08501 , 2020."
        },
        "Learning to detect scene landmarks for camera localization": {
            "authors": [
                "Tien Do",
                "Ondrej Miksik",
                "Joseph De",
                "Hyun Soo",
                "Sudipta N. Sinha"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Do_Learning_To_Detect_Scene_Landmarks_for_Camera_Localization_CVPR_2022_paper.pdf",
            "ref_texts": "[52] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019. 3[53] No \u00b4e Pion, Martin Humenberger, Gabriela Csurka, Yohann Cabon, and Torsten Sattler. Benchmarking image retrieval for visual localization. In 3DV, 2020. 2",
            "ref_ids": [
                "52",
                "53"
            ],
            "1": "Further, image retrieval-based methods use scalable techniques [25, 27,45,79] to estimate the query camera pose by interpolating poses of the retrieved database images [11, 53,79,80].",
            "2": "These were initially proposed to find the 6DoF pose of small objects using random forests [34], random ferns [49], and nowadays, CNNs [48, 51,52,55].",
            "3": "3[53] No \u00b4e Pion, Martin Humenberger, Gabriela Csurka, Yohann Cabon, and Torsten Sattler."
        },
        "Single shot 6d object pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2004.12729",
            "ref_texts": "[10] S. Peng, Y . Liu, Q. Huang, H. Bao, and X. Zhou, \u201cPVNet: Pixelwise voting network for 6dof pose estimation,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 2019.",
            "ref_ids": [
                "10"
            ],
            "1": "Recent works regress the 2D image coordinates of the object\u2019s 3D bounding box and use a P nP algorithm to estimate the object\u2019s 6D pose [8], [9], [10].",
            "2": "Approaches for 6D OPE relying on a prior semantic segmentation step [8], [13], [14], [15], [10] cannot be used for scenes of many parts of the same type in bulk.",
            "3": "[10] S."
        },
        "Object level depth reconstruction for category level 6d object pose estimation from monocular rgb image": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2204.01586",
            "ref_texts": "19. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561{4570 (2019)",
            "ref_ids": [
                "19"
            ],
            "1": "To improve keypoint detection performance, PVNet [19] formulates a voting scheme, which is more robust towards occlusion and truncation."
        },
        "Cloudaae: Learning 6d object pose regression with on-line data synthesis on point clouds": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2103.01977",
            "ref_texts": "[30] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR , 2019.",
            "ref_ids": [
                "30"
            ],
            "1": "Depending on the applicability of the methods, for each dataset, we compare to a subset of stateof-the-art methods SSD-6D [29], EEPG-AAE [15], CloudPose [8], PVNet [30], PoseCNN [4], DenseFusion [12], PVN3D [13] and PointV oteNet [31].",
            "2": "9 PVNet [30] 47.",
            "3": "[30] S."
        },
        "Uni6dv2: Noise elimination for 6d pose estimation": {
            "authors": [],
            "url": "https://proceedings.mlr.press/v206/sun23b/sun23b.pdf",
            "ref_texts": "2125. Mallick, T., Das, P. P., and Majumdar, A. K. (2014). Characterizations of noise in kinect depth images: A review. IEEE Sensors journal , 14(6):1731\u20131740. Marchand, E., Uchiyama, H., and Spindler, F. (2015). Pose estimation for augmented reality: a hands-on survey. IEEE transactions on visualization and computer graphics, 22(12):2633\u20132651. Mo, N., Gan, W., Yokoya, N., and Chen, S. (2022). Es6d: A computation efficient and symmetry-aware 6d pose regression framework. arXiv preprint arXiv:2204.01080 . Mingshan Sun, Ye Zheng, Tianpeng Bao, Jianqiu Chen, et al. Newell, A., Yang, K., and Deng, J. (2016). Stacked hourglass networks for human pose estimation. In European conference on computer vision , pages 483\u2013499. Oberweger, M., Rad, M., and Lepetit, V . (2018). Making deep heatmaps robust to partial occlusions for 3d object pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV) , pages 119\u2013134. Peng, S., Liu, Y ., Huang, Q., Zhou, X., and Bao, H. (2019a). Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "2125"
            ]
        },
        "Monocular relative pose estimation pipeline for uncooperative resident space objects": {
            "authors": [],
            "url": "https://re.public.polimi.it/bitstream/11311/1216796/3/PIAZM_OA_01-22.pdf",
            "ref_texts": "[24]Peng, S., Liu, Y., Huang, Q., Zhou, X., and Bao, H., \u201cPVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation,\u201d 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , IEEE, 2019, pp. 4556\u20134565. https: //doi.org/10.1109/CVPR.2019.00469.",
            "ref_ids": [
                "24"
            ],
            "1": "[24]Peng, S."
        },
        "Es6d: A computation efficient and symmetry-aware 6d pose regression framework": {
            "authors": [
                "Ningkai Mo",
                "Wanshui Gan",
                "Naoto Yokoya",
                "Shifeng Chen"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Mo_ES6D_A_Computation_Efficient_and_Symmetry-Aware_6D_Pose_Regression_Framework_CVPR_2022_paper.pdf",
            "ref_texts": "[26] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019. 1",
            "ref_ids": [
                "26"
            ],
            "1": "In recent years, methods based on the deep neural network (DNN) have gradually emerged [17, 22, 25, 26, 40]."
        },
        "Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin Picking": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2204.07049",
            "ref_texts": "37. Peng, S., Zhou, X., Liu, Y., Lin, H., Huang, Q., Bao, H.: Pvnet: pixel-wise voting network for 6dof object pose estimation. TPAMI (2020) 1, 3",
            "ref_ids": [
                "37"
            ],
            "1": "Recently, learning-based models [22,37,44,51] thatarXiv:2204.",
            "2": "1 6D Object Pose Estimation for Bin-Picking Though recent works [10,26,37,40] show superior performance on household object datasets (e."
        },
        "Optimal pose and shape estimation for category-level 3d object perception": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2104.08383",
            "ref_texts": "[57] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. PVNet: Pixel-wise V oting Network for 6DoF Pose Estimation. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , pages 4561\u20134570, 2019. 2",
            "ref_ids": [
                "57"
            ],
            "1": "Such approaches first recover the position of semantic keypoints [56] in the images with neural networks, and then recover the 3D pose of the object by solving a geometric optimization problem [31, 53, 56, 57, 64].",
            "2": "1, 2, 3\n[57] S."
        },
        "Refine-net: Normal refinement neural network for noisy point clouds": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2203.12514",
            "ref_texts": "[2] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
            "ref_ids": [
                "2"
            ],
            "1": "As standard outputs of these 3D sensors, point clouds have been flexibly used in various applications, ranging from 6-degree virtual reality [1], [2], robotics [3] to autonomous driving [4], [5].",
            "2": "[2] S."
        },
        "Pfrl: Pose-free reinforcement learning for 6d pose estimation": {
            "authors": [
                "Jianzhun Shao",
                "Yuhang Jiang",
                "Gu Wang",
                "Zhigang Li",
                "Xiangyang Ji"
            ],
            "url": "http://openaccess.thecvf.com/content_CVPR_2020/papers/Shao_PFRL_Pose-Free_Reinforcement_Learning_for_6D_Pose_Estimation_CVPR_2020_paper.pdf",
            "ref_texts": "[25] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "25"
            ],
            "1": "Some people [24, 29, 25] followed the conventional way to build the 2D3D correspondences and subsequently solve the pose via Figure 1."
        },
        "Learning deep network for detecting 3d object keypoints and 6d poses": {
            "authors": [
                "Wanqing Zhao",
                "Shaobo Zhang",
                "Ziyu Guan",
                "Wei Zhao",
                "Jinye Peng",
                "Jianping Fan"
            ],
            "url": "https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhao_Learning_Deep_Network_for_Detecting_3D_Object_Keypoints_and_6D_CVPR_2020_paper.pdf",
            "ref_texts": "[25] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019.",
            "ref_ids": [
                "25"
            ],
            "1": "Other methods [33, 25] are 6D object pose detection pipelines containing a CNN architecture for object detection.",
            "2": "[25] S."
        },
        "Polarimetric pose prediction": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2112.03810",
            "ref_texts": "41. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
            "ref_ids": [
                "41"
            ],
            "1": "Some methods in this field use sparse correspondences [43,41,47,25], while others establish dense 2D-3D pairs [57,40,36,22]."
        },
        "Pointvotenet: Accurate object detection and 6 dof pose estimation in point clouds": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1912.09057",
            "ref_texts": "[6]S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in IEEE Conference on Computer Vision and Pattern Recognition , pp. 4561\u20134570, 2019.",
            "ref_ids": [
                "6"
            ],
            "1": "The method in [6] uses a compromising approach where a semi-dense set of keypoints are predicted over the image.",
            "2": "[2,3,6], the training examples are gathered from real scenes, each annotated with one or more ground truth poses of objects.",
            "3": "15/85 % for train/test) for the LINEMOD dataset as in earlier works, such as [1 \u20133, 6].",
            "4": "Both PoseCNN [5] and PVNet [6], which both use image data, produce much less accurate poses for this dataset.",
            "5": "On the[8] [2] [6] [10] Ours Ape 65.",
            "6": "The competing methods are SSD-6D [8], BB8 [2], PVNet [6], and DenseFusion [10].",
            "7": "[5] [6] Ours [5] Ours Ape 9.",
            "8": "[6]S."
        },
        "Visual identification of articulated object parts": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2012.00284",
            "ref_texts": "[29] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise V oting Network for 6DoF Pose Estimation,\u201d in Conf. on Computer Vision and Pattern Recognition (CVPR) , 2019.",
            "ref_ids": [
                "29"
            ],
            "1": "Of these works, PVNet [29] also regresses to a residual (pointing to object keypoints for pose estimation), whereas our motion residual is used to infer the kinematic constraint.",
            "2": "[29] S."
        },
        "Monocular localization with vector HD map (MLVHM): A low-cost method for commercial IVs": {
            "authors": [],
            "url": "https://www.mdpi.com/1424-8220/20/7/1870/pdf",
            "ref_texts": "31. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 16\u201320 June 2019.",
            "ref_ids": [
                "31"
            ],
            "1": "In recent years, with the development of deep learning and the improvement of on-board computing ability, the information contained in an image can be interpreted down to pixel-level resolution [30], thereby enabling more complex image feature recognition such as semantic information extraction [31,32]."
        },
        "Clearpose: Large-scale transparent object dataset and benchmark": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2203.03890",
            "ref_texts": "14. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561{4570 (2019)",
            "ref_ids": [
                "14"
            ],
            "1": "StereObj1M [12] benchmarked KeyPose and another RGB-based object pose estimator, PVNet [14], on more challenging objects and scenes, where both methods achieved lower accuracy with respect to the ADD-S AUC metric (introduced in [19]) with both monocular and stereo input."
        },
        "DCL-Net: Deep Correspondence Learning Network for 6D Pose Estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2210.05232",
            "ref_texts": "31. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019) 1, 3, 14 DCL-Net: Deep Correspondence Learning Network for 6D Pose Estimation 17",
            "ref_ids": [
                "31"
            ],
            "1": "Many of the data-driven methods [3,14,20,23,28,31,33,34,38,41] thus achieve the estimation by learning point correspondence between camera and object coordinate systems.",
            "2": "2 Related Work 6D Pose Estimation from RGB Data This body of works can be broadly categorized into three types: i) holistic methods [11,15,18] for directly estimating object poses; ii) keypoint-based methods [28,33,34], which establish 2D-3D correspondence via 2D keypoint detection, followed by a PnP/RANSAC algorithm to solve the poses; iii) dense correspondence methods [3,20,23,31], which make dense pixel-wise predictions and vote for the final results.",
            "3": "PoseCNN\n[45]DeepHeat [29]SS\n[17]Pix2pose [30]PVNet [31]HybridPose [35]PVN3D\n[14]PR-GCN\n[47]FFB6D\n[13]DCL-Net ape 9."
        },
        "Sc6d: Symmetry-agnostic and correspondence-free 6d object pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2208.02129",
            "ref_texts": "[32] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 2",
            "ref_ids": [
                "32"
            ],
            "1": "These methods [34, 32, 40, 33, 45, 13, 14] are also called keypoint-based approaches which detect the predefined keypoints (known 3D coordinates on the object 3D model) from the input data.",
            "2": "proposed an occlusion-robust approach PVNet [32], which predicts the pixel-wise voting vectors to localize the keypoints defined based on the object 3D model instead of on the 3D bounding box."
        },
        "Spatial feature mapping for 6DoF object pose estimation": {
            "authors": [
                "Jianhan Mei",
                "Xudong Jiang",
                "Henghui Ding"
            ],
            "url": "https://arxiv.org/pdf/2206.01831",
            "ref_texts": "[25] S. Peng, Y. Liu, Q. Huang, X. Zhou, H. Bao, Pvnet: Pixel-wise voting network for 6dof pose estimation, in: IEEE Conf. Comput. Vis. Pattern Recog., 2019, pp.",
            "ref_ids": [
                "25"
            ],
            "1": "Recently, inspired by the traditional methods of inferring pose parameters using the correspondence, [6, 25, 26] deffne and calculate the keypoints of the objects, and then ffnd the pose based on the corresponding keypoints through the \"Perspective-n-Point\" (PnP) algorithm.",
            "2": "Following the above-mentioned pose estimation idea based on keypoints, [6, 25, 26, 27] predict the coordinate map directly from the 2D convolutional features where 3D information is insuflciently used.",
            "3": "[6, 25] learn the keypoints of 2D to 3D matching through the instance segmentation framework to enhance the description of the object pose, which brings the system performance to a new level.",
            "4": "For keypoint selection, we follow the settings in [25, 28].",
            "5": "We create the training set following [25] that crops object masks under ground-truth poses in each image and render them to difierent background images from PASCAL VOC [84].",
            "6": "Methods only based on RGB image (DeepIM [57], PVNet [25], CDPN [27]) and methods using the depth information (Point-Fusion [9], DF (perpixel) [9], DF (iterative) [9], PVN3D [28]) are compared.",
            "7": "So, the 24 proposed method still achieves comparable results with the state-of-the-art three methods [57, 25, 27].",
            "8": "[25] S."
        },
        "PoET: Pose Estimation Transformer for Single-View, Multi-Object 6D Pose Estimation": {
            "authors": [
                "Anonymous Submission"
            ],
            "url": "https://proceedings.mlr.press/v205/jantos23a/jantos23a.pdf",
            "ref_texts": "[20] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "20"
            ],
            "1": "In recent years, advancements in deep learning for computer vision tasks have been applied to singleview, image-based 6D pose estimation, either to replace components of classical approaches [17, 18, 19, 20], or as end-to-end learned methods, where the 6D pose is directly estimated from the input using convolutional neural networks (CNNs).",
            "2": "[20] S."
        },
        "Detarnet: Decoupling translation and rotation by siamese network for point cloud registration": {
            "authors": [
                "Zhi Chen",
                "Fan Yang",
                "Wenbing Tao"
            ],
            "url": "https://ojs.aaai.org/index.php/AAAI/article/view/19917/19676",
            "ref_texts": "408 Li, Y .; Wang, G.; Ji, X.; Xiang, Y .; and Fox, D. 2018. DeepIM: Deep Iterative Matching for 6D Pose Estimation. In European Conference on Computer Vision (ECCV). Liu, Y .; Liu, L.; Lin, C.; Dong, Z.; and Wang, W. 2021. Learnable Motion Coherence for Correspondence Pruning. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 3237\u20133246. Ma, Y .; Soatto, S.; Kosecka, J.; and Sastry, S. S. 2012. An invitation to 3-d vision: from images to geometric models , volume 26. Springer Science & Business Media. Moo Yi, K.; Trulls, E.; Ono, Y .; Lepetit, V .; Salzmann, M.; and Fua, P. 2018. Learning to find good correspondences. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2666\u20132674. Myronenko, A.; and Song, X. 2010. Point set registration: Coherent point drift. IEEE transactions on pattern analysis and machine intelligence, 32(12): 2262\u20132275. Pais, G. D.; Ramalingam, S.; Govindu, V . M.; Nascimento, J. C.; Chellappa, R.; and Miraldo, P. 2020. 3DRegNet: A deep neural network for 3D point registration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 7193\u20137203. Peng, S.; Liu, Y .; Huang, Q.; Zhou, X.; and Bao, H. 2019. PVNet: Pixel-wise V oting Network for 6DoF Pose Estimation. In CVPR. Pl\u00a8otz, T.; and Roth, S. 2018. Neural nearest neighbors networks. In Advances in Neural Information Processing Systems, 1087\u20131098. Qi, C. R.; Su, H.; Mo, K.; and Guibas, L. J. 2017. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, 652\u2013660. Rusinkiewicz, S.; and Levoy, M. 2001. Efficient variants of the ICP algorithm. In Proceedings third international conference on 3-D digital imaging and modeling, 145\u2013152. IEEE. Rusu, R. B.; Blodow, N.; and Beetz, M. 2009. Fast point feature histograms (FPFH) for 3D registration. In 2009 IEEE international conference on robotics and automation, 3212\u2013"
        },
        "6D pose estimation with combined deep learning and 3D vision techniques for a fast and accurate object grasping": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2111.06276"
        },
        "Optimal and robust category-level perception: Object pose and shape estimation from 2D and 3D semantic keypoints": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2206.12498",
            "ref_texts": ""
        },
        "SymmetryNet: Learning to predict reflectional and rotational symmetries of 3D shapes from single-view RGB-D images": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2008.00485",
            "ref_texts": "2001, volume 1, pages I\u2013I. IEEE, 2001. Yanxi Liu, Hagit Hel-Or, Craig S Kaplan, Luc Van Gool, et al. Computational symmetry in computer vision and computer graphics. Foundations and Trends \u00aein Computer Graphics and Vision , 5(1\u20132):1\u2013195, 2010. Giovanni Marola. On the detection of the axes of symmetry of symmetric and almost symmetric planar images. IEEE Transactions on Pattern Analysis and Machine Intelligence , 11(1):104\u2013108, 1989. Aur\u00e9lien Martinet, Cyril Soler, Nicolas Holzschuch, and Fran\u00e7ois X Sillion. Accurate detection of symmetries in 3d shapes. ACM Transactions on Graphics (TOG) , 25(2): 439\u2013464, 2006. Niloy J Mitra, Leonidas J Guibas, and Mark Pauly. Partial and approximate symmetry detection for 3d geometry. In ACM Transactions on Graphics (TOG) , volume 25, pages 560\u2013568. ACM, 2006. Niloy J Mitra, Mark Pauly, Michael Wand, and Duygu Ceylan. Symmetry in 3d geometry: Extraction and applications. In Computer Graphics Forum , volume 32, pages 1\u201323. Wiley Online Library, 2013. Hideo Ogawa. Symmetry analysis of line drawings using the hough transform. Pattern Recognition Letters , 12(1):9\u201312, 1991. Maks Ovsjanikov, Jian Sun, and Leonidas Guibas. Global intrinsic symmetries of shapes. InComputer graphics forum , volume 27, pages 1341\u20131348. Wiley Online Library, 2008. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems , pages 8024\u20138035, 2019. Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6dof pose estimation. In Proc. CVPR , pages 4561\u20134570, 2019. Joshua Podolak, Philip Shilane, Aleksey Golovinskiy, Szymon Rusinkiewicz, and Thomas Funkhouser. A planar-reflective symmetry transform for 3d shapes. In ACM SIGGRAPH 2006 Papers , pages 549\u2013559. 2006. Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 652\u2013660, 2017. ACM Trans. Graph., Vol. 1, No. 1, Article 1. Publication date: August 2020. SymmetryNet: Learning to Predict Reflectional and Rotational Symmetries of 3D Shapes from Single-View RGB-D Images \u20221:15 D. Raviv, A. M. Bronstein, M. M. Bronstein, and R. Kimmel. Symmetries of non-rigid shapes. In Proc. ICCV , pages 1\u20137. IEEE, 2007. Dan Raviv, Alexander M Bronstein, Michael M Bronstein, and Ron Kimmel. Full and partial symmetries of non-rigid shapes. International journal of computer vision , 89"
        },
        "PointPoseNet: Point pose network for robust 6D object pose estimation": {
            "authors": [
                "Wei Chen",
                "Jinming Duan",
                "Hector Basevi",
                "Hyung Jin",
                "Ales Leonardis"
            ],
            "url": "http://openaccess.thecvf.com/content_WACV_2020/papers/Chen_PonitPoseNet_Point_Pose_Network_for_Robust_6D_Object_Pose_Estimation_WACV_2020_paper.pdf",
            "ref_texts": "[22] S. Peng, Y . Liu, Q. Huang, H. Bao, and X. Zhou. Pvnet: Pixel-wise voting network for 6dof pose estimation. arXiv preprint arXiv:1812.11788 , 2018.",
            "ref_ids": [
                "22"
            ],
            "1": "Instead of regressing 6D pose directly, some other methods [25, 27, 20, 33, 22, 21] make use of 2D keypoints as an intermediate representation for pose estimation.",
            "2": "Inspired by [23, 22], we estimate 6D object pose via multi-stage.",
            "3": "Our method shares features of PVNet [22]: both methods use unit vector regression to estimate pose, however, our method takes point cloud, and instead of using 2D keypoints we use 3D keypoints, then we use our proposed scoring mechanism to access the final pose which is different to the optimization based method in [22].",
            "4": "Learning-based methods [25, 22, 27, 20] address this problem by training their model to predict the 2D keypoints and estimate the pose by PnP algorithm [3, 19].",
            "5": "Different to [22, 38] which predict dense vectors pointing to 2D keypoints, our network predicts dense vectors pointing to 3D keypoints.",
            "6": "However, different from 2D cases in [22, 38] where two nonparallel lines always have an intersection, two nonparallel lines in 3D can have no intersection.",
            "7": "Another way is proposed in [22] which uses the farthest point sampling (FPS) algorithm to sample the keypoints.",
            "8": "We refer to the mechanism which uses the mean value of all pose hypotheses without scoring mechanism as \u201cMEA\u201d and the mechanism using similar optimization method as [22] to compute the final pose from pose hypotheses as \u201cOPT\u201d.",
            "9": "OPT means using similar optimization method as [22] to compute final pose from pose hypotheses.",
            "10": "From Table 2, we can see that our method outperforms its 2D counterpart PVNet [22], the baseline and other state-of-the-art methods, which shows that our method can better utilize 3D information from depth image.",
            "11": "For symmetric objects Egg Box andGlue we use ADD-S metric Method PVNet [22]PoseCNN + DeepIM [38, 13]Frustum-P [23] Hinterstoisser [5] DenseFusion [35] Ours Ape 43.",
            "12": "[22] S."
        },
        "6dof pose estimation of transparent object from a single rgb-d image": {
            "authors": [],
            "url": "https://www.mdpi.com/1424-8220/20/23/6790/pdf",
            "ref_texts": "13. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation. In Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 16\u201318 June 2019; pp. 4561\u20134570. [CrossRef]",
            "ref_ids": [
                "13"
            ],
            "1": "The recovered surface normal, the plane, and the UV map are essential components of the extended point-cloud representation, which contains rich geometric information for 6DoF pose estimation: (1) the recovered surface normal contains important hint to estimate the object\u2019s relative pose (3DoF rotation); (2) the plane where the object placed is closely related to the object\u2019s 3D position; and (3) the UV map encodes the 2D coordinates of points on image, which is crucial for 6DoF object pose estimation [1,13].",
            "2": "The roles of these three components are as follows: (1) PUV(x) indicates the 2D location of a pixel on image plane, and it is an important hint for 6DoF pose estimation [1,13]; (2) Pnorm(x)indicates the surface normal of object, and it is important for relative pose (3DoF rotation) estimation; and (3) Pplane (x)provides the hint where the object is placed, and it helps conduct more accurate 3DoF translation estimation."
        },
        "Interacting Hand-Object Pose Estimation via Dense Mutual Attention": {
            "authors": [
                "Rong Wang",
                "Wei Mao",
                "Hongdong Li"
            ],
            "url": "https://openaccess.thecvf.com/content/WACV2023/papers/Wang_Interacting_Hand-Object_Pose_Estimation_via_Dense_Mutual_Attention_WACV_2023_paper.pdf",
            "ref_texts": "[31] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "31"
            ],
            "1": "Hand-Object Pose Estimation Most previous works tackle 3D hand pose estimation [17, 25, 40, 50, 47] and object pose estimation [27, 31, 44, 49] separately."
        },
        "DemoGrasp: Few-shot learning for robotic grasping with human demonstration": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2112.02849",
            "ref_texts": "[35] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR , 2019.",
            "ref_ids": [
                "35"
            ],
            "1": "The last and most prominent branch of works establishes 2D-3D correspondences and optimize for pose using a variant of the P nP/RANSAC paradigm [33], [34], [35], [36].",
            "2": "[35] S."
        },
        "E2EK: End-to-end regression network based on keypoint for 6D pose estimation": {
            "authors": [],
            "url": "https://uwe-repository.worktribe.com/index.php/preview/9655564/E2EK_RAL_finalversion_plain.pdf",
            "ref_texts": ""
        },
        "Knowledge Distillation for 6D Pose Estimation by Aligning Distributions of Local Predictions": {
            "authors": [
                "Shuxuan Guo",
                "Yinlin Hu",
                "Jose M. Alvarez",
                "Mathieu Salzmann"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Guo_Knowledge_Distillation_for_6D_Pose_Estimation_by_Aligning_Distributions_of_CVPR_2023_paper.pdf",
            "ref_texts": "[33] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise V oting Network for 6DoF Pose Estimation. In Conference on Computer Vision and Pattern Recognition , 2019. 1, 2, 3, 4",
            "ref_ids": [
                "33"
            ],
            "1": "Modern methods that tackle this task [7,20,21,25,28,33,40,45,47] all rely on deep neural networks.",
            "2": "ject surface [33], others produce dense representations, such as 3D locations [7,45] or binary codes [40], from which the pose can be obtained.",
            "3": "In particular, several techniques jointly segment the object and predict either the 2D image locations of the corners of the 3D object bounding box [19\u201321] or the 2D displacements from the cells\u2019 center of points on the object\u2019s surface [33]; Oberweger et al.",
            "4": "As discussed above, we focus on approaches that produce local predictions, such as sparse 2D keypoints [19\u201321, 33] or dense quantities [7, 28, 40, 45].",
            "5": "In particular, we consider the case of predicting the 2D locations of the 8 object bounding box corners [19\u201321, 33]."
        },
        "Multi-Object Manipulation via Object-Centric Neural Scattering Functions": {
            "authors": [
                "Stephen Tian",
                "Yancheng Cai",
                "Xing Yu",
                "Sergey Zakharov",
                "Katherine Liu",
                "Adrien Gaidon",
                "Yunzhu Li",
                "Jiajun Wu"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Tian_Multi-Object_Manipulation_via_Object-Centric_Neural_Scattering_Functions_CVPR_2023_paper.pdf",
            "ref_texts": "[41] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. In CVPR , pages 4561\u20134570, 2019. 2",
            "ref_ids": [
                "41"
            ],
            "1": "Many prior works investigate the problem of estimating rigid object poses from RGB images, including deep-learning approaches based on correspondences [24, 35, 40, 41, 47, 58] as well as direct regression [9, 13, 29, 53, 59]."
        },
        "Roft: Real-time optical flow-aided 6d object pose and velocity tracking": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2111.03821",
            "ref_texts": "[2]S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVnet: Pixel-wise voting network for 6DoF pose estimation,\u201d in 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . IEEE, 2019, pp.",
            "ref_ids": [
                "2"
            ],
            "1": "Several approaches have been proposed to tackle the problems of 6D object pose estimation [1], [2], [3], refinement [4], [5] and tracking [6], [7].",
            "2": "MkandTkcan be obtained using either separate deep learning-based networks for segmentation and 6D object pose estimation, or a single network which performs both tasks jointly [1], [2].",
            "3": "[2]S."
        },
        "Pixel2mesh++: 3d mesh generation and refinement from multi-view images": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2204.09866",
            "ref_texts": "[68] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2019, pp. 4561\u20134570.",
            "ref_ids": [
                "68"
            ],
            "1": "[68] regard camera pose estimation as a key point regression problem, and use voting strategy to densely estimatethe key point offset.",
            "2": "We use four types of standard metrics to evaluate our camera pose estimation method: 2D reprojection errord2D, mean distance d3D[4], 2D reprojection accuracy Acc 2Dand average 3D distance of model points (ADD) accuracy metric ADD 3D[68].",
            "3": "Different from the threshold selected for general 6 DOF estimation tasks [68], we use more strict criteria.",
            "4": "[68] S."
        },
        "A pose proposal and refinement network for better 6d object pose estimation": {
            "authors": [
                "Ameni Trabelsi",
                "Mohamed Chaabane",
                "Nathaniel Blanchard",
                "Ross Beveridge"
            ],
            "url": "https://openaccess.thecvf.com/content/WACV2021/papers/Trabelsi_A_Pose_Proposal_and_Refinement_Network_for_Better_6D_Object_WACV_2021_paper.pdf",
            "ref_texts": "[21] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6DoF pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "21"
            ],
            "1": "Most existing RGB-based methods [13, 20, 21, 23] take advantage of deep learning techniques used for object detection [5, 10, 16, 25] or image segmentation [17] and leverage them for 6D pose estimation.",
            "2": "First, the 2D-projection error, analogously to [21], measures the average distance between the 2D projections in the image space 2387\n Table 1.",
            "3": "We use a threshold of 2 cm for the ADD(-S) metric Methods HMap[20] PVNet[21] DeepIM\u2020[15] OURS\u2020\n2D-Proj 39.",
            "4": "We report percentages of correctly estimated poses averaged over all object classes Method Tekin[28] PVNet[21] SSD6D\u2020[13] DeepIM\u2020[15] OURS\u2020 ADD(-S) 55.",
            "5": "We report percentages of correctly estimated poses averaged over all object classes Method HMap[20] PVNet[21] BB8\u2020[23] DeepIM\u2020[15] OURS\u2020 ADD(-S) 30."
        },
        "SO (3)\u2010Pose: SO (3)\u2010Equivariance Learning for 6D Object Pose Estimation": {
            "authors": [
                "Haoran Pan"
            ],
            "url": "https://arxiv.org/pdf/2208.08338"
        },
        "Ikea-manual: Seeing shape assembly step by step": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/b645d1a085bcb39bece5c03703b62464-Paper-Datasets_and_Benchmarks.pdf",
            "ref_texts": "[37] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DOF pose estimation. In CVPR , 2019. 6",
            "ref_ids": [
                "37"
            ],
            "1": "Combined with the annotation of assembly parts, this annotation can be leveraged in pose estimation [36,37] and single-view 3D reconstruction [38, 39] tasks."
        },
        "Shape-Constraint Recurrent Flow for 6D Object Pose Estimation": {
            "authors": [
                "Yang Hai",
                "Rui Song",
                "Jiaojiao Li",
                "Yinlin Hu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Hai_Shape-Constraint_Recurrent_Flow_for_6D_Object_Pose_Estimation_CVPR_2023_paper.pdf",
            "ref_texts": "[36] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-Wise V oting Network for 6DoF Pose Estimation. In Conference on Computer Vision and Pattern Recognition , 2019. 2, 5, 6",
            "ref_ids": [
                "36"
            ],
            "1": "Related Work Object pose estimation , has shown significant improvement [36, 47, 50] after the utilization of deep learning techniques [13, 51].",
            "2": "Most recent methods create the correspondence either by predicting 2D points of some predefined 3D points [18, 21, 36, 37] or predicting the corresponding 3D point for every 2D pixel location within a segmentation mask [3, 10, 29, 42, 48, 53].",
            "3": "Some previous methods [10,48,52] use different training settings for LM and LM-O, and some methods train a separated model for every single object [23, 36].",
            "4": "Comparison to the State of the Art We compare our method with most state-of-the-art methods, including PoseCNN [50], PVNet [36], SO-Pose [10], DeepIM [28], RePose [23], RNNPose [52], and PFA [16]."
        },
        "Reconstruct locally, localize globally: A model free method for object pose estimation": {
            "authors": [
                "Ming Cai",
                "Ian Reid"
            ],
            "url": "https://openaccess.thecvf.com/content_CVPR_2020/papers/Cai_Reconstruct_Locally_Localize_Globally_A_Model_Free_Method_for_Object_CVPR_2020_paper.pdf",
            "ref_texts": "[40] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 1,3,8",
            "ref_ids": [
                "40"
            ],
            "1": "This predefined structural information contributes variously to the classical geometry methods[10,57,31,25,39] and recent machine learning based methods[41,22,48,54,27,47,20,40,52,55,7].",
            "2": "As for the CNN-based approaches, this model acts such as the supervision for network learning[2,38,4,22], a source for synthetic image generation[40,27,22,9] and/or an agent for post-process refinement[27,41,22]etc.",
            "3": "PVNet[40] proposes a method that automatically discovers a set of keypoints on the 3D object surface based on the physical structure, to ensure that their 2D projection are all within the silhouette.",
            "4": "[40,42,35] use the textured object model and random poses to generate a large amount of synthetic images to augment (or replace) the limited training images, preventing the network from overfitting.",
            "5": "LINEMOD contains 13 objects sequences 3159\n w/ CAD model w/o CAD model methodBB8\n[41]BB8 w/ rSSD-6D w/ r [22]Tekin [48]DeepIM w/ r [27]DenseFusion [52]Pix2Pose [38]PVNet w/ r [40]SSD-6D\n[22]LieNet [11]Ours ape 27.",
            "6": "Our method outperforms more than half of the learning-based methods and achieves comparable result with the state-of-the-art method, which use a large amount of synthetic training images from new viewpoints [40] and/or 3D model for refinement [52,27].",
            "7": "ADD-10 results are shown in Table 3following the test scheme of [40].",
            "8": "The additional landmark branch provides consistence for objects across mul-Tekin [48]PoseCNN\n[54]Oberweger [35]PVNet [40]Pix2Pose [38]Ours ape 2."
        },
        "Introducing pose consistency and warp-alignment for self-supervised 6d object pose estimation in color images": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2003.12344",
            "ref_texts": "[27] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019.",
            "ref_ids": [
                "27"
            ],
            "1": "The most popular approach is to estimate the correspondence between 2D images and 3D object models to obtain both 3D pose and translation via a PerspectivenPoint (PnP) algorithm [20, 28, 27, 25, 44].",
            "2": "One branch of such methods [28, 39, 27], including BB8 [28], directly regresses pixel locations of 3D bounding box positions on 2D images where positions of 3D bounding boxes are predetermined.",
            "3": "[27] S.",
            "4": "We followed the standard practice to generate training dataset [28, 27]."
        },
        "Sequential voting with relational box fields for active object detection": {
            "authors": [
                "Qichen Fu",
                "Xingyu Liu",
                "Kris Kitani"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Fu_Sequential_Voting_With_Relational_Box_Fields_for_Active_Object_Detection_CVPR_2022_paper.pdf",
            "ref_texts": "[26] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "26"
            ],
            "1": "Our method overcomes inconsistencies across the RBF by using a technique similar to [9,26,32,36], where our voting function finds the consensus from pixel-wise predictions by selecting the bounding box with a majority vote.",
            "2": "[26, 36] use pixel-wise predictions with Hough voting to localize keypoint for pose estimation."
        },
        "Multi-view object pose refinement with differentiable renderer": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2207.02811",
            "ref_texts": "[14] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR , 2019.",
            "ref_ids": [
                "14"
            ],
            "1": "In contrast, PVNet [14] used per-pixel Hough votingto allow all object\u2019s pixels to vote for the few keypoints lying on the object.",
            "2": "Train data Synthetic Real GT labels Method OURS SSD6D [12] OURSOURS Pix2Pose [16] PVNet [14] DPOD [4] HybPose [15]closest views random views farthest views Refinement DL [5] DR1 DR2 DR4 DR2 DR4 DR2 DR4 Ape 26.",
            "3": "Train data SyntReal Autolabels Real GT labelsDR2 DR4 Method OURS OURS OURS DPOD [4] PVNet [14] OURS CDPN [17] HybP.",
            "4": "[14] S."
        },
        "ACR-Pose: Adversarial canonical representation reconstruction network for category level 6D object pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2111.10524",
            "ref_texts": "[29] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "29"
            ],
            "1": "Over the last decade, numerous 6D object pose estimation works [13,21, 29,34,37, 47,51] have emerged and been deployed in industrial products, demonstrating the usefulness of this line of research.",
            "2": "6D Object Pose Estimation Instance-level 6D object pose estimation only estimates the 6D pose of a particular object can be divided into five parts: direct-methods [17, 47, 53], keypoint-based methods [29, 34, 37], dense coordinate-based methods [22, 27, 51], refinement based methods [21, 25, 48] and self-supervised methods [33, 42]."
        },
        "ConvPoseCNN: Dense convolutional 6D object pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1912.07333",
            "ref_texts": "[22] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
            "ref_ids": [
                "22"
            ],
            "1": "[22] also removed the RoIpooled orientation prediction branch, but with a different method: Here, 2D directions to a fixed number of keypoints are densely predicted.",
            "2": "[22] S."
        },
        "Rigidity-Aware Detection for 6D Object Pose Estimation": {
            "authors": [
                "Yang Hai",
                "Rui Song",
                "Jiaojiao Li",
                "Mathieu Salzmann",
                "Yinlin Hu"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Hai_Rigidity-Aware_Detection_for_6D_Object_Pose_Estimation_CVPR_2023_paper.pdf",
            "ref_texts": "[33] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-Wise V oting Network for 6DoF Pose Estimation. In Conference on Computer Vision and Pattern Recognition , 2019. 2",
            "ref_ids": [
                "33"
            ],
            "1": "Related Work Object pose estimation , whose goal is to estimate the 3D rotation and 3D translation of a target object with respect tothe camera, nowadays typically involves a pose regression network to establish 3D-to-2D correspondences [12,15\u201317, 19, 20, 33]."
        },
        "DGECN: A depth-guided edge convolutional network for end-to-end 6D pose estimation": {
            "authors": [
                "Tuo Cao",
                "Fei Luo",
                "Yanping Fu",
                "Wenxiao Zhang",
                "Shengjie Zheng",
                "Chunxia Xiao"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Cao_DGECN_A_Depth-Guided_Edge_Convolutional_Network_for_End-to-End_6D_Pose_CVPR_2022_paper.pdf",
            "ref_texts": "[28] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "28"
            ],
            "1": "It is widely used in the three-dimensional registration of AR [1, 28, 45], robotic vision [27, 31] and 3D reconstruction [9, 10].",
            "2": "Current object pose estimation methods can be divided into two types: 1) the object poses are estimated using a single RGB image [17, 27, 28, 31, 45] or 2) an RGB image accompanying a depth image [14,39,41].",
            "3": "PVNet [28] and Seg-Driven [17] conducted segmentation coupled with voting for each correspondence to make the estimation more robust.",
            "4": "Afterwards, like GDR-Net [42] and PVNet [28], we locate each object in the image with the method of FCN [24].",
            "5": "To deal with multiple objects segmentation, previous works [17, 28, 41, 45] use existing detection or semantic segmentation algorithms.",
            "6": "The 3D keypoints are selected from the 3D object model as in [14, 28].",
            "7": "We follow [28] and adopt the farthest point sampling (FPS) algorithm to select keypoints on object surface.",
            "8": "4 PVNet [28]DG-PnP(Ours) 23.",
            "9": "Our DGECN is comparable to [7, 21, 42] and outperforms [16, 28].",
            "10": "9 PVNet [28] % 47."
        },
        "6d-vit: Category-level 6d object pose estimation via transformer-based instance representation learning": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2110.04792",
            "ref_texts": "[7] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp.",
            "ref_ids": [
                "7"
            ],
            "1": "[7] presented the prediction of a unit vector for each pixel pointing toward the keypoints.",
            "2": "[7] S."
        },
        "SMOC-Net: Leveraging Camera Pose for Self-Supervised Monocular Object Pose Estimation": {
            "authors": [
                "Tao Tan",
                "Qiulei Dong"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Tan_SMOC-Net_Leveraging_Camera_Pose_for_Self-Supervised_Monocular_Object_Pose_Estimation_CVPR_2023_paper.pdf",
            "ref_texts": "[22] Sida Peng, Xiaowei Zhou, Yuan Liu, Haotong Lin, Qixing Huang, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "22"
            ],
            "1": "Some existing methods for self-supervised monocular object pose estimation [16, 31] use only synthetic images with object poses (which are generated via Blender [22] or some other rendering tools [24, 29]) for training.",
            "2": "However, due to the domain gap between real and synthetic data, the performances of these self-supervised methods are significantly lower compared to the fully-supervised methods [22, 35].",
            "3": "Keypoint-based methods [22, 28, 32] established correspondences by detecting keypoints in 2D images.",
            "4": "[22] proposed a pixel level voting network (PVNet) by using the direction vector field to predict keypoints, which achieved good performance under severe truncation and occlusion.",
            "5": "6 PVNet [22] 43.",
            "6": "Here, we evaluate the proposed SMOC-Net on the LineMOD dataset in comparison to some state-of-the-art methods, including three fullysupervised methods (DPOD [41], PVNet [22], CDPN [17]), three self-supervised methods that are trained with only synthetic data (AAE [31], MHP [20], DPOD [41]), one selfsupervised method that is trained with both synthetic data and un-annotated real images (DSC-PoseNet [39]), and two self-supervised methods that are trained with synthetic data + un-annotated real images + depth images (Self6D [34], Self6D++ [33])."
        },
        "TANet: towards fully automatic tooth arrangement": {
            "authors": [],
            "url": "https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123600477.pdf",
            "ref_texts": "25. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4561{4570 (2019)",
            "ref_ids": [
                "25"
            ],
            "1": "It aims to infer the three-dimensional pose, which has six degrees of freedom, of an object present in an RGB image, [3, 7, 45, 5, 33, 34, 18, 25], RGB-D image [39, 40, 35], or point cloud data [26, 44, 29, 30]."
        },
        "Fast uncertainty quantification for deep object pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2011.07748",
            "ref_texts": "[25] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR , 2019.",
            "ref_ids": [
                "25"
            ],
            "1": "Recent leading methods rely on an approach similar to the one used in our work: A network is trained to predict object keypoints in the 2D image, followed by P nP [28] to estimate the pose of the object in the camera coordinate frame [2, 5, 24, 25, 27, 29].",
            "2": "[25] S."
        },
        "T6d-direct: Transformers for multi-object 6d pose direct regression": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2109.10948",
            "ref_texts": "20. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: PVNet: Pixel-wise voting network for 6DOF pose estimation. In: CVPR, pp. 4561{4570 (2019)",
            "ref_ids": [
                "20"
            ],
            "1": "Indirect approaches aim to recover the 6D pose from the 2D-3D correspondences using the P nP algorithm, where P nP is often used in combination with T6D-Direct: Transformers for Multi-Object 6D Pose Direct Regression 3 the RANSAC algorithm to increase the robustness against outliers in correspondence prediction [10, 20, 23, 29].",
            "2": "We compare our results with PoseCNN [35], PVNet [20] and DeepIM [14]."
        },
        "Cullnet: Calibrated and pose aware confidence scores for object pose estimation": {
            "authors": [
                "Kartik Gupta",
                "Lars Petersson",
                "Richard Hartley"
            ],
            "url": "http://openaccess.thecvf.com/content_ICCVW_2019/papers/R6D/Gupta_CullNet_Calibrated_and_Pose_Aware_Confidence_Scores_for_Object_Pose_ICCVW_2019_paper.pdf",
            "ref_texts": "[17] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019. 3",
            "ref_ids": [
                "17"
            ],
            "1": "Recently proposed PV-Net [17] tries to address the problem of partial occlusion in RGB based object pose estimation by regressing for dense pixel-wise unit vectors pointing to the keypoints, which are combined together using RANSAC like voting scheme."
        },
        "Occlusion-aware region-based 3D pose tracking of objects with temporally consistent polar-based local partitioning": {
            "authors": [],
            "url": "https://zx007zls.github.io/zls_ch/TIP2020_preprint.pdf",
            "ref_texts": "[46] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 4561\u2013",
            "ref_ids": [
                "46"
            ],
            "1": "Apart from template matching, a lot of deep learning-based 3D object detection methods have been proposed recently [41]\u2013[46].",
            "2": "These methods train deep neural networks either to directly predict the pose parameters [41]\u2013\n[43], or to first predict the keypoint locations and then estimate the 6-DOF pose via the PnP algorithm [44]\u2013[46].",
            "3": "When GPU is available, deep learning-based methods (such as [44], [46]) could be incorporated for better performance.",
            "4": "[46] S."
        },
        "Occlusion-robust object pose estimation with holistic representation": {
            "authors": [
                "Bo Chen",
                "Jun Chin",
                "Marius Klimavicius"
            ],
            "url": "http://openaccess.thecvf.com/content/WACV2022/papers/Chen_Occlusion-Robust_Object_Pose_Estimation_With_Holistic_Representation_WACV_2022_paper.pdf",
            "ref_texts": "[44] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 1, 2, 5, 7, 8",
            "ref_ids": [
                "44"
            ],
            "1": "Rather than directly regressing the pose, two-stage approaches [23, 25, 35, 41, 42, 44, 46, 64, 51, 43, 57] first predict landmarks on the object to establish 2D-3D correspondences, then use a Perspective-n-Point (PnP) like algorithm to solve for the pose.",
            "2": "Current works to improve robustness often take the pixel-wise or patch-wise approach [44, 41, 23, 25, 35, 42], i.",
            "3": "PVNet [44] predicts the object mask and, for each pixel within the mask, unit vectors that points to the landmarks.",
            "4": "For LINEMOD, we follow the convention of previousworks [46, 57, 44, 64] by using 15% of the images of each object as training set and the remaining 85% as testing set.",
            "5": "For the YCB-Video dataset we also report the AUC metric proposed in [62] and adopted in [41, 44].",
            "6": "Implementation details For each object model we apply the farthest point sampling (FPS) algorithm [44] on the 3D point cloud and select 11 landmarks.",
            "7": "We also define a measure of incoherence ci=\u2225(xi\u2212x\u2217 i)\u2212m\u22252 (4)\n2934\n ADD(-S)Without refinement With refinement PVNet Pix2Pose DPOD CDPN GDR Ours SSD-6D DPOD+ HybridPose DeepIM\n[44] [42] [64] [35] [59] [27] [64] [51] [33] ape 43.",
            "8": "ADD(-S)Without refinement With refinement HM PVNet Hu Pix2Pose DPOD Hu2 GDR Ours DPOD+ HybridPose [41] [44] [23] [42] [64] [22] [59] [64] [51] ape 15.",
            "9": "Data efficiency The LINEMOD dataset has about 1200 images for each object, which results in approximately 180 images (15%)\n2935\n ADD(-S) AUC of ADD(-S) Without refinement Without refinement With refinement HM Hu Hu2 GDR Ours HM PVNet GDR Ours DeepIM CosyPose [41] [23] [22] [59] [41] [44] [59] [34] [30] master chef can 31.",
            "10": "For example, PVNet [44] renders 20000 images for each object and the same strategy is adopted in [52]."
        },
        "Stability-driven contact reconstruction from monocular color images": {
            "authors": [
                "Zimeng Zhao",
                "Binghui Zuo",
                "Wei Xie",
                "Yangang Wang"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Zhao_Stability-Driven_Contact_Reconstruction_From_Monocular_Color_Images_CVPR_2022_paper.pdf",
            "ref_texts": "[43] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , pages 4561\u20134570, 2019. 2",
            "ref_ids": [
                "43"
            ],
            "1": "With the rapid increase of 3D hand datasets [16,33,61,64,68] and object datasets [23, 33, 61], data-driven methods [2, 15, 25, 26, 30, 37, 43, 54, 60, 63, 66, 67] become popular in the community."
        },
        "CAD2Render: A Modular Toolkit for GPU-accelerated Photorealistic Synthetic Data Generation for the Manufacturing Industry": {
            "authors": [
                "Steven Moonen",
                "Bram Vanherle",
                "Taoufik Bourgana",
                "Abdellatif Bey",
                "Nick Michiels"
            ],
            "url": "https://openaccess.thecvf.com/content/WACV2023W/PIES-CV/papers/Moonen_CAD2Render_A_Modular_Toolkit_for_GPU-Accelerated_Photorealistic_Synthetic_Data_Generation_WACVW_2023_paper.pdf",
            "ref_texts": "[17] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019.",
            "ref_ids": [
                "17"
            ],
            "1": "The identification of the objects and subsequent position and pose estimation was done with two state of the art networks: YoloV4 [2] for object detection and PVNET [17] for pose estimation."
        },
        "VoteHMR: Occlusion-aware voting network for robust 3D human mesh recovery from partial point clouds": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2110.08729",
            "ref_texts": "[32] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. 2019. PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019 . Computer Vision Foundation / IEEE, Long Beach, 4561\u20134570. https: //doi.org/10.1109/CVPR.2019.00469",
            "ref_ids": [
                "32"
            ],
            "1": "PVN [32] predicted the unit vector field for a given image and PVN3D [12] predicted the keypoints offsets and instance semantic segmentation label for each pixel on a given RGBD image."
        },
        "Super-BPD: Super boundary-to-pixel direction for fast image segmentation": {
            "authors": [
                "Jianqiang Wan",
                "Yang Liu",
                "Donglai Wei",
                "Xiang Bai",
                "Yongchao Xu"
            ],
            "url": "https://openaccess.thecvf.com/content_CVPR_2020/papers/Wan_Super-BPD_Super_Boundary-to-Pixel_Direction_for_Fast_Image_Segmentation_CVPR_2020_paper.pdf",
            "ref_texts": "[32] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6dof pose estimation. In Proc. of CVPR , pages 4561\u20134570, 2019. 3",
            "ref_ids": [
                "32"
            ],
            "1": "PifPaf [24] and PVNet [32] leverage direction cue for 2D human pose estimation and 6 DoF pose estimation, respectively."
        },
        "Pyrapose: Feature pyramids for fast and accurate object pose estimation under domain shift": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2010.16117",
            "ref_texts": "[5] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixelwise voting network for 6DoF pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
            "ref_ids": [
                "5"
            ],
            "1": "The best performing deep-learning approaches for single-shot object pose estimation employ encoder-decoder architectures [2], [3], [4], [5], [6].",
            "2": "The dominant strategy is to employ encoder-decoder architectures for dense hypotheses generation and subsequent pose estimation using the PnP algorithm [13], [14], [3], [15], [4], [5], [16], [6].",
            "3": "Therefore, only one model needs to be trained per dataset in contrast to the majority of state-of-the-art approaches [30], [4], [5], [17], [24], [27], [16], [6].",
            "4": "The dominant approach to recover the most likely reference frame transformation that aligns a set of 2D points to a set of noisy 3D points [2], [4], [5], [16], [6] is PnP.",
            "5": "[5] S."
        },
        "Rede: End-to-end object 6d pose robust estimation using differentiable outliers elimination": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2010.12807",
            "ref_texts": "[13] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp.",
            "ref_ids": [
                "13"
            ],
            "1": "Therefore, PVNet [13] employs the farthest point sampling (FPS) algorithm to select more representative keypoints.",
            "2": "As [13], we employ the farthest point sampling (FPS) algorithm to sample 3D keypoints fmkgK k=1from the CAD model of each object.",
            "3": "However, this process is not differentiable, thus [13] has no back-propagation after keypoint regression.",
            "4": "10000 images using the \u201cCut and Paste\u201d strategy are further synthesized for training as [13].",
            "5": "RGB RGB-D1 PVNet [13]DPOD\n[14]DPVL\n[27]PointFusion [3]DenseFusion [11]P2GNet [34]Tian et al.",
            "6": "PVNet [13]DPOD\n[14]DPVL\n[27]PVN3D\n[15]REDE1 ape 15.",
            "7": "4\n1With the same mask as PVNet [13].",
            "8": "4\n1With the same mask as PVNet [13] on Occlusion LineMOD dataset.",
            "9": "As for Occlusion LineMOD dataset with many hard cases, we use the same masks as PVNet [13].",
            "10": "[13] S.",
            "11": "2 APPENDIX A IMPLEMENTATION DETAILS During implementation, center point and 8 points selected by FPS algorithm are picked up as keypoints following PVNet [13].",
            "12": "10000 images using the \u201cCut and Paste\u201d strategy are further synthesized for training on LineMOD dataset as [13].",
            "13": "The second and third experiments extend PVNet [13] to 3D and the second experiment also employs SVD 3D-3D estimator."
        },
        "Single-shot scene reconstruction": {
            "authors": [
                "Anonymous Submission"
            ],
            "url": "https://openreview.net/pdf?id=CGn3XKSf7vf",
            "ref_texts": "[7] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019.",
            "ref_ids": [
                "7"
            ],
            "1": "Alternatively, 3D detection pipelines detect separate objects and recover their masks and 3D bounding boxes [3, 6, 7, 4], or incorporate relationships between objects by using a graph or physical simulation [8, 9, 10].",
            "2": "The current state-of-the-art methods in object pose estimation almost exclusively belong to the latter group with such representatives as PVNet [7], CDPN [28], EPOS [27], Pix2Pose [4], GDR-Net [29] and DPOD [3, 6].",
            "3": "[7] S."
        },
        "TexPose: Neural Texture Learning for Self-Supervised 6D Object Pose Estimation": {
            "authors": [
                "Hanzhi Chen",
                "Fabian Manhardt",
                "Nassir Navab",
                "Benjamin Busam"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_TexPose_Neural_Texture_Learning_for_Self-Supervised_6D_Object_Pose_Estimation_CVPR_2023_paper.pdf",
            "ref_texts": "[39] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019 , pages 4561\u20134570. Computer Vision Foundation / IEEE, 2019. 1,2",
            "ref_ids": [
                "39"
            ],
            "1": "Noteworthy, accuracy and runtime have both recently made a huge leap forward thanks to deep learning [19,23,31,39,40].",
            "2": "Unfortunately, most of these methods heavily rely on a massive amount of labelled data for supervision to learn precise models withstrong generalisation capabilities [16,39,53,59].",
            "3": "Correspondence-based methods establish 2D-3D correspondences [37,39,40,42,59], prior to leveraging a variant of the RANSAC&PnP paradigm to solvefor pose."
        },
        "Structure-Aware NeRF without Posed Camera via Epipolar Constraint": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2210.00183",
            "ref_texts": "[47] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pix el-wise voting network for 6dof pose estimation,\u201d in Proc. IEEE Conf . Comput. Vis. Pattern Recognit., Jun. 2019, pp. 4561-4570.",
            "ref_ids": [
                "47"
            ],
            "1": "In contrast, keypoi ntbased approaches [47], [48] detect the keypoints of objects and solve for pose using the PnP-RANSAC algorithm.",
            "2": "[47] S."
        },
        "Unseen object 6D pose estimation: a benchmark and baselines": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2206.11808",
            "ref_texts": "[41] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4561\u20134570, 2019. 1, 2",
            "ref_ids": [
                "41"
            ],
            "1": "Recently, deep learning methods based on 2D image [31, 41, 44, 56] or 3D point cloud [21, 22, 54] are proposed to tackle this problem and yield better performances, benefiting from the powerful feature extraction ability of neural network.",
            "2": "Prior knowledge of object models such as keypoint location [22,41] or voting offsets [22,41] is also encoded by the networks.",
            "3": "In the former case, methods extract 2D keypoints [41, 44, 60, 61] of the target objects and apply PnP-RANSAC [19] algorithms to obtain their 6D poses."
        },
        "Edge enhanced implicit orientation learning with geometric prior for 6D pose estimation": {
            "authors": [],
            "url": "https://haopan.github.io/papers/6dpose.pdf",
            "ref_texts": "[27] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 4561\u20134570.",
            "ref_ids": [
                "27"
            ],
            "1": "While [28] and [33] use bounding box corners as keypoints, a recent work [27] explores using designated surface keypoints for more robust 2D keypoint localization.",
            "2": "[27] S."
        },
        "Neural object learning for 6d pose estimation using a few cluttered images": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2005.03717",
            "ref_texts": "28. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (June 2019) 2, 3",
            "ref_ids": [
                "28"
            ],
            "1": "Recently, state-of-the-art performance has been accomplished by using both synthetic and real images [23,24,28].",
            "2": "To overcome this limitation, both real images and synthetic images are used for training [23,24,28,45], which currently achieves state-of-theart performance."
        },
        "Keypoint-based category-level object pose tracking from an RGB sequence with uncertainty estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2205.11047",
            "ref_texts": "[21] J. Tremblay, T. To, B. Sundaralingam, Y . Xiang, D. Fox, and S. Birchfield, \u201cDeep object pose estimation for semantic robotic grasping of household objects,\u201d in Conference on Robot Learning (CoRL) , 2018, pp. 306\u2013316.[22] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixelwise voting network for 6DoF pose estimation,\u201d in Proceedings of the European Conference on Computer Vision (ECCV) , 2019, pp. 4561\u2013",
            "ref_ids": [
                "21",
                "22"
            ],
            "1": "State-of-the-art methods can be generally categorized into template matching [17], [18] and regression techniques [19], [20], [21], [22].",
            "2": "[21] J.",
            "3": "[22] S."
        },
        "Indirect object-to-robot pose estimation from an external monocular rgb camera": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2008.11822",
            "ref_texts": "[3] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d in CVPR , 2019.",
            "ref_ids": [
                "3"
            ],
            "1": "INTRODUCTION Determining the pose of an object with respect to the camera has received much attention in the computer vision and robotics communities [1], [2], [3], [4], [5], [6].",
            "2": "[3] S."
        },
        "Learning-based point cloud registration for 6d object pose estimation in the real world": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2203.15309",
            "ref_texts": "44. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation. In: Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570. Long Beach, California (2019) 1",
            "ref_ids": [
                "44"
            ],
            "1": "In this context, great progress has been made by learning-based methods operating on RGB(D) images [32,48,44,58,42,74,62,36,61,57]."
        },
        "Bcot: A markerless high-precision 3d object tracking benchmark": {
            "authors": [
                "Jiachen Li",
                "Bin Wang",
                "Shiqiang Zhu",
                "Xin Cao",
                "Fan Zhong",
                "Wenxuan Chen",
                "Te Li",
                "Jason Gu",
                "Xueying Qin"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Li_BCOT_A_Markerless_High-Precision_3D_Object_Tracking_Benchmark_CVPR_2022_paper.pdf",
            "ref_texts": "[32] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. In CVPR , pages 4561\u20134570. Computer Vision Foundation / IEEE, 2019. 1",
            "ref_ids": [
                "32"
            ],
            "1": "Despite the rapid development of single-frame 6DOF pose estimation methods [32, 41], for video analysis 3D tracking can be more accurate and more efficient, and thus is indispensable."
        },
        "Single-image 3D face reconstruction under perspective projection": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2205.04126",
            "ref_texts": "[15] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
            "ref_ids": [
                "15"
            ],
            "1": "Besides, 6DoF pose estimation in other objects [15]\u2013[17] always assumes a pre-defined 3D shape but we only have a face image as input.",
            "2": "For 6DoF face pose estimation, we use a two-stage pipeline [15]\u2013[17], that first chooses m2D pixelsV2R2\u0002min 2D image and learns their corresponding 3D points in reconstructed 3D face shape and 6D face pose parameters can be calculated by a PnP [18] algorithm.",
            "3": "worse performance than our method due to the nonlinearity of the rotation space [15].",
            "4": "[15] S."
        },
        "Deepurl: Deep pose estimation framework for underwater relative localization": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2003.05523",
            "ref_texts": "[32] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise V oting Network for 6DoF Pose Estimation,\u201d in Proc. CVPR , 2019.",
            "ref_ids": [
                "32"
            ],
            "1": "[32] used a pixel-wise voting network to regress pixel-wise unit vectors pointing to the keypoints and used these vectors to vote for keypoint locations using RANSAC.",
            "2": "39% 54 PVNet [32] 0.",
            "3": "[15] and PVNet [32] trained on a synthetic dataset and tested on real pool dataset.",
            "4": "[15] and PVNet [32] in terms of rotation and translation errors along with REP-10px and ADD-0.",
            "5": "Moreover, the runtime performance is realtime, outperforming PVNet [32] and only slightly slower than that of Tekin et al.",
            "6": "PVNet [32], compared to DeepURL, performed slightly inferior on REP-10px and ADD0.",
            "7": "We also present a detection bounding box based keypoint sampling strategy that is more robust to related work [15], [32] which leads to a better estimate of the pose of the observed robot, up to an order of magnitude is some cases; see Table I.",
            "8": "[32] S."
        },
        "Stereopose: Category-level 6d transparent object pose estimation from stereo images via back-view nocs": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2211.01644",
            "ref_texts": "[10] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
            "ref_ids": [
                "10"
            ],
            "1": ", instance-level voting field [10] and category-level NOCS map [1]) used on nontransparent objects only describe dense correspondences for the front-view1of the object.",
            "2": "[10] S."
        },
        "HS-Pose: Hybrid Scope Feature Extraction for Category-level Object Pose Estimation": {
            "authors": [
                "Linfang Zheng",
                "Chen Wang",
                "Yinghan Sun",
                "Esha Dasgupta",
                "Hua Chen",
                "Ales Leonardis",
                "Wei Zhang",
                "Hyung Jin"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_HS-Pose_Hybrid_Scope_Feature_Extraction_for_Category-Level_Object_Pose_Estimation_CVPR_2023_paper.pdf",
            "ref_texts": "[32] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-Wise V oting Network for 6DoF Pose Estimation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 2019. 2",
            "ref_ids": [
                "32"
            ],
            "1": "The correspondences can be sparse bounding box corners [33, 41], or distinguishable points on the object\u2019s surface [19, 31, 32]."
        },
        "POPE: 6-DoF Promptable Pose Estimation of Any Object, in Any Scene, with One Reference": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2305.15727",
            "ref_texts": "[11] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019. 2",
            "ref_ids": [
                "11"
            ],
            "1": "Traditional instance-level [6,7,8,9,10,11,12] or category-level [13,14, 15] pose estimators exhibit limitations in handling diverse objects, as they are specifically designed for particular instances or categories."
        },
        "W-posenet: Dense correspondence regularized pixel pair pose regression": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1912.11888",
            "ref_texts": "[20] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR , 2019.",
            "ref_ids": [
                "20"
            ],
            "1": "Intuitively, compared to sparse semantic keypoints pre-defined in keypoint-based methods [16], [17], [18], [19], [20], dense correspondence mapping in our scheme treats each pixel as a keypoint to regress its corresponding 3D coordinate in object model space, which makes each pixel-wise feature more discriminative, and thus our pixel-wise feature encoding is more robust to occlusion.",
            "2": "Recently, PVNet [20] is proposed to detect keypoints via voting on pixel-wise predictions of the directional vector that points to keypoints and is robust to truncation and occlusion.",
            "3": "6 PVNet [20] 43.",
            "4": "[20] S."
        },
        "CenDerNet: Center and Curvature Representations for Render-and-Compare 6D Pose Estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2208.09829",
            "ref_texts": "20. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
            "ref_ids": [
                "20"
            ],
            "1": "CNNs are used to detect local features [31,26,27,15] or find 2D-3D correspondences [20,19,33,30].",
            "2": "Representations based on global geometry [30] or category-level semantics [5,20] do not generalize to unseen object types.",
            "3": "On DIMO, we show our method significantly outperforms PVNet [20], a strong single-view baseline.",
            "4": "PVNet [20] is based on estimating 2D keypoints followed by perspective n-point optimization."
        },
        "CHAIRS: Towards Full-Body Articulated Human-Object Interaction": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2212.10621",
            "ref_texts": "[40] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Conference on Computer Vision and Pattern Recognition (CVPR) , 2019. 2, 3",
            "ref_ids": [
                "40"
            ],
            "1": "Complex and subtle relations between human body parts and object parts Interacting with articulated objects involves complicated spatial and physical relationships, with severe occlusions and rich contacts that incapacitate conventional pose estimation methods that rely on point cloud template-matching [66,50,19,40,28].",
            "2": ", 6-DOF pose estimation) of rigid objects has recently attracted significant attentions [25,19, 3,49,40,38,9]."
        },
        "YOLOPose: Transformer-based multi-object 6D pose estimation using keypoint regression": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2205.02536",
            "ref_texts": "[21] Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: PVNet: Pixel-wise voting network for 6DOF pose estimation. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4561{4570 (2019)",
            "ref_ids": [
                "21"
            ],
            "1": "Some examples for direct regression methods include [1, 22, 29, 30] and examples for keypoint-based methods include [8, 9, 21, 24, 28].",
            "2": "[21] argued that predicting the projection of 3D bounding boxes of an object might be diflcult for a CNN-based model since the projection might lie outside of the object boundary in the RGB image.",
            "3": "Method PoseCNN [30] PVNet [21] GDR-Net [29] T6D-Direct [1] YOLOPose (Ours) DeepIM [15] P.",
            "4": "In: European Conference on Computer Vision (ECCV) (2018)\n[21] Peng, S."
        },
        "3D object tracking with adaptively weighted local bundles": {
            "authors": [],
            "url": "https://jcst.ict.ac.cn/fileup/1000-9000/PDF/2021-3-7-1272.pdf",
            "ref_texts": "[6] Peng S, Liu Y, Huang Q, Zhou X, Bao H. PVNet: Pixel-wise voting network for 6DoF pose estimation. In Proc. the 2019 IEEE Conference on Computer Vision and Pattern Recognition , June 2019, pp.4561-4570. DOI: 10.1109/CVPR.2019.00469.",
            "ref_ids": [
                "6"
            ],
            "1": "3 this is difierent from the 3D object detection and 6DOF pose estimation from a single image, which has been greatly advanced using learning-based approaches[6,7].",
            "2": "[6] Peng S, Liu Y, Huang Q, Zhou X, Bao H."
        },
        "Data-driven object pose estimation in a practical bin-picking application": {
            "authors": [],
            "url": "https://www.mdpi.com/1424-8220/21/18/6093/pdf",
            "ref_texts": "16. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation. In Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 15\u201320 June 2019; pp. 4556\u20134565.",
            "ref_ids": [
                "16"
            ],
            "1": "Some recent methods use CNNs to regress 2D keypoints, using them as an intermediate representation for the Perspective-n-Point (PnP) algorithm to compute 6DoF pose parameters [13\u201315], and other methods improve on this approach by using pixel-wise predictions to provide a flexible representation for localizing occluded or truncated keypoints [8,16]."
        },
        "6D Robotic Assembly Based on RGB-only Object Pose Estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2208.12986",
            "ref_texts": "[17] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
            "ref_ids": [
                "17"
            ],
            "1": "To enhance the robustness, SegDriven [16] and PVNet [17] employ segmentation paired with voting for each correspondence.",
            "2": "Some 6D pose estimation methods [17], [18], [22] render 3D object models in an arbitrary pose and randomly choose a VisualPerceptionGrasp&PlacePose-guided6DTransformation3-axisCalibrationCollision-freeAssembly NextCycleFig.",
            "3": "[17] S."
        },
        "6D object pose estimation using keypoints and part affinity fields": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2107.02057",
            "ref_texts": "15. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: PVNet: Pixel-wise voting network for 6DoF pose estimation. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4561\u20134570 (2019)",
            "ref_ids": [
                "15"
            ],
            "1": "In recent years, two-stage approaches, which first detect keypoints and then solve a Perspective-n-Point (PnP) problem to infer the object pose [14,15,16], have been shown to provide robust and accurate results.",
            "2": "PVNet[15]alsodefineskeypointsontheobjectsurfacebutinferstheminadense manner: Each pixel in the object segmentation mask predicts vectors that point to every keypoint.",
            "3": "The automatically defined set of keypoints is chosen with the farthest-point-algorithm, inspired by PVNet [15]: Starting with the object center, points on the object surface which are farthest from the already chosen points are added to the keypoint set."
        },
        "A visual navigation perspective for category-level object pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2203.13572",
            "ref_texts": "40. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) (2019) 3",
            "ref_ids": [
                "40"
            ],
            "1": "2 Related Work Object Pose Estimation: Extensive studies have been conducted for object pose estimation of known instances [10, 12, 19, 22, 26\u201328, 35, 39, 40, 53]."
        },
        "CorNet: generic 3D corners for 6D pose estimation of new objects without retraining": {
            "authors": [
                "Giorgia Pitteri",
                "Slobodan Ilic",
                "Vincent Lepetit"
            ],
            "url": "http://openaccess.thecvf.com/content_ICCVW_2019/papers/R6D/Pitteri_CorNet_Generic_3D_Corners_for_6D_Pose_Estimation_of_New_ICCVW_2019_paper.pdf",
            "ref_texts": "[24] S. Peng, Y . Liu, Q. Huang, H. Bao, and X. Zhou. Pvnet: Figure 9: Some qualitative results on Object #20 in Scene #13 of the T-LESS dataset. Figure 10: Some qualitative results on Object #20 in Scene #14 of the T-LESS dataset. Figure 11: Some qualitative results on Object #26 and Object #29 in Scene #15 of the T-LESS dataset. Pixel-Wise V oting Network for 6DoF Pose Estimation. CoRR , abs/1812.11788, 2018.",
            "ref_ids": [
                "24"
            ],
            "1": "It is therefore often desirable to rely on color images, and many methods to do so have been proposed recently [19, 25, 31, 18, 35, 24].",
            "2": "Also focusing on occlusion handling, PVNet [24] proposed a network that for each pixel regresses an offset to predefined keypoints.",
            "3": "Somewhat related to our approach, [18, 4, 37, 24] first predict the 3D coordinates of the image locations lying on the objects, in the object coordinate system, and predict the 3D object pose through hypotheses sampling with preemptive RANSAC.",
            "4": "[24] S."
        },
        "Strumononet: Structure-aware monocular 3d prediction": {
            "authors": [
                "Zhenpei Yang",
                "Li Erran",
                "Qixing Huang"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_StruMonoNet_Structure-Aware_Monocular_3D_Prediction_CVPR_2021_paper.pdf",
            "ref_texts": "[29] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019 , pages 4561\u20134570. Computer Vision Foundation / IEEE, 2019.",
            "ref_ids": [
                "29"
            ],
            "1": "Examples include learning a machine translator between two minor languages by composing machine translators via a mother language [19], solving 6D object pose prediction via intermediate keypoint detections [1,31,28,36,27,29,34], and predicting 3D human poses through 2D keypoint predictions [44]."
        },
        "Multi-view fusion for multi-level robotic scene understanding": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2103.13539",
            "ref_texts": "[6] S. Peng, Y . Liu, Q. Huang, H. Bao, and X. Zhou, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d in CVPR , 2019.",
            "ref_ids": [
                "6"
            ],
            "1": "Despite the tremendous progress made in the computer vision community on solving problems such as 3D reconstruction [1], [2], [3], [4] and object pose estimation [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], existing deployed robotic manipulators have limited, if any, perception of their surroundings.",
            "2": "[6] S."
        },
        "ARShoe: Real-time augmented reality shoe try-on system on smartphones": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2108.10515",
            "ref_texts": "[23] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. 2019. PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . 4556\u2013",
            "ref_ids": [
                "23"
            ],
            "1": "Pixel-wise voting network (PVNet) [23] predicts pixel-wise vectors pointing to the object keypoints and uses these vectors to vote for keypoint locations."
        },
        "Disp6d: Disentangled implicit shape and pose learning for scalable 6d pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2107.12549",
            "ref_texts": "44. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
            "ref_ids": [
                "44"
            ],
            "1": ", by direct pose regression [59,29,2], by registering 2D and 3D points [4,47,51,44,42,22,34], and by template retrieval 4 Y."
        },
        "Online object searching by a humanoid robot in an unknown environment": {
            "authors": [],
            "url": "https://raw.githubusercontent.com/aescande/website/master/papers/2021_RAL_Tsuru.pdf",
            "ref_texts": "[14] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixe lwise voting network for 6dof pose estimation. In IEEE/CVF Conf. on Computer Vision and Pattern Recognition , pages 4556\u20134565, 2019.",
            "ref_ids": [
                "14"
            ],
            "1": "The methods for recognizing objects in the 6-DoF format from RGB images [14][15][16][17] requires the correct answer data in the exact 6-DoF format for each training image.",
            "2": "[14] S."
        },
        "Video based Object 6D Pose Estimation using Transformers": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2210.13540",
            "ref_texts": "[40] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "40"
            ],
            "1": "The classical solution for such 6-DOF pose estimation problems utilises a feature point matching mechanism, followed by Perspective-n-Point (PnP) to correct the estimated pose [41,47,40,21].",
            "2": "Due to this limitation, the YCB-Video [56], T-LESS [19], and OccludedLINEMOD datasets [26,40] were introduced.",
            "3": "They have enabled the emergence of novel network designs such as PoseCNN [56], DPOD [59], PVNet [40], and others [8,52,16,10]."
        },
        "Combining local and global pose estimation for precise tracking of similar objects": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2201.13278",
            "ref_texts": "(2020). Tga: Two-level group attention for assembly state detection. In Proc. ISMAR . Masood, T. and Egger, J. (2020). Adopting augmented reality in the age of industrial digitalisation. Computers in Industry , 115:103112. Park, K., Patten, T., and Vincze, M. (2019). Pix2pose: Pixelwise coordinate regression of objects for 6d pose estimation. In Proc. ICCV . Peng, S., Liu, Y ., Huang, Q., Zhou, X., and Bao, H. (2019). Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proc. CVPR . Prisacariu, V . A. and Reid, I. D. (2012). Pwp3d: Real-time segmentation and tracking of 3d objects. International Journal of Computer Vision , 98(3):335\u2013354. Rad, M. and Lepetit, V . (2017). Bb8: A scalable, accurate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth. InProc. ICCV . Seibold, C., Hilsmann, A., and Eisert, P. (2017). Modelbased motion blur estimation for the improvement of motion tracking. Computer Vision and Image Understanding , 160:45\u201356. Seo, B.-K., Park, H., Park, J.-I., Hinterstoisser, S., and Ilic, S. (2013). Optimal local searching for fast and robust textureless 3d object tracking in highly cluttered backgrounds. IEEE Transactions on Visualization and Computer Galraphics , 20(1):99\u2013110. Sock, J., Castro, P., Armagan, A., Garcia-Hernando, G., and Kim, T.-K. (2020). Tackling two challenges of 6d object pose estimation: Lack of real annotated rgb images and scalability to number of objects. arXiv preprint arXiv:2003.12344v1 . Song, C., Song, J., and Huang, Q. (2020). Hybridpose: 6d object pose estimation under hybrid representations. In Proc. CVPR . Steinbach, E., Eisert, P., and Girod, B. (2001). Model-based 3-d shape and motion estimation using sliding textures. InProc. VMV . Su, Y ., Rambach, J., Minaskan, N., Lesur, P., Pagani, A., and Stricker, D. (2019). Deep multi-state object pose estimation for augmented reality assembly. In Proc. ISMAR . Sun, D., Roth, S., and Black, M. J. (2010). Secrets of optical flow estimation and their principles. In Proc. CVPR . Sun, X., Zhou, J., Zhang, W., Wang, Z., and Yu, Q. (2021). Robust monocular pose tracking of less-distinct objects based on contour-part model. IEEE Transactions on Circuits and Systems for Video Technology , 31(11):4409\u20134421. Tan, Z., Chen, D., Chu, Q., Chai, M., Liao, J., He, M., Yuan, L., Hua, G., and Yu, N. (2021). Efficient semantic image synthesis via class-adaptive normalization. IEEE Transactions on Pattern Analysis and Machine Intelligence . Thalhammer, S., Leitner, M., Patten, T., and Vincze, M."
        },
        "Collaborative Viewpoint Adjusting and Grasping via Deep Reinforcement Learning in Clutter Scenes": {
            "authors": [
                "Firstname Lastname",
                "Firstname Lastname",
                "Firstname Lastname"
            ],
            "url": "https://www.mdpi.com/2075-1702/10/12/1135/pdf",
            "ref_texts": "9. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 15\u201320 June 2019; pp. 4561\u20134570.",
            "ref_ids": [
                "9"
            ],
            "1": "According to [9], PVNet is used to vote on the projected 2D feature points, and then find their corresponding relationship to calculate the 6D pose of the object."
        },
        "WeLSA: Learning to Predict 6D Pose from Weakly Labeled Data Using Shape Alignment": {
            "authors": [],
            "url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680633.pdf",
            "ref_texts": "28. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: CVPR (2019)",
            "ref_ids": [
                "28"
            ],
            "1": "Alternatively, [33,28,16] estimate a predefined set of sparse keypoints instead of dense correspondences, which has proven to be more robust to occlusions."
        },
        "DeepFlux for skeleton detection in the wild": {
            "authors": [],
            "url": "http://www.cs.toronto.edu/~sven/Papers/IJCVDeepFlux21.pdf",
            "ref_texts": "1. Ahn, J., Cho, S., & Kwak, S. (2019). Weakly supervised learningof instance segmentation with inter-pixel relations. InProceedingsof IEEE international conference on computer vision and patternrecognition(pp. 2209\u20132218).2. Bai, M., & Urtasun, R. (2017). Deep watershed transform forinstance segmentation. InProceedings of IEEE internationalconference on computer vision and pattern recognition(pp. 2858\u20132866).3. Bai, X., Wang, X., Latecki, L. J., Liu, W., & Tu, Z. (2009). Activeskeleton for non-rigid object detection. InProceedings of IEEEinternational conference on computer vision(pp. 575\u2013582).4. Blum, H. (1973). Biological shape and visual science (part i).Jour-nal of Theoretical Biology,38(2), 205\u2013287.5. Borenstein, E., & Ullman, S. (2002). Class-specific, top-down seg-mentation. InProceedings of European conference on computervision(pp. 109\u2013122).6. Chen, L. C., Hermans, A., Papandreou, G., Schroff, F., Wang, P.,& Adam, H. (2018). Masklab: Instance segmentation by refiningobject detection with semantic and direction features. InProceed-ings of IEEE international conference on computer vision andpattern recognition(pp. 4013\u20134022).7. Chen, L. C., Papandreou, G., Kokkinos, I., Murphy, K., & Yuille,A. L. (2018). Deeplab: Semantic image segmentation with deepconvolutional nets, atrous convolution, and fully connected crfs.IEEE Transactions on Pattern Analysis and Machine Intelligence,40(4), 834\u2013848.8. Chen, X., Fang, H., Lin, T. Y., Vedantam, R., Gupta, S., Doll\u00e1r, P.,& Zitnick, C. L. (2015). Microsoft coco captions: Data collectionand evaluation server. CoRRabs/1504.00325.9. Ci, H., Wang, C., & Wang, Y. (2018). Video object segmentationby learning location-sensitive embeddings. InProceedings of Euro-pean conference on computer vision(pp. 501\u2013516).10. Deng, J., Dong, W., Socher, R., Li, L. J., Li, K., & Li, F. F. (2009).Imagenet: A large-scale hierarchical image database. InProceed-ings of IEEE international conference on computer vision andpattern recognition(pp. 248\u2013255).11. Dickinson, S. J. (2009).Object categorization: Computer andhuman vision perspectives.C a m b r i d g e :C a m b r i d g eU n i v e r s i t yPress.12. Dimitrov, P., Damon, J. N., & Siddiqi, K. (2013). Flux invariants forshape. InProceedings of IEEE international conference on com-puter vision and pattern recognition.13. Ding, J., Xue, N., Long, Y., Xia, G. S., & Lu, Q. (2019). LearningRoI transformer for oriented object detection in aerial images. InProceedings of IEEE international conference on computer visionand pattern recognition(pp. 2849\u20132858).14. Doll\u00e1r, P., & Zitnick, C. L. (2015). Fast edge detection using struc-tured forests.IEEE Transactions on Pattern Analysis and MachineIntelligence,37(8), 1558\u20131570.15. Dufresne-Camaro, C. O., Rezanejad, M., Tsogkas, S., Siddiqi, K.,&D i c k i n s o n ,S .(2 0 2 0 ) .A p p e a r a n c es h o c kg r a m m a rf o rf a s tm e d i a laxis extraction from real images. InProceedings of IEEE interna-tional conference on computer vision and pattern recognition.16. Everingham, M., Van Gool, L., Williams, C. K., Winn, J., & Zisser-man, A. (2010). The pascal visual object classes (voc) challenge.International Journal of Computer Vision,88(2), 303\u2013338.17. Felzenszwalb, P. F., & Huttenlocher, D. P. (2005). Pictorial struc-tures for object recognition.International Journal of ComputerVision,61(1), 55\u201379.18. Felzenszwalb, P. F., & Huttenlocher, D. P. (2012). Distance trans-forms of sampled functions.Theory of Computing,8(1), 415\u2013428.19. Girshick, R., Shotton, J., Kohli, P., Criminisi, A., & Fitzgibbon, A.(2011). Efficient regression of general-activity human poses fromdepth images. InProceedings of IEEE international conference oncomputer vision(pp. 415\u2013422).20. He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learningfor image recognition. InProceedings of IEEE international con-ference on computer vision and pattern recognition(pp. 770\u2013778).21. Jang, J. H., & Hong, K. S. (2001). A pseudo-distance map for thesegmentation-free skeletonization of gray-scale images. InPro-ceedings of IEEE international conference on computer vision(vol. 2, pp. 18\u201323).22. Jerripothula, K. R., Cai, J., Lu, J., & Yuan, J. (2017). Objectco-skeletonization with co-segmentation. InProceedings of IEEEinternational conference on computer vision and pattern recogni-tion(pp. 3881\u20133889).23. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick,R., Guadarrama, S., & Darrell, T. (2014). Caffe: Convolutionalarchitecture for fast feature embedding. InProceedings of ACMmultimedia(pp. 675\u2013678).24. Jiang, Y., Zhu, X., Wang, X., Yang, S., Li, W., Wang, H., Fu, P.,&L u o ,Z .(2 0 1 7 ) .R 2 C N N :R o t a t i o n a lr e g i o nC N Nf o ro r i e n t a t i o nrobust scene text detection. PreprintarXiv:1706.09579.123 International Journal of Computer Vision25. Ke, W., Chen, J., Jiao, J., Zhao, G., & Ye, Q. (2017) SRN: Side-output residual network for object symmetry detection in the wild.InProceedings of IEEE international conference on computervision and pattern recognition(pp. 302\u2013310).26. Kinga, D., & Adam, J. B.: A method for stochastic optimization.InProceedings of international conference on learning represen-tations(vol. 5).27. Kreiss, S., Bertoni, L., & Alahi, A. (2019) PifPaf: Composite fieldsfor human pose estimation. InProceedings of IEEE internationalconference on computer vision and pattern recognition(pp. 11977\u201311986).28. Levinshtein, A., Sminchisescu, C., & Dickinson, S. (2013). Multi-scale symmetric part detection and grouping.International Journalof Computer Vision,104(2), 117\u2013134.29. Lindeberg, T. (1998). Edge detection and ridge detection with auto-matic scale selection.International Journal of Computer Vision,30(2), 117\u2013156.30. Lindeberg, T. (2013). Scale selection properties of generalizedscale-space interest point detectors.Journal of Mathematical Imag-ing and Vision,46(2), 177\u2013210.31. Liu, C., Ke, W., Qin, F., & Ye, Q. (2018). Linear span network forobject skeleton detection. InProceedings of European conferenceon computer vision(pp. 136\u2013151).32. Liu, T. L., Geiger, D., & Yuille, A. L. (1998). Segmenting by seek-ing the symmetry axis. InProceedings of international conferenceon pattern recognition(vol. 2, pp. 994\u2013998).33. Liu, X., Lyu, P., Bai, X., & Cheng, M. M. (2017). Fusing image andsegmentation cues for skeleton extraction in the wild. InProceed-ings of ICCV workshop on detecting symmetry in the wild(vol. 6,p. 8).34. Liu, Y., Cheng, M. M., Hu, X., Wang, K., & Bai, X. (2017). Richerconvolutional features for edge detection. InProceedings of IEEEinternational conference on computer vision and pattern recogni-tion(pp. 5872\u20135881).35. Long, J., Shelhamer, E., & Darrell, T. (2015) Fully convolutionalnetworks for semantic segmentation. InProceedings of IEEE inter-national conference on computer vision and pattern recognition(pp. 3431\u20133440).36. Luo, W., Li, Y., Urtasun, R., & Zemel, R. (2016). Understanding theeffective receptive field in deep convolutional neural networks. InProceedings of advances in neural information processing systems(pp. 4898\u20134906).37. Ma, J., Shao, W., Ye, H., Wang, L., Wang, H., Zheng, Y., et al.(2018). Arbitrary-oriented scene text detection via rotation pro-posals.IEEE Transactions on Multimedia,20(11), 3111\u20133122.38. Maninis, K. K., Pont-Tuset, J., Arbel\u00e1ez, P., & Van Gool, L.(2018). Convolutional oriented boundaries: From image segmen-tation to high-level tasks.IEEE Transactions on Pattern Analysisand Machine Intelligence,40(4), 819\u2013833.39. Marr, D., & Nishihara, H. K. (1978). Representation and recog-nition of the spatial organization of three-dimensional shapes.Proceedings of the Royal Society of London B: Biological Sciences,200(1140), 269\u2013294.40. Martin, D., Fowlkes, C., Tal, D., & Malik, J. (2001). A database ofhuman segmented natural images and its application to evaluatingsegmentation algorithms and measuring ecological statistics. InProceedings of IEEE international conference on computer vision(vol. 2, pp. 416\u2013423).41. Martin, D. R., Fowlkes, C. C., & Malik, J. (2004). Learning todetect natural image boundaries using local brightness, color, andtexture cues.IEEE Transactions on Pattern Analysis and MachineIntelligence,26(5), 530\u2013549.42. M\u00e1ttyus, G., Luo, W., & Urtasun, R. (2017). Deeproadmapper:Extracting road topology from aerial images. InProceedings of theIEEE international conference on computer vision.43. Mattyus, G., Wang, S., Fidler, S., & Urtasun, R. (2015). Enhancingroad maps by parsing aerial images around the world. InProceed-ings of the IEEE international conference on computer vision(pp.1689\u20131697).44. Nedzved, A., Ablameyko, S., & Uchida, S. (2006). Gray-scalethinning by using a pseudo-distance map. InProceedings of IEEEinternational conference on pattern recognition.45. Peng, S., Liu, Y., Huang, Q., Zhou, X., & Bao, H. (2019). PVNet:Pixel-wise voting network for 6dof pose estimation. InProceedingsof IEEE international conference on computer vision and patternrecognition(pp. 4561\u20134570).46. Ren, Z., Yuan, J., Meng, J., & Zhang, Z. (2013). Robust part-basedhand gesture recognition using kinect sensor.IEEE Transactionson Multimedia,15(5), 1110\u20131120.47. Shen, W., Bai, X., Hu, R., Wang, H., & Latecki, L. J. (2011).Skeleton growing and pruning with bending potential ratio.Pat-tern Recognition,44(2), 196\u2013209.48. Shen, W., Bai, X., Hu, Z., & Zhang, Z. (2016). Multiple instancesubspace learning via partial random projection tree for localreflection symmetry in natural images.Pattern Recognition,52,306\u2013316.49. Shen, W., Zhao, K., Jiang, Y., Wang, Y., Bai, X., & Yuille, A.(2017). Deepskeleton: Learning multi-task scale-associated deepside outputs for object skeleton extraction in natural images.IEEETransactions on Image Processing,26(11), 5298\u20135311.50. Shen, W., Zhao, K., Jiang, Y., Wang, Y., Zhang, Z., & Bai, X.(2016). Object skeleton extraction in natural images by fusingscale-associated deep side outputs. InProceedings of IEEE inter-national conference on computer vision and pattern recognition(pp. 222\u2013230).51. Shotton, J., Fitzgibbon, A., Cook, M., Sharp, T., Finocchio, M.,Moore, R., Kipman, A., & Blake, A. (2011) Real-time humanpose recognition in parts from single depth images. InProceedingsof IEEE international conference on computer vision and patternrecognition(pp. 1297\u20131304).52. Siddiqi, K., Bouix, S., Tannenbaum, A., & Zucker, S. W. (2002).Hamilton-jacobi skeletons.International Journal of ComputerVision,48(3), 215\u2013231.53. Siddiqi, K., & Pizer, S. M. (2008).Medial Representations: Math-ematics., Algorithms and Applications Berlin: Springer.54. Siddiqi, K., Shokoufandeh, A., Dickinson, S. J., & Zucker, S. W.(1999). Shock graphs and shape matching.International Journalof Computer Vision,35(1), 13\u201332.55. Sie Ho Lee, T., Fidler, S., & Dickinson, S. (2013). Detecting curvedsymmetric parts using a deformable disc model. InProceedingsof IEEE international conference on computer vision(pp. 1753\u20131760).56. Simonyan, K., & Zisserman, A. (2015). Very deep convolutionalnetworks for large-scale image recognition. InProceedings of inter-national conference on learning representations.57. Sironi, A., Lepetit, V., & Fua, P. (2014). Multiscale centerline detec-tion by learning a scale-space distance transform. InProceedingsof IEEE international conference on computer vision and patternrecognition(pp. 2697\u20132704).58. Trinh, N. H., & Kimia, B. B. (2011). Skeleton search: Category-specific object recognition and segmentation using a skeletal shapemodel.International Journal of Computer Vision,2,2 1 5 \u2013 2 4 0 .59. Tsogkas, S., & Dickinson, S. (2017) AMAT: Medial axis transformfor natural images. InProceedings of IEEE international confer-ence on computer vision(pp. 2727\u20132736).60. Tsogkas, S., & Kokkinos, I. (2012). Learning-based symmetrydetection in natural images. InProceedings of European confer-ence on computer vision(pp. 41\u201354).61. Wang, Y., Xu, Y., Tsogkas, S., Bai, X., Dickinson, S., & Siddiqi, K.(2019). Deepflux for skeletons in the wild. InProceedings of IEEE123 International Journal of Computer Visioninternational conference on computer vision and pattern recogni-tion(pp. 5287\u20135296).62. Wei, S. E., Ramakrishna, V., Kanade, T., & Sheikh, Y. (2016).Convolutional pose machines. InProceedings of IEEE interna-tional conference on computer vision and pattern recognition(pp.4724\u20134732).63. Xia, G., Hu, J., Hu, F., Shi, B., Bai, X., Zhong, Y., et al. (2017).AID: A benchmark data set for performance evaluation of aerialscene classification.IEEE Transactions Geoscience and RemoteSensing,55(7), 3965\u20133981.64. Xia, G. S., Bai, X., Ding, J., Zhu, Z., Belongie, S., Luo, J., Datcu,M., Pelillo, M., & Zhang, L. (2018) DOTA: A large-scale datasetfor object detection in aerial images. InProceedings of IEEE inter-national conference on computer vision and pattern recognition(pp. 3974\u20133983).65. Xie, S., & Tu, Z. (2015). Holistically-nested edge detection. InProceedings of IEEE international conference on computer vision(pp. 1395\u20131403).66. Xu, W., Parmar, G., & Tu, Z. (2019). Geometry-aware end-to-endskeleton detection. InBritish Machine Vision Conference.67. Xu, Y., Wang, Y., Zhou, W., Wang, Y., Yang, Z., & Bai, X. (2019).Textfield: Learning a deep direction field for irregular scene textdetection.IEEE Transactions on Image Processing,28(11), 5566\u20135579.68. Yang, X., Sun, H., Fu, K., Yang, J., Sun, X., Yan, M., et al. (2018).Automatic ship detection in remote sensing images from googleearth of complex scenes based on multiscale rotation dense featurepyramid networks.Remote Sensing,10(1), 132.69. Yu, Z., & Bajaj, C. (2004). A segmentation-free approach for skele-tonization of gray-scale images via anisotropic vector diffusion. InProceedings of IEEE international conference on computer visionand pattern recognition(pp. 415\u2013420).70. Zhang, Q., & Couloigner, I. (2007). Accurate centerline detectionand line width estimation of thick lines using the radon transform.IEEE Transactions on Image Processing,16(2), 310\u2013316.71. Zhang, Z., Shen, W., Yao, C., & Bai, X. (2015). Symmetry-basedtext line detection in natural scenes. InProceedings of IEEE inter-national conference on computer vision and pattern recognition(pp. 2558\u20132567).72. Zhao, K., Shen, W., Gao, S., Li, D., & Cheng, M. M. (2018). Hi-fi:Hierarchical feature integration for skeleton detection. InProceed-ings of international joint conference on artificial intelligence(pp.1191\u20131197).73. Zhu, S. C., & Yuille, A. L. (1996). Forms: A flexible object recog-nition and modelling system.International Journal of ComputerVision,20(3), 187\u2013212.74. Zucker, S. W. (2012). Local field potentials and border owner-ship: A conjecture about computation in visual cortex.Journal ofPhysiology-Paris,106,2 9 7 \u2013 3 1 5 .Publisher\u2019s NoteSpringer Nature remains neutral with regard to juris-dictional claims in published maps and institutional affiliations.",
            "ref_ids": [
                "1"
            ],
            "1": "O u rw o r ki sa l s or e l a t e dt othe approaches in [1,2,6,9,27,38,45,67]w h i c hl e a r nd i r e c -tion cues for edge detection, instance segmentation, and poseestimation.",
            "2": "Finally, direction cues are also used to improveinstance segmentation in [1]a n dd i r e c t i o nfi e l d sp o i n t i n gtowards keypoints are used for pose estimation in [27,45]."
        },
        "Synpo-net\u2014accurate and fast cnn-based 6dof object pose estimation using synthetic training": {
            "authors": [
                "Yongzhi Su",
                "Jason Rambach",
                "Alain Pagani",
                "Didier Stricker"
            ],
            "url": "https://www.mdpi.com/1424-8220/21/1/300/pdf"
        },
        "Estimating 6D aircraft pose from keypoints and structures": {
            "authors": [
                "Runze Fan",
                "Bing X",
                "Zhenzhong Wei"
            ],
            "url": "https://www.mdpi.com/2072-4292/13/4/663/pdf"
        },
        "HMD-EgoPose: Head-mounted display-based egocentric marker-less tool and hand pose estimation for augmented surgical guidance": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2202.11891",
            "ref_texts": "[18] Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4561{4570 (2019)",
            "ref_ids": [
                "18"
            ],
            "1": "have presented several strategies for surgical drill and hand pose estimation from monocular RGB data for the synthetic drill dataset based ofi of the PVNet [18] and HandObjectNet [26] frameworks.",
            "2": "PVNet focuses solely 10 HMD-EgoPose: Marker-Less Pose Estimation for Surgical Guidance on 6DoF object pose estimation and does not consider the joint interaction of the hand of a user with the object [18].",
            "3": "Our HMDEgoPose framework outperformed the HandObjectNet [26, 27] and PVNet [18, 27] techniques in measures of tool ADD and rotational error at the drill bit tip.",
            "4": "Our HMDEgoPose framework outperformed the HandObjectNet [26, 27] and PVNet [18, 27] techniques in measures of tool ADD and rotational and translational error at the drill bit tip.",
            "5": "04307 (2020)\n[18] Peng, S."
        },
        "Fusing local similarities for retrieval-based 3D orientation estimation of unseen objects": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2203.08472",
            "ref_texts": "24. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
            "ref_ids": [
                "24"
            ],
            "1": "Motivated by the tremendous success of deep learning, much effort [36,24,32] has been dedicated to developing deep networks able to recognize the objects depicted in the input image and estimate their 3D orientation.",
            "2": "PVNet [24] estimates the 2D projections of 3D points using a voting network."
        },
        "Dprost: Dynamic projective spatial transformer network for 6d pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2112.08775",
            "ref_texts": "31. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
            "ref_ids": [
                "31"
            ],
            "1": "Therefore, researchers have proposed methods for applying deep learning to the object pose estimation problem with great performance [2,8,14,15,19,21,22,26,28,31,32,37,40,41].",
            "2": "On the other hand, image-space representation based methods [5, 22, 28, 29, 31, 41] may learn the perspective effect implicitly by the projected image.",
            "3": "For example, [15, 32] used the corners of a 3D bounding box, [31,37] detected projected 3D keypoints of an object, and [28] used 2D-3D coordinates to train the network.",
            "4": "Then, based on the geodesic distance of the rotation matrix, we use the farthest point sampling (FPS) algorithm in [31] to select the other reference images.",
            "5": "MethodPoseCNN\n[42]DeepIM\n[21]PVNet [31]S.",
            "6": "MethodPoseCNN\n[42]PVNet [31]DeepIM\n[21]Cosypose [19]GDR-Net [40]SO-Pose [8]RePOSE\n[16]DProST DProST A."
        },
        "MaskedFusion: Mask-based 6D object pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1911.07771",
            "ref_texts": "17. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4561{4570 (2019)",
            "ref_ids": [
                "17"
            ],
            "1": "Methods [2], [15], [16], [17], [23], [25], [26] that use RGB images as input usually rely on the detection and matching of keypoints from the objects in a scene with the 3D render and use the PnP [6] algorithm to solve the pose of that object.",
            "2": "Methods that use RGB images as input [2], [15], [16], [17], [23], [25], [26] usually rely on the detection and matching of keypoints from the objects in a scene with the 3D render and use the PnP [6] algorithm to solve the pose of that object.",
            "3": "One of the most accurate method in 6D pose using RGB images is PVNet [17].",
            "4": "3 Metrics As in previous works [8], [17], [28], [30] we used the Average Distance of Model Points (ADD) [7] as metric of evaluation for non-symmetric objects and for the egg-box and glue we used the Average Closest Point Distance (ADD-S) [30]."
        },
        "Semantic keypoint-based pose estimation from single RGB frames": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2204.05864",
            "ref_texts": "551. Lee, C.-Y., Xie, S., Gallagher, P., Zhang, Z., and Tu, Z. (2015). Deeply-supervised nets. In AISTATS , volume 2, page 6. Lepetit, V., Moreno-Noguer, F., and Fua, P. (2009). EP nP: An accurate O(n) solution to the P nP problem. IJCV , 81(2):155{166. Li, Y., Wang, G., Ji, X., Xiang, Y., and Fox, D. (2018). Deepim: Deep iterative matching for 6d pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV) , pages 683{698. Li, Z., Wang, G., and Ji, X. (2019). Cdpn: Coordinates-based disentangled pose network for real-time rgb-based 6-dof object pose estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 7678{7687. Liu, J., Zou, Z., Ye, X., Tan, X., Ding, E., Xu, F., and Yu, X. (2020). Leaping from 2d detection to eflcient 6dof object pose estimation. In Computer Vision { ECCV 2020 Workshops , pages 707{714. Long, J., Zhang, N., and Darrell, T. (2014). Do convnets learn correspondence? In NIPS , pages 1601{1609. Lowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. IJCV , 60(2):91{110. Manuelli, L., Gao, W., Florence, P., and Tedrake, R. (2019). Kpam: Keypoint afiordances for category-level robotic manipulation. International Symposium on Robotics Research (ISRR) . Marion, P., Florence, P. R., Manuelli, L., and Tedrake, R. (2018). Label fusion: A pipeline for generating ground truth labels for real rgbd data of cluttered scenes. In ICRA . Massa, F., Aubry, M., and Marlet, R. (2014). Convolutional neural networks for joint object detection and pose estimation: A comparative study. CoRR , abs/1412.7190. Michel, F., Kirillov, A., Brachmann, E., Krull, A., Gumhold, S., Savchynskyy, B., and Rother, C. (2017). Global hypothesis generation for 6D object pose estimation. In CVPR . Montserrat, D. M., Chen, J., Lin, Q., Allebach, J. P., and Delp, E. J. (2019). Multi-view matching network for 6d pose estimation. arXiv preprint arXiv:1911.12330 . Mousavian, A., Anguelov, D., Flynn, J., and Kosecka, J. (2017). 3d bounding box estimation using deep learning and geometry. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 7074{7082. Muja, M., Rusu, R. B., Bradski, G. R., and Lowe, D. G. (2011). REIN A fast, robust, scalable recognition infrastructure. In ICRA , pages 2939{2946. Murthy, J. K., Krishna, G., Chhaya, F., and Krishna, K. M. (2017). Reconstructing vechicles from a single image: Shape priors for road scene understanding. ICRA . Narayanan, P., Yeh, B., Holmes, E., Martucci, S., Schmeckpeper, K., Mertz, C., Osteen, P., and Wigness, M. (2020). An integrated perception pipeline for robot mission execution in unstructured environments. InArtiffcial Intelligence and Machine Learning for Multi-Domain Operations Applications II , volume 11413, page 1141318. International Society for Optics and Photonics. Newell, A., Yang, K., and Deng, J. (2016). Stacked hourglass networks for human pose estimation. In ECCV . Osteen, P. R., Owens, J. L., and Kaukeinen, B. (2019). Reducing the cost of visual DL datasets. In Pham, T., editor, Artiffcial Intelligence and Machine Learning for Multi-Domain Operations Applications , volume 11006, pages 121 { 139. International Society for Optics and Photonics, SPIE. Park, K., Patten, T., and Vincze, M. (2019). Pix2pose: Pixel-wise coordinate regression of objects for 6d pose estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 7668{7677. Pavlakos, G., Zhou, X., Chan, A., Derpanis, K. G., and Daniilidis, K. (2017). 6-DoF object pose from semantic keypoints. ICRA , pages 2011{2018. Peng, S., Liu, Y., Huang, Q., Zhou, X., and Bao, H. (2019). Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , pages 4561{4570. Pepik, B., Stark, M., Gehler, P. V., and Schiele, B. (2012). Teaching 3D geometry to deformable part models. InCVPR , pages 3362{3369. Qin, Z., Fang, K., Zhu, Y., Fei-Fei, L., and Savarese, S. (2019). Keto: Learning keypoint representations for tool manipulation. International Conference on Robotics and Automation (ICRA) . Rad, M. and Lepetit, V. (2017). Bb8: A scalable, accurate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth. In Proceedings of the IEEE International Conference on Computer Vision , pages 3828{3836. Ramakrishna, V., Kanade, T., and Sheikh, Y. (2012). Reconstructing 3D human pose from 2D image landmarks. In ECCV , pages 573{586. Ren, S., He, K., Girshick, R., and Sun, J. (2015). Faster R-CNN: Towards real-time object detection with region proposal networks. In NIPS , pages 91{99. Rios-Cabrera, R. and Tuytelaars, T. (2013). Discriminatively trained templates for 3D object detection: A real time scalable approach. In ICCV , pages 2048{2055. Rusu, R. B., Blodow, N., and Beetz, M. (2009). Fast point feature histograms (FPFH) for 3D registration. InICRA , pages 3212{3217. Salti, S., Tombari, F., and di Stefano, L. (2014). Shot: Unique signatures of histograms for surface and texture description. CVIU , 125:251{264. Sohn, K., Berthelot, D., Li, C.-L., Zhang, Z., Carlini, N., Cubuk, E. D., Kurakin, A., Zhang, H., and Rafiel, C. (2020). Fixmatch: Simplifying semi-supervised learning with consistency and conffdence. Advances in Neural Information Processing Systems (NeurIPS) . Song, C., Song, J., and Huang, Q. (2020). Hybridpose: 6d object pose estimation under hybrid representations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 431{440. Su, H., Qi, C. R., Li, Y., and Guibas, L. J. (2015a). Render for CNN: viewpoint estimation in images using CNNs trained with rendered 3D model views. In ICCV , pages 2686{2694. Su, H., Qi, C. R., Li, Y., and Guibas, L. J. (2015b). Render for cnn: Viewpoint estimation in images using cnns trained with rendered 3d model views. In Proceedings of the IEEE International Conference on Computer Vision , pages 2686{2694. Sundermeyer, M., Marton, Z.-C., Durner, M., and Triebel, R. (2020). Augmented autoencoders: Implicit 3d orientation learning for 6d object detection. International Journal of Computer Vision , 128(3):714{729. Tekin, B., Sinha, S. N., and Fua, P. (2018). Real-time seamless single shot 6d object pose prediction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 292{301. Tieleman, T. and Hinton, G. (2012). Lecture 6.5-rmsprop, coursera: Neural networks for machine learning. University of Toronto, Technical Report . Toshev, A. and Szegedy, C. (2014). DeepPose: Human pose estimation via deep neural networks. In CVPR , pages 1653{1660. Tremblay, J., To, T., Sundaralingam, B., Xiang, Y., Fox, D., and Birchffeld, S. (2018). Deep object pose estimation for semantic robotic grasping of household objects. Conference on Robot Learning (CoRL) . Tulsiani, S. and Malik, J. (2015). Viewpoints and keypoints. In CVPR , pages 1510{1519. Vasilopoulos, V. and Koditschek, D. E. (2018). Reactive navigation in partially known non-convex environments. In International Workshop on the Algorithmic Foundations of Robotics , pages 406{421. Springer. Vasilopoulos, V., Pavlakos, G., Bowman, S. L., Caporale, J. D., Daniilidis, K., Pappas, G. J., and Koditschek, D. E. (2020a). Reactive Semantic Planning in Unexplored Semantic Environments Using Deep Perceptual Feedback. RAL , 5(3):4455{4462. Vasilopoulos, V., Pavlakos, G., Schmeckpeper, K., Daniilidis, K., and Koditschek, D. E. (2020b). Reactive navigation in partially familiar planar environments using semantic perceptual feedback. arXiv preprint arXiv:2002.08946 . Wang, C., Mart\u0013 \u0010n-Mart\u0013 \u0010n, R., Xu, D., Lv, J., Lu, C., Fei-Fei, L., Savarese, S., and Zhu, Y. (2019). 6-pack: Category-level 6D pose tracker with anchor-based keypoints. International Conference on Robotics and Automation (ICRA) . Wang, G., Manhardt, F., Shao, J., Ji, X., Navab, N., and Tombari, F. (2020). Self6d: Self-supervised monocular 6d object pose estimation. In European Conference on Computer Vision , pages 108{125. Springer. Wei, S.-E., Ramakrishna, V., Kanade, T., and Sheikh, Y. (2016). Convolutional pose machines. In CVPR . Whelan, T., Kaess, M., Johannsson, H., Fallon, M., Leonard, J. J., and McDonald, J. (2015). Real-time large-scale dense rgb-d slam with volumetric fusion. The International Journal of Robotics Research , 34(4-5):598{626. Whelan, T., Salas-Moreno, R. F., Glocker, B., Davison, A. J., and Leutenegger, S. (2016). Elasticfusion: Real-time dense slam and light source estimation. The International Journal of Robotics Research , 35(14):1697{1716. Xiang, Y., Mottaghi, R., and Savarese, S. (2014). Beyond PASCAL: A benchmark for 3D object detection in the wild. In WACV , pages 75{82. Xiang, Y., Schmidt, T., Narayanan, V., and Fox, D. (2017). Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes. Robotics: Science and Systems (RSS) . Xie, Q., Dai, Z., Hovy, E., Luong, M.-T., and Le, Q. V. (2019). Unsupervised data augmentation for consistency training. Advances in Neural Information Processing Systems (NeurIPS) . Xie, Z., Singh, A., Uang, J., Narayan, K. S., and Abbeel, P. (2013). Multimodal blending for high-accuracy instance recognition. In IROS , pages 2214{2221. Zeng, A., Yu, K.-T., Song, S., Suo, D., Walker, E., Rodriguez, A., and Xiao, J. (2017). Multi-view selfsupervised deep learning for 6d pose estimation in the amazon picking challenge. In 2017 IEEE international conference on robotics and automation (ICRA) , pages 1386{1383. IEEE. Zhou, X., Karpur, A., Luo, L., and Huang, Q. (2018). Starmap for category-agnostic keypoint and viewpoint estimation. In ECCV , pages 318{334. Zhou, X., Leonardos, S., Hu, X., and Daniilidis, K. (2015a). 3D shape estimation from 2D landmarks: A convex relaxation approach. In CVPR , pages 4447{4455. Zhou, X., Zhu, M., Leonardos, S., Derpanis, K., and Daniilidis, K. (2015b). Sparseness meets deepness: 3D human pose estimation from monocular video. In CVPR . Zhu, M., Derpanis, K. G., Yang, Y., Brahmbhatt, S., Zhang, M., Phillips, C., Lecce, M., and Daniilidis, K.",
            "ref_ids": [
                "551"
            ]
        },
        "SSP-Pose: Symmetry-Aware Shape Prior Deformation for Direct Category-Level Object Pose Estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2208.06661",
            "ref_texts": "[25] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR , 2019.",
            "ref_ids": [
                "25"
            ],
            "1": "As for RGBonly methods, end-to-end methods [18], [19], [20], [21], [22], [23], [6] regress the pose parameters directly, whiletwo-stage methods [24], [25], [26], [27], [28] first establish 2D-3D correspondences by predicting the 3D coordinate for each pixel or detecting pre-defined keypoints, and then utilize PnP/RANSAC algorithm to solve the pose from intermediate results.",
            "2": "[25] S."
        },
        "Synpick: A dataset for dynamic bin picking scene understanding": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2107.04852",
            "ref_texts": "[16] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise voting network for 6DOF pose estimation,\u201d in Conf. on Computer Vision and Pattern Recognition (CVPR) , 2019.",
            "ref_ids": [
                "16"
            ],
            "1": "[16] S."
        },
        "Robust rgb-based 6-dof pose estimation without real pose annotations": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2008.08391",
            "ref_texts": "23. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4561{4570 (2019)",
            "ref_ids": [
                "23"
            ],
            "1": "Therefore, recent advances in the ffeld have focused on a deeplearning-based approach [10,32,28,22,24,29,7,23,14,3,33,31,20].",
            "2": "To construct the correspondences, the network is trained to either detect pre-deffned object keypoints [24,29,7,23], or regress 3D object coordinates from the image [14,3,33,31,20].",
            "3": "In [23,22] the keypoints are deffned as semantic object parts.",
            "4": "For keypoints-based methods, such as [7,23], where the model is trained to predict a keypoint-relevant representation, self-supervision can be applied using the MCI orMCV loss, as in our approach.",
            "5": "4 PVNet[23] 43.",
            "6": "4 PVNet[23] 15."
        },
        "Sim2real object-centric keypoint detection and description": {
            "authors": [
                "Chengliang Zhong",
                "Chao Yang",
                "Fuchun Sun",
                "Jinshan Qi",
                "Xiaodong Mu",
                "Huaping Liu",
                "Wenbing Huang"
            ],
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/20482/20241",
            "ref_texts": "2019. Key. net: Keypoint detection by handcrafted and learned cnn filters. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 5836\u20135844. Bay, H.; Tuytelaars, T.; and Van Gool, L. 2006. SURF: Speeded Up Robust Features. In Leonardis, A.; Bischof, H.; and Pinz, A., eds., Computer Vision \u2013 ECCV 2006, 404\u2013417. Chai, C.-Y .; Hsu, K.-F.; and Tsao, S.-L. 2019. Multi-step pick-andplace tasks using object-centric dense correspondences. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 4004\u20134011. IEEE. Chan, J.; Addison Lee, J.; and Kemao, Q. 2017. BIND: Binary integrated net descriptors for texture-less object recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2068\u20132076. DeTone, D.; Malisiewicz, T.; and Rabinovich, A. 2018. SuperPoint: Self-Supervised Interest Point Detection and Description. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. Dusmanu, M.; Rocco, I.; Pajdla, T.; Pollefeys, M.; Sivic, J.; Torii, A.; and Sattler, T. 2019. D2-net: A trainable cnn for joint description and detection of local features. In Proceedings of the ieee/cvf conference on computer vision and pattern recognition, 8092\u20138101. Fischler, M. A.; and Bolles, R. C. 1981. Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography. Commun. ACM, 24(6): 381\u2013395. Florence, P.; Manuelli, L.; and Tedrake, R. 2018. Dense Object Nets: Learning Dense Visual Object Descriptors By and For Robotic Manipulation. Conference on Robot Learning. Godard, C.; Mac Aodha, O.; Firman, M.; and Brostow, G. J. 2019. Digging into Self-Supervised Monocular Depth Prediction. Han, X.; Leung, T.; Jia, Y .; Sukthankar, R.; and Berg, A. C. 2015. MatchNet: Unifying feature and metric learning for patch-based matching. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 3279\u20133286. He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep Residual Learning for Image Recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770\u2013778. Ioffe, S.; and Szegedy, C. 2015. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, 448\u2013456. PMLR.J. Lee, J. P. B. H., D. Kim. 2019. SFNet: Learning Object-aware Semantic Flow. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Kendall, A.; and Gal, Y . 2017. What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision? In Guyon, I.; Luxburg, U. V .; Bengio, S.; Wallach, H.; Fergus, R.; Vishwanathan, S.; and Garnett, R., eds., Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc. Kingma, D. P.; and Ba, J. 2017. Adam: A Method for Stochastic Optimization. arXiv:1412.6980. Kulkarni, T. D.; Gupta, A.; Ionescu, C.; Borgeaud, S.; Reynolds, M.; Zisserman, A.; and Mnih, V . 2019. Unsupervised Learning of Object Keypoints for Perception and Control. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc. Liu, Z.; Liu, W.; Qin, Y .; Xiang, F.; Gou, M.; Xin, S.; Roa, M. A.; Calli, B.; Su, H.; Sun, Y .; and Tan, P. 2021. OCRTOC: A CloudBased Competition and Benchmark for Robotic Grasping and Manipulation. arXiv:2104.11446. Lowe, D. G. 2004. Distinctive Image Features from Scale-Invariant Keypoints. International Journal of Computer Vision, 60: 91\u2013110. Mikolajczyk, K.; and Schmid, C. 2005. A performance evaluation of local descriptors. IEEE transactions on pattern analysis and machine intelligence, 27(10): 1615\u20131630. Paszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.; Chanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.; Desmaison, A.; Kopf, A.; Yang, E.; DeVito, Z.; Raison, M.; Tejani, A.; Chilamkurthy, S.; Steiner, B.; Fang, L.; Bai, J.; and Chintala, S. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In Advances in Neural Information Processing Systems 32, 8024\u20138035. Curran Associates, Inc. Peng, S.; Liu, Y .; Huang, Q.; Zhou, X.; and Bao, H. 2019. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 4561\u20134570. Piasco, N.; Sidib \u00b4e, D.; Demonceaux, C.; and Gouet-Brunet, V .",
            "ref_ids": [
                "2019"
            ]
        },
        "Dynamical pose estimation": {
            "authors": [
                "Heng Yang",
                "Chris Doran",
                "Jacques Slotine"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Yang_Dynamical_Pose_Estimation_ICCV_2021_paper.pdf",
            "ref_texts": "[39] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , pages 4561\u20134570, 2019. 2",
            "ref_ids": [
                "39"
            ],
            "1": "4 Problem (1), when specialized to the primitives 1-7, includes a broad class of fundamental perception problems concerning pose estimation from visual measurements, and finds extensive applications to object detection and localization [30,39], motion estimation and 3D reconstruction [58,56], and simultaneous localization and mapping [10,52,42]."
        },
        "Digital Twin Tracking Dataset (DTTD): A New RGB+ Depth 3D Dataset for Longer-Range Object Tracking Applications": {
            "authors": [
                "Weiyu Feng",
                "Seth Z. Zhao",
                "Chuanyu Pan",
                "Adam Chang",
                "Yichen Chen",
                "Zekun Wang",
                "Allen Y. Yang"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023W/VDU/papers/Feng_Digital_Twin_Tracking_Dataset_DTTD_A_New_RGBDepth_3D_Dataset_CVPRW_2023_paper.pdf",
            "ref_texts": "[23] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 3",
            "ref_ids": [
                "23"
            ],
            "1": "6 DoF Object Pose Estimation Most data-driven methods for object pose estimation take RGB [18, 23, 29, 30] or RGB-D images [10, 11, 15, 22, 27] as input."
        },
        "Dronepose: photorealistic uav-assistant dataset synthesis for 3d pose estimation via a smooth silhouette loss": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2008.08823",
            "ref_texts": "48. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4561{4570 (2019)",
            "ref_ids": [
                "48"
            ],
            "1": "PVNet [48] votes for keypoints, which are then used to estimate the object's 6DOF pose using PnP."
        },
        "Vote from the center: 6 dof pose estimation in rgb-d images by radial keypoint voting": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2104.02527",
            "ref_texts": "36. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570 (2019)",
            "ref_ids": [
                "36"
            ],
            "1": "Keywords: 6 DoF pose estimation, keypoint voting 1 Introduction Object pose estimation is an enabling technology for many applications including robot manipulation, human-robot interaction, augmented reality, and autonomous driving [36,35,45].",
            "2": "They have generally relied on hand-crafted features and therefore fail when objects are featureless or when scenes are very cluttered and occluded [18,36].",
            "3": "As an alternate to directly regressing keypoint coordinates, methods which vote for keypoints have been shown to be highly effective [36,49,18,37], especially when objects are partially occluded.",
            "4": "While recent voting methods have shown great promise and leading performance, they require the regression of either a 2-channel (for 2D voting) [36] or 3-channel (for 3D voting ) [14] activation map where voting quantities are accumulated in order to vote for keypoints.",
            "5": "Notably, RCVPose requires only 3 keypoints per object, which is fewer than existing methods that use 4 or more keypoints [36,14,37].",
            "6": "Both [18] and [36] conclude with RANSAC-based keypoint voting, whereas Deep Hough Voting [37] proposed a complete MLP pipeline of keypoint localization using a series of convolutional layers as the voting module.",
            "7": "To estimate keypoints, two different deep learning-based voting schemes have appeared [36,49,18,37], the proposed scheme introducing a third.",
            "8": "Accumulator spaces can cover the 2D [49,18,37] image space, or more recently the 3D [36] camera reference frame.",
            "9": "Specifically, our method is inspired by PVNet [36], and is most closely related to the recently proposed PVN3D of He et al.",
            "10": "Alternately, the 3D quantity mvfrom the second vector scheme [36,49] is the unit vector pointing to k\u03b8 jfrom pi, denoted as mv=(dx, dy, dz )=mo \u2225mo\u2225.",
            "11": "The network structure was based on a Fully Convolutional ResNet-152 [12], similar to PVNet [36], albeit with two main differences.",
            "12": "This is analogous to the approach of [14], and is efficient compared to previous pure RGB approaches [36] which employ an iterative PnP method.",
            "13": "The training set contains only 180 training samples using the standard 15% /85% training/testing split [49,36,5,14,18].",
            "14": "2 and 3 (with \u03b2=1) were used, as in PVNet [36] (albeit therein using a 2D accumulator space).",
            "15": "FPS selects points on the surface of an object which are well separated, and is a popular keypoint generation strategy [36,14,38,37].",
            "16": "5 Keypoint Dispersion Impact on Transformation Estimation: It was suggested in [36] that 6 DoF pose estimation accuracy is improved by selecting keypoints that lie on the object surface, rather than the bounding box corners which lie just beyond the object surface.",
            "17": "8 PVNet [36] 86."
        },
        "Fully convolutional geometric features for category-level object alignment": {
            "authors": [
                "Qiaojun Feng",
                "Nikolay Atanasov"
            ],
            "url": "https://arxiv.org/pdf/2103.04494",
            "ref_texts": "[13] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-Wise V oting Network for 6DoF Pose Estimation,\u201d in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , June 2019, pp.",
            "ref_ids": [
                "13"
            ],
            "1": "PVNet [13] estimates sparse object keypoints on the RGB image via voting mechanism.",
            "2": "[13] S."
        },
        "Large-Displacement 3D Object Tracking with Hybrid Non-local Optimization": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2207.12620",
            "ref_texts": "15. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation. In: IEEE/CVF Conference on CVPR. pp. 4556{4565. IEEE, Long Beach, CA, USA (Jun 2019). https://doi.org/10.1109/CVPR.2019.00469",
            "ref_ids": [
                "15"
            ],
            "1": "This approach actually bridges 3D tracking with detection-based 6D pose estimation [11, 15, 27]."
        },
        "A dynamic keypoint selection network for 6DoF pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2110.12401",
            "ref_texts": "[25] Peng, S., Liu, Y., Huang, Q., Zhou, X., & Bao, H. (2019). Pvnet: Pixel -wise voting network for 6dof pose estimation. In Proceedin gs of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 4561 -4570). ",
            "ref_ids": [
                "25"
            ],
            "1": "Recently, the explosive growth of deep learning techniques motivates several works to tackle this proble m by convolution neural networks (CNNs) on RGB images [19, 25, 40] and reveal promising improvements .",
            "2": "However, due to the nonlinearity of the rotation space illustrated by [25], these methods typically have poor generalization.",
            "3": "Previous methods [25, 9] try to use the farthest point sampling (FP S) algorithm to choose the keypoints on the mesh.",
            "4": "Table 2 shows the evaluation results on the LineMOD dataset compared with PoseCNN [19 , 25, 26, \n21, 9, 15].",
            "5": "RGB RGB-D Object PoseCNN [19] PVNe [25]t PointFusion[26] DenseFusion[21] PVN3D [9] FFB6D [15] Ours Ape 77.",
            "6": "[25] Peng, S."
        },
        "MV6D: Multi-View 6D Pose Estimation on RGB-D Frames Using a Deep Point-wise Voting Network": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2208.01172",
            "ref_texts": "[13] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixelwise voting network for 6DoF pose estimation,\u201d in CVPR , 2019, pp.",
            "ref_ids": [
                "13"
            ],
            "1": "Traditional pose estimation methods mostly rely on single RGB(-D) images [8], [13], [14], [7], [6] or point cloud data [15], [16], [17].",
            "2": "Pose Estimation on Single RGB Images Traditional pose estimation methods [23], [24], [25], [26], [27], [5], [28], [13] extract local features from the given RGB image and match them to the corresponding features in its 3D model.",
            "3": "However, often the generalization of direct methods is an issue due to the non-linearity of the rotation space [13].",
            "4": "[13] S."
        },
        "Semi-automatic 3d object keypoint annotation and detection for the masses": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2201.07665",
            "ref_texts": "[16] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp.",
            "ref_ids": [
                "16"
            ],
            "1": "[16] S."
        },
        "Parapose: Parameter and domain randomization optimization for pose estimation using synthetic data": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2203.00945",
            "ref_texts": "[27] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "27"
            ],
            "1": "These methods are generally seen in many pose estimation methods [23], [27], [16].",
            "2": "In PVNet [27] 10000 images are rendered and cut and pasted onto images from the SUN397 [37] dataset.",
            "3": "PVN3D [16] use the training data and domain randomization from PVNet [27], but expand with 3D pointclouds.",
            "4": "As in other methods [27], the level of noise in each sample is a random Gaussian based on a selected max level."
        },
        "SD-pose: Semantic decomposition for cross-domain 6D object pose estimation": {
            "authors": [
                "Zhigang Li",
                "Yinlin Hu",
                "Mathieu Salzmann",
                "Xiangyang Ji"
            ],
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/16298/16105",
            "ref_texts": "2019 IEEE/CVF International Conference on Computer Vision (ICCV) . Massa, F.; Marlet, R.; and Aubry, M. 2016. Crafting a multi-task CNN for viewpoint estimation. arXiv preprint arXiv:1609.03894 . Park, K.; Patten, T.; and Vincze, M. 2019. Pix2Pose: PixelWise Coordinate Regression of Objects for 6D Pose Estimation. Pavlakos, G.; Zhou, X.; Chan, A.; Derpanis, K. G.; and Daniilidis, K. 2017. 6-dof object pose from semantic keypoints. In Robotics and Automation (ICRA), 2017 IEEE International Conference on, 2011\u20132018. IEEE. Peng, S.; Liu, Y .; Huang, Q.; Zhou, X.; and Bao, H. 2019. PVNet: Pixel-wise V oting Network for 6DoF Pose Estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 4561\u20134570. Rad, M.; and Lepetit, V . 2017. BB8: A Scalable, Accurate, Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects without Using Depth. In IEEE International Conference on Computer Vision (ICCV). Rad, M.; Oberweger, M.; Lepetit, V .; Lepetit, V .; and Lepetit, V . 2018. Domain Transfer for 3D Pose Estimation from Color Images without Manual Annotations. Asian Conference on Computer Vision (ACCV) ."
        },
        "Geometric change detection in digital twins": {
            "authors": [
                "Tiril Sundby",
                "Julia Maria",
                "Adil Rasheed",
                "Mandar Tabib",
                "Omer San"
            ],
            "url": "https://www.mdpi.com/2673-6470/1/2/9/pdf",
            "ref_texts": "21. Peng, S.; Liu, Y.; Huang, Q.; Bao, H.; Zhou, X. PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation. arXiv 2018 , arXiv:1812.11788.",
            "ref_ids": [
                "21"
            ]
        },
        "Canonical voting: Towards robust oriented bounding box detection in 3d scenes": {
            "authors": [
                "Yang You",
                "Zelin Ye",
                "Yujing Lou",
                "Chengkun Li",
                "Lu Li",
                "Lizhuang Ma",
                "Weiming Wang",
                "Cewu Lu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/You_Canonical_Voting_Towards_Robust_Oriented_Bounding_Box_Detection_in_3D_CVPR_2022_paper.pdf",
            "ref_texts": "[14] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "14"
            ],
            "1": "PVNet [14] regresses pixel-wise unit vectors pointing to the predefined keypoints and solves a Perspective-n-Point (PnP) problem for pose estimation in RGB images."
        },
        "Keypoint cascade voting for point cloud based 6DoF pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2210.08123",
            "ref_texts": "[42] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "42"
            ],
            "1": "A number of recent leading ML approaches [42, 20, 66] have been proposed based on keypoint voting, in which the 3D scene coordinates of specific keypoints defined within an object\u2019s reference frame, are voted on and accumulated independently for each image pixel.",
            "2": "Recent works such as PVNet [42], PVN3D [20], and RCVPose [66] that exhibit leading SOTA performance, have merged voting-based methods, which are well established in the classical literature [61, 54], with recent ML-based keypoint estimation approaches.",
            "3": "Keypoints have been defined in a variety of ways, including the object bounding box corners [39, 46, 56], farthest point sampling [42, 20], and disperse sampling [66].",
            "4": "Radial Pair Loss LP Existing methods calculate loss during training individually for each keypoint [42, 20, 66].",
            "5": "Experiments We evaluate the performance of our proposed RCVPose3D, and compare it with the best performing 6DoF PE methods on the two challenging 6DoF datasets that are commonly used in related SOTA work [42, 20].",
            "6": "We follow previous works [42, 20, 66] and use a train/test split of 85%/15%.",
            "7": "3 PVNet [42] \u0017 73.",
            "8": "Initially, the Smooth L1 LossLsis used to train both the segmentation and regression components, as in PVNet [42]."
        },
        "Shape Enhanced Keypoints Learning with Geometric Prior for 6D Object Pose Tracking": {
            "authors": [
                "Mateusz Majcher",
                "Bogdan Kwolek"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022W/DLGC/papers/Majcher_Shape_Enhanced_Keypoints_Learning_With_Geometric_Prior_for_6D_Object_CVPRW_2022_paper.pdf",
            "ref_texts": "[20] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. In CVPR , pages 4556\u20134565, 2019. 2, 3",
            "ref_ids": [
                "20"
            ],
            "1": "To mitigate this effect, many top performing two-stage approaches are either based on pixel-vise voting [20] or on generating an ensemble of predictions from each image pixel or patch [17], and then aggregating them to improve final predictions.",
            "2": "Relevant Work Top-performing methods on existing benchmarks, rather than directly regressing the object pose, are based on twostage approaches [12, 16, 17, 20, 21, 24, 25, 31, 33], which first predict landmarks of the object (intermediate features) with established 2D-3D correspondences, and then utilize a PnP like algorithm to determine the pose.",
            "3": "To better cope with occluded objects, [20] proposed a neural network for pixel-wise voting for the 2D keypoints location.",
            "4": "While [21, 25] utilize bounding box corners as keypoints, more recent approaches [20] use designated surface keypoints.",
            "5": "In contrast to [20] which predicts dense vectors pointing to 2D keypoints, our algorithm votes for keypoint confidences on the basis of the (segmented) object shape.",
            "6": "2\n[20] S."
        },
        "Research on non-pooling YOLOv5 based algorithm for the recognition of randomly distributed multiple types of parts": {
            "authors": [
                "Zehua Yu",
                "Ling Zhang",
                "Xingyu Gao",
                "Yang Huang",
                "Xiaoke Liu"
            ],
            "url": "https://www.mdpi.com/1424-8220/22/23/9335/pdf",
            "ref_texts": "2. Peng, S.; Zhou, X.; Liu, Y.; Lin, H.; Huang, Q.; Bao, H. PVNet: Pixel-Wise Voting Network for 6DoF Object Pose Estimation. IEEE Trans. Pattern. Anal. Mach. Intell. 2022 ,44, 3212\u20133223. [CrossRef] [PubMed]",
            "ref_ids": [
                "2"
            ],
            "1": "[2] proposed a pixel-wise voting network (PVNet) that first classified objects in images using a CNN, and then computed the object pose information."
        },
        "Robust 6-DoF Pose Estimation under Hybrid Constraints": {
            "authors": [
                "Hong Ren",
                "Lin Lin",
                "Yanjie Wang",
                "Xin Dong"
            ],
            "url": "https://www.mdpi.com/1424-8220/22/22/8758/pdf",
            "ref_texts": "27. Peng, S.; Zhou, X.; Liu, Y.; Lin, H.; Huang, Q.; Bao, H. PVNet: Pixel-Wise Voting Network for 6DoF Object Pose Estimation. IEEE Trans. Pattern Anal. Mach. Intell. 2022 ,44, 3212\u20133223. [CrossRef]",
            "ref_ids": [
                "27"
            ],
            "1": "proposed the PVNet [27] algorithm, which predicts the direction of all pixels in the object to the keypoints, and uses a RANSAC algorithm-based voting method to determine the keypoints, thus improving the robustness of the pose estimation against occlusion greatly.",
            "2": "Therefore, the strategy of [27] was used to add synthetic images to the training set, resulting in a total of 20,000 training images per class."
        },
        "ASM-Net: Category-level pose and shape estimation using parametric deformation": {
            "authors": [],
            "url": "https://www.bmvc2021-virtualconference.com/assets/papers/1277.pdf",
            "ref_texts": "[19] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixelwise V oting Network for 6DoF Pose Estimation. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "19"
            ],
            "1": "If a 3D model of the object represented as a mesh or point cloud data is available, it can be solved in an approach called instance-level pose estimation [10, 11, 14, 19, 32].",
            "2": "Recently, this process has been replaced by deep learning [10, 11, 19]."
        },
        "6 dof pose estimation of textureless objects from multiple rgb frames": {
            "authors": [],
            "url": "https://campar.cs.tum.edu/pub/kaskman2020eccvw/kaskman2020eccvw.pdf",
            "ref_texts": "40. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4561{4570 (2019)",
            "ref_ids": [
                "40"
            ],
            "1": "In Pixel-wise Voting Network (PVNet) [40].",
            "2": "The network outputs per-seed point classiffcation labels as well as estimation of the keypoint direction vectors, which are further used for the RANSAC-based voting for the object keypoint locations, similar to PVNet [40]."
        },
        "Active 6d multi-object pose estimation in cluttered scenarios with deep reinforcement learning": {
            "authors": [
                "Anonymous Submission"
            ],
            "url": "https://arxiv.org/pdf/1910.08811",
            "ref_texts": "[15] S. Peng, Y . Liu, Q. Huang, H. Bao, and X. Zhou. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019.",
            "ref_ids": [
                "15"
            ],
            "1": "To handle occlusion and truncation, keypoint detection with PnP [13, 14] or per-pixel regression/patch-based approaches [6, 9, 7] followed by Hough voting [15, 7] or RANSAC have been proposed.",
            "2": "[15] S."
        },
        "Vipose: Real-time visual-inertial 6d object pose tracking": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2107.12617",
            "ref_texts": "[10] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
            "ref_ids": [
                "10"
            ],
            "1": "[10] S."
        },
        "A deep learning framework for accurate vehicle yaw angle estimation from a monocular camera based on part arrangement": {
            "authors": [
                "Wenjun Huang",
                "Wenbo Li",
                "Luqi Tang",
                "Xiaoming Zhu",
                "Bin Zou"
            ],
            "url": "https://www.mdpi.com/1424-8220/22/20/8027/pdf",
            "ref_texts": "9. Peng, S.D.; Liu, Y.; Huang, Q.X.; Zhou, X.W.; Bao, H.J.; Soc, I.C. PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 15\u201320 June 2019.",
            "ref_ids": [
                "9"
            ],
            "1": "Image algorithm researchers pursue information about the shape [3], distance [4,5], velocity [6,7], position, and orientation [8,9] of objects.",
            "2": "The first form involves the construction of a 2D\u20133D correspondence by matching images with 3D model renderings and then using the perspective-n-point method [18] to solve the object pose [9,29,30]."
        },
        "Prima6d: Rotational primitive reconstruction for enhanced and robust 6d pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2006.07789",
            "ref_texts": "[1] S. Peng, Y . Liu, Q. Huang, H. Bao, and X. Zhou, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d 12 2018.",
            "ref_ids": [
                "1"
            ],
            "1": "When solving 6D pose estimation, reaching accurate, fast and robust solutions has been a key element of augmented reality [1], robotic tasks [2, 3, 4], and autonomous vehicles [5].",
            "2": "The latter approaches reported current state-of-the-art (SOTA) performance by leveraging a two-step approach for pose estimation [1, 7, 8].",
            "3": "To alleviate this issue, PVNet [1] predicted the unit vectors that point to predefined keypoints for each pixel in an object.",
            "4": "Evaluation Metric (i) 2D Projection error metric To evaluate the pose estimation in terms of the 2D projection error we also use the same metric as in [1] and measure the mean pixel distance between projection of the 3D model and the image pixel points.",
            "5": "We chose the average distance metric (ADD) [28] as the evaluation metric, which computes the distance of transformed 3D model points methodsHolistic ApproachPnP based Approach w/o refinement w/ refinement PoseCNN [19] Deep-6D Pose [16] BB8 [11] Tekin [12] Pix2Pose [15] DPOD [26] PVNet [1] PrimA6D-S PrimA6D-SR BB8 Tekin HybridPose [27] apey38.",
            "6": "All values are imported from [1] except pix2pose and hybridpose.",
            "7": "[1] S."
        },
        "Instancepose: Fast 6dof pose estimation for multiple objects from a single rgb image": {
            "authors": [
                "Lee Aing",
                "Nung Lie",
                "Chiu Chiang",
                "Shiang Lin"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021W/CVinHRC/papers/Aing_InstancePose_Fast_6DoF_Pose_Estimation_for_Multiple_Objects_From_a_ICCVW_2021_paper.pdf",
            "ref_texts": "[19] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4556\u20134565, 2019. 1, 2, 6, 7",
            "ref_ids": [
                "19"
            ],
            "1": "Introduction Many studies [8, 14, 9, 10, 27, 15, 1, 18, 19, 25, 17, 4, 13, 24, 11] involve 6DoF object pose estimation using a single RGB image.",
            "2": "After adding and stacking several stages to fulfill a condition of multiple object pose estimation, some systems [8, 14, 19] become slow or the network structures become more complex and performance decreases significantly.",
            "3": "Two studies, [9, 19] predict the unit-vector fields to estimate the pose.",
            "4": "The maximum processing speed for this method with an 2625\n Metrics 2DPro ADD(S) MethodsPoseCNN S-Driven PVNet S-StageOursPoseCNN S-Driven Pix2Pose PVNet S-StageOurs[26] [10] [19] [9] [26] [10] [18] [19] [9] ape 34.",
            "5": "Datasets The datasets that are used for training come from the Normal LINEMOD dataset [7] and the rendered dataset [19].",
            "6": "However, for the object \u201d glue\u201d, 2626\n Metrics 5CMD 2CMD 5CMD 10CMD MethodsDeepIM PVNetOurs[13] [19] ape 51.",
            "7": "Metrics Time consumption (ms) Methods [10] [18] [19] [9] Ours Data loading 10.",
            "8": "1, 4, 6, 7\n[19] S."
        },
        "DRNet: A depth-based regression network for 6D object pose estimation": {
            "authors": [
                "Lei Jin",
                "Xiaojuan Wang",
                "Mingshu He",
                "Jingyue Wang"
            ],
            "url": "https://www.mdpi.com/1424-8220/21/5/1692/pdf",
            "ref_texts": "28. Peng, S.; Liu, Y.; Huang, Q.; Bao, H.; Zhou, X. PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation. In Proceedings of the CVPR, Long Beach, CA, USA, 15\u201321 June 2019; pp. 4561\u20134570. Sensors 2021 ,21, 1692 14 of 14",
            "ref_ids": [
                "28"
            ],
            "1": "[28] proposed a Pixel-wise Voting Network (PVNet) to identify keypoints with the aid of RANSAC-based voting.",
            "2": "We render 10,000 images for each object in the Linemod dataset as [28].",
            "3": "In addition, we also compare our method with PVNet [28], which uses key points to figure out poses.",
            "4": "4 for the average ADD(S) [28], which calculates the ADD AUC for asymmetric objects and the ADD-S AUC for symmetric objects.",
            "5": "Methods BB8 [53] PoseCNN [20] Pix2Pose [54] PVNet [28] CDPN [51] Our Mean 43."
        },
        "L6dnet: Light 6 dof network for robust and precise object pose estimation with small datasets": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2002.00911",
            "ref_texts": "[7]S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise V oting Network for 6DoF pose estimation,\u201d in IEEE Conf. on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
            "ref_ids": [
                "7"
            ],
            "1": "To address these limitations most recent methods solve the problem of object pose estimation with a data driven strategy using for example Convolutional Neural Networks (CNNs) [2], [6], [7], [8], [9], [10], [11], [12], [13], [14].",
            "2": "While some methods are hybrid, using a learning-based approach followed by a geometrical solver [6], [7], [8], [9], [10], [14], others use an end-to-end CNN to predict the pose [2], [11], [12].",
            "3": "On the other side some methods [6], [7], [8], [9], [23], [14], [24], [25] are inspired by classical pose estimation from 2D-3D correspondence.",
            "4": "PVNet[7] proposes to apply an offset based approach to predict the 2D location of a set of keypoints on the object surface.",
            "5": "Inspired by [7], we select the keypoints using the farthest point sampling algorithm which allows us to get a good coverage of the object.",
            "6": "We use the same method as [2], [6], [7] to select training and testing images.",
            "7": "This makes some methods like [7], [12] need synthetic data.",
            "8": "Using data augmentation on the brightness we manage to get state of the art results without needing additional synthetic data, contrary to [14], [7] that use 20 000 new synthetic images per object.",
            "9": "Input RGB RGB-D Method [7] [2] w.",
            "10": "[7]S."
        },
        "Deep soft procrustes for markerless volumetric sensor alignment": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2003.10176",
            "ref_texts": "[30] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. PVNet: Pixel-wise voting network for 6dof pose estimation. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR , pp. 4561\u20134570, 2019.",
            "ref_ids": [
                "30"
            ],
            "1": "PVNet [30] densely regresses vectors pointing at the keypoints to improve robustness to occlusions.",
            "2": "[30] S."
        },
        "HRPose: Real-Time High-Resolution 6D Pose Estimation Network Using Knowledge Distillation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2204.09429"
        },
        "Learning 6d pose estimation from synthetic rgbd images for robotic applications": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2208.14288",
            "ref_texts": "[5]S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
            "ref_ids": [
                "5"
            ],
            "1": "Additionally, as in many computer vision tasks, the performance of the algorithms is vulnerable to environmental factors, such as lighting changes and cluttered backgrounds [5].",
            "2": "[5]S."
        },
        "End-to-end learning improves static object geo-localization from video": {
            "authors": [
                "Mohamed Chaabane",
                "Lionel Gueguen",
                "Ameni Trabelsi",
                "Ross Beveridge",
                "Stephen O"
            ],
            "url": "http://openaccess.thecvf.com/content/WACV2021/papers/Chaabane_End-to-End_Learning_Improves_Static_Object_Geo-Localization_From_Video_WACV_2021_paper.pdf",
            "ref_texts": "[24] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6DoF pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "24"
            ],
            "1": "5D Pose Estimation Many state-of-the-art methods for object pose estimation [15, 22, 24, 30, 35] use 3D models of the objects."
        },
        "Bico-net: Regress globally, match locally for robust 6d pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2205.03536",
            "ref_texts": "[Peng et al. , 2019 ]Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019.",
            "ref_ids": [
                "Peng et al\\. , 2019 "
            ]
        },
        "Top-1 CORSMAL challenge 2020 submission: Filling mass estimation using multi-modal observations of human-robot handovers": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2012.01311",
            "ref_texts": "22. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4561{4570 (2019)",
            "ref_ids": [
                "22"
            ],
            "1": "[22], [28], [31])."
        },
        "CASAPose: Class-Adaptive and Semantic-Aware Multi-Object Pose Estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2210.05318",
            "ref_texts": "[30] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proc. CVPR , 2019.",
            "ref_ids": [
                "30"
            ],
            "1": "Many methods train a separate CNN per object [20,28,30,34,35,39].",
            "2": "The trivial multi-object extension to expand this secondary output for each object [30] results in GPU-intensive slow training and performance degradation [7].",
            "3": "Approaches from last category either predict object specific keypoints [14,30,31,34,39] or dense correspondence/coordinate maps [11, 13, 20, 28, 37, 50].",
            "4": "To deal with multiple objects, most approaches train a separate network per object and need multiple inferences per image [20,28,30,34,35,39].",
            "5": "Alternatively, increasing the number of output maps is proposed as multi-object extension [11,30,31], which risks serious accuracy drops [32] or complex and slow processing [11].",
            "6": "The first estimates a segmentation mask that guides the second in estimating vectors pointing to 2D projections of predefined 3D object keypoints [30].",
            "7": "We use this property to avoid the non-differentiable RANSAC estimation, commonly used to get 2D points from vector fields [30].",
            "8": "41for13objects (and 9 keypoints), compared to 248outputs for PVNet [30] or3342 for EPOS [11].",
            "9": "GARD, HILSMANN, EISERT: CASAPOSE 7\n4 Implementation Details Architecture In CASAPose, a shared ResNet-18 [9,30] provides features for two decoders.",
            "10": "The first resembles [30] and predicts a semantic mask by multiple blocks of consecutive skip connections, convolutions, batch normalisation (BN), leaky ReLU, and upsampling.",
            "11": "The simplest model (Base ) uses the merged vector field, but otherwise resembles PVNet [30].",
            "12": "It applies DKR on the largest connected component of each object class and clearly outperforms RANSAC voting (PVRANSAC ) [30] used with the same trained model.",
            "13": "Previous work weightedLSegandLVecequally (l1=l2=1:0) [30], or additionally added LPVas a regularizer with much smaller weight [49].",
            "14": "3 Further Details \u2022The Farthest Point Sampling (FPS) algorithm is used to calculate the 3D locations of the keypoints [30]."
        },
        "Spatial attention improves iterative 6D object pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2101.01659",
            "ref_texts": "[21] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019. 1, 2, 3, 5, 6, 16, 17",
            "ref_ids": [
                "21"
            ],
            "1": "To solve this challenging task, a variety of methods such as BB8 [22], PoseCNN [28], PVNet [21] and DPOD [29] have been proposed recently.",
            "2": "The 6D pose estimation task is typically separated into two subtasks: initial detection and pose estimation of objects from raw images [21, 10, 12, 28, 22] and subsequent pose refinement [20, 29].",
            "3": "With modern CNN architectures, the one-shot pose estimation sub-task has saturated somewhat [22, 28, 21, 29, 12] but such methods have not yet been able to precisely align spatial details, leaving much room for improvement via iterative pose refinement, which is the focus of our work.",
            "4": "Inspired by this scheme, SoA methods for one-shot pose estimation rely on RANSAC to extract the most reliable points for prediction [21, 29].",
            "5": "Thus, the accuracy of estimation on occlusion data is far below the non-occluded case [21, 29].",
            "6": "State-of-the-art methods [29, 21] do not use hand-designed keypoints.",
            "7": "Contrary, papers for initial pose estimation use advanced techniques, such as RANSAC for keypoint selection [21] or masking for removing occlusions [12].",
            "8": "Neural Network Model At the initial step, we obtain the object pose from an existing one-shot pose estimation algorithm, such as PVNet [21].",
            "9": "Evaluation Metric For evaluation, we use the standard ADD(-S) evaluation metric as is done in most 6D pose estimation papers [21, 20, 29, 17, 11, 1].",
            "10": "The red object outlines show the initial pose obtained from PVNet [21].",
            "11": "We use the SoA in the one-shot setting, PVNet [21], for initialization.",
            "12": "Note that here the accuracy of the initialization method is significantly lower than PVNet [21] which implies that some initial estimates maybe outside of the distribution used during training of our method.",
            "13": "1, 3, 4, 5, 6\n[21] S.",
            "14": "For initialization, we use PVNet [21].",
            "15": "For initialization, we use PVNet [21]."
        },
        "How to track your dragon: A multi-attentional framework for real-time rgb-d 6-dof object pose tracking": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2004.10335",
            "ref_texts": "31. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4561{4570 (2019)",
            "ref_ids": [
                "31"
            ],
            "1": "In PVNet [31], Peng et al."
        },
        "3d-aware ellipse prediction for object-based camera pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2105.11494",
            "ref_texts": "[24] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019 , pages 4561\u20134570. Computer Vision Foundation / IEEE, 2019.",
            "ref_ids": [
                "24"
            ],
            "1": "Many works exist on this subjects [12, 18, 24, 25, 30, 31, 36].",
            "2": "[24] S."
        },
        "Kdfnet: Learning keypoint distance field for 6d object pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2109.10127",
            "ref_texts": "[5] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR , 2019. 1, 2, 4, 5, 6",
            "ref_ids": [
                "5"
            ],
            "1": "There are mainly two types of methods for localizing 2D keypoints: heatmap-based [3, 4] and voting-based [2, 5, 6].",
            "2": "In voting-based methods, the visible parts of the object hallucinate and vote for the 2D locations of the invisible keypoints [5].",
            "3": "every object pixel predicts the 2D direction to the keypoints and the keypoint hypotheses are the intersections of the direction votes [2, 5, 6].",
            "4": "3%, significantly outperforms related baselines such as [5] and the current state-of-the-art HybridPose [6].",
            "5": "Besides heatmap, 2D direction field representation has also been proposed for localizing keypoints [2, 5].",
            "6": "Specifically, direction-based voting methods have been adopted by previous works to robustly localize object centers [2] or object keypoints [5, 10] on the images where the voting scores are based on the number of direction inliers.",
            "7": "The intuition behind this method is that given an object, the location of the invisible keypoint can be inferred from the visible parts [2, 5, 6].",
            "8": "Inspired by recent works [4, 5], we first predict KDF to localize 2D keypoints through voting, then compute object 6D pose by solving a PnP problem.",
            "9": "method PVNet [5] KDFNet (ours) GT mask 93.",
            "10": "DISTANCE -BASED VOTING : A T OYEXPERIMENT The key difference between our method and previous works [2, 5, 6] is the predicted representation used in voting, i.",
            "11": "The baseline we compare our KDFNet against is PVNet [5], a direction-based keypoint method.",
            "12": "The data used to train our model are the same as [5]: real images from LINEMOD and synthetically rendered images using the scanned 3D object models.",
            "13": "We adopt the same object keypoint set as [5], which are generated by Farthest Point Sampling (FPS) of the object 3D point set.",
            "14": "We evaluate and compare our model against previous baselines [2, 5, 6, 17, 24] on Occlusion LINEMOD dataset.",
            "15": "Among these baselines, the most relevant is PVNet [5], a direction-based keypoint voting method which was also compared against in the toy experiment in Section IV.",
            "16": "methodsPoseCNN\n[2]Oberweger [24]Pix2Pose [17]PVNet [5]HybridPose [6]KDFNet (ours) ape 9.",
            "17": "3methodsPoseCNN\n[2]Oberweger [24]PVNet [5]KDFNet (ours) ape 34.",
            "18": "5 particular, our method outperforms PVNet [5] by a margin of 9.",
            "19": "1, 2, 3, 5, 6, 7\n[5] S."
        },
        "Category-agnostic 6d pose estimation with conditional neural processes": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2206.07162",
            "ref_texts": "[12] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof poseestimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , June 2019. 1, 2",
            "ref_ids": [
                "12"
            ],
            "1": "Prior work has investigated instance-level 6D pose estimation [6, 7, 12, 13], where the objects are predefined.",
            "2": "Similar to other works [6, 12, 14], we report the ADD-0."
        },
        "Vision-based Neural Scene Representations for Spacecraft": {
            "authors": [
                "Anne Mergy",
                "Gurvan Lecuyer",
                "Dawa Derksen",
                "Dario Izzo"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2021W/AI4Space/papers/Mergy_Vision-Based_Neural_Scene_Representations_for_Spacecraft_CVPRW_2021_paper.pdf",
            "ref_texts": "[37] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "37"
            ],
            "1": "In theory, this information could be inferred using Machine Learning methods [37,12] which usually rely on the detection of keypoints and thus requires extensive annotation for training."
        },
        "Ensemble of 6 DoF Pose estimation from state-of-the-art deep methods.": {
            "authors": [
                "Ibon Merino"
            ],
            "url": "https://addi.ehu.es/bitstream/handle/10810/61846/1-s2.0-S0925231223003934-main.pdf?sequence=1&isAllowed=y",
            "ref_texts": "[27] S. Peng, Y. Liu, Q. Huang, X. Zhou, H. Bao, Pvnet: Pixel-wise voting network for 6dof pose estimation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u20134570.",
            "ref_ids": [
                "27"
            ],
            "1": "[27] introduces a pixel-wise voting network (PVNet) to regress pixel-wise vectors pointing to the keypoints and uses those vectors to vote for the location of keypoints.",
            "2": "[27] S."
        },
        "6d pose estimation with correlation fusion": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1909.12936",
            "ref_texts": "[10] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp.",
            "ref_ids": [
                "10"
            ],
            "1": "[10] S."
        },
        "Collision-aware In-hand 6D Object Pose Estimation using Multiple Vision-based Tactile Sensors": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2301.13667",
            "ref_texts": "[3]S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: PixelWise Voting Network for 6DoF Pose Estimation,\u201d in 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . IEEE, 2019, pp. 4556\u20134565.",
            "ref_ids": [
                "3"
            ],
            "1": "[3]S."
        },
        "Pixel-pair occlusion relationship map (P2ORM): formulation, inference and application": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2007.12088",
            "ref_texts": "36. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: PVNet: Pixel-wise voting network for 6DoF pose estimation. In: Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4561{4570 (2019) P2ORM: Formulation, Inference & Application 17",
            "ref_ids": [
                "36"
            ],
            "1": "Besides the joint treatment of occlusion when developing techniques for speciffc tasks [40,19,54,36,35,37,18], task-independent occlusion reasoning [42,24,49,53,51,30] ofiers valuable occlusion-related features for high-level scene understanding tasks."
        },
        "Joint Hand and Object Pose Estimation from a Single RGB Image using High\u2010level 2D Constraints": {
            "authors": [],
            "url": "https://diglib.eg.org/xmlui/bitstream/handle/10.1111/cgf14685/v41i7pp383-394.pdf?sequence=1"
        },
        "Primitive shape recognition for object grasping": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2201.00956",
            "ref_texts": "10344\u201310353. Paschalidou D, van Gool L and Geiger A (2020) Learning unsupervised hierarchical part decomposition of 3D objects from a single rgb image. arXiv preprint arXiv:2004.01176 . Peng S, Liu Y , Huang Q, Zhou X and Bao H (2019) PVNet: Pixelwise voting network for 6DoF pose estimation. In: IEEE Conference on Computer Vision and Pattern Recognition . pp."
        },
        "SD-Pose: Structural Discrepancy Aware Category-Level 6D Object Pose Estimation": {
            "authors": [
                "Guowei Li",
                "Dongchen Zhu",
                "Guanghui Zhang",
                "Wenjun Shi",
                "Tianyu Zhang",
                "Xiaolin Zhang",
                "Jiamao Li"
            ],
            "url": "https://openaccess.thecvf.com/content/WACV2023/papers/Li_SD-Pose_Structural_Discrepancy_Aware_Category-Level_6D_Object_Pose_Estimation_WACV_2023_paper.pdf",
            "ref_texts": "[27] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "27"
            ],
            "1": "So far, instancelevel 6D pose estimation works [19, 29, 22, 27, 38, 17, 16] have made considerable progress.",
            "2": "For the correspondence between 2D and 3D [27, 29, 30], the pose is obtained by solving a PnP problem [21].",
            "3": "Indirect voting [27, 17] first selects key point positions through RANSAC [10] voting and then calculates the 6D pose of the object according to the correspondence between key points."
        },
        "From IR images to point clouds to pose: point cloud-based AR glasses pose estimation": {
            "authors": [
                "Ahmet Firintepe",
                "Carolin Vey",
                "Stylianos Asteriadis",
                "Alain Pagani",
                "Didier Stricker"
            ],
            "url": "https://www.mdpi.com/2313-433X/7/5/80/pdf",
            "ref_texts": "1. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation. In Proceedings of the The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 15\u201320 June 2019.",
            "ref_ids": [
                "1"
            ],
            "1": "Given that depth information usually necessitates the use of dedicated hardware, image-only approaches for pose estimation have received significant attention over the last years [1,3,4,6,8].",
            "2": "Because handcrafting features is time-consuming and prone to errors, Deep Learning-based approaches have gained popularity and outperform traditional approaches [1,3,4,8].",
            "3": "Recent feature-based Deep Learning methods use Deep Neural Networks to estimate the objects\u2019 keypoints and combine them with PnP , partly relying on traditional methods [1,2,11].",
            "4": "[1] provide a state-of-the-art approach based on keypoint regression and further PnP execution.",
            "5": "Especially the definition of keypoints benefits pose estimation when dealing with occlusions and truncation [1]."
        },
        "Robust 2D/3D vehicle parsing in arbitrary camera views for CVIS": {
            "authors": [
                "Hui Miao",
                "Feixiang Lu",
                "Zongdai Liu",
                "Liangjun Zhang",
                "Dinesh Manocha",
                "Bin Zhou"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Miao_Robust_2D3D_Vehicle_Parsing_in_Arbitrary_Camera_Views_for_CVIS_ICCV_2021_paper.pdf",
            "ref_texts": "[36] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "36"
            ],
            "1": ", AM3D [30], DPOD [60], PV-Net [36], D4LCN [9])."
        },
        "Pose estimation of primitive-shaped objects from a depth image using superquadric representation": {
            "authors": [],
            "url": "https://www.mdpi.com/2076-3417/10/16/5442/pdf",
            "ref_texts": "3. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation. In Proceedings of the CVPR 2019, Long Beach, CA, USA, 15\u201321 June 2019.",
            "ref_ids": [
                "3"
            ]
        },
        "Manifold-Aware Self-Training for Unsupervised Domain Adaptation on Regressing 6D Object Pose": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2305.10808",
            "ref_texts": "[Peng et al. , 2019 ]Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019.",
            "ref_ids": [
                "Peng et al\\. , 2019 "
            ]
        },
        "PIZZA: A Powerful Image-only Zero-Shot Zero-CAD Approach to 6 DoF Tracking": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2209.07589",
            "ref_texts": "[37] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-Wise V oting Network for 6DoF Pose Estimation. In IEEE Conference on Computer Vision and Pattern Recognition , 2019. 2",
            "ref_ids": [
                "37"
            ],
            "1": "These methods can be roughly divided into three types of approaches: direct regression of the object pose [23, 56], template-based matching [19, 43] that encodes images in latent spaces and compares them against a dictionary of predefined viewpoints, and keypoint prediction for predicting the 6D pose [39, 48, 37, 22, 2] or dense 2D-3D correspondences [28, 51, 58, 34] as in earlier works [46, 7]."
        },
        "SyMFM6D: Symmetry-aware Multi-directional Fusion for Multi-View 6D Object Pose Estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2307.00306",
            "ref_texts": ""
        },
        "Multi-view shape estimation of transparent containers": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1911.12354",
            "ref_texts": "[6] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. , Long Beach, CA, USA, 16\u201320 June 2019.",
            "ref_ids": [
                "6"
            ],
            "1": "CAD) [5, 6, 7, 8] or motion capture systems [4, 9, 10].",
            "2": "[2] LGP X X X X\n[18] DeepIM X X 3DM\n[7] StoCS X X 3DM\n[6] PVNet X X X 3DM\n[5] DenseFusion X X 3DM\n[16] SegOPE X X X 3DM\n[15] NOCS X X X X LoDE X X X X X X to estimate the 6 DoF object pose quite accurately, but their training requires large amount of data, usually annotated only for the highlevel object class [14], including depth information and/or known dense 3D models in addition to colour images [5, 6, 7, 15, 16].",
            "3": "For example, PoseCNN [17], DenseFusion [5], SegOPE [16] and PVNet [6] are evaluated only with objects whose high-quality 3D models and depth were available [17], discarding testing objects that are transparent as the segmentation may fail or be inaccurate.",
            "4": "Pixel-wise V oting Network (PVNet) [6] estimates the pose of occluded or truncated objects with an uncertainty-driven PnP, learning a vector-field representation to localise a sparse set of 2D keypoints and their spatial uncertainty.",
            "5": "As most of these works target object pose estimation, related comprehensive reviews can be found in [5, 6, 15, 16].",
            "6": "[6] S."
        },
        "Deep quaternion pose proposals for 6D object pose tracking": {
            "authors": [
                "Mateusz Majcher",
                "Bogdan Kwolek"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021W/DSC/papers/Majcher_Deep_Quaternion_Pose_Proposals_for_6D_Object_Pose_Tracking_ICCVW_2021_paper.pdf",
            "ref_texts": "[21] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. PVNet: Pixel-Wise V oting Network for 6DoF Pose Estimation. In IEEE Conf. CVPR , pages 4556\u20134565, 2019. 1, 4",
            "ref_ids": [
                "21"
            ],
            "1": "Most existing methods for object pose estimation output a single guess of each object\u2019s pose [13, 36, 21, 26].",
            "2": "In [21], a Pixel-wise V oting Network (PVNet) to regress pixelwise unit vectors pointing to the keypoints and then using these vectors to vote for keypoint locations via RANSAC has been proposed.",
            "3": "1\n[21] S."
        },
        "Lit: Light-field inference of transparency for refractive object localization": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1910.00721",
            "ref_texts": "[15] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "15"
            ],
            "1": "Other end-to-end method methods have explored using synthetic data in training [3], [13], pixel-wise voting over keypoints [14], [15], and residual networks to iteratively refine object poses [5], [2].",
            "2": "In addition, the center point estimation branch does not regress multiple keypoints which is common in texture-rich object pose estimation networks [14], [15]."
        },
        "6D Pose Estimation for Textureless Objects on RGB Frames using Multi-View Optimization": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2210.11554",
            "ref_texts": "[26] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 4561\u20134570, 2019.",
            "ref_ids": [
                "26"
            ],
            "1": "comboost the object pose estimation performance using only RGB images [23], [24], [25], [26], [27].",
            "2": "In comparison, some recent works leverage CNNs to first predict 2D object keypoints [36], [37], [26] or dense 2D-3D correspondences [38], [39], [27], [40], and then compute the pose through 2D-3D correspondences with a PnP algorithm [41].",
            "3": "Our network architecture is based on PVNet [26].",
            "4": "For more details of the object center localization prediction, we refer the reader to [26].",
            "5": "[26] S."
        },
        "Chain-of-Thought Predictive Control": {
            "authors": [
                "Zhiwei Jia",
                "Fangchen Liu",
                "Vineet Thumuluri",
                "Linghao Chen",
                "Zhiao Huang",
                "Hao Su"
            ],
            "url": "https://arxiv.org/pdf/2304.00776",
            "ref_texts": "15th International Conference on Humanoid Robots (Humanoids) , pp. 657\u2013663. IEEE, 2015. Laskey, M., Lee, J., Fox, R., Dragan, A., and Goldberg, K. Dart: Noise injection for robust imitation learning. InConference on robot learning , pp. 143\u2013156. PMLR, 2017. Levine, S., Kumar, A., Tucker, G., and Fu, J. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643 , 2020.Liang, Z., Bethard, S., and Surdeanu, M. Explainable multihop verbal reasoning through internal monologue. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , 2021. Ling, W., Yogatama, D., Dyer, C., and Blunsom, P. Program induction by rationale generation: Learning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146 , 2017. Liu, F., Liu, H., Grover, A., and Abbeel, P. Masked autoencoding for scalable and generalizable decision making. arXiv preprint arXiv:2211.12740 , 2022. Lynch, C., Khansari, M., Xiao, T., Kumar, V ., Tompson, J., Levine, S., and Sermanet, P. Learning latent plans from play. In Conference on robot learning , pp. 1113\u20131132. PMLR, 2020. Mandi, Z., Liu, F., Lee, K., and Abbeel, P. Towards more generalizable one-shot visual imitation learning. arXiv preprint arXiv:2110.13423 , 2021. Mandlekar, A., Xu, D., Wong, J., Nasiriany, S., Wang, C., Kulkarni, R., Fei-Fei, L., Savarese, S., Zhu, Y ., and Mart \u00b4\u0131n-Mart \u00b4\u0131n, R. What matters in learning from offline human demonstrations for robot manipulation. arXiv preprint arXiv:2108.03298 , 2021. Mu, T., Ling, Z., Xiang, F., Yang, D., Li, X., Tao, S., Huang, Z., Jia, Z., and Su, H. Maniskill: Generalizable manipulation skill benchmark with large-scale demonstrations. arXiv preprint arXiv:2107.14483 , 2021. Nair, A., Dalal, M., Gupta, A., and Levine, S. Accelerating online reinforcement learning with offline datasets. arXiv preprint arXiv:2006.09359 , 2020. Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114 , 2021. Paster, K., McIlraith, S., and Ba, J. You can\u2019t count on luck: Why decision transformers fail in stochastic environments. arXiv preprint arXiv:2205.15967 , 2022. Peng, S., Liu, Y ., Huang, Q., Zhou, X., and Bao, H. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 4561\u20134570, 2019. Pertsch, K., Lee, Y ., and Lim, J. Accelerating reinforcement learning with learned skill priors. In Conference on robot learning , pp. 188\u2013204. PMLR, 2021. Chain-of-Thought Predictive Control Pfrommer, S., Halm, M., and Posa, M. Contactnets: Learning discontinuous contact dynamics with smooth, implicit representations. In Conference on Robot Learning , pp."
        },
        "CheckerPose: Progressive Dense Keypoint Localization for Object Pose Estimation with Graph Neural Network": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2303.16874",
            "ref_texts": "[51] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4561\u20134570, 2019. 1, 2, 4, 5, 7",
            "ref_ids": [
                "51"
            ],
            "1": "Instead of direct estimation, correspondence guided methods [50, 56, 68, 46, 22, 51, 21, 23, 81, 48, 38, 75, 10, 64] follow a two-stage framework: they first predict a set of correspondences between 3D object frame coordinates and 2D image plane coordinates, and then recover the pose from the 3D-2D correspondences with a PnP algorithm [32, 30, 11, 73, 6].",
            "2": "Keypoint-localization based methods [50, 56, 68, 46, 22, 51, 21, 23] estimate the 2D coordinates for a sparse set of predefined 3D keypoints, while dense methods [81, 48, 38, 75, 10, 64] predict the 3D object frame coordinate of each 2D image pixel.",
            "3": ", heatmaps [50, 46] and vector-fields [51, 22]), our representation needs only 2d+ 1binary bits for each keypoint, thus greatly reduces the memory usage for dense keypoint localization.",
            "4": ", voting for the vector-field representations [51].",
            "5": "As shown in Table 1, the accuracy decreases significantly without Method PVNet [51] S."
        },
        "A 3D Keypoints Voting Network for 6DoF Pose Estimation in Indoor Scene": {
            "authors": [
                "Huikai Liu",
                "Gaorui Liu",
                "Yue Zhang",
                "Linjian Lei",
                "Hui Xie",
                "Yan Li",
                "Shengli Sun"
            ],
            "url": "https://www.mdpi.com/2075-1702/9/10/230/pdf",
            "ref_texts": "7. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 15\u201320 June 2019; pp. 4561\u20134570.",
            "ref_ids": [
                "7"
            ],
            "1": "CNN is also used in pose estimation, PVNet [7] regress the 2d keypoint through the end-to-end network, and then use the PnP algorithm, estimate the 6d pose by calculating the 2d-3d correspondence relationship of the object.",
            "2": "PVNet [7] first votes the keypoints through RANSAC, then utilizes the 2D-3D correspondence to calculate the 6D pose.",
            "3": "We compare our method with the RGB based methods PoseCNN [9], PVNet [7] and RGDB based methods PointFusion [49], Densefusion [12], PVN3D [15]."
        },
        "Linear-Covariance Loss for End-to-End Learning of 6D Pose Estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2303.11516",
            "ref_texts": "[40] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , June 2019. 1, 2",
            "ref_ids": [
                "40"
            ],
            "1": "More recently, most works [5, 20, 22, 32, 34, 37, 38, 40, 41, 42, 43] draw inspiration from geometry and seek to predict 2D-3D corresponImageNoisy 2D-3D matchingPosteriordistributionResidual variances of correspondencesSolution distributionlossposepg.",
            "2": ", via the farthest point sampling algorithm [40].",
            "3": "[22] aggregate the 2D keypoint predictions from all pixels belonging to the given target; Similarly, PVNet [40] regresses the vector-field pointing from each object pixel to the 2D locations."
        },
        "DeepRM: Deep Recurrent Matching for 6D Pose Refinement": {
            "authors": [
                "Alexander Avery",
                "Andreas Savakis"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023W/RHOBIN/papers/Avery_DeepRM_Deep_Recurrent_Matching_for_6D_Pose_Refinement_CVPRW_2023_paper.pdf",
            "ref_texts": "[22] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNET: Pixel-wise voting network for 6dof pose estimation. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition , 2019June:4556\u20134565, dec 2019. 2, 5, 6",
            "ref_ids": [
                "22"
            ],
            "1": "To further address the problem of occlusion, PVNet [22] introduced a pixel-wise voting network using RANSAC, resulting in an estimator that is capable of detecting keypoints, even when they are occluded.",
            "2": "Evaluation Metrics To evaluate the performance against other state-of-theart methods, we follow [7, 17, 18, 22, 31, 32] and use the ADD metric [12].",
            "3": "31 PVNet [22] 1 73.",
            "4": "9 PVNet [22] \u22c6 1 40.",
            "5": "Initial predictions are obtained from PVNet [22], where DeepRM outperforms all existing methods except for ZebraPose [27] and CRT-6D [4]."
        },
        "Reflective texture-less object registration using multiple edge features for augmented reality assembly": {
            "authors": [],
            "url": "https://www.researchsquare.com/article/rs-1578869/latest.pdf",
            "ref_texts": "23. S. Peng, Y. Liu, Q. Huang, H. Bao, and X. Zhou, \"PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation,\" 2018.",
            "ref_ids": [
                "23"
            ],
            "1": "PVNet [23] predicted the direction from each pixel to each key point, so the spatial probability distribution of two-dimensional key points can be obtained just like RANSAC."
        },
        "Learning to Estimate Object Poses without Real Image Annotations.": {
            "authors": [
                "Haotong Lin",
                "Sida Peng",
                "Zhize Zhou",
                "Xiaowei Zhou"
            ],
            "url": "https://www.ijcai.org/proceedings/2022/0162.pdf",
            "ref_texts": "[Park et al., 2019 ]Kiru Park, Timothy Patten, and Markus Vincze. Pix2pose: Pixel-wise coordinate regression of objects for 6d pose estimation. In ICCV, pages 7668\u20137677, 2019.[Peng et al., 2019 ]Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, pages 4561\u2013",
            "ref_ids": [
                "Park et al\\., 2019 ",
                "Peng et al\\., 2019 "
            ]
        },
        "Smart Task Assistance in Mixed Reality for Astronauts": {
            "authors": [
                "Qingwei Sun",
                "Wei Chen",
                "Jiangang Chao",
                "Wanhong Lin",
                "Zhenying Xu",
                "Ruizhi Cao"
            ],
            "url": "https://www.mdpi.com/1424-8220/23/9/4344/pdf",
            "ref_texts": "29. Peng, S.; Zhou, X.; Liu, Y.; Lin, H.; Huang, Q.; Bao, H. PVNet: Pixel-Wise Voting Network for 6DoF Object Pose Estimation. IEEE Trans. Pattern Anal. Mach. Intell. 2022 ,44, 3212\u20133223. [CrossRef] [PubMed]",
            "ref_ids": [
                "29"
            ]
        },
        "Pose estimation from RGB images of highly symmetric objects using a novel multi-pose loss and differential rendering": {
            "authors": [],
            "url": "https://vbn.aau.dk/ws/files/458094775/IROS_2021_Pose_Estimation_from_RGB_Images_of_Highly_Symmetric_Objects_using_a_Novel_Multi_Pose_Loss_and_Differential_Rendering.pdf",
            "ref_texts": "[9] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR , June 2019.",
            "ref_ids": [
                "9"
            ],
            "1": "Lately, many of these methods have started to be replaced or complemented by machine learning methods [1], [9], [10], [11], [12].",
            "2": "[9] S."
        },
        "Prior-free Category-level Pose Estimation with Implicit Space Transformation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2303.13479",
            "ref_texts": "[26] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "26"
            ],
            "1": "Unlike instancelevel pose estimation [14, 16, 19, 26, 13, 37], which requires a 3D CAD model for each object instance, this task aims at exploiting category-specific information and thus can furPrior-based methodsPrior-free methodsFigure 1."
        },
        "EasyHeC: Accurate and Automatic Hand-eye Calibration via Differentiable Rendering and Space Exploration": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2305.01191",
            "ref_texts": "[26] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
            "ref_ids": [
                "26"
            ],
            "1": "In our work, we adopt the PVNet [26] to perform the pose initialization.",
            "2": "We rendered 10,000 images for the PointRend [22] and another 10,000 images for the PVNet [26] training to obtain the observed segmentation mask and initial camera pose for our method.",
            "3": "Specifically, we use PVNet [26] to estimate the object poses in the camera coordinate system and then use the hand-eye calibration results to transform the object poses from the camera coordinate system to the base coordinate system as the input states to the CoTPC network.",
            "4": "Although the restricted field of view for the camera in this scenario may introduce inaccuracies when using the PVNet [26] for initialization, the distribution range of the initial camera pose will be greatly constrained for eye-in-hand configurations, enabling manual initialization or the selection of one of several predefined camera poses for initialization.",
            "5": "[26] S."
        },
        "Region pixel voting network (RPVNet) for 6D pose estimation from monocular image": {
            "authors": [
                "Feng Xiong",
                "Chengju Liu",
                "Qijun Chen"
            ],
            "url": "https://www.mdpi.com/2076-3417/11/2/743/pdf",
            "ref_texts": "29. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation. In Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Los Alamitos, CA, USA, 16\u201320 June 2019; IEEE Computer Society: Los Alamitos, CA, USA, 2019; pp. 4556\u20134565.",
            "ref_ids": [
                "29"
            ],
            "1": "Thus, PVNet [29], which is the state-of-the-art, predicts vector pointing to a keypoint in the image for every pixel that belongs to the object, and produce the keypoints via a voting procedure, and then a PnP procedure is adopted for final pose estimation.",
            "2": "Instead of using the whole features for keypoints detection as in PVNet [29], the regions specified by the predicted bounding boxes are utilized to select local features by RoIAlign [9].",
            "3": "As suggested by PVNet [29], Nkis set to 8 in our model.",
            "4": "The proposed method adopts a strategy of direction map [29].",
            "5": "Truncation LINEMOD [29] dataset is created by randomly cropping each image of LINEMOD.",
            "6": "Second, the experimental results of many studies [10,12,17,29] show that 2D projection error is more \u201ctolerant\u201d than ADD metric, i.",
            "7": "Two-Stage End-to-End RPVNet Heatmap Tekin [16] BB8 [19] Cull [17] PVNet [29] Deep6D [10] SSD-6D [12] Ape 55.",
            "8": "A similar phenomenon was found in the study of PVNet [29], when comparing predicting keypoints on object, and predicting bounding-box corners.",
            "9": "Given an input of a 480\u0002640 RGB image, the average processing speed was 42 ms, or 23 fps, which is close to PVNet (25 fps) [29]."
        },
        "Robust Category-Level 3D Pose Estimation from Synthetic Data": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2305.16124",
            "ref_texts": "[25] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , June 2019. 1",
            "ref_ids": [
                "25"
            ],
            "1": "Pose estimation has been studied in depth on the instance level [14, 17, 19, 25, 38], and on the category-level for very specific object classes like cars [11] and faces [26]."
        },
        "Soft-jig-driven assembly operations": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2010.10843",
            "ref_texts": "[36] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixelwise voting network for 6DoF pose estimation,\u201d in CVPR , 2019, pp.",
            "ref_ids": [
                "36"
            ],
            "1": "To confirm the 6D pose estimation task for fixed parts, we apply PVNet [36], one of deep learning-based algorithms [37], [38].",
            "2": "[36] S."
        },
        "Learning to Estimate 6DoF Pose from Limited Data: A Few-Shot, Generalizable Approach using RGB Images": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2306.07598",
            "ref_texts": "[74] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "74"
            ],
            "1": "However, traditional methods [45, 95, 104, 109, 51, 74, 46] often require instance*Equal contribution Full shots 32 shots 16 shotsOnePose++ Gen6D(32 shots) Ours (32 shots)GT Cas6D Support View Gen6D OnePose++\n\ud835\udc65\ud835\udc66\ud835\udc67 GT Detec\ufffdon Model Detec\ufffdonSparse (8) Support view 6DoF Estimation Figure 1.",
            "2": "Recent advances in generalizable pose estimators aim to address the limitations of prior instance-specific methods [104, 94, 40, 74, 46, 41, 39, 42, 46, 102, 23, 57, 89, 78, 90, 108] which typically utilize CAD models or depth maps and categoryspecific [98, 17, 101, 22, 52, 17, 16, 53, 96, 24, 30] methods that require per-category training, enabling pose estimation generalizing to unseen objects and categories."
        },
        "3D object reconstruction and 6D-pose estimation from 2D shape for robotic grasping of objects": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2203.01051",
            "ref_texts": "[25] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. 2019. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 4561\u20134570.",
            "ref_ids": [
                "25"
            ],
            "1": "2 Pose estimation based on deep learning In the last years, many approaches for pose estimation based on deep learning have been proposed [22,24,25,30,32] and evaluated using data sets of the BOP challenge [15,16]."
        },
        "ParametricNet: 6DoF pose estimation network for parametric shapes in stacked scenarios": {
            "authors": [],
            "url": "https://yongjinliu.github.io/files/2021-ParametricNet-6DoF-Pose-Estimation-Network-for-Parametric-Shapes-in-Stacked-Scenarios.pdf",
            "ref_texts": "[15] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d in IEEE/CVF Conference on CVPR , 2019, pp. 4556\u20134565.",
            "ref_ids": [
                "15"
            ],
            "1": "However, their generalization abilities were not satisfied due to the non-linearity of the rotation space [15].",
            "2": "[15] S."
        },
        "Ambiguity-Aware Multi-Object Pose Optimization for Visually-Assisted Robot Manipulation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2211.00960"
        },
        "GoferBot: A Visual Guided Human-Robot Collaborative Assembly System": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2304.08840",
            "ref_texts": "[26] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR , 2019.",
            "ref_ids": [
                "26"
            ],
            "1": "Despite continuous advancement in object pose estimation algorithms [26], [42], [12], [38], comparatively fewer learningbased pose estimation algorithms have been integrated into real-time, closed-loop robotic grasping systems due to domain gap and computational expensive inference.",
            "2": "[26] S."
        },
        "Fast-Learning Grasping and Pre-Grasping via Clutter Quantization and Q-map Masking": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2107.02452",
            "ref_texts": "[20] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2019, pp. 4561\u2013",
            "ref_ids": [
                "20"
            ],
            "1": "We also extend the FCN by utilizing a combination of skip connections [20] and upsampling to improve learning efficiency.",
            "2": "In addition, the combination of skip connections [20] and bilinear upsampling in our FCN further boosts accuracy.",
            "3": "[20] S."
        },
        "Module-Wise Network Quantization for 6D Object Pose Estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2303.06753",
            "ref_texts": "[30] Sida Peng, Yuan Liu, Qixing Huang, Hujun Bao, and Xiaowei Zhou. PVNet: Pixel-wise V oting Network for 6DoF Pose Estimation. In Proceedings of the IEEE/CVF Computer Vision and Pattern Recognition , 2019. 1, 2, 3, 5, 7",
            "ref_ids": [
                "30"
            ],
            "1": "In this context, single-stage methods [6, 15, 18, 30, 34, 36, 41, 44] offer a more pragmatic alternative, yielding models with a good accuracy-footprint tradeoff.",
            "2": "These methods typically employ a backbone,a feature decoder and one or multiple heads [6, 17\u201320, 29, 30, 34, 40, 41, 44, 48].",
            "3": "[34, 40] estimate these correspondences in a single global fashion, while [17, 20, 29, 30, 48] aggregate multiple local predictions to improve robustness.",
            "4": "On this basis, we therefore focus on single-stage 6D pose estimation networks [18, 30, 40, 41, 44].",
            "5": "We use 512x512 resolution input for the SwissCube dataset and 640x480 for LINEMOD and O-LINEMOD as in [30].",
            "6": "We follow the settings of PVNet [30] and GDRNet [43] and utilize 15% of the images for training and the remaining 85% for testing.",
            "7": "For both datasets, additional rendered images are used during training [9, 18, 30, 38, 43].",
            "8": "and O-LINEMOD as used by [22, 30, 38, 41, 43]."
        },
        "Iterative Coarse-to-Fine 6D-Pose Estimation Using Back-propagation": {
            "authors": [],
            "url": "http://mprg.jp/data/MPRG/C_group/C20210929_araki.pdf",
            "ref_texts": "[21] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise V oting Network for 6DoF Pose Estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
            "ref_ids": [
                "21"
            ],
            "1": "PVNet [21] estimates pose in two stages: feature point extraction and Perspective-n-Point.",
            "2": "[21] S."
        },
        "A Benchmark for Cycling Close Pass Near Miss Event Detection from Video Streams": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2304.11868",
            "ref_texts": "[40] Peng, S., Liu, Y ., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570",
            "ref_ids": [
                "40"
            ],
            "1": "On the other hand, end-to-end approaches [54], [37], [40] aim to directly return the 3D information and pose parameters of the camera [26], avoiding the nonlinear space for rotation regression and improving efficiency.",
            "2": "medRxiv (2022)\n[40] Peng, S."
        },
        "Towards an egocentric framework for rigid and articulated object tracking in virtual reality": {
            "authors": [],
            "url": "https://wevr.adalsimeone.me/2020/WEVR2020_Taylor.pdf",
            "ref_texts": "[24] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conf. on Comp. Vision and Pattern Recognition , 2019.",
            "ref_ids": [
                "24"
            ],
            "1": "Works such as PoseCNN [39] and PVNet [24] demonstrate accurate 6DoF pose predictions from RGB images, even in complex, uncontrolled environments [1, 21, 38, 40].",
            "2": "[24] S."
        },
        "Review on 6D Object Pose Estimation with the focus on Indoor Scene Understanding": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2212.01920",
            "ref_texts": "[58] S. Peng, Y . Liu, Q. Huang, H. Bao, and X. Zhou. Pvnet: Pixel-wise voting network for 6dof pose estimation. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4556\u20134565, 2019.",
            "ref_ids": [
                "58"
            ],
            "1": "Problem Formulation Pose estimation problem can be formulated as direct classification [53], regression [1], 2D-3D correspondences [58], or 3D-3D correspondences [93].",
            "2": "To address the occlusion problem for keypoint detection, PVNet [58] predicts unit vectors pointing to keypoints for each pixel in the mask of the object and localize 2D keypoints in a RANSAC voting scheme.",
            "3": "3DPVNet [44], inspired from V oteNet [12] and pvnet [58], employed deep learning and Hough voting simultaneously to achieve a patch-level 3D Hough voting method for object 6D pose estimation.",
            "4": "The promise of votingbased techniques [58, 44] are being robust to these challenges.",
            "5": "[58] S."
        },
        "Shape-coded aruco: Fiducial marker for bridging 2d and 3d modalities": {
            "authors": [
                "Lilika Makabe",
                "Hiroaki Santo",
                "Fumio Okura",
                "Yasuyuki Matsushita"
            ],
            "url": "https://openaccess.thecvf.com/content/WACV2022/papers/Makabe_Shape-Coded_ArUco_Fiducial_Marker_for_Bridging_2D_and_3D_Modalities_WACV_2022_paper.pdf",
            "ref_texts": "[27] Sida Peng, Xiaowei Zhou, Yuan Liu, Haotong Lin, Qixing Huang, and Hujun Bao. PVNet: Pixel-wise V oting Network for 6DoF Object Pose Estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) , 2020. early access.",
            "ref_ids": [
                "27"
            ],
            "1": "Some recent methods use deep neural networks to infer the 6 degrees-of-freedom (DoF) object poses from a textured 3D shape and image observations [13, 39, 41, 27, 9]."
        },
        "Pose proposal critic: Robust pose refinement by learning reprojection errors": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2005.06262",
            "ref_texts": "[18] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 2019.",
            "ref_ids": [
                "18"
            ],
            "1": "[18] take a similar approach, but simplify the output space by regressing to the direction from each pixel to each of the projected keypoints, but not the corresponding distance.",
            "2": "[18] does indeed prove robust to partial occlusions, and yields accurate estimates of rotation as well as lateral translation on the Occlusion LINEMOD benchmark.",
            "3": "DeepIM [3] used initializations from PoseCNN [6], but as these predictions are not publicly available, we instead rely on initial pose proposals from PVNet [18].",
            "4": "[13] PVNet [18]PoseCNN [6]\n+ DeepIM [3]PVNet [18]\n+ PPC (Ours) ADD (-S)-0:1D 30:40 41:37 55:50 55:33 REPROJ -S-5PX 60:86 61:84 56:61 66:37\n5CM/5fl-S \u2013 33:36 30:93 41:52 Table 1: Results on Occlusion LINEMOD.",
            "5": "[18] on LINEMOD.",
            "6": "On the Occlusion LINEMOD benchmark, we initialize our method with pose proposals from PVNet [18], yielding state-ofthe-art results for two out of three metrics on this competitive benchmark, while performing on-par with previous methods for the third metric.",
            "7": "Despite the sub-optimal pose proposals from PVNet [18], the poses are accurately recovered.",
            "8": "1 Negative Depth Correction of Pose Proposals We observed that the pose proposals from PVNet [18] sometimes have negative depth, and in this case we switched sign for the object center position, and rotated the object 180 degrees around the principal axis of the camera, in order to yield a feasible estimate with similar projection (the projection is identical for points on the plane which goes through the object center and is parallel to the principal plane of the camera).",
            "9": "This correction is done both when reporting the results of [18], and when reporting the results of our refinement.",
            "10": "[13] PVNet [18]PoseCNN [6]\n+ DeepIM [3]PVNet [18]\n+ PPC (Ours) ape 17:60 15:04 59:18 40:85 can 53:90 63:21 63:52 82:44 cat 3:31 20:30 26:24 35:64 driller 62:40 64:00 55:58 71:33 duck 19:20 33:86 52:41 49:08 eggbox\u000325:90 43:32 62:95 57:28 glue\u000339:60 49:83 71:66 62:90 holepuncher 21:30 41:40 52:48 43:14 Mean 30:40 41:37 55:50 55:33 Table 5: Results on Occlusion LINEMOD according to the ADD (-S)-0:1Dmetric.",
            "11": "[13] PVNet [18]PoseCNN [6]\n+ DeepIM [3]PVNet [18]\n+ PPC (Ours) ape 69:60 66:84 69:02 68:97 can 82:60 82:85 56:14 79:29 cat 65:10 62:34 50:95 66:47 driller 73:80 70:68 52:94 76:52 duck 61:40 59:58 60:54 66:93 eggbox\u000313:10 34:55 49:18 49:28 glue\u000354:90 47:72 52:92 48:06 holepuncher 66:40 70:17 61:16 75:45 Mean 60:86 61:84 56:61 66:37 Table 6: Results on Occlusion LINEMOD according to the REPROJ -S-5PXmetric.",
            "12": "PVNet [18]PoseCNN [6]\n+ DeepIM [3]PVNet [18]\n+ PPC (Ours) ape 37:18 51:75 47:69 can 63:38 35:82 63:63 cat 19:43 12:75 33:19 driller 60:21 45:24 67:46 duck 15:31 22:48 23:01 eggbox\u000310:47 17:81 33:87 glue\u000320:93 42:73 25:80 holepuncher 40:00 18:84 37:52 Mean 33:36 30:93 41:52 Table 7: Results on Occlusion LINEMOD according to the 5 CM/5fl-Smetric."
        },
        "Self-supervised Vision Transformers for 3D Pose Estimation of Novel Objects": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2306.00129",
            "ref_texts": "38. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pix el-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/C VF Conference on Computer Vision and Pattern Recognition (2019)",
            "ref_ids": [
                "38"
            ],
            "1": "In contrast, learning-based solutions using Convolution al Neural Networks (CNN) learn a feature representation to infer object class and geometric correspondencesduring testing [34, 38, 47, 26, 48, 50, 1, 43, 9].",
            "2": "Yet, training pose estimatorsforeachobjectinstance[34, 38],oreachsetofobjec tinstances[48,50] is insufficient to be usable in real world scenarios where object instan ces are manifold and constantly changing."
        },
        "PoseMatcher: One-shot 6D Object Pose Estimation by Deep Feature Matching": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2304.01382",
            "ref_texts": "[30] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 2, 6, 7, 8",
            "ref_ids": [
                "30"
            ],
            "1": "PVNet [30] found that choosing keypoints that lie within the object\u2019s silhouette would yield better results.",
            "2": "We supervise the zoom and 2D location offset directly with a sparse loss: (Lz=||\u03b4z\u2212\u03b5z||1 L2d=||\u03b42d\u2212\u03b52d||1(5) Type Fully Supervised Self-Supervised One-Shot Method PVNet [30] GDR [42] SO-Pose [7] Self6D [41] Sock et al.",
            "3": "It is interesting to observe that we also outperform PVNet[30], an instance-level pose estimator.",
            "4": "To sample which images to use, we sample from the available viewpoints using furthest point sampling [30] w."
        },
        "KRF: Keypoint Refinement with Fusion Network for 6D Pose Estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2210.03437",
            "ref_texts": "[13] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
            "ref_ids": [
                "13"
            ],
            "1": "[2], [13], [14] utilized DNNs to detect keypoints of each object, and then computed the 6D pose parameters with Perspective-n-Point (PnP) for 2D keypoints or Least Squares methods for 3D keypoints.",
            "2": "PVNet [13] tried to predict a unit vector to each keypoint for each pixel, then voted the 2D location for each keypoint and computed the final pose using the PnP algorithm.",
            "3": "Pix2Pose [37] PVNet [13] HybridPose [36] DeepIM [22] PVN3D [14] FFB6D [15] KRF ape 22.",
            "4": "[13] S."
        },
        "Perceiving Unseen 3D Objects by Poking the Objects": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2302.13375",
            "ref_texts": "[4] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
            "ref_ids": [
                "4"
            ],
            "1": "Currently, most 3D perception tasks are passive, such as object detection [1]\u2013[3], object pose estimation [4, 5], object reconstruction [6, 7], etc.",
            "2": "Here, we use the PVNet [4] to demonstrate how to learn an object pose estimator based on the reconstructed object model.",
            "3": "3, given the reconstructed object model, we use the analytic method Graspit! [16] to compute Tgoand PVNet [4] to estimateToc.",
            "4": "[4] S."
        },
        "Finer-Grained Correlations: Location Priors for Unseen Object Pose Estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2211.16290",
            "ref_texts": "[25] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019. 2, 7",
            "ref_ids": [
                "25"
            ],
            "1": "Most existing deep learning methods are instancespecific [41, 25, 35, 37], meaning that the training and testing data contain the same object instances.",
            "2": "36 PVNet [25] 3 7 83.",
            "3": "PVNet [25] 3 7 38.",
            "4": "As the instancespecific methods such as PVNet [25] cannot generalize to unseen objects, the models are retrained on the testing objects."
        },
        "Pose estimation of specular and symmetrical objects": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2011.00372",
            "ref_texts": "[17] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
            "ref_ids": [
                "17"
            ],
            "1": "Deep learning method Recently, deep learning methods[17][9][7][18] have been used to detect robust keypoint.",
            "2": "[17] S."
        },
        "FC-TrackNet: Fast Convergence Net for 6D Pose Tracking in Synthetic Domains": {
            "authors": [
                "Di Jia",
                "Qian Wang",
                "Jun Cao",
                "Peng Cai",
                "Zhiyang Jin"
            ],
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/27077/26849",
            "ref_texts": "7487184. Jonathan, T.; Thang, T.; Balakumar, S.; Yu, X.; Dieter, F.; and Stan, B. 2018. Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects. Paper presented at the 2nd Con-ference on Robot Learning. Zurich, CH, October 29-31. Kehl, W.; Manhardt, F.; Tombari, F.; Ilic, S.; and Navab, N. 2017. SSD-6D: Making RGB -Based 3D Detection and 6D Pose Estimation Great Again. In proceedings of the International Conference on Computer Vision.Venice: Institute of Electrical and Electronics Engineers. doi.org/10.1109/ICCV. 2019. 00777. Li, Y.; Wang, G.; Ji, X.; and Fox, D. 2020. DeepIM: Deep Iterative Matching for 6D Pose Estimation. Int J Comput Vis 128: 657 \u2013678. doi.org/10.1007/s11263019-01250 -9. Li, Z.; Wang, G.; and Ji, X. 2019. CDPN: Coordinates -Based Disentangled Pose Network for Real -Time RGB -Based 6 -DoF Object Pose Estimation. In proceedings of the International Conference on Computer Vision.Se oul: Institute of Electrical and Electronics Engineers. doi.org/10.1109/ICCV. 2019. 00777. Mitash, C.; Bowen, W.; Kostas, B.; and Abdeslam, B. 2020. Scene-level Pose Estimation for Multiple Instances of Densely Packed Objects. Paper presented at the Confer ence on Robot Learning. Boston, MA, October 30-November 1. Pavlakos, G.; Zhou, X.; Chan, A.; Derpanis, K. G.; and Daniilidis, K. 2017. 6-DoF object pose from semantic keypoints. In proceedings of the IEEE International Conference on Robotics and Auto-mation .Singapore: Institute of Electrical and Electronics Engineers. doi.org/10.1109/ICRA. 2017. 7989233. Peng, S.; Zhou, X.; Liu, Y.; Lin, H.; Huang, Q.; and Bao, H. 2020. PVNet: Pixel-Wise Voting Network for 6DoF Object Pose Estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence 44(6): 3212 \u20133223. doi.org/10.1109/TPAMI. 2020. 3047388. Sundermeyer, M.; Marton, Z.-C.; Durner, M.; Brucker, M.; and Triebel, R. 2018. Implicit 3D Orientation Learning for 6D Object Detection from RGB Images. Paper presented at the European Conference on Computer Vision. Munich, DE, September 8-14. Tobin, J.; Fong, R.; Ray, A.; Schneider, J.; Zaremba, W.; and Abbeel, P. 2017. Domain randomization for transferring deep neural networks from simulation to the real world. In proceedings of the Intelligent Robots and Systems.Venice: Institute of Electrical and Electronics Engineers. doi.org/10.1109/IROS. 2017. 8202133. Wang, C.; Xu, D.; Zhu, Y.; Mart\u00edn -Mart\u00edn, R.; Lu, C.; Fei-Fei, L.; and Savarese, S. 2019. DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion. In proceedings of the Computer Vision and Pattern Recognition.California: Computer Vision and Pattern Recog nition. doi.org/10.1109/CVPR. 2019. 00346. Wen, B.; Mitash, C.; Ren, B.; and Bekris, K. E. 2020. se(3) -TrackNet: Data -driven 6D Pose Tracking by Calibrating Image Residuals in Synthetic Domains. In proceedings of the Intelligent Robots and Systems.Nevada: Institute of Electrical and Electronics Engineers. doi.org/10.1109/IROS45743. 2020. 9341314. W\u00fcthrich, M.; Pastor, P.; Kalakrishnan, M.; Bohg, J.; and Schaal, S. 2013. Probabilistic object tracking using a range camera. In proceedings of the Intelligent Ro bots and Systems.Tokyo: Institute of Electrical and Electronics Engineers. doi.org/10.1109/IROS. 2013. ",
            "ref_ids": [
                "7487184"
            ]
        },
        "Lyrn (lyapunov reaching network): A real-time closed loop approach from monocular vision": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2005.12072",
            "ref_texts": "[11] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp.",
            "ref_ids": [
                "11"
            ],
            "1": "Object pose estimation algorithms [9], [11] are designed to find classspecific objects in an image.",
            "2": "[11] S."
        },
        "Bridging the reality gap for pose estimation networks using sensor-based domain randomization": {
            "authors": [
                "Frederik Hagelskjaer",
                "Anders Glent"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021W/3DODI/papers/Hagelskjaer_Bridging_the_Reality_Gap_for_Pose_Estimation_Networks_Using_Sensor-Based_ICCVW_2021_paper.pdf",
            "ref_texts": "[24] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019. 2, 7",
            "ref_ids": [
                "24"
            ],
            "1": "In PVNet [24], the network instead locates keypoints by first segmenting the object and then letting all remaining pixels vote for keypoint locations.",
            "2": "However, as we have trained only on synthetic data, our method is tested both using the 85% split and using all images in the dataset;\n940\n Training DataReal Synthetic Modality RGB RGB-D RGB-D\n[24] [32] [35] [7] [10] [35] [19] Ours Ape 43.",
            "3": "The competing methods are DPOD [35], SSD-6D [19] (obtained from [32]), PVNet [24], DenseFusion [32], PointV oteNet [7] and PVN3D [10].",
            "4": "Training DataReal Synthetic Modality RGB RGB-D RGB-D\n[34] [24] [34] [7] [10] Ours Ape 9."
        },
        "Cross-Attention-Based Reflection-Aware 6D Pose Estimation Network for Non-Lambertian Objects from RGB Images": {
            "authors": [
                "Chenrui Wu",
                "Long Chen",
                "Shiqing Wu"
            ],
            "url": "https://www.mdpi.com/2075-1702/10/12/1107/pdf",
            "ref_texts": "17. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 15\u201320 June 2019; pp. 4561\u20134570.",
            "ref_ids": [
                "17"
            ],
            "1": "PVNet [17] selected eight key points from an object\u2019s surface via the farthest-point-sampling algorithm.",
            "2": "Methods Used for Comparison and Metrics We compared our method with other state-of-the-art 6D pose estimation methods, including PVNet [17] and PSGMN [23].",
            "3": "PVNet [17] PSGMN [23] Ours Object ADDS VSD ADDS VSD ADDS VSD Obj_01 73."
        },
        "MSDA: Monocular Self-supervised Domain Adaptation for 6D Object Pose Estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2302.07300",
            "ref_texts": "23. Peng, S., Zhou, X., Liu, Y., Lin, H., Huang, Q., Bao, H.: Pvnet: pixel-wise voting network for 6dof object pose estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence (2020)",
            "ref_ids": [
                "23"
            ],
            "1": "Different from BB8, PVNet [23] selects the pre-defined 3D keypoints from the surface of the 3D object CAD model and then localizes the 2D pixel coordinates of these 3D keypoints in RGB images based on the pixel-wise voting schema.",
            "2": "3 PVNet [23] 73."
        },
        "VideoPose: Estimating 6D object pose from videos": {
            "authors": [],
            "url": "https://apoorvabeedu.github.io/assets/pdf/VideoPose.pdf",
            "ref_texts": "[25] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4561\u2013",
            "ref_ids": [
                "25"
            ],
            "1": "The classical solution for such 6-DOF pose estimation problems utilises a feature point matching mechanism, followed by Perspective-n-Point (PnP) to correct the estimated pose [26, 29, 25, 12].",
            "2": "Thus, the YCB-Video dataset [36], TLESS [10], and OccludedLINEMOD dataset [16, 25] were introduced.",
            "3": "These datasets have enabled the emergence of novel network designs such as PoseCNN, DPOD [37] and PVNet [36, 25].",
            "4": "To address the problems of heavy occlusions and ambiguities,[25, 12, 23, 22] learn to detect keypoints and then perform PnP."
        },
        "TransPose: A Transformer-based 6D Object Pose Estimation Network with Depth Refinement": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2307.05561",
            "ref_texts": "[49] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
            "ref_ids": [
                "49"
            ],
            "1": "Some other methods have considered using models encompassing classical algorithm such as PnP algorithm to increase the accuracy of estimation [44], [49], [50].",
            "2": "[49] S."
        },
        "6d object pose estimation in cluttered scenes from RGB images": {
            "authors": [],
            "url": "http://jcst.ict.ac.cn/fileup/1000-9000/PDF/2022-3-16-1311.pdf",
            "ref_texts": "[46] Peng S, Liu Y, Huang Q, Zhou X, Bao H. PVNet: Pixelwise voting network for 6DoF pose estimation. In Proc. the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition , June 2019, pp.4561-4570. DOI: 10.1109/CVPR.2019.00469.",
            "ref_ids": [
                "46"
            ],
            "1": "We compare our proposed approach with the following recent advanced methods: PoseCNN[11], Seg-Driven[14], Tekin[28], SilhoNet[34], Pix2Pose[35], BB8[44], Heatmaps[45], PVnet[46], CDPN[47], and ourprevious work (OCP)[17].",
            "2": "80 PVnet[46]15.",
            "3": "[46] Peng S, Liu Y, Huang Q, Zhou X, Bao H."
        },
        "Sim2Real Instance-Level Style Transfer for 6D Pose Estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2203.02069",
            "ref_texts": "[25] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
            "ref_ids": [
                "25"
            ],
            "1": "The network architecture and loss function are the same as the single image translation [25].",
            "2": "Training We selected PVNet [25] as a PoseNet for our experiments.",
            "3": "[25] S."
        },
        "Category-Level 6D Object Pose Estimation with Flexible Vector-Based Rotation Representation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2212.04632",
            "ref_texts": "[23] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
            "ref_ids": [
                "23"
            ],
            "1": "Correspondences-based methods trained their model to establish 2D-3D correspondences [6], [22], [23], [24] or 3D-3D correspondences [19], [25].",
            "2": "Method Input ADD-(S) Speed(FPS) PVNet [23] RGB 86.",
            "3": "[23] S."
        },
        "Towards Two-view 6D Object Pose Estimation: A Comparative Study on Fusion Strategy": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2207.00260",
            "ref_texts": "[18] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 4561\u20134570, 2019.",
            "ref_ids": [
                "18"
            ],
            "1": "To use more reliable correspondence, PVNet [18] selects keypoints from the object\u2019s model, and train a CNN to predict the vertex from every pixel to those keypoints then vote with confidence.",
            "2": "Since feature extraction is not the focus of this paper, we follow [18] to build a multi-scale Fig.",
            "3": "Then, for a fair comparison with the other two approaches, we predict 2D keypoints from the input reference image following [18].",
            "4": "Likewise, we follow [18] to predict 2D keypoint for equitable contrast.",
            "5": "For a fair comparison, we follow [18] [6] to split it into train, valid and test data.",
            "6": "Also, we follow [18] to further create 10000 images by \u201dCut and Paste\u201d strategy to train all the objects.",
            "7": "Implementation Details In implementation, we follow [18] to select 9 keypoints for every object, and perform the same data augmentation.",
            "8": "RGB RGBD Pix2Pose PVNet DPOD CDPN HybridPose SO-Pose GDR-Net Ours DenseFusion REDE\n[2] [18] [3] [40] [19] [22] [21] [4] [6] ape 58.",
            "9": "RGB RGBD PoseCNN PVNet DPOD HybridPose SO-Pose GDR-Net Ours PVN3D REDE FFB6D\n[1] [18] [3] [19] [22] [21] [9] [6] [24] ape 9.",
            "10": "Compared with RGB-based method PVNet [18], our method achieves more accurate results in textureless surfaces, bizarre viewing angle, and some normal situations.",
            "11": "Also, our method beats both [18] and RGBD-based method [6] in occluded occasions.",
            "12": "[18] S.",
            "13": "Compared with RGB-based method PVNet [18], our method achieves more accurate results in textureless surfaces (e), bizarre viewing angle (c), and some normal situations (a)(b)(d).",
            "14": "Also, our methods beats [18] and RGBD-based method [6] in occluded occasions (f)(g)."
        },
        "Iterative pose refinement for object pose estimation based on RGBD data": {
            "authors": [],
            "url": "https://www.mdpi.com/1424-8220/20/15/4114/pdf",
            "ref_texts": "13. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. Pvnet: pixel-wise voting network for 6dof pose estimation. In Proceedings of the 2019 IEEE /CVF Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 15\u201321 June 2019.",
            "ref_ids": [
                "13"
            ],
            "1": "Unlike the approaches [23,30] that use regression to make pose estimate, keypoint-based methods [13,15,17] provide an alternative solution by using pnp solver.",
            "2": "To deal with such problem, plenty of existing methods [13] focus on how to extract reliable object key points for pnp solver to make accurate pose estimate.",
            "3": "PVNet [13] determines the key points with pixel-wise voting network to avoid estimation error under occlusion, and then utilizes uncertainty-driven pnp to estimate object pose.",
            "4": "To deal with such problem, plenty of existing methods [13] focu s on how to extract reliable object key points for pnp solver to make accurate pose estimate.",
            "5": "PVNet [13] determines the key points with pixel-wise voting network to avoid estimation error under occl usion, and then utilizes uncertainty-driven pnp to estimate object pose.",
            "6": "Table 1 shows the accuracy in ADD(-S) metric of the proposed method, in comparison with the state-of-the-art approaches, including BB8 [15], SSD-6D [16], PVNet [13], Tien [10], and DenseFusion [11].",
            "7": "[16] PVNet [13] Tien [10] DenseFusion [11] Proposed Method Ape 40."
        },
        "Uni6Dv3: 5D Anchor Mechanism for 6D Pose Estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2210.10959",
            "ref_texts": "6727. Peng, S.; Liu, Y .; Huang, Q.; Zhou, X.; and Bao, H. 2019. Pvnet: Pixel-wise voting network for 6dof pose estimation. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 4561\u20134570. Rad, M.; and Lepetit, V . 2017. Bb8: A scalable, accurate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth. In Proceedings of the IEEE international conference on computer vision , 3828\u20133836. Su, Y .; Saleh, M.; Fetzer, T.; Rambach, J.; Navab, N.; Busam, B.; Stricker, D.; and Tombari, F. 2022. ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 6738\u20136748. Sun, M.; Zheng, Y .; Bao, T.; Chen, J.; Jin, G.; Zhao, R.; Wu, L.; and Jiang, X. 2022. Uni6Dv2: Noise Elimination for 6D Pose Estimation. arXiv preprint arXiv:2208.06416 . Wang, C.; Xu, D.; Zhu, Y .; Mart \u00b4\u0131n-Mart \u00b4\u0131n, R.; Lu, C.; FeiFei, L.; and Savarese, S. 2019. Densefusion: 6d object pose estimation by iterative dense fusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , 3343\u20133352. Wang, G.; Manhardt, F.; Shao, J.; Ji, X.; Navab, N.; and Tombari, F. 2020. Self6d: Self-supervised monocular 6d object pose estimation. In European Conference on Computer Vision , 108\u2013125. Wang, G.; Manhardt, F.; Tombari, F.; and Ji, X. 2021. Gdrnet: Geometry-guided direct regression network for monocular 6d object pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 16611\u201316621. Xiang, Y .; Schmidt, T.; Narayanan, V .; and Fox, D. 2018. PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes. In Robotics: Science and Systems (RSS) . Xu, D.; Anguelov, D.; and Jain, A. 2018. Pointfusion: Deep sensor fusion for 3d bounding box estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition , 244\u2013253. Zeng, L.; Lv, W. J.; Dong, Z. K.; and Liu, Y . J. 2022. PPRNet++: Accurate 6-D Pose Estimation in Stacked Scenarios. IEEE Transactions on Automation Science and Engineering , 19(4): 3139\u20133151.",
            "ref_ids": [
                "6727"
            ]
        },
        "Rigidity preserving image transformations and equivariance in perspective": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2201.13065",
            "ref_texts": "44.Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: IEEE/CVF Conf. Computer Vision and Pattern Recognition (2019)",
            "ref_ids": [
                "44"
            ],
            "1": "[10, 44, 8], with \u0018200samples for training and\u00181000samples for test.",
            "2": "As expected, there is some discrepancy between the baseline trained by us and the original reported results, in particular Rigidity Preserving Image Transformations and Equivariance in Perspective 13 EP:Baseline EP:PYEP:RHaugPVNet[44] CDPN[38] BPnP[11] RNNPose[55] DFPN-6D[12]\n95:56%(97:35%)97:43% 97:21% 86:27% 89:86% 93:27% 97:37% 98:06% Table 2."
        },
        "Depth-based 6DoF Object Pose Estimation using Swin Transformer": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2303.02133",
            "ref_texts": "[10] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise V oting Network for 6DOF Pose Estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "10"
            ],
            "1": "6D pose estimation techniques are generally classified into three groups based on the type of input data: RGB input [10], [11], [12], RGB-D input [13], [14], [15], and depth image input [16], [17], [18], [19].",
            "2": "INPUTS RGB RGB-D Depth-Only METHODS PVNet [10] Pix2Pose [42] RNNPose [43] PVN3D [15] DenseFusion [13] KPD [44] CloudAAE [16] CATRE [19]OVE6D [17]\n(w Mask R-CNN)OVE6D [17]\n(w GT Masks)OURS\n(w/o GT Mask)OURS\n(w GT Masks) ape 43.",
            "3": "INPUTS RGB RGB-D Depth-Only METHODS PVNet [10] Pix2Pose [42] Keypoint [45] Point-to-Keypoint [46] FFB6D [14] KPD [44] CloudAAE [16]OVE6D [17]\n(w Mask R-CNN)OVE6D [17]\n(w GT Masks)OURS\n(w/o GT Mask)OURS\n(w GT Masks) ape 15."
        },
        "6D pose estimation of object based on fused region-level feature in cluttered scenes": {
            "authors": [
                "Xiangpeng Liu",
                "Huiping Duanmu",
                "Kang An",
                "Wancheng Wang",
                "Yaqing Song",
                "Qingying Gu",
                "Bo Yuan",
                "Danning Wang"
            ],
            "url": "http://www.lib.shnu.edu.cn/_upload/article/files/a7/4e/a6110c2a40a7b8abb3c1d00023d5/a70d3159-043b-40ac-8e16-4c1fe2f9bd59.pdf",
            "ref_texts": "[23]Peng S, Zhou X, Liu Y, Lin H and Bao H 2019 PVNet: pixel-wise voting network for 6DoF object pose estimation Proc. of IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) (16\u201320 June 2019) pp 4561\u201370",
            "ref_ids": [
                "23"
            ],
            "1": "For instance, pixel-wise voting network (PVNet) [23] regresses pixel-wise vectors pointing to the key points and uses these vectors to vote for the key points\u2019 locations.",
            "2": "On the LINEMOD data set, we used the average distance measurement ADD [42] and 2D reprojection [23] for the measurement.",
            "3": "RGB RGB-D Object categoryBB8 [24]\n(%)PVNet [23] (%)HybridPose [25]PoseCNN\n[14] (%)AAE\n[44] (%)SSD-6D\n[20] (%)DenseFusion [16] (%)Proposed (%) Ape 40.",
            "4": "on Computer Vision and Pattern Recognition (CVPR) (18\u201322 June 2018) pp 292\u2013301\n[23]Peng S, Zhou X, Liu Y, Lin H and Bao H 2019 PVNet: pixel-wise voting network for 6DoF object pose estimation Proc."
        },
        "Attention voting network with prior distance augmented loss for 6DoF pose estimation": {
            "authors": [],
            "url": "https://www.jstage.jst.go.jp/article/transinf/E104.D/7/E104.D_2020EDP7235/_pdf",
            "ref_texts": "[7]S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp.4561\u20134570, 2019.",
            "ref_ids": [
                "7"
            ],
            "1": "Most recent state-of-the-artmethods, such as [7]\u2013[10],[19], adopt a two-stage pipeline based on dense prediction.",
            "2": "Unit direction vector-field representation and Hough voting scheme were proposed for robust 2D keypoint localization and demonstrated its superiority in [6],[7],[17].",
            "3": "However, the Hough voting scheme is non-di fferentiable so that it cannot be integrated into the learning of vector-field represen-tation for joint training, and [6],[7],[17]just use smooth/lscript1 loss[24]to learn it.",
            "4": "A U-structure fully convolutional network is adopted by[6],[7],[14],[17]for 6DoF pose estimation.",
            "5": "xEmbedding the AFAM into the PVNet [7](a standard UNet), we build an Attention Voting Network to further im-prove the performance of our method.",
            "6": "[7],[22]demonstrate that selecting the surface point of the 3D model as the keypoint can acquire more accurate poses.",
            "7": "[7] achieve state-of-the-art performance via their unit directionvector-field representation and Hough voting scheme.",
            "8": "1, the Attention Voting Network predicts pixelwise semantic labels and vector-field representation, then the Hough voting layer [7]uses the intermediate representation to locate the 2D keypoint as described in 3.",
            "9": "tion points of predefined 3D keypoints associated with the 3D object models, where the keypoint localization is implemented through the Hough voting scheme [7]based on the semantic mask and vector-field representation.",
            "10": "Instead, we follow [7],[14] to define the 3D point set for each object by the Farthest Point Sampling (FPS) algorithm [50].",
            "11": "Vector-field representation consists of pixel-wise unit direction vectors for each 2Dkeypoint [6],[7],[17].",
            "12": "[7], Capellen et al.",
            "13": "To learn pixel-wise prediction, the low-resolution feature maps be repeatedly performed feature fusion, convolution, and bi-linear upsampling until the size is restored to H\u00d7W, where the feature fusion can be implemented via channel connec-tion as in [7],[25] or our attention module.",
            "14": "The number of images in each sequence isnot enough to train the deep network, thus we use the codeprovided by [7]to render 10,000 synthetic images for each sequence.",
            "15": "For multiple instances scenes, we generate voting centers through clustering and assign masks to the nearest voting center as [7],[14].",
            "16": "Wefollow previous works [7]to divide the training and testing set.",
            "17": "For YCB-Video dataset,we follow [6],[7]and compute the area under the accuracythreshold curve, i.",
            "18": "MethodsBBS Pix2pose DPOD CDPN PVNet Ours [15] [23] [8] [10] [7] PDAL AFAM PDAL +AFAM ape 27.",
            "19": "MethodsBBS CDPN Oberweger PVNet Ours [15] [10] [20] [7] PDAL AFAM PDAL +AFAM LINEOMD 83.",
            "20": "MethodsPoseCNN Pix2pose DPOD PVNet Ours [6] [23] [8] [7] PDAL AFAM PDAL +AFAM ape 9.",
            "21": "Comparing our methods with PVNet [7], both PDAL and AFAM have significant improvements on most objects, especiallyon ape, cat, duck, etc.",
            "22": "MethodsPoseCNN Oberweger PVNet Ours [6] [20] [7] PDAL AFAM PDAL +AFAM\n2D Projection 39.",
            "23": "Green 3D bounding boxes represent the ground truth poses, blue 3D bounding boxes correspond to the poses predicted by thebaseline (PVNet [7]), yellow and red 3D bounding boxes respectively represent the predicted poses of PDAL and AFAM.",
            "24": "CDPN [10] and the PVNet [7].",
            "25": "[7]S."
        },
        "3D point-to-keypoint voting network for 6D pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2012.11938",
            "ref_texts": "[6] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp.",
            "ref_ids": [
                "6"
            ],
            "1": "With the rapid development of deep learning on image, a lot of methods for pose estimation using RGB image have been proposed [5], [6], [7], [8].",
            "2": "Existing methods can be categorized as extended 2D object detection method [9], [10], predicting pose directly by CNN [11], [12], predicting 2D projection points [5], [6] and template matching [13].",
            "3": "To overcome this problem, PVNet [6] selects kepoints using the farthest point sampling (FPS) algorithm.",
            "4": "Inspired by recent 2D methods [5], [6], we estimate pose by 3D keypoints instead of regression directly.",
            "5": "Before training, FPS algorithm is employed to sample K 3D keypoints on CAD model surface like PVNet [6].",
            "6": "[6] S."
        },
        "SpyroPose: Importance Sampling Pyramids for Object Pose Distribution Estimation in SE (3)": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2303.05308",
            "ref_texts": "[24] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "24"
            ],
            "1": "Most work [30, 26, 24, 19] on pose estimation uses manual annotations to explicitly handle one cause of visual ambiguity, symmetries, and infers a \u201cbest guess\u201d pose estimate without expressing uncertainty."
        },
        "Pam: Point-wise attention module for 6d object pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2008.05242",
            "ref_texts": "[20] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp.",
            "ref_ids": [
                "20"
            ],
            "1": "In [16], [17], [18], [19], [20], [21], [22] regressed the coordinates of sparse 2D keypoints by learning features from RGB using CNN structure.",
            "2": "[20] to compensate for the occlusion and truncation, which are the problems covered by the sparse keypoints and heatmap method above, an attempt was made to find the keypoints by representing the direction toward the keypoints in pixel-wise.",
            "3": "[20] S."
        },
        "NeRF-Loc: Visual Localization with Conditional Neural Radiance Field": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2304.07979",
            "ref_texts": "[21] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "21"
            ],
            "1": "PVNet[21] 25."
        },
        "External Camera-based Mobile Robot Pose Estimation for Collaborative Perception with Smart Edge Sensors": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2303.03797",
            "ref_texts": "[28] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d in IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , 2019, pp. 4561\u20134570.",
            "ref_ids": [
                "28"
            ],
            "1": "[28] S."
        },
        "Self-guided instance-aware network for depth completion and enhancement": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2105.12186",
            "ref_texts": "[12] S. Y . Q. H.Bao and X.Zhou, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d IEEE Conference on Computer Vision and Pattern Recognition , pp. 4556\u20134565, 2018.",
            "ref_ids": [
                "12"
            ],
            "1": "In contrast, as convolutional neural network (CNN) have been successful in learning effective representations in object detection [9]\u2013[11], and pose estimation [12], [13].",
            "2": "[12] S."
        },
        "Iterative 3D Deformable Registration from Single-view RGB Images using Differentiable Rendering.": {
            "authors": [],
            "url": "https://www.scitepress.org/Papers/2022/108171/108171.pdf",
            "ref_texts": "468. Peng, S., Liu, Y ., Huang, Q., Zhou, X., and Bao, H. (2019). PVNet: Pixel-wise voting network for 6DOF pose estimation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4561\u20134570. Periyasamy, A. S., Schwarz, M., and Behnke, S. (2019). Refining 6D object pose predictions using abstract render-and-compare. In IEEE-RAS 19th International Conference on Humanoid Robots (Humanoids) , pages 739\u2013746. Pharr, M., Jakob, W., and Humphreys, G. (2016). Physically based rendering: From theory to implementation. Morgan Kaufmann. Ravi, N., Reizenstein, J., Novotny, D., Gordon, T., Lo, W.Y ., Johnson, J., and Gkioxari, G. (2020). Accelerating 3D deep learning with PyTorch3D. In European Conference on Computer Vision (ECCV) . Rodriguez, D., Cogswell, C., Koo, S., and Behnke, S.",
            "ref_ids": [
                "468"
            ]
        },
        "A 3D real object recognition and localization on SLAM based augmented reality environment": {
            "authors": [
                "Jongin Choe",
                "Ang University",
                "Sanghyun Seo",
                "Ang University"
            ],
            "url": "https://american-cse.org/sites/csci2020proc/pdfs/CSCI2020-6SccvdzjqC7bKupZxFmCoA/762400a745/762400a745.pdf",
            "ref_texts": "[8] Peng, S., Liu, Y., Huang, Q., Zhou, X., and Bao, H, \"Pvnet: Pix el-wise voting network for 6dof pose estimation,\" Proceedings of the IE EE Conference on Computer Vision and Pattern Recognition, pp. 4561 4570, 2019. ",
            "ref_ids": [
                "8"
            ],
            "1": "Sida Peng et al also proposed a system that estimates the position on 3D coordinates after recognizing real-world objects[8].",
            "2": "[8] Peng, S."
        },
        "Learning Bifunctional Push-grasping Synergistic Strategy for Goal-agnostic and Goal-oriented Tasks": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2212.01763",
            "ref_texts": "[24] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2019, pp. 4561\u2013",
            "ref_ids": [
                "24"
            ],
            "1": "2)Grasp module establishes contact with the perception module through skip connections [24] and outputs a dense pixel-wise map of Qvalues as Grasp Qmap for the grasping action.",
            "2": "[24] S."
        },
        "Generalizable Pose Estimation Using Implicit Scene Representations": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2305.17252",
            "ref_texts": "[13] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
            "ref_ids": [
                "13"
            ],
            "1": "The majority of approaches can only be used for a specific object or for categories ([8], [9], [10], [11], [12], [13], [14]) that are similar to the ones in the training data.",
            "2": "b) Instanceand Category-Specific Pose Estimation: Most state-of-the-art object pose estimators are either instance-specific ([8], [9], [10], [11], [12], [13], [14]) or category-specific ([29], [30]).",
            "3": "Keypoint-based approaches ([13], [14]) utilized deep neural networks to detect 2D keypoints of an object and computed 6D pose parameters with Perspective-n-Point (PnP) algorithms, improving pose estimates by a large margin.",
            "4": "[13] S."
        },
        "Real-time detection of 2d tool landmarks with synthetic training data": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2210.11991",
            "ref_texts": "(2019). Photorealistic image synthesis for object instance detection. In 2019 IEEE International Conference on Image Processing (ICIP) , pages 66\u201370. Ioffe, S. and Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. CoRR , abs/1502.03167. Kingma, D. P. and Ba, J. (2017). Adam: A method for stochastic optimization. Law, H. and Deng, J. (2020). Cornernet: Detecting objects as paired keypoints. International Journal of Computer Vision , 128. Long, J., Zhang, N., and Darrell, T. (2014). Do convnets learn correspondence? CoRR , abs/1411.1091. Lowe, D. (2004). Distinctive image features from scaleinvariant keypoints. International Journal of Computer Vision , 60:91\u2013.Newell, A., Yang, K., and Deng, J. (2016). Stacked hourglass networks for human pose estimation. In Leibe, B., Matas, J., Sebe, N., and Welling, M., editors, Computer Vision \u2013 ECCV 2016 , pages 483\u2013499, Cham. Springer International Publishing. Pavlakos, G., Zhou, X., Chan, A., Derpanis, K. G., and Daniilidis, K. (2017). 6-dof object pose from semantic keypoints. In 2017 IEEE International Conference on Robotics and Automation (ICRA) , pages 2011\u20132018. Peng, S., Liu, Y ., Huang, Q., Zhou, X., and Bao, H. (2019). Pvnet: Pixel-wise voting network for 6dof pose estimation. pages 4556\u20134565. P\u00b4erez, P., Gangnet, M., and Blake, A. (2003). Poisson image editing. ACM Trans. Graph. , 22(3):313\u2013318. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research , 15(56):1929\u20131958. Toshev, A. and Szegedy, C. (2014). Deeppose: Human pose estimation via deep neural networks. In 2014 IEEE Conference on Computer Vision and Pattern Recognition, pages 1653\u20131660. Tulsiani, S. and Malik, J. (2014). Viewpoints and keypoints. CoRR , abs/1411.6067. Wei, S., Ramakrishna, V ., Kanade, T., and Sheikh, Y ."
        },
        "SaMfENet: Self-Attention Based Multi-Scale Feature Fusion Coding and Edge Information Constraint Network for 6D Pose Estimation": {
            "authors": [
                "Zhuoxiao Li",
                "Xiaobing Li",
                "Shihao Chen",
                "Jialong Du",
                "Yong Li"
            ],
            "url": "https://www.mdpi.com/2227-7390/10/19/3671/pdf",
            "ref_texts": "2. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Padua, Italy, 18\u201323 July 2019; pp. 4561\u20134570.",
            "ref_ids": [
                "2"
            ],
            "1": ", PVNet [2], BB8 [3], and PoseCNN [4], and RGB-D data input-based networks, e.",
            "2": "Moreover, PVNet [2] extracts the features of the object through a convolutional neural network at the beginning and adopts pixel-level unit vector representation.",
            "3": "Input RGB RGB-D MethodBB8\n[3]PVNet [2]PoseCNN [4]\n+ DeepIM [18]HRPose [37]SSD-6D [14]\n+ ICP [21]MSCNet [9]DenseFusion [5]EANet [10]Ours ape 40."
        },
        "MBAPose: Mask and bounding-box aware pose estimation of surgical instruments with photorealistic domain randomization": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2103.08105",
            "ref_texts": "[23] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
            "ref_ids": [
                "23"
            ],
            "1": "They estimate 2D information such as key points [23], or projected corner points of 3D bounding boxes [24], in the first stage and subsequently use an optimization algorithm in the second stage using the information of the first stage to obtain the instrument\u2019s pose.",
            "2": "[23] S."
        },
        "Investigations on output parameterizations of neural networks for single shot 6d object pose estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2104.07528",
            "ref_texts": "[28] S. Peng, Y . Liu, Q. Huang, H. Bao, and X. Zhou, \u201cPVNet: Pixelwise voting network for 6dof pose estimation,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2019.",
            "ref_ids": [
                "28"
            ],
            "1": "Other single shot approaches to object pose estimation output the 2D projections of the 3D bounding box and use a P nP algorithm [27] to compute the 6D pose [17], [18], [28], [29].",
            "2": "[28] S."
        },
        "End-to-end differentiable 6DoF object pose estimation with local and global constraints": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2011.11078",
            "ref_texts": "[9]S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
            "ref_ids": [
                "9"
            ],
            "1": "A recent study regressed direction vectors to the 2D keypoints from the segmented regions of the object [9].",
            "2": "Their end-to-end trainable model showed improved results compared to the two stage approach as validated with two state of the art correspondence estimators [8] [9].",
            "3": "We follow up on their model with [9] as the correspondence estimator as it shows superior performance and refer to it as SSPE.",
            "4": "This is because a 2D keypoint is given by the intersection of a pair of direction vectors pointing to that keypoint [9].",
            "5": "Part I: Occlusion Linemod Part II: Linemod PVNet [9] DPVR [13] SSPE [10] SSPE-r2SSPE-ours PVNet [9] DPVR [13] SSPE-r SSPE-ours Ape 15.",
            "6": "The final lossLto optimize is a linear combination of the cross entropy segmentation loss Lsand L1 vector regression loss Lkfrom the correspondence estimator [9], and the pose loss Lpand triplet regularization term Ltfrom the pose estimator.",
            "7": "As per previous studies [9][10], we train separate models for each object.",
            "8": "Similar to previous approaches [9][10], we augment the Linemod train data using synthetic data.",
            "9": "We generate 10000 images containing multiple objects using the cut and paste [17] technique, and8\u000210000 images of single objects using the rendering technique in [9].",
            "10": "[9]S."
        },
        "Multi-View Keypoints for Reliable 6D Object Pose Estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2303.16833",
            "ref_texts": "[7] Peng, S., Liu, Y ., Huang, Q., Zhou, X., Bao, H. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (2019)",
            "ref_ids": [
                "7"
            ],
            "1": "These methods have been revisited using the advantages of CNNs to automatically learn features and implicitly estimate pose [9] [7].",
            "2": "The network is derived from YOLO object detection [15], specifically trained using multi-view RGB-D data that is rendered synthetically using domain randomization [7].",
            "3": "Single-view Keypoint and Heatmap Estimation The keypoint detection network is based off PVNet, a pixel-wise voting network for 6D pose estimation [7].",
            "4": "In Conference on Computer Vision and Pattern Recognition , 770\u2013778, (2016)\n[7] Peng, S."
        },
        "3DPVNet: Patch-level 3D Hough Voting Network for 6D Pose Estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2009.06887",
            "ref_texts": "[26] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixelwise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019. 1, 2",
            "ref_ids": [
                "26"
            ],
            "1": "5D images, such as PVNet [26].",
            "2": "Existing methods [26,36,11] employ point/pixel-wise voting, which account for a large amount of computation redundancy.",
            "3": "PVNet [26] predicts vectors pointing to 2D keypoints, forming dense correspondences between 2D and 3D data.",
            "4": "Our work is inspired by PVNet [26] and V oteNet [27], and employs deep learning and Hough voting simultaneously to achieve a patch-level 3D Hough voting method for object 6D pose estimation.",
            "5": "2\n[26] S."
        },
        "Go Beyond Point Pairs: A General and Accurate Sim2Real Object Pose Voting Method with Efficient Online Synthetic Training": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2211.13398",
            "ref_texts": "[19] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "19"
            ],
            "1": "PVNet [19] regresses the vector fields toward the 2D projections of the chosen 3D key points and uses weighted PnP to estimate the final location."
        },
        "A Flexible-Frame-Rate Vision-Aided Inertial Object Tracking System for Mobile Devices": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2210.12476",
            "ref_texts": "[29] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. PVNet: Pixelwise voting network for 6DoF pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 4561\u20134570, 2019.",
            "ref_ids": [
                "29"
            ],
            "1": "PVNet [29] selects keypoints by the distance from the center to the surface of the 3D object model.",
            "2": "For example, PVNet [29] may be a good choice for general use.",
            "3": "[29] S."
        },
        "An improved estimation algorithm of space targets pose based on multi-modal feature fusion": {
            "authors": [
                "Jiang Hua",
                "Tonglin Hao",
                "Liangcai Zeng",
                "Gui Yu"
            ],
            "url": "https://www.mdpi.com/2227-7390/9/17/2085/pdf",
            "ref_texts": "3. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation. In Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 15\u201320 June 2019; IEEE: Long Beach, CA, USA; pp. 4556\u20134565.",
            "ref_ids": [
                "3"
            ],
            "1": "With the development of deep learning technology in the field of two-dimensional images, the method of remote pose estimating from images directly based on CNN architecture has been proposed multiple times [3]."
        },
        "Weakly Supervised Learning of Keypoints for 6D Object Pose Estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2203.03498",
            "ref_texts": "[26] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019. 1, 2, 4, 6, 7, 8",
            "ref_ids": [
                "26"
            ],
            "1": "Recent deep learning-based approaches either directly regress the 6D pose from an image [20, 38] or build 2D-3D correspondences and indirectly solve for the pose via PnP\n[24, 26, 41].",
            "2": "PVNet [26] and its extensions [10, 32] predict the 2D projections of a sparse point set selected on the surface of the object model.",
            "3": "We also make same modifications as PVNet [26].",
            "4": "On LINEMOD and OCCLUSION, we use exactly the same training and test splits as prior works [26, 28, 35].",
            "5": "In contrast to previous fully supervised method [26] with increasing accuracy after 8 keypoints, our accuracy drops when there are too many keypoints, i.",
            "6": "We conjecture two reasons for this observation: 1) weak supervision makes it harder for the network to learn to detect more keypoints; 2) unlike [26] where the keypoints are obtained from the 3D object, our 3D virtual keypoints are randomly generated and thus have weak relation to the 3D object.",
            "7": "methodsRelative Pose Synthetic Data RGB with Pose Annotations OK-POSE [42] Ours AAE [34] DPOD [41] NOL [25] PVNet [26] CDPN [20] DPOD [41] ape 35.",
            "8": "PVNet [26], CDPN [20], and DPOD [41]) trained on real images annotated with full 6D object poses.",
            "9": "MethodsPVNet Pix2Pose HybridPose RLLGOurs[26] [24] [32] [5] ape 15.",
            "10": "3 summaries the comparison with PVNet [26], Pix2Pose [24], HybridPose [32], and RLLG [5]on the OCCLUSION dataset in terms of ADD(-S) metric."
        },
        "Multi-view object pose distribution tracking for pre-grasp planning on mobile robots": {
            "authors": [],
            "url": "https://findresearcher.sdu.dk/ws/files/206261625/Multi_view_pose_distribution_tracking_ICRA2022_camera_ready_2_.pdf",
            "ref_texts": "[28] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp.",
            "ref_ids": [
                "28"
            ],
            "1": "To improve robustness against occlusions,recent works have focused on the prediction of object key-points [26], [27], [28] and then solve the Perspective-n-Point problem for pose estimation.",
            "2": "[28] S."
        },
        "Learning 6-DoF Object Poses to Grasp Category-Level Objects by Language Instructions": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2205.04028",
            "ref_texts": "[24] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570. 2",
            "ref_ids": [
                "24"
            ],
            "1": "The former ones [2], [12], [24], [33], [35] rely on pre-scanned CAD models of objects, which is impractical in real-world scenarios.",
            "2": "1, 2\n[24] S."
        },
        "Knowledge Distillation for 6D Pose Estimation by Keypoint Distribution Alignment": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2205.14971",
            "ref_texts": "[32] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Conference on Computer Vision and Pattern Recognition , 2019. 1, 2, 3, 4",
            "ref_ids": [
                "32"
            ],
            "1": "Modern methods that tackle this task [7,19,20,24,27,32,39,44,46] all rely on deep neural networks.",
            "2": "the 8 3D bounding box corners [18\u201320] or points on the object surface [32], others produce dense representations, such as 3D locations [7,44] or binary codes [39], from which the pose can be obtained.",
            "3": "In particular, several techniques jointly segment the object and predict either the 2D image locations of the corners of the 3D object bounding box [18\u201320] or the 2D displacements from the cells\u2019 center of points on the object\u2019s surface [32].",
            "4": "As discussed above, we focus on approaches that produce local predictions, such as sparse 2D keypoints [18\u201320, 32] or dense quantities [7, 27, 39, 44].",
            "5": "In particular, we consider the case of predicting the 2D locations of the 8 object bounding box corners [18\u201320, 32]."
        },
        "MDS-Net: A Multi-scale Depth Stratification Based Monocular 3D Object Detection Algorithm": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2201.04341",
            "ref_texts": "2017. 3D Bounding Box Estimation Using Deep Learning and Geometry. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . Naiden, A.; Paunescu, V .; Kim, G.; Jeon, B.; and Leordeanu, M. 2019. Shift r-cnn: Deep monocular 3d object detection with closed-form geometric constraints. In 2019 IEEE International Conference on Image Processing (ICIP) , 61\u201365. IEEE. Nguyen, T.; and Grishman, R. 2018. Graph convolutional networks with argument-aware pooling for event detection. InProceedings of the AAAI Conference on Artificial Intelligence , volume 32. Peng, S.; Liu, Y .; Huang, Q.; Zhou, X.; and Bao, H. 2019. PVNet: Pixel-Wise V oting Network for 6DoF Pose Estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . Qi, C. R.; Liu, W.; Wu, C.; Su, H.; and Guibas, L. J. 2018. Frustum PointNets for 3D Object Detection From RGB-D Data. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . Qi, C. R.; Su, H.; Mo, K.; and Guibas, L. J. 2017. PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . Qin, Z.; Wang, J.; and Lu, Y . 2019. Monogrnet: A geometric reasoning network for monocular 3d object localization. InProceedings of the AAAI Conference on Artificial Intelligence , volume 33, 8851\u20138858. Ravanbakhsh, M.; Nabi, M.; Sangineto, E.; Marcenaro, L.; Regazzoni, C.; and Sebe, N. 2017. Abnormal event detection in videos using generative adversarial nets. In 2017 IEEE International Conference on Image Processing (ICIP) , 1577\u20131581. IEEE. Redmon, J.; Divvala, S.; Girshick, R.; and Farhadi, A. 2016. You Only Look Once: Unified, Real-Time Object Detection. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . Ren, S.; He, K.; Girshick, R.; and Sun, J. 2017. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. IEEE Transactions on Pattern Analysis and Machine Intelligence , 39(6): 1137\u20131149.Shi, W.; and Rajkumar, R. 2020. Point-gnn: Graph neural network for 3d object detection in a point cloud. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , 1711\u20131719. Shi, X.; Ye, Q.; Chen, X.; Chen, C.; Chen, Z.; and Kim, T.-K.",
            "ref_ids": [
                "2017"
            ]
        },
        "Iterative optimisation with an innovation CNN for pose refinement": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2101.08895",
            "ref_texts": "[PLH+19] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. InCVPR , 2019.",
            "ref_ids": [
                "PLH\\+19"
            ],
            "1": "Pose via an Intermediate Representation: A recent, popular approach to pose estimation is to first regress to an intermediate representation such as keypoints, from which pose can be obtained via 2D-3D correspondences and a PnP algorithm [TSF17 , RL17 ,PLH+19,SSH20 ,ZSI19 ,lWJ19b ,HBM20 ].",
            "2": "Alternatively, [PLH+19] predicts pixel-wise unit-vectors that in turn indicate direction to keypoints.",
            "3": "[SSH20 ] implements these vector directions from [PLH+19] while also estimating object edge vectors and symmetry correspondences.",
            "4": "We choose PVNet [PLH+19] to be the baseline 3 PREPRINT object pose estimation network.",
            "5": "The function hmaps a vector field representation of keypoints to object pose via a RANSAC and uncertainty-driven PnP framework (EPnP), as discussed in [PLH+19].",
            "6": "We use a pretrained PVNet [PLH+19] to provide the initial estimate Xk ij(0)for each object.",
            "7": "We compute 8 keypoints via farthest point sampling, from which object pose is obtained via uncertainty-driven PnP [PLH+19].",
            "8": "7 PREPRINT Methods CDPN [lWJ19b] YOLO6D [RDGF16] PVNet [PLH+19] OURS ape 92.",
            "9": "Methods DPOD [ZSI19] Pix2Pose [PPV19] PVNet [PLH+19] OURS ape 22.",
            "10": "Methods DPOD [ZSI19] CDPN [lWJ19b] PVNet [PLH+19] OURS ape 53."
        },
        "6D object pose estimation with pairwise compatible geometric features": {
            "authors": [],
            "url": "https://dspace.mit.edu/bitstream/handle/1721.1/138123/ICRA2021PoseEstimation.pdf?sequence=1&isAllowed=y",
            "ref_texts": "[27] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp.",
            "ref_ids": [
                "27"
            ],
            "1": "Recent work such as [27] and [25] predicts either a direction vector pointing to keypoints or offsets to the center of target objects for each point and achieves satisfactory accuracy.",
            "2": "Local reasoning via RANSAC and Hough-voting is frequently used [7], [8], [27].",
            "3": "Most previous work uses RANSAC [24], [27] to iteratively sample a least set of points and find a valid hypothesis with the maximum consensus.",
            "4": "Comparison with the State-of-the-Art on LineMOD and Occlusion LineMOD Datasets a) Performance on LineMOD dataset: We evaluate performance of our method and compare pose estimation recall against state-of-the-art methods [15], [27], [24], [42], [25], [40], [8].",
            "5": "The first observation we draw from the row of the mean recall is that, with a more consistent geometric representation, we can eliminate the gap with methods using RGB [15], [27] even without ICP refinement.",
            "6": "RGB RGBD Depth only Method [15] [27] [24] [42] [25] [40] [8] Ours Ours+MCS Refinement DeepIM ICP [25] ICPICPvariantape 40.",
            "7": "RGB RGBD Depth only Method [16] [27] [24] [40]\u2020[28]\u2020[8]\u2020Ours Ours+MCS ape 2.",
            "8": "[27] S."
        },
        "Model-Based Underwater 6D Pose Estimation from RGB": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2302.06821",
            "ref_texts": "[22] S. Peng, Y . Liu, Q. Huang, H. Bao, and X. Zhou, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d CoRR , vol. abs/1812.11788, 2018.",
            "ref_ids": [
                "22"
            ],
            "1": "These metrics were chosen as the most famous and usually cited in this field ([21], [22], [23], [24], [25], [20]).",
            "2": "[22] S."
        },
        "Object-based visual camera pose estimation from ellipsoidal model and 3d-aware ellipse prediction": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2203.04613",
            "ref_texts": "32. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pp. 4561{4570. Computer Vision Foundation / IEEE (2019)",
            "ref_ids": [
                "32"
            ],
            "1": "Many methods exist for estimating the pose of an object with respect to the camera frame [4, 17, 23, 30, 32, 36, 48, 50, 57], however, they usually require a detailed textured model of the object and suflcient training images.",
            "2": "Many works exist on this subjects [17, 23, 32, 36, 48, 50, 57]."
        },
        "Data-Link: High Fidelity Manufacturing Datasets for Model2Real Transfer under Industrial Settings": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2306.05766"
        },
        "Deep learning on point clouds with applications in vehicle self-localization": {
            "authors": [],
            "url": "https://oparu.uni-ulm.de/xmlui/bitstream/handle/123456789/48402/diss_nico_engel.pdf?sequence=3",
            "ref_texts": "[PLH+19]Peng, Sida; Liu, Yuan; Huang, Qixing; Zhou, Xiaowei; and Bao, Hujun: PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation . In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR",
            "ref_ids": [
                "PLH\\+19"
            ],
            "1": "[PLH+19]Peng, Sida; Liu, Yuan; Huang, Qixing; Zhou, Xiaowei; and Bao, Hujun: PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation ."
        },
        "One-Shot General Object Localization": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2211.13392",
            "ref_texts": "[15] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "15"
            ],
            "1": "During inference, inspired by PVNet [15], we randomly sample point pairs and cast a center vote for each pair.",
            "2": "However, the variance of corner votes is much larger than center, which is also observed in previous work [15]."
        },
        "Object pose estimation for robotic grasping based on multi-view keypoint detection": {
            "authors": [
                "Zheyuan Hu",
                "Beihang University",
                "Renluan Hou",
                "Beihang University",
                "Jianwei Niu",
                "Beihang University",
                "Zhengzhou University",
                "Xiaolong Yu",
                "Beihang University",
                "Tao Ren",
                "Beihang University",
                "Qingfeng Li",
                "Beihang University"
            ],
            "url": "http://www.cloud-conf.net/ispa2021/proc/pdfs/ISPA-BDCloud-SocialCom-SustainCom2021-3mkuIWCJVSdKJpBYM7KEKW/264600b295/264600b295.pdf",
            "ref_texts": "[12] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE/CVF Conf. on Comp. Vision and Pattern Recog., pages 4561\u20134570, 2019.",
            "ref_ids": [
                "12"
            ],
            "1": "6DoF pose estimation can be divided into correspondencebased methods [10], templat-based methods [11], and votingbased methods [12]."
        },
        "Development of Vision Guided Real-Time Trajectory Planning System for Autonomous Ground Refuelling Operations using Hybrid Dataset": {
            "authors": [],
            "url": "https://dspace.lib.cranfield.ac.uk/bitstream/handle/1826/19049/Development_of_vision_guided_real-time_trajectory_planning_system-2023.pdf?sequence=1"
        },
        "Generative model for spacecraft image synthesis using limited dataset": {
            "authors": [],
            "url": "https://taehajeffpark.com/files/papers/parkdamico_aas2020.pdf",
            "ref_texts": "[19] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise V oting Network for 6DoF Pose Estimation,\u201d The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Oral , 2019.",
            "ref_ids": [
                "19"
            ],
            "1": "[19] S."
        },
        "Kosnet: A unified keypoint, orientation and scale network for probabilistic 6d pose estimation": {
            "authors": [],
            "url": "http://groups.csail.mit.edu/robotics-center/public_papers/Hashimoto20.pdf",
            "ref_texts": "[19] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise V oting Network for 6DoF Pose Estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
            "ref_ids": [
                "19"
            ],
            "1": "More recent works focus on fixing this problem by using local patches to reduce the effect of occlusion [17] or adding a segmentation head to aggregate information only from pixels in the object regions [18], [19].",
            "2": "Several other works [15], [19] realize the benefits of heat maps of keypoints in enabling probabilistic fusion.",
            "3": "[19] S."
        },
        "Learning Stereopsis from Geometric Synthesis for 6D Object Pose Estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2109.12266",
            "ref_texts": "[14] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 4561\u20134570, 2019.",
            "ref_ids": [
                "14"
            ],
            "1": "To use more reliable correspondence, PVNet [14] selects keypoints from the object\u2019s model, and train a CNN to predict the vertex from every pixel to those keypoints.",
            "2": "Since feature extraction is not the focus of this paper, we follow [14] to build a multi-scale convolutional neural network, to extract features at multiple resolutions.",
            "3": "Many recent works adopt RANSAC(RANdom SAmple Consensus) algorithm [24] or its variants to pick inliers [14] [16].",
            "4": "Implementation Details In inplementation, we follow [14] to select 9 keypoints for every object, and perform the same data augmentation.",
            "5": "RGB 2-view RGB PoseCNN PVNet Hu Late-fusion Ours [1] [14] [28] ape 9.",
            "6": "For fair comparison, we take the 2D predicted keypoints from PVNet [14], and triangulate the two keypoints to 3D space by classic method [29].",
            "7": "It can be observed that compared with [14] and late-fusion approach, our method can accurately estimate the pose of objects, especially in some hard cases with occlusion.",
            "8": "[14] S."
        },
        "Experimental Evaluation of Affordance Detection Applied to 6-DoF Pose Estimation for Intelligent Robotic Grasping of Household Objects": {
            "authors": [
                "Aidan Keaveny"
            ],
            "url": "https://uwspace.uwaterloo.ca/bitstream/handle/10012/17716/Keaveny_Aidan.pdf?sequence=3",
            "ref_texts": "[19] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao. PVNET: Pixel-wise Voting Network for 6DoF Pose Estimation. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition , pages 4556\u20134565, 2019. ISSN 10636919. doi: 10.1109/CVPR.2019.00469.",
            "ref_ids": [
                "19"
            ],
            "1": "3 Classical Pose Estimation Classical methods for 6-DoF pose estimation can be broadly classified as template-based methods [15, 16, 17] or feature-based methods [7, 11, 18, 19, 20].",
            "2": "PnP is still widely used in many deep learning frameworks for 6-DoF pose estimation today [11, 18, 19] but feature extractors swap classical algorithms, such as SIFT [21] or SURF [22], for more powerful and robust CNNs.",
            "3": "More recently, the trend has been to learn robust keypoints, descriptors, and matching through a large number of sample images [11, 18, 19].",
            "4": "An alternative to learning sparse keypoints is offered by dense methods in Pixel-wise Voting Network (PVNet) [19], which predict unit vectors pointing to keypoints for each pixel.",
            "5": "The latter has been more effective in estimating pose under heavy occlusion [19].",
            "6": "Thus, the P nP algorithm is still widely used in many deep learning frameworks for 6-DoF pose estimation today and such frameworks can be deployed in real-time [18, 19].",
            "7": "2 Heterogeneous RGB-D Architectures One of the main issues with architectures, such as DOPE [18] or PVNet [19], that utilize only 2D RGB images is that they can perform poorly in terms of accuracy, as errors that are small on the image plane can be large in 3D space.",
            "8": "[19] S."
        },
        "Keypoint Matching via Random Network Consensus": {
            "authors": [],
            "url": "https://openreview.net/pdf?id=WhbWzFg8cZ",
            "ref_texts": "11 Under review as a conference paper at ICLR 2023 David G Lowe. Distinctive image features from scale-invariant keypoints. International journal of computer vision , 60(2):91\u2013110, 2004. Zixin Luo, Tianwei Shen, Lei Zhou, Jiahui Zhang, Yao Yao, Shiwei Li, Tian Fang, and Long Quan. Contextdesc: Local descriptor augmentation with cross-modality context. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 2527\u20132536, 2019. Zixin Luo, Lei Zhou, Xuyang Bai, Hongkai Chen, Jiahui Zhang, Yao Yao, Shiwei Li, Tian Fang, and Long Quan. Aslfeat: Learning local features of accurate shape and localization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 6589\u20136598, 2020. Krystian Mikolajczyk and Cordelia Schmid. Scale & affine invariant interest point detectors. International journal of computer vision , 60(1):63\u201386, 2004. Anastasiia Mishchuk, Dmytro Mishkin, Filip Radenovic, and Jiri Matas. Working hard to know your neighbor\u2019s margins: Local descriptor learning loss. Advances in neural information processing systems , 30, 2017. Dmytro Mishkin, Filip Radenovic, and Jiri Matas. Repeatability is not enough: Learning affine regions via discriminability. In Proceedings of the European Conference on Computer Vision (ECCV) , pp. 284\u2013300, 2018. Raul Mur-Artal and Juan D Tard \u00b4os. Orb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras. IEEE transactions on robotics , 33(5):1255\u20131262, 2017. Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan D Tardos. Orb-slam: a versatile and accurate monocular slam system. IEEE transactions on robotics , 31(5):1147\u20131163, 2015. Hyeonwoo Noh, Andre Araujo, Jack Sim, Tobias Weyand, and Bohyung Han. Large-scale image retrieval with attentive deep local features. In Proceedings of the IEEE international conference on computer vision , pp. 3456\u20133465, 2017. Yuki Ono, Eduard Trulls, Pascal Fua, and Kwang Moo Yi. Lf-net: Learning local features from images. Advances in neural information processing systems , 31, 2018. F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V . Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research , 12:2825\u20132830, 2011. Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 4561\u20134570, 2019. Charles R Qi, Or Litany, Kaiming He, and Leonidas J Guibas. Deep hough voting for 3d object detection in point clouds. In proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 9277\u20139286, 2019. Jerome Revaud, Cesar De Souza, Martin Humenberger, and Philippe Weinzaepfel. R2d2: Reliable and repeatable detector and descriptor. Advances in neural information processing systems , 32, 2019. Edward Rosten and Tom Drummond. Machine learning for high-speed corner detection. In European conference on computer vision , pp. 430\u2013443. Springer, 2006. Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary Bradski. Orb: An efficient alternative to sift or surf. In 2011 International conference on computer vision , pp. 2564\u20132571. Ieee, 2011. Paul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, and Marcin Dymczyk. From coarse to fine: Robust hierarchical localization at large scale. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 12716\u201312725, 2019. Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue: Learning feature matching with graph neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 4938\u20134947, 2020."
        },
        "End-to-end learning improves static object geo-localization in monocular video": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2004.05232",
            "ref_texts": "[32] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6DoF pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "32"
            ],
            "1": "3 5D Pose Estimation Many state-of-the-art methods for object pose estimation [30,31,32,33] use 3D models of the objects."
        },
        "DFBVS: Deep Feature-Based Visual Servo": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2201.08046",
            "ref_texts": "[25] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 4561\u20134570, 2019.",
            "ref_ids": [
                "25"
            ],
            "1": "[24], [25] are some of the best performing methods which score highly on the LINEMOD dataset [26].",
            "2": "Here, we use accuracy in the image space as the accuracy metric as inspired from pose estimation field [24], [25].",
            "3": "[25] S."
        },
        "Joint Learning of Object Detection and Pose Estimation using Augmented Autoencoder": {
            "authors": [],
            "url": "https://www.mva-org.jp/Proceedings/2021/papers/P1-10.pdf",
            "ref_texts": "[13] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019.[14] Yinlin Hu, Pascal Fua, Wei Wang, and Mathieu Salzmann. Single-stage 6d object pose estimation. In CVPR , 2020.",
            "ref_ids": [
                "13",
                "14"
            ],
            "1": ", local feature detection and matching, appearance matching, and pose refinement [7, 8, 9, 10, 11, 12, 13, 14, 15]) for pose estimation.",
            "2": "[14] Yinlin Hu, Pascal Fua, Wei Wang, and Mathieu Salzmann."
        },
        "Towards real-time Scan-versus-BIM: methods applications and challenges": {
            "authors": [],
            "url": "https://ec-3.org/publications/conferences/EC32021/papers/EC32021_176.pdf",
            "ref_texts": ""
        },
        "Advances in biplanar X-ray imaging: calibration and 2D/3D registration": {
            "authors": [],
            "url": "https://repository.uantwerpen.be/docstore/d:irua:15079",
            "ref_texts": "[41]S. Peng, Y. Liu, Q. Huang, H. Bao, and X. Zhou, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d in 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , (Long Beach, CA, USA, USA), June 2019.",
            "ref_ids": [
                "41"
            ],
            "1": "PVNet [41] is another deep learning 67 CHAPTER 4.",
            "2": "The bounding landmarks [41] are selected from the object voxels, while the SIFT landmarks are obtained from 3D SIFT keypoints extracted for conventional image matching [92].",
            "3": "Finally, BoneNet, inspired by PVNet [41], is trained to detect 2D landmarks in fluoroscopy images automatically.",
            "4": "1) where the distances between the measured and reference landmarks are penalized by different weights \u03c9ikbased on their hypothesis covariances [41], which will be further discussed in Section 4.",
            "5": "[41] introduced a technique based on Euclidean distance between voxels and the object\u2019s center (C) to define 3D landmarks of an object given its 3D model.",
            "6": "Two types of landmarks are determined, namely bounding (similar to [41]) and SIFT (Scale-Invariant Feature Transform) landmarks [92].",
            "7": "[41] trained a deep neural network (PVNet) to automatically detect 2D landmarks in an optical image scene.",
            "8": "PVNet was designed for accurate inference of only nine landmarks in optical images [41], which resulted in an unstable and slow convergence when applying to X-ray images with a higher number of landmarks.",
            "9": "Landmark 2D coordinates are then converted to 2D vector fields as in [41].",
            "10": "The exact coordinates of each landmark are computed from its voting vector field using the voting scheme described in [41].",
            "11": "1) is penalized with the inverse of the covariance \u03c3as a higher \u03c3represents a less accurate estimation of the corresponding landmark [41].",
            "12": "The learning loss is composed of smooth L1and cross entropy loss \u2113(\u00b7)for vector field and segment training, respectively [41].",
            "13": "Like PVNet [41], the adam optimizer [38] minimizes the smooth L1 loss, which is equivalent to the Huber loss [101], and cross entropy loss (chapter 9, Murphy 2012 [102]) for the vector fields and object segment learning, respectively.",
            "14": "A multistep learning rate scheduler [41] that adjusts the base learning rate of 10\u22125by a multiplication rate of 0.",
            "15": "[41] was applied to compute the exact coordinates of each landmark based on its masked voting vector field.",
            "16": "One of the most relevant models is PVNet [41], which was introduced to detect 2D landmarks in optical images.",
            "17": "The method employed an automated procedure to robustly detect 3D landmarks compared to the CoM-based technique [41].",
            "18": "[41]S."
        },
        "Fiducial Points-supported Object Pose Tracking on RGB Images via Particle Filtering with Heuristic Optimization.": {
            "authors": [],
            "url": "https://www.scitepress.org/Papers/2021/102371/102371.pdf",
            "ref_texts": "(2017). SSD-6D: Making RGB-Based 3D Detection and 6D Pose Estimation Great Again. In IEEE Int. Conf. on Computer Vision , pages 1530\u20131538. Lepetit, V ., Moreno-Noguer, F., and Fua, P. (2009). EPnP: An accurate O(n) solution to the PnP problem. Int. J. Comput. Vision , 81(2):155\u2013166. Lepetit, V ., Pilet, J., and Fua, P. (2004). Point matching as a classification problem for fast and robust object pose estimation. In CVPR , pages 244\u2013250. Lin, T.-Y ., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., and Zitnick, C. L. (2014). Microsoft COCO: Common Objects in Context. In ECCV , pages 740\u2013755. Springer. Lopatin, V . and Kwolek, B. (2020). 6D pose estimation of texture-less objects on RGB images using CNNs. In The 19th Int. Conf. on Artificial Intelligence and Soft Computing , pages 180\u2013192. Springer. Majcher, M. and Kwolek, B. (2020). 3D Model-Based 6D Object Pose Tracking on RGB Images Using Particle Filtering and Heuristic Optimization. In 15th Int. The Int. Conf. on Computer Vision Theory and Applications (VISAPP) , pages 690\u2013697, vol. 5. SciTePress. Marchand, E., Uchiyama, H., and Spindler, F. (2016). Pose estimation for augmented reality: A hands-on survey. IEEE Trans. on Vis. and Comp. Graphics , 22(12):2633\u20132651. Peng, S., Liu, Y ., Huang, Q., Zhou, X., and Bao, H. (2019). PVNet: Pixel-Wise V oting Network for 6DoF Pose Estimation. In IEEE Conf. on Comp. Vision and Patt. Rec., pages 4556\u20134565. Prisacariu, V . A. and Reid, I. D. (2012). PWP3D: RealTime Segmentation and Tracking of 3D Objects. Int. J. Comput. Vision , 98(3):335\u2013354. Rad, M. and Lepetit, V . (2017). BB8: A scalable, accurate, robust to partial occlusion method for predicting the 3D poses of challenging objects without using depth. InIEEE Int. Conf. on Comp. Vision , pages 3848\u20133856. Ronneberger, O., Fischer, P., and Brox, T. (2015). UNet: Convolutional networks for biomedical image segmentation. In MICCAI , pages 234\u2013241. Springer."
        },
        "Simultaneous Object Detection and Pose Estimation under Domain Shift": {
            "authors": [
                "Stefan Thalhammer"
            ],
            "url": "https://scholar.archive.org/work/frqksitta5brpfxfjmkw6rgeqe/access/wayback/https://repositum.tuwien.at/bitstream/20.500.12708/120374/1/Thalhammer%20Stefan%20-%202022%20-%20Simultaneous%20Object%20Detection%20and%20Pose%20Estimation...pdf",
            "ref_texts": "[22] S. Peng,Y. Liu, Q.Hua ng,X. Zhou, and H. Bao,\u201cPVNe t:Pixel-wisevoting networkfor6DoFposeestima tion,\u201d in Proceedings of the IEEEConfer enc eon Compu ter Vision and Patter nRecognition ,2019,pp. 4561\u20134570(cit.onpp .2,6, 14,18,19,31,38,40).",
            "ref_ids": [
                "22"
            ],
            "1": "Imp ortantdimensions ofthisdistribut ionare scene illumi nation,occlusionpatterns,objectconfi gurations and objectapp earance[20], [22],[23].",
            "2": "Though, overcoming the doma in gapin RGBis mo rediffic ult due tothe hig hervariations ofthe ima genoiseand the variations ofthe \n6 1Introduc tion latentdatacharacteristics,nume rousworksthus preferusing RGBand eitheraddr essing the doma in gapbyusingannotateddatawiththe expectedcharacteristics[22],[39],[40], [46]\u2013[49]orbyusingnotannotatedsample softhe expecteddoma in [49]\u2013[52].",
            "3": "As aconsequence,researcherstendtotrain onmix eddata,consisting ofreal-world sample s,withthe expectedcharacteristics,and renderedsamples[22],[40],[46],[47], [53],[54].",
            "4": "Some oftheseand others[22], [49],[53],[58],[59]adopt geometriccorresponde ncesastraining targetstoreduc ethe dime nsionalityofthe output space,and thus toimproveperformance.",
            "5": "Researchforcombining detectionand geometriccorresponde nce estima tionisalsoconducted[22],[49],[58],[59],yetonly [58]provide sresult sindic ating performa ncecomparable tostate-of-the-artappr oacheswithmorestagesand endto-end traina bilit y.",
            "6": "org/\n13\n\n14 2RelatedWork beensuccessfully adoptedbythe objectposeestima tioncommuni ty[22],[40].",
            "7": "Thisapproachiswidelyadoptedbythe comm unit y[22],[47],[53],[59], [106],[107].",
            "8": "An alternativestrategyistouseencoder-decoderarchit ecturesfordense hypothesesgenerationand subs equentposeestim ationusing the PnP algorithm [22], [39],[40],[46],[49],[54],[58],[107].",
            "9": "4 M ulti-m odelPoseEstimation Sinc esing le-mo delappr oachesresult in inf eriorperforma nceascompa redmulti-mo del appr oaches[56],researcherstend totrain separatemodelsforeachobjectofinterestto impr oveperformance[22],[43],[47],[49],[59].",
            "10": "4 6D Localisation 19 whenfirstdetecting objectsin ima ges,secondly predic ting geome triccorresponde nces assurrogatetargetsand lastly estima ting the poseusing the PnPalgorithm [22],[39], [40],[46],[49],[54],[107].",
            "11": "An arguablybettersuitedrepresentation forlearning -basedposeestima tionaregeome triccorresponde nces[22],[39],[40],[49],[53], [54],[58],[59].",
            "12": "The twostanda rdwaystousegeome triccorresponde ncesasregression targetsareeithertopredic tuv -coordina tes[39], [40],[49],[58]orkeypoints[22], [53],[54],[59].",
            "13": "No tably,the authors of[22]showthatproviding alargesetofkeypointhypothesesalsoleads togoodpose estima tionresult s.",
            "14": "The bestperforming deeplearningappr oachesformo no cula r6D objectposeestim ationemplo yencoder-decoderarchit ectures[22],[39],[40],[49],[53].",
            "15": "Therefore, only one mo delistraine dperdatasetin contrasttothe majorityofstate-of-the-art appr oaches[22],[23],[40],[46],[47],[49],[129].",
            "16": "Biblio graph y 91\n[22] S."
        },
        "MULTI LEVEL REFINEMENT ENRICHED FEATURE PYRAMID NETWORK FOR SCALE AND CLASS IMBALANCE IN OBJECT DETECTION": {
            "authors": [],
            "url": "http://eprints.utm.my/id/eprint/101479/1/LubnaAzizPSC2022.pdf.pdf",
            "ref_texts": "185 Pang, Y., Wang, T., Anwer, R. M., Khan, F. S., and Shao, L. (2019a). Efficie nt featurized image pyramid network for single shot detector. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Pang, Y., Wang, T., Anwer, R. M., Khan, F. S., and Shao, L. (2019b). Efficient featurized image pyramid networ k for single shot detector. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Peng, S., Liu, Y., Huang, Q., Zhou, X., and Bao, H. (2019). Pvnet: Pixel -wise voting network for 6dof pose estimation. Proceedings of the IEEE/CVF C onference on Computer Vision and Pattern Recognition, Picron, C., and Tuytelaars, T. (2021). Trident Pyramid Networks: The importance of processing at the feature pyramid level for better object detection. arXiv preprint arXiv:2110.04004 . Pitts, W., and McCulloch, W. S. (1947). How we know universals the perception of auditory and visual forms. The Bulletin of mathematical biophysics , 9(3), "
        },
        "OLF: RGB-D Adaptive Late Fusion for Robust 6D Pose Estimation": {
            "authors": [],
            "url": "https://hal.science/hal-04085729/document",
            "ref_texts": "[25] Peng, S., Liu, Y., Huang, Q., Zhou, X., and Bao, H., \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in [CVPR ], (2019).",
            "ref_ids": [
                "25",
                "CVPR "
            ],
            "1": ", \u201cDensefusion: 6d object pose estimation by iterative dense fusion,\u201d in [CVPR ], (2019).",
            "2": ", \u201cPvn3d: A deep point-wise 3d keypoints voting network for 6dof pose estimation,\u201d in [CVPR ], (2020).",
            "3": ", \u201cLearning descriptors for object recognition and 3d pose estimation,\u201d in [CVPR ], (2015).",
            "4": ", \u201cPointfusion: Deep sensor fusion for 3d bounding box estimation,\u201d in [CVPR ], (2018).",
            "5": ", \u201cViewpoints and keypoints,\u201d in [CVPR ], (2015).",
            "6": ", \u201cData-driven 3d voxel patterns for object category recognition,\u201d in [CVPR ], (2015).",
            "7": ", \u201c3d bounding box estimation using deep learning and geometry,\u201d in [CVPR ], (2017).",
            "8": ", \u201cFrustum pointnets for 3d object detection from rgb-d data,\u201d in [CVPR ], (2018).",
            "9": ", \u201cPointnet: Deep learning on point sets for 3d classification and segmentation,\u201d in [CVPR ], (2017).",
            "10": ", \u201cVoxelnet: End-to-end learning for point cloud based 3d object detection,\u201d in [CVPR ], (2018).",
            "11": ", \u201cindependent object class detection using 3d feature maps,\u201d in [CVPR ], (2008).",
            "12": ", \u201cRecovering 6d object pose and predicting next-best-view in the crowd,\u201d in [CVPR ], (2016).",
            "13": "[25] Peng, S.",
            "14": ", \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in [CVPR ], (2019).",
            "15": ", \u201cMorefusion: Multi-object reasoning for 6d pose estimation from volumetric fusion,\u201d in [CVPR ], (2020).",
            "16": ", \u201cFfb6d: A full flow bidirectional fusion network for 6d pose estimation,\u201d in [CVPR ], (June 2021)."
        },
        "Pose Guided Feature Learning for 3D Object Tracking on RGB Videos.": {
            "authors": [],
            "url": "https://www.scitepress.org/Papers/2022/108868/108868.pdf",
            "ref_texts": "(2017). Nonlinear Bayesian filtering and learning: A neuronal dynamics for perception. Scientific Reports , 7(1).Majcher, M. and Kwolek, B. (2021). Deep quaternion pose proposals for 6D object pose tracking. In Proceedings of the IEEE/CVF Int. Conf. on Computer Vision (ICCV) Workshops , pages 243\u2013251. Manhardt, F., Wang, G., Busam, B., Nickel, M., Meier, S., Minciullo, L., Ji, X., and Navab, N. (2020). CPS++: Improving class-level 6D pose and shape estimation from monocular images with self-supervised learning. arXiv 2003.05848. Newell, A., Yang, K., and Deng, J. (2016). Stacked hourglass networks for human pose estimation. In ECCV , pages 483\u2013499. Springer. Pavlakos, G., Zhou, X., Chan, A., Derpanis, K. G., and Daniilidis, K. (2017). 6-DoF object pose from semantic keypoints. In IEEE Int. Conf. on Robotics and Automation (ICRA) , pages 2011\u20132018. Peng, S., Liu, Y ., Huang, Q., Zhou, X., and Bao, H. (2019). PVNet: Pixel-Wise V oting Network for 6DoF Pose Estimation. In IEEE Conf. on Comp. Vision and Patt. Rec., pages 4556\u20134565. Prisacariu, V . A. and Reid, I. D. (2012). PWP3D: RealTime Segmentation and Tracking of 3D Objects. Int. J. Comput. Vision , 98(3):335\u2013354. Rad, M. and Lepetit, V . (2017). BB8: A scalable, accurate, robust to partial occlusion method for predicting the 3D poses of challenging objects without using depth. InIEEE Int. Conf. on Comp. Vision , pages 3848\u20133856. Tekin, B., Sinha, S. N., and Fua, P. (2018). Real-time seamless single shot 6D object pose prediction. In IEEE/CVF Conf. on Comp. Vision and Pattern Rec."
        },
        "Instance-specific 6-dof object pose estimation from minimal annotations": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2207.13264",
            "ref_texts": "[2] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2019.",
            "ref_ids": [
                "2"
            ],
            "1": "In recent years, advances in deep learning based approaches have used powerful convolutional neural networks (CNNs) to process the input image data and generate a pose prediction [1], [2], [3].",
            "2": "In the case of CNNs designed for detecting pre-defined object keypoints in the input RGB image [4], [5], [3], [2], [6] this implies that the training dataset should be composed of annotated images of the object in many different backgrounds with varying 1Rohan P.",
            "3": "The more recent PVNet method [2] regresses to vector fields for each of the pre-defined object keypoints and uses these vectors to vote for keypoint locations using RANSAC.",
            "4": "[2] S."
        },
        "Pose Tracking vs. Pose Estimation of AR Glasses with Convolutional, Recurrent, and Non-local Neural Networks: A Comparison": {
            "authors": [],
            "url": "https://www-live.dfki.de/fileadmin/user_upload/import/11858_Firintepe2021_EUROXR.pdf",
            "ref_texts": "26. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (June 2019)",
            "ref_ids": [
                "26"
            ],
            "1": "The keypoint-based methods either use the keypoints from the object's surface [28, 34, 35] or directly predict the 2D projections from 3D models of the object using furthest point algorithm [26, 33, 6].",
            "2": "[26] also removed the ROI pooled orientation prediction branch and used 2D keypoints predicted using Hough-based voting to estimate the pose, creating a hybrid between the two major categories."
        },
        "Robust 2D/3D Vehicle Parsing in CVIS": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2103.06432",
            "ref_texts": "[34] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "34"
            ],
            "1": ", AM3D [29], DPOD [58], PV-Net [34], D4LCN [9])."
        },
        "Learned Perception Systems for Self-Driving Vehicles": {
            "authors": [],
            "url": "https://mountainscholar.org/bitstream/handle/10217/235337/Chaabane_colostate_0053A_17155.pdf?sequence=1",
            "ref_texts": "[43] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise V oting Network for 6DoF Pose Estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2019.",
            "ref_ids": [
                "43"
            ]
        },
        "Learning and Leveraging Kinematics for Robot Motion Planning under Uncertainty": {
            "authors": [],
            "url": "https://repositories.lib.utexas.edu/bitstream/handle/2152/87739/JAIN-DISSERTATION-2021.pdf?sequence=1",
            "ref_texts": "[107] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561{4570, 2019.",
            "ref_ids": [
                "107"
            ],
            "1": "4 Rigid Body Pose Estimation Articulation model estimation for objects can be viewed as a subset of the body of work on rigid body pose estimation [15, 16, 21, 73, 104, 107, 120, 126, 136, 146, 148, 150].",
            "2": "Some selected recent work on estimating point estimates for rigid body poses are [21, 73, 104, 107, 126, 146, 148, 150]."
        },
        "DenseTransformer: Direct 6D OPE using self-attention on dense representations": {
            "authors": [],
            "url": "https://fse.studenttheses.ub.rug.nl/28988/1/mAI_2022_DesaiN.pdf",
            "ref_texts": "[33] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 4561\u20134570, 2019.",
            "ref_ids": [
                "33"
            ],
            "1": "[33], [16] further reduces complexity of this method by using a pixel wise or regional voting schemes within estimated keypoints.",
            "2": "e, the RGB information is first used to extract features, which are then refined using depth information ([17], [42], [6], [33]).",
            "3": "[39] proposed an end-to-end framework called the PPR-Net for 3D data where the technique is similar to the point-wise voting used in PVNet [33].",
            "4": "1 Preparing Joint Representation The network attempts to create a jointly learnt representation ([63], [21]) as opposed to a mere concatenation of individually learnt representations ([64], [65], [33]).",
            "5": "[33] S."
        },
        "Recovering 6D pose of rigid object from point cloud at the level of instance and category": {
            "authors": [],
            "url": "https://etheses.bham.ac.uk/id/eprint/12424/7/Chen2022PhD.pdf",
            "ref_texts": "[60] Sida Peng, Yuan Liu, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Pvnet: Pixel-wise voting network for 6dof pose estimation. arXiv preprint arXiv:1812.11788 , 2018.",
            "ref_ids": [
                "60"
            ]
        },
        "Perception Systems for Robust Autonomous Navigation in Natural Environments": {
            "authors": [],
            "url": "https://mountainscholar.org/bitstream/handle/10217/235264/Trabelsi_colostate_0053A_17016.pdf?sequence=1",
            "ref_texts": "[32] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise V oting Network for 6DoF Pose Estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "32"
            ],
            "1": "Two-Stage Methods Based on 2D \u2013 3D Correspondences Despite the many successes, many researchers have argued that deep neural networks have a major limitation [31, 32]: Though deep networks have proven their effectiveness in the task of object detection and classification, their effectiveness in regression tasks is still controversial due to the large output space which can grow to infinity when regressing real-valued outputs.",
            "2": "Thanks to the robustness of CNNs in key-point localization, these methods managed to overcome the difficulty in handling texture-less objects and low-resolution images [32].",
            "3": "[32] introduced PVNet, which stands for \u201cPixel-wise V oting Network\u201d.",
            "4": "42 PVNet [32] 86.",
            "5": "7 End-to-end RGB PVNet [32] + [40] 43.",
            "6": "For instance, the end-to-end extension of PVNet ([32] + [40]) outperforms its two-stage version ([32]) by 6.",
            "7": "First, the 2D-projection error, analogously to [32], measures the average distance between the 2D projections in the image space of the 3D model points, transformed using the ground-truth pose and the predicted pose.",
            "8": "Methods HMap [36] PVNet [32] DeepIM\u2020[45] OURS\u2020\n2D-Proj 39.",
            "9": "2: Detailed results of the proposed approach and other existing RGB-based methods on the different objects of the YCB-Video dataset in terms of ADD AUC Methods HMap [36] PVNet [32] DeepIM\u2020[45] OURS\u2020\n002-master-chef-can 81.",
            "10": "Method Tekin [31] PVNet [32] SSD6D\u2020[27] DeepIM\u2020[45] OURS\u2020 ADD(-S) 55.",
            "11": "5: Detailed Results of the proposed approach and other existing RGB-based methods on the different objects of the LINEMOD dataset in terms of ADD metric Method Tekin [31] PVNet [32] BB8\u2020[34] SSD6D\u2020[27] DeepIM\u2020[45] OURS\u2020 ape 21.",
            "12": "5, the proposed approach is compared with existing state-of-the-art methods: Tekin [31], PVNet [32], BB8 [34], SS6D [27] and DeepIM [45] on LINEMOD dataset [20].",
            "13": "Method HMap [36] PVNet [32] BB8\u2020[34] DeepIM\u2020[45] OURS\u2020 ADD(-S) 30.",
            "14": "8: Evaluation Results of PPN compared to other state-of-the-art RGB-based methods that do not use refinement on three datasets: YCB-Video ,LINEMOD andOcclusion using the 2D-Proj metric Methods PoseCNN [39] HMap [36] PVNet [32] PPN(ours) YCB-Video 3."
        },
        "Alignment of rendered images with photographs for testing appearance models": {
            "authors": [],
            "url": "http://www2.imm.dtu.dk/~jerf/papers/alignment_lowres.pdf",
            "ref_texts": "9. S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d in Proceedings of CVPR",
            "ref_ids": [
                "9"
            ],
            "1": "For pose estimation, a large dataset is usually employed to train a statistical model [8,9].",
            "2": "[9] present an improved method inspired by Tekin and others that indeed seems not to require a posteriori pose refinement."
        },
        "Object 6DoF Pose Estimation for Power Grid Manipulating Robots": {
            "authors": [],
            "url": "http://mvr.whu.edu.cn/pubs/6dof-ICIG-v3-final.pdf",
            "ref_texts": "15. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561{4570 (2019)",
            "ref_ids": [
                "15"
            ],
            "1": "In such conditions, the prediction ability of these method will be signiffcantly degraded [8, 15]."
        },
        "An intelligent robotic vision system with environment perception": {
            "authors": [],
            "url": "https://etheses.whiterose.ac.uk/31259/1/Yixiang_s_PhD_thesis_final.pdf",
            "ref_texts": "[23] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the 131 IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "23"
            ],
            "1": "However, this paradigm suffers from severe shortcomings in terms of low detection accuracy for texture-less objects and expensive post-processing steps [23].",
            "2": "[101] \u00d7 PVNet[23]\u221a Point CloudCorrespondence-based3DMatch[81]\u221a PPFnet[86]\u221a\n3DFeat-Net[82]\u221a Rpm-net[102]\u221a Template-basedFrustum PointNets[103]\u221a PointFusion[104]\u221a DenseFusion[15]\u221a CloudPose[105]\u221a Voting-basedTombariandDiStefano[18] \u00d7 Woodford et al.",
            "3": "[23], Pixel-wise Voting Network (PVNet), which detect vectors between pixel and keypoints rather than directly regressing keypoints.",
            "4": "[9] proposed PVN3D, that is, an extension of PV-Net [23] in the 3D domain shown in the Figure 2.",
            "5": "As we reviewed in Chapter 2, correspondence-based 6D pose estimation [94, 14, 23, 96] commonly use a PnP algorithm or its variant to compute the 6D pose of an object."
        },
        "Object Recognition and Pose Estimation from RGB-D Data Using Active Sensing": {
            "authors": [],
            "url": "http://web-ext.u-aizu.ac.jp/conference/ieeeuoas/files/CFP_GS_Info_2022/Posters/24.pdf"
        },
        "Graph Neural Network based on Geometric and Appearance Attention for 6D Pose Estimation": {
            "authors": [],
            "url": "https://dl.acm.org/doi/pdf/10.1145/3488933.3488959",
            "ref_texts": "[18] Peng, S., Liu, Y., Huang, Q., Zhou, X., & Bao, H. (2019). Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 4561-4570).",
            "ref_ids": [
                "18"
            ],
            "1": "Instead, indirect methods [6,18,20] establish 2D-3D correspondences between 2D keypoints and 3D keypoints, then solve poses by PnP.",
            "2": "Specifically, we use MLP to estimate the vector from the point pto the center point cfrom the feature: v=c\u2212p |c\u2212p|2(5) where vis a unit vector from point to the center, and use RANSAC for voting [18, 23] to get the coordinate of center.",
            "3": "We fellow previous works [18] to only use this dataset for evaluation by a model trained on LineMOD dataset.",
            "4": "125 Graph Neural Network based on Geometric and Appearance Attention for 6D Pose Estimation AIPR 2021, September 24\u201326, 2021, Xiamen, China Table 2: ADD(-S) performance on LineMOD dataset RGB RGB-D PoseCNN DeepIM [10, 23]PVNet [18] CDPN [11] Dense-Fusion [22]Tian et al.",
            "5": "Table 3: ADD(-S) performance on Occlusion LineMOD dataset PVNet [18] Pix2Pose [17] DPOD [26] DPVL [25] Ours + Colored ICP ape 15.",
            "6": "[18] Peng, S."
        },
        "Optimization of visual SLAM by semantic analysis of the environment": {
            "authors": [],
            "url": "https://theses.hal.science/tel-03967982/document",
            "ref_texts": "2018. Peng, J., Shi, X., Wu, J., & Xiong, Z., (2019), An object-oriented semantic SLAM system towards dynamic environments for mobile manipulation, 2019 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM) , 199\u2013204. Peng, S., Liu, Y., Huang, Q., Zhou, X., & Bao, H., (2019), PVNet: Pixel-wise Voting Network for 6DoF pose estimation, IEEE Conf. on Computer Vision and Pattern Recognition , 4561\u20134570. Pham, Q.-H., Hua, B.-S., Nguyen, T., & Yeung, S.-K., (2019), Real-time progressive 3d semantic segmentation for indoor scenes, 2019 IEEE Winter Conference on Applications of Computer Vision (WACV) , 1089\u20131098. Pham, Q.-H., Nguyen, T., Hua, B.-S., Roig, G., & Yeung, S.-K., (2019), Jsis3d: joint semantic-instance segmentation of 3d point clouds with multi-task pointwise networks and multi-value conditional random fields, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 8827\u20138836. Pizer, S. M., Amburn, E. P., Austin, J. D., Cromartie, R., Geselowitz, A., Greer, T., ter HaarRomeny,B.,Zimmerman,J.B.,&Zuiderveld,K.,(1987),Adaptivehistogram equalization and its variations, Computer vision, graphics, and image processing , 39(3), 355\u2013368. Pizer, S. M., Johnston, R. E., Ericksen, J. P., Yankaskas, B. C., & Muller, K. E., (1990), Contrast-limited adaptive histogram equalization: speed and effectiveness, [1990]",
            "ref_ids": [
                "2018"
            ],
            "1": "[Sucar and Hayet, 2018] formulates the problem of scale estimation as a bayesian estimation using object bounding boxes and a priori sizes.",
            "2": "To decrease inference time we retrained a light 2D detection algorithm (namely tiny YOLOv3 [Redmon and Farhadi, 2018] with a darknet backbone) on the driller.",
            "3": ", 2018] have been using LiDAR scans instead of images as an input for SLAM: [Behley and Stachniss, 2018] is a full SLAM system based only on LiDAR data, which represents the map using a set of surfels.",
            "4": ", 2019] improves on [Behley and Stachniss, 2018] by integrating a CNN to segment LiDAR scans [Milioto et al."
        },
        "\u4f4d\u59ff\u89c6\u89c9\u6d4b\u91cf\u65b9\u6cd5\u53ca\u5e94\u7528\u7efc\u8ff0": {
            "authors": [],
            "url": "https://www.researching.cn/ArticlePdf/m00002/2023/60/3/0312010.pdf",
            "ref_texts": ""
        },
        "6D Pose Estimation of Weakly Textured Object Driven by Decoupling Analysis and Algorithm Fusion Strategy": {
            "authors": [],
            "url": "https://www.researchsquare.com/article/rs-3105669/latest.pdf",
            "ref_texts": "32. Peng S, Liu Y , Huang Q, Zhou X, Bao H (2019) Pvnet: Pixel-wise voting netwo rk for 6dof pose estimation. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 4561-4570 ",
            "ref_ids": [
                "32"
            ],
            "1": "developed the PVNet [32] network, which combines 3D shape information and 2D projection information for pose estim ation."
        },
        "6D UAV pose estimation for ship landing guidance": {
            "authors": [],
            "url": "https://welcome.isr.tecnico.ulisboa.pt/wp-content/uploads/2021/12/2021141083.pdf",
            "ref_texts": "[14] G. Billings and M. Johnson-Roberson. SilhoNet: An RGB Method for 6D Object Pose Estimation. IEEE Robotics and Automation Letters, 2019.[15] S. Peng, Y . Liu, Q. Huang, X. Zhou and H. Bao. PVNet: Pixelwise V oting Network for 6DoF Pose Estimation. IEEE conference on computer vision and pattern recognition, 2019.",
            "ref_ids": [
                "14",
                "15"
            ],
            "1": "Whereas some methods directly regress the pose hypotheses [12], others form a pipeline of different sub-networks to regress pose parameters [13], [14].",
            "2": "Alternatively, some Convolution Neural Networks (CNNs) architectures produce features, as keypoints, subsequently used to compute the pose estimation [15].",
            "3": "[14] G.",
            "4": "[15] S."
        },
        "Model-free Bin-Picking: Food Processing and Parcel Processing Use Cases": {
            "authors": [],
            "url": "https://i-rim.it/wp-content/uploads/2020/12/I-RIM_2020_paper_153.pdf",
            "ref_texts": "[5] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR , 2019.",
            "ref_ids": [
                "5"
            ],
            "1": "This problem, often referred to as random bin picking , rely on robust 3D pose estimation algorithms that exploit either 2D or 3D vision technologies [1], [6], with an increasingly trend towards datadriven approcehs based on deep models [3], [5].",
            "2": "[5] S."
        },
        "MixedFusion: 6D Object Pose Estimation from Decoupled RGB-Depth Features": {
            "authors": [],
            "url": "https://ailb-web.ing.unimore.it/icpr/media/posters/10918.pdf"
        },
        "An Exploration of the Virtual Digital Twin Capture for Spatial Tasks and its Applications": {
            "authors": [],
            "url": "https://hammer.purdue.edu/ndownloader/files/34929147",
            "ref_texts": "[4]S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
            "ref_ids": [
                "4"
            ],
            "1": "[4]S."
        },
        "Direct pose estimation from RGB images using 3D objects": {
            "authors": [],
            "url": "https://dergipark.org.tr/en/download/article-file/2402755",
            "ref_texts": ""
        },
        "\u57fa\u4e8e\u865a\u62df\u76f8\u673a\u7684\u4f4d\u59ff\u4f30\u8ba1\u7814\u7a76\u8fdb\u5c55": {
            "authors": [],
            "url": "https://www.researching.cn/ArticlePdf/m00002/2022/59/14/1415003.pdf"
        },
        "Deep Learning for Object Detection: Training Data Generation using Parametric CAD Modelling and Gazebo Simulation": {
            "authors": [
                "Akber Khan"
            ],
            "url": "https://trepo.tuni.fi/bitstream/handle/10024/135807/KhanAkberAli.pdf?sequence=4",
            "ref_texts": "[29] S. Peng, Y. Liu, Q. Huang, X. Zhou and H. Bao, \"PVNET: Pixel -wise voting network for 6dof pose estimation,\" in Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019 06-01, vol. 2019, pp. 4556 \u20134565 , 2019. ",
            "ref_ids": [
                "29"
            ],
            "1": "PVNet [29] is an example of an indirect voting -based technique and outperforms some of the earlier method s.",
            "2": "[29] S."
        },
        "A Survey on Deep Learning Based Methods and Datasets for Monocular 3D Object Detection. Electronics 2021, 10, 517": {
            "authors": [],
            "url": "https://pdfs.semanticscholar.org/076b/052fe9aa43e1f8619cc9e8aab29966a32f6d.pdf",
            "ref_texts": "72. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. PVNet: Pixel-wise Voting Network for 6DOF Pose Estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 16\u201320 June 2019; pp. 4561\u20134570.",
            "ref_ids": [
                "72"
            ],
            "1": "For example, a pixel-wise voting network (PVNet) [72] predicts pixel-level indicators corresponding to the key points so that they can handle truncation or occlusion of object parts.",
            "2": "PVNet [72] also uses a denser key point prediction method, as shown in Figure 11.",
            "3": "Overview of the keypoint localization in PVNet [72].",
            "4": "The most recent trend in monocular 3D object detection is learning deep neural networks to directly regress the 6D pose from a single image [25\u201327,68,75] or to estimate the 2D positions of 3D key points and solve the PnP algorithm [28\u201330,72,76,78,79]."
        },
        "Impact of Segmentation and Color Spaces in 6D Pose Estimation": {
            "authors": [],
            "url": "http://www.di.ubi.pt/~lfbaa/pubs/icarsc2021.pdf",
            "ref_texts": "[4] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp.",
            "ref_ids": [
                "4"
            ],
            "1": "These methods try to solve the 6D pose estimation problem [3], [4], [5].",
            "2": "As in previous methods [2], [3], [4], [5] that tackle the 6D pose estimation using the LineMOD dataset, we use the Average Distance of Model Points (ADD) [6] as an evaluation metric for non-symmetric objects, and for the egg-box and glue (symmetric objects) we use the Average Closest Point Distance (ADD-S) [5].",
            "3": "As in previous works in 6D pose estimation [1], [2], [3], [4], [5] we use the same evaluation metrics for the LineMOD dataset.",
            "4": "[4] S."
        },
        "Graph-Theoretic Outlier Rejection: From Instance to Category-Level Perception": {
            "authors": [],
            "url": "https://dspace.mit.edu/bitstream/handle/1721.1/139117/shi-jnshi-sm-AeroAstro-2021-thesis.pdf?sequence=1&isAllowed=y",
            "ref_texts": "[83] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "83"
            ],
            "1": "Such approaches first recover the position of semantic keypoints [82] in the images with neural networks, and then recover the 3D pose of the object by solving a geometric optimization problem [78, 82, 83, 96, 52]."
        },
        "Efficient learning methods for robot grasping oriented pose estimation": {
            "authors": [],
            "url": "https://tsukuba.repo.nii.ac.jp/record/2002128/files/DA010090_abstract.pdf",
            "ref_texts": ""
        },
        "Real-time embedded reconstruction of dynamic objects for a 3D maritime situational awareness picture": {
            "authors": [],
            "url": "https://elib.dlr.de/193059/1/MARESEC_2022_11_final.pdf",
            "ref_texts": "[20] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , June 2019.",
            "ref_ids": [
                "20"
            ],
            "1": "The tasks of object detection, instance segmentation and pose estimation can be trained using a single back-end, as shown by [20].",
            "2": "[20] S."
        },
        "Network and system for pose and size estimation": {
            "authors": [],
            "url": "https://patentimages.storage.googleapis.com/00/a6/37/ab887606a738b2/US20220292698A1.pdf"
        },
        "Automatic Generation of Machine Learning Synthetic Data Using ROS": {
            "authors": [
                "Markus Richter"
            ],
            "url": "https://arxiv.org/pdf/2106.04547",
            "ref_texts": ""
        },
        "Supplementary Material for EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation": {
            "authors": [],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/supplemental/Chen_EPro-PnP_Generalized_End-to-End_CVPR_2022_supplemental.pdf",
            "ref_texts": "[13] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 5",
            "ref_ids": [
                "13"
            ],
            "1": "9 ms overhead to the base pose estimator PVNet [13] at the same batch size, measured on RTX 2080 Super GPU, which is slower than ours."
        },
        "A Comprehensive Review on 3D Object Detection and 6D Pose Estimation With Deep Learning": {
            "authors": [
                "Sabera Hoque"
            ],
            "url": "https://figshare.utas.edu.au/ndownloader/files/40754126",
            "ref_texts": "[191] S. Peng, Y. Liu, Q. Huang, H. Bao, and X. Zhou, ``PVNet: Pixel-wise voting network for 6DoF pose estimation,'' Tech. Rep., 2018.",
            "ref_ids": [
                "191"
            ],
            "1": "Also, [191] recently removed the ROI pooled orientation layer and introduced PVNet (Pixelwise Voting Network) to deny pixel-based vectors and use them for key-point positions.",
            "2": "[191] S."
        },
        "An Open-source Recipe for Building Simulated Robot Manipulation Benchmarks": {
            "authors": [],
            "url": "https://jiayuan-gu.github.io/pdf/icra23-compare.pdf",
            "ref_texts": "[25] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
            "ref_ids": [
                "25"
            ],
            "1": "In the real-world setup, we first estimate the 6-DoF object poses in the camera space by PVNet [25] and then transform the object poses to the robot base using the relative pose between the camera and the base of the robot arm obtained by hand-eye-calibration.",
            "2": "[25] S."
        },
        "\u57fa\u4e8e\u4e09\u7ef4\u68c0\u6d4b\u7f51\u7edc\u7684\u673a\u5668\u4eba\u6293\u53d6\u65b9\u6cd5": {
            "authors": [],
            "url": "http://femt.cnjournals.com/yqyb/article/pdf/20210817",
            "ref_texts": ""
        },
        "Review on 6D Object Pose Estimation With the Focus on Indoor Scene Understanding. 2022\u037e 2 (4): 41": {
            "authors": [],
            "url": "https://www.oajaiml.com/uploads/archivepdf/24821141.pdf",
            "ref_texts": "[41] Peng S, Liu Y , Huang Q, Zhou X, Bao H. Pvnet: Pixel-Wise V oting Network for 6DOF Pose Estimation. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019:4561-4570.",
            "ref_ids": [
                "41"
            ],
            "1": "PROBLEM FORMULATION Pose estimation problem can be formulated as direct classification [14], regression [40], 2D-3D correspondences [41], or 3D-3D correspondences [42].",
            "2": "To address the occlusion problem for keypoint detection, PVNet [41] predicts unit vectors pointing to keypoints for each pixel in the mask of the object and localize 2D keypoints in a RANSAC voting scheme.",
            "3": "3DPVNet [72], inspired from V oteNet [73] and pvnet [41], employed deep learning 600 https://www.",
            "4": "The promise of voting-based techniques [41,72] are being robust to these challenges.",
            "5": "[41] Peng S, Liu Y , Huang Q, Zhou X, Bao H."
        },
        "Robotic assembly, using RGBD-based object pose estimation & grasp detection.": {
            "authors": [
                "Jaana Kunnari"
            ],
            "url": "https://trepo.tuni.fi/bitstream/handle/10024/123320/AhmadSaad.pdf?sequence=2",
            "ref_texts": ""
        },
        "Planung und Simulation taktiler, intelligenter und kollaborativer Roboterf\u00e4higkeiten in der Montage": {
            "authors": [
                "Metzner Maximilian"
            ],
            "url": "https://opus4.kobv.de/opus4-fau/files/21521/Metzner_Diss_MB_414.pdf",
            "ref_texts": ""
        },
        "D\u00e9tection 3D pour la r\u00e9alit\u00e9 mixte en maintenance industrielle": {
            "authors": [],
            "url": "https://www.theses.fr/2022BORD0419.pdf",
            "ref_texts": ""
        },
        "Augmented Reality Pilot Assistance System for Helicopter Shipboard Operations": {
            "authors": [],
            "url": "https://mediatum.ub.tum.de/doc/1655458/document.pdf",
            "ref_texts": "[114] Peng, S., Liu, Y., Huang, Q., Bao, H., Zhou, X. (2018). PVNet: Pixel -wise Voting Network for 6DoF Pose Estimation. Retrieved from https://arxiv.org/pdf/1812.11788 ",
            "ref_ids": [
                "114"
            ],
            "1": "Therefore, well known approaches for real -time 6D pose estimation are analyzed within this work: Pose CNN [157] , PV Net [114] , Dense Fusion \n[148] and Single Shot 6D Pose [136] .",
            "2": "051406 \n[114] Peng, S."
        },
        "Learning Innovations for State Estimation": {
            "authors": [],
            "url": "http://www.gerard-kennedy.com/files/iros-2021.pdf",
            "ref_texts": "[31] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019.",
            "ref_ids": [
                "31"
            ],
            "1": "A recent popular approach is to first regress to an intermediate representation such as keypoints, from which pose can be obtained via 2D-3D correspondences and a PnP algorithm [36], [32], [31], [40], [41], [28], [17].",
            "2": "Of these approaches, some, including [36], [32] regress to a set of bounding box corners, while others regress to vector fields [31], [34], [40], or dense correspondences.",
            "3": "State Estimation for Object Pose and Depth Refinement For object pose estimation we choose PVNet [31] to be the baseline network that provides bX(0).",
            "4": "The high dimensional regressor \u2018state\u2019 is the collection of unit vector fields Xk ij=\u0011k ij k\u0011k ijk22R2\u0002K\u0002 M\u0002N: (9) The function hmaps this vector field representation of keypoints to object pose via a RANSAC and uncertaintydriven PnP framework (EPnP), as discussed in [31].",
            "5": "Forobject pose estimation the baseline network is PVNet [31].",
            "6": "Forobject pose estimation we use a pretrained PVNet [31] to provide the initial estimate for each object.",
            "7": "We compute 8 keypoints via farthest point sampling, from which object pose is obtained via uncertainty-driven PnP [31]."
        },
        "Car Pose in Context: Accurate Pose Estimation with Ground Plane Constraints": {
            "authors": [],
            "url": "https://arxiv.org/pdf/1912.04363",
            "ref_texts": "[36] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 2",
            "ref_ids": [
                "36"
            ],
            "1": "More recent methods, including SOTA method [36] in 6-DoF pose estimation, adopts a two-stage method."
        },
        "Detecting and tracking outdoor gym geometry for AR display of exercise suggestions": {
            "authors": [],
            "url": "https://upcommons.upc.edu/bitstream/handle/2117/360080/M_Thesis_Aina_Maki.pdf?sequence=2"
        },
        "\u57fa\u4e8e\u5149\u573a EPI \u56fe\u50cf\u6808\u7684 6D \u4f4d\u59ff\u4f30\u8ba1\u65b9\u6cd5": {
            "authors": [],
            "url": "http://jemi.cnjournals.com/jemi/article/pdf/20230414"
        },
        "Vote from the Center: 6 DoF Pose Estimation in RGB-D Images by Radial Keypoint Voting (Supplementary Material)": {
            "authors": [],
            "url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700331-supp.pdf",
            "ref_texts": "[S.17] Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570",
            "ref_ids": [
                "S\\.17"
            ]
        },
        "Monocular Markerless 6D Pose Estimation of ANYmal": {
            "authors": [],
            "url": "https://tenhearts.github.io/assets/pdf/plr_report.pdf",
            "ref_texts": "[14] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "14"
            ],
            "1": "Most recent state-of-the-art works with RGB images focus on first detecting 2D targets of the object in the given image and subsequently solving a Perspective-n-Point(PnP) problem with predicting 2D-3D correspondences for 6D poses[11, 12, 13, 14, 15, 16].",
            "2": "[14] S."
        },
        "Direct pose estimation from RGB images using 3D objects 3 Boyutlu nesneleri kullanarak imgelerden poz kestirimi": {
            "authors": [],
            "url": "https://jag.journalagent.com/z4/download_fulltext.asp?pdir=pajes&plng=tur&un=PAJES-08566",
            "ref_texts": ""
        },
        "R-SAC: Reinforcement Sample Consensus": {
            "authors": [],
            "url": "https://openreview.net/pdf?id=Urn3WzRwhXO",
            "ref_texts": "[13] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "13"
            ],
            "1": "[13] S."
        },
        "6D Pose Estimation for Texture-less Objects": {
            "authors": [],
            "url": "https://www.sublimeforest.com/static/pdf/6dpose.pdf",
            "ref_texts": "[17] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019. 1, 3, 4",
            "ref_ids": [
                "17"
            ],
            "1": "[17] In the second stage, the refinement technique predicts a relative SE(3) transformation that matches a synthetic mesh rendered view of the object against the observed image.",
            "2": "[17]\n3.",
            "3": "PVNet: Inspired by [17], \u201dassuming there are C classes of objects and K keypoints for each class, PVNet takes as input the H*W*3 image, processes it with a fully convolutional architecture, and outputs the H*W*(K*2*C)tensor representing unit vectors and H*W*(C+1)tensor representing class probabilities\u201d.",
            "4": "5\n[17] S."
        },
        "Supplementary Material-FFB6D: A Full Flow Bidirectional Fusion Network for 6D Pose Estimation": {
            "authors": [],
            "url": "https://openaccess.thecvf.com/content/CVPR2021/supplemental/He_FFB6D_A_Full_CVPR_2021_supplemental.pdf",
            "ref_texts": "[15] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019. 3",
            "ref_ids": [
                "15"
            ],
            "1": "RGB RGB-D PoseCNN DeepIM\n[18, 10]PVNet[15] CDPN[11] DPOD[20] PointFusion[19]DenseFusion[17]G2LNet[1]PVN3D[5] Our FFB6D ape 77.",
            "2": "[9]Pix2Pose [14]PVNet [15]DPOD\n[20]Hu et al."
        },
        "Self-supervised Geometric Perception": {
            "authors": [],
            "url": "https://openaccess.thecvf.com/content/CVPR2021/supplemental/Yang_Self-Supervised_Geometric_Perception_CVPR_2021_supplemental.pdf",
            "ref_texts": "[12] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , pages 4561\u20134570, 2019. 3",
            "ref_ids": [
                "12"
            ],
            "1": "Recent works such as YOLO6D [14], PVNet [12], and DPOD [16] can all serve as the student network, despite using different methodologies.",
            "2": ", by rendering synthetic projections of the 3D models under different simulated poses, which is common in [16, 12, 14, 2].",
            "3": "2There are many different ways to establish 2D-3D correspondences, see PVNet [12], YOLO6D [14] and references therein."
        },
        "RNNPose: Recurrent 6-DoF Object Pose Refinement with Robust Correspondence Field Estimation and Pose Optimization": {
            "authors": [],
            "url": "https://decayale.github.io/publication/rnnpose/paper.pdf",
            "ref_texts": "[34] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "34"
            ],
            "1": "These methods may estimate the object\u2019s bounding box corners [35,45], predict dense 2D-3D correspondence maps [33] or vote the keypoints by all object pixels [34].",
            "2": "At the beginning of the first rendering cycle, a reference image Iref is rendered with the object\u2019s CAD model according to its initial pose Pinit(estimated by any direct methods [34,53]).",
            "3": "Here, the initial poses for pose refinement are originally from PVNet [34] but added with significant disturbances for robustness testing.",
            "4": "We follow similar conventions in data processing and synthetic data generation as the previous works [20,34].",
            "5": "For the initial poses, we mainly rely on PoseCNN [52] and PVNet [34], two typical direct estimation methods, following [23] and [20].",
            "6": "1 cm following [34].",
            "7": "Robustness comparison with RePOSE by degrading the initial poses (from PVNet [34]) with Gaussian noise on LINEMOD dataset.",
            "8": "The comparison of estimation accuracy with competitive direct methods (PoseCNN [52], PVNet [34] and HybridPose [38]) and refinement methods (DPOD [59], DeepIM [23] and RePOSE\n[20]) on LINEMOD dataset in terms of the ADD(-S) metric.",
            "9": "Object PoseCNN [52] PVNet [34] HybridPose [38] GDR-Net [51] DPOD [59] RePOSE [20] Ours Ape 9.",
            "10": "For the LINEMOD dataset, we compare with the recent pose refinement methods RePOSE [20], DPOD [59] and DeepIM [23] as well as some direct estimation baselines [34, 38, 52].",
            "11": "4 the PVNet [34], although the pose accuracy of PVNet is much better as exhibited in Table 3."
        },
        "Supplementary Material for OnePose: One-Shot Object Pose Estimation without CAD Models": {
            "authors": [],
            "url": "http://www.cad.zju.edu.cn/home/gfzhang/papers/OnePose/onepose_supp.pdf",
            "ref_texts": "[8] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 3",
            "ref_ids": [
                "8"
            ],
            "1": "Implementation Details of the Evaluation of PVNet For the experiments of evaluating PVNet [8], we directly use the original implementation and training configurations provided by the authors at [1]."
        },
        "Supplementary Material: Gen6D: Generalizable Model-Free 6-DoF Object Pose Estimation from RGB Images": {
            "authors": [],
            "url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136920297-supp.pdf",
            "ref_texts": "5. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6-dof pose estimation. In: CVPR (2019)",
            "ref_ids": [
                "5"
            ],
            "1": "On the LINEMOD [2] dataset, we use the training set of previous instance-specific estimators [8,5] as reference images and the other images are selected as query images.",
            "2": "Note that reference images are used in inference of Gen6D but not in training the Gen6D estimator while instance-specific estimators like PVNet [5] actually use these reference images to train their models.",
            "3": "Note PVNet [5] is trained on the specific test object with both synthetic and real images while our Gen6D is not trained on the test object.",
            "4": "1d PVNet [5] Ours PVNet [5] Ours Eggbox 99."
        },
        "What Supervision Scales? Practical Learning Through Interaction": {
            "authors": [],
            "url": "https://digitalassets.lib.berkeley.edu/techreports/ucb/incoming/EECS-2020-128.pdf"
        },
        "VS-Net: Voting with Segmentation for Visual Localization-Supplementary Material": {
            "authors": [],
            "url": "http://openaccess.thecvf.com/content/CVPR2021/supplemental/Huang_VS-Net_Voting_With_CVPR_2021_supplemental.pdf",
            "ref_texts": "[6] Jeremie Papon, Alexey Abramov, Markus Schoeler, and Florentin W \u00a8org\u00a8otter. V oxel cloud connectivity segmentation supervoxels for point clouds. In Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on , Portland, Oregon, June 22-27 2013. 1[7] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019. 1",
            "ref_ids": [
                "6",
                "7"
            ],
            "1": "SuperV oxel [6] is a 3D oversegmentation algorithm that uniformly sets vast seeds in 3D space and raises a patch for each pre-defined seeds.",
            "2": "COLMAP [9] and the 3D surface we use reconstructed by Algorithm 1 Landmarks from V otes INPUT: pi: Pixel coordinates in current patch label di: Pixel votes at pi ^l(0) j: Initial landmark of landmark jfrom RANSAC-based approach [7]\n\u0012min: Threshold of minimum neighbor pixels \u0012dist: Threshold of neighbor pixel distance \u000fstep: Epsilon of landmark coordinate change OUTPUT: ^lbest j: Best voting landmark bvalid: Validity of the landmark ^lbest j ^l(0) j bvalid true t 1 whilet<Max Iteration do S=fkpi\u0000^l(t\u00001) jk2<\u0012distg ifjSj<\u0012minthen bvalid false break end if for all pi2Sdo Compute normal of voting map ni \u00140\u00001\n1 0\u0015 di end for ^l(t) j \u0000P Snin> i\u0001\u00001\u0000P Snin> ipi\u0001 ifjj^l(t) j\u0000^l(t\u00001) jjj2<\u000fstepthen break end if ^lbest j ^l(t) j t t+ 1 end while 7Scenes Cambridge Landmarks Ches.",
            "3": "2\n[6] Jeremie Papon, Alexey Abramov, Markus Schoeler, and Florentin W \u00a8org\u00a8otter."
        },
        "Neural Object Learning for 6D Pose Estimation Using a Few Cluttered Images": {
            "authors": [],
            "url": "https://openreview.net/pdf?id=Z6zwbn0i3Z1",
            "ref_texts": "28. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4556{4565 (June 2019)",
            "ref_ids": [
                "28"
            ],
            "1": "Recently, state-of-the-art performance has been accomplished by using both synthetic and real images [23, 24, 28].",
            "2": "To overcome this limitation, both real images and synthetic images are used for training [23, 24, 28, 45], which currently achieves state-of-the-art performance."
        },
        "Computer Vision-assisted Battery-free RFID Systems for Object Recognition, Localization and Orientation": {
            "authors": [
                "Zhongqin Wang"
            ],
            "url": "https://opus.lib.uts.edu.au/bitstream/10453/147331/2/02whole.pdf",
            "ref_texts": "[73] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE CVPR, pages 4561\u20134570. IEEE, 2019.",
            "ref_ids": [
                "73"
            ],
            "1": "Most computer vision-based solutions [52, 85, 73] require large-scale labeled image data."
        },
        "Deep object 6-DoF pose estimation using instance segmentation": {
            "authors": [],
            "url": "https://alife-robotics.co.jp/members2020/icarob/data/html/data/OS/OS24/OS24-1.pdf",
            "ref_texts": "7. Peng, Sida & Liu, Yuan & Huang, Qixing & Bao, Hujun & Zhou, PVNet: Pixel-wise Voti ng Network for 6DoF Pose Estimation, Xiaowei, 2018. ",
            "ref_ids": [
                "7"
            ]
        },
        "Evaluating Computer Vision Methods for Detection and Pose Estimation of Textureless Objects": {
            "authors": [
                "Name Last"
            ],
            "url": "https://uis.brage.unit.no/uis-xmlui/bitstream/handle/11250/2620242/Skutvik_Harald_Thirud.pdf?sequence=1",
            "ref_texts": "[10]Sida Peng, Yuan Liu, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Pvnet: Pixelwise voting network for 6dof pose estimation. CoRR, abs/1812.11788, 2018. URL http://arxiv.org/abs/1812.11788 .",
            "ref_ids": [
                "10"
            ],
            "1": "2 Pose estimation techniques DeepIM [8], Keypoint detector localization [9] and PVNet [10] are examples of the current cutting edge pose estimation methods."
        },
        "Model-based 3D Tracking for Augmented Orthopedic Surgery": {
            "authors": [],
            "url": "https://hal.science/hal-03022939/document",
            "ref_texts": "[9] S Peng, Y Liu, Q Huang, X Zhou, H Bao. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . (2018) . Pvnet: Pixel -wise voting network for 6dof pose estimation . ",
            "ref_ids": [
                "9"
            ],
            "1": "Machine learning based pose inference [9] could also be investigated to initialize registration.",
            "2": "[9] S Peng, Y Liu, Q Huang, X Zhou, H Bao."
        },
        "Deep Snake for Real-Time Instance Segmentation": {
            "authors": [],
            "url": "https://ask.qcloudimg.com/draft/6837186/79j1ad13cu.pdf",
            "ref_texts": "[30] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 3",
            "ref_ids": [
                "30"
            ],
            "1": "An alternative method is to use standard CNNs to regress a pixel-wise vector field from the input image to guide the evolution of the initial contour [34, 30, 37]."
        },
        "Odhadov\u00e1n\u00ed rotace a translace netexturovan\u00e9ho objektu z jedn\u00e9 kamery": {
            "authors": [],
            "url": "https://dspace.cvut.cz/bitstream/handle/10467/96699/F3-BP-2021-Lukes-Michal-Michal%20Lukes%20thesis%20-%20final2.pdf?sequence=-1"
        },
        "Supplement to \u201cPose Proposal Critic: Robust Pose Refinement by Learning Reprojection Errors\u201d": {
            "authors": [],
            "url": "https://research.chalmers.se/publication/519547/file/519547_AdditionalFile_dad0f2b5.pdf",
            "ref_texts": "[9]Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixelwise voting network for 6DoF pose estimation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 2019. BRYNTE, KAHL: SUPPLEMENT TO \u201cPOSE PROPOSAL CRITIC\u201d 9",
            "ref_ids": [
                "9"
            ],
            "1": "Despite the sub-optimal pose proposals from PVNet [9], the poses are accurately recovered.",
            "2": "[8]PVNet [9]PoseCNN [11]\n+ DeepIM [6]PVNet [9]\n+ PPC (Ours) ape 17.",
            "3": "[8]PVNet [9]PoseCNN [11]\n+ DeepIM [6]PVNet [9]\n+ PPC (Ours) ape 69.",
            "4": "BRYNTE, KAHL: SUPPLEMENT TO \u201cPOSE PROPOSAL CRITIC\u201d 5 PVNet [9]PoseCNN [11]\n+ DeepIM [6]PVNet [9]\n+ PPC (Ours) ape 37.",
            "5": "1 Negative Depth Correction of Pose Proposals We observed that the pose proposals from PVNet [9] sometimes have negative depth, and in this case we switched sign for the object center position (in the camera frame), and rotated the object 180 degrees around the principal axis of the camera, in order to yield a feasible estimate with similar projection (the projection is identical for points on the plane which goes through the object center and is parallel to the principal plane of the camera).",
            "6": "This correction is done both when reporting the results of [9], and when reporting the results of our refinement."
        },
        "System and method for image inpainting": {
            "authors": [],
            "url": "https://patentimages.storage.googleapis.com/cc/b1/59/882ff0d691b4cd/US20220292651A1.pdf",
            "ref_texts": ""
        },
        "Systems and methods for pose detection and measurement": {
            "authors": [],
            "url": "https://patentimages.storage.googleapis.com/f6/43/5d/8fc8d44f24b8b1/US11295475.pdf"
        },
        "A Multi-view Pixel-wise Voting Network for 6DoF Pose Estimation": {
            "authors": [],
            "url": "https://thesis.unipd.it/bitstream/20.500.12608/31496/1/tesi_3d_pose_estimation_pdfA.pdf"
        },
        "\u7269\u54c1\u63b4\u307f\u4e0a\u3052\u4f5c\u696d\u3067\u6c42\u3081\u3089\u308c\u308b\u8996\u899a\u6a5f\u80fd": {
            "authors": [],
            "url": "https://www.jstage.jst.go.jp/article/jrsj/38/6/38_38_538/_pdf"
        },
        "Learning to Estimate 3D Object Pose from Synthetic Data": {
            "authors": [
                "Sergey Zakharov"
            ],
            "url": "https://mediatum.ub.tum.de/doc/1550255/document.pdf",
            "ref_texts": "[93]S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2019.",
            "ref_ids": [
                "93"
            ],
            "1": ", BB8 [38], YOLO6D [40], PVNet [93], use the training split of the real dataset.",
            "2": "Analogously to other related papers [40, 50, 93, 101], we measure the accuracy of pose estimation using the ADD score [58].",
            "3": "Train data Synthetic +R e fi n e m e n t Real +R e fi n e m e n t Object SSD6D [50]AAE [92]Ours SSD6D [56]Ours YOLO6D [40]PoseCNN [101] PVNet [93]Ours DeepIM [9]Ours Ape 2.",
            "4": "Method YOLO6D [40] PoseCNN [101] SSD6D+Ref [56] HMap [102] PVNet [93] Ours Ours+Ref Mean 6.",
            "5": "If trained on real data, our method is the second best after [93].",
            "6": "Method Frames per second Refinement AAE [92] 4 200 ms/object SSD6D [50] 10 24 ms/object PVNet [93] 25 Ours 33 5 ms/object YOLO6D [40] 50 68\n4.",
            "7": "We demonstrated that for both, real and synthetic training data, our detector outperforms other related works, such as [40, 101], by a large margin and performs similarly to [93].",
            "8": "[92]\n[93]S."
        },
        "\ub85c\ubd07 \ud314\uc744 \ud65c\uc6a9\ud55c \uc815\ub9ac\uc791\uc5c5\uc744 \uc704\ud55c \ubb3c\uccb4 \uc790\uc138\ucd94\uc815 \ubc0f \uc774\ubbf8\uc9c0 \ub9e4\uce6d": {
            "authors": [
                "Media Contents"
            ],
            "url": "https://jkros.org/xml/31147/31147.pdf",
            "ref_texts": "[3] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , Long Beach, CA, USA, 2019, DOI: 10.1109/CVPR.2019.00469.",
            "ref_ids": [
                "3"
            ],
            "1": "\uae30\uc874\uc758 \uc790\uc138\ucd94\uc815 \uc54c\uace0\ub9ac\uc998 \uc911\uc5d0\uc11c \uc2ec\uce35\ud559\uc2b5\uc5d0 \uae30\ubc18\ud55c \uc5f0\uad6c\n\ub4e4[2,3]\uc774 \uc18d\ub3c4\uc640 \uc815\ud655\ub3c4\uba74\uc5d0\uc11c \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc774\uace0 \uc788\ub2e4 .",
            "2": "[3] S."
        },
        "Visual slam in dynamic environments": {
            "authors": [],
            "url": "https://zaguan.unizar.es/record/100672/files/TESIS-2021-083.pdf",
            "ref_texts": "104 Mur-Artal, R. & Tard\u0013 os, J. D. (2017), `ORB-SLAM2: An open-source slam system for monocular, stereo, and RGB-D cameras', IEEE T-RO . Newcombe, R. A., Lovegrove, S. J. & Davison, A. J. (2011), DTAM: Dense tracking and mapping in real-time, in `ICCV', IEEE. Oquab, M., Bottou, L., Laptev, I. & Sivic, J. (2014), Learning and transferring midlevel image representations using convolutional neural networks, in `Proceedings of the IEEE conference on computer vision and pattern recognition', pp. 1717{1724. Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T. & Efros, A. A. (2016), Context encoders: Feature learning by inpainting, in `Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition', pp. 2536{2544. Paz, L. M., Pini\u0013 es, P., Tard\u0013 os, J. D. & Neira, J. (2008), `Large-scale 6-dof slam with stereo-in-hand', IEEE transactions onrobotics 24(5), 946{957. Peng, S., Liu, Y., Huang, Q., Zhou, X. & Bao, H. (2019), Pvnet: Pixel-wise voting network for 6dof pose estimation, in `Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition', pp. 4561{4570. Peris, M., Martull, S., Maki, A., Ohkawa, Y. & Fukui, K. (2012), Towards a simulation driven stereo vision system, in `Pattern Recognition (ICPR), 2012 21st International Conference on', IEEE, pp. 1038{1042. Pinheiro, P. O., Lin, T.-Y., Collobert, R. & Doll\u0013 ar, P. (2016), Learning to reffne object segments, in `European conference on computer vision', Springer, pp. 75{91. Porav, H., Maddern, W. & Newman, P. (2018), `Adversarial training for adverse conditions: Robust metric localisation using appearance transfer', IEEE International Conference onRobotics andAutomation . Ren, S., He, K., Girshick, R. & Sun, J. (2015), Faster r-cnn: Towards real-time object detection with region proposal networks, in `Advances in neural information processing systems', pp. 91{99. Ren, X. & Malik, J. (2003), Learning a classiffcation model for segmentation, in `nternational Conference on Computer Vision', IEEE, p. 10. Riazuelo, L., Montano, L. & Montiel, J. M. M. (2017), `Semantic visual SLAM in populated environments', ECMR . Rogers, J. G., Trevor, A. J., Nieto-Granda, C. & Christensen, H. I. (2010), SLAM with expectation maximization for moveable object tracking, in `2010 IEEE/RSJ International Conference on Intelligent Robots and Systems', IEEE, pp. 2077{2082. Romera, E., Alvarez, J. M., Bergasa, L. M. & Arroyo, R. (2017), `ERFNet', https: //github.com/Eromera/erfnet. Romera, E., Alvarez, J. M., Bergasa, L. M. & Arroyo, R. (2018), `ERFNet: Eflcient Residual Factorized ConvNet for Real-Time Semantic Segmentation', IEEE Transactions onIntelligent Transportation Systems 19(1), 263{272."
        }
    }
}