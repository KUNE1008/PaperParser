{
    "title": "Human motion capture using a drone",
    "id": 34,
    "valid_pdf_number": "15/33",
    "matched_pdf_number": "10/15",
    "matched_rate": 0.6666666666666666,
    "citations": {
        "Detecting human actions in drone images using YOLOv5 and stochastic gradient boosting": {
            "authors": [
                "Tasweer Ahmad",
                "Marc Cavazza",
                "Yutaka Matsuo",
                "Helmut Prendinger"
            ],
            "url": "https://www.mdpi.com/1424-8220/22/18/7020/pdf",
            "ref_texts": "31. Zhou, X.; Liu, S.; Pavlakos, G.; Kumar, V .; Daniilidis, K. Human motion capture using a drone. In Proceedings of the IEEE International Conference on Robotics and Automation, Brisbane, QLD, Australia, 21\u201325 May 2018.",
            "ref_ids": [
                "31"
            ],
            "1": "However, Sensors 2022 ,22, 7020 3 of 14 the recognition and detection of actions in aerial images is a less developed area, and differs from previous work that simply adopts the perspective of pedestrians in the scene [31,35]."
        },
        "Activemocap: Optimized viewpoint selection for active human motion capture": {
            "authors": [
                "Sena Kiciroglu",
                "Helge Rhodin",
                "Sudipta N. Sinha",
                "Mathieu Salzmann",
                "Pascal Fua"
            ],
            "url": "https://openaccess.thecvf.com/content_CVPR_2020/papers/Kiciroglu_ActiveMoCap_Optimized_Viewpoint_Selection_for_Active_Human_Motion_Capture_CVPR_2020_paper.pdf",
            "ref_texts": "[45] X. Zhou, A. S. Liu, A. G. Pavlakos, A. V . Kumar, and K. Daniilidis. Human Motion Capture Using a Drone. In International Conference on Robotics and Automation , 2018.",
            "ref_ids": [
                "45"
            ],
            "1": "To achieve this, the algorithm of [45] uses 2D human pose estimation in a monocular video and non-rigid structure from motion to reconstruct the articulated 3D pose of a subject, while that of [18] reacts online to the subject\u2019s motion to keep them in view and to optimize for screen-space framing objectives.",
            "2": "Either the human is followed from a constant angle and the angle is set externally by the user [19] or the drone undergoes a constant rotation around the human [45].",
            "3": "[45] X."
        },
        "A Deep Bi-directional Attention Network for Human Motion Recovery.": {
            "authors": [
                "Qiongjie Cui",
                "Huaijiang Sun",
                "Yupeng Li",
                "Yue Kong"
            ],
            "url": "https://www.ijcai.org/proceedings/2019/0099.pdf",
            "ref_texts": "[Zhou et al., 2018 ]Xiaowei Zhou, Sikang Liu, Georgios Pavlakos, Vijay Kumar, and Kostas Daniilidis. Human motion capture using a drone. 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 2027\u2013",
            "ref_ids": [
                "Zhou et al\\., 2018 "
            ]
        },
        "A Drone Video Clip Dataset and its Applications in Automated Cinematography": {
            "authors": [],
            "url": "https://diglib.eg.org/xmlui/bitstream/handle/10.1111/cgf14668/v41i7pp189-203.pdf?sequence=1"
        },
        "Multi-Agent Deep Reinforcement Learning for Online 3D Human Poses Estimation": {
            "authors": [
                "Zhen Fan",
                "Xiu Li",
                "Yipeng Li"
            ],
            "url": "https://www.mdpi.com/2072-4292/13/19/3995/pdf",
            "ref_texts": "24. Zhou, X.; Liu, S.; Pavlakos, G.; Kumar, V .; Daniilidis, K. Human Motion Capture Using a Drone. In Proceedings of the 2018 IEEE International Conference on Robotics and Automation, Brisbane, Australia, 21\u201325 May 2018; pp. 2027\u20132033.",
            "ref_ids": [
                "24"
            ],
            "1": "[24] formulated this task as a nonrigid structure from motion (NRSFM) problem and recovered human motion from the video recorded in fast varying viewpoints."
        },
        "RobCap: A mobile motion capture system mounted on a robotic arm": {
            "authors": [],
            "url": "https://hal.science/hal-03539339/document",
            "ref_texts": "[44] X. Zhou, S. Liu, G. Pavlakos, V . Kumar, and K. Daniilidis, \u201cHuman motion capture using a drone,\u201d in 2018 IEEE International Conference on Robotics and Automation (ICRA) , pp. 2027\u20132033, 2018.",
            "ref_ids": [
                "44"
            ],
            "1": "The first mobile Mocap system is an RGB Camera-based solution using deep learning models for pose estimation [44].",
            "2": "[37] 0 \u224860 high high Pose estimation [44] 100\u2212150 >500 low low Kersting et al.",
            "3": "[44] X."
        },
        "KUMITRON: A Multimodal Psychomotor Intelligent Learning System to Provide Personalized Support when Training Karate Combats.": {
            "authors": [],
            "url": "https://ceur-ws.org/Vol-2902/paper7.pdf",
            "ref_texts": "17. X. Zhou, S. Liu, G. Pavlakos, V. Kumar, and K. Daniilidis, \u201cHuman Motion Capture Using a Drone,\u201d Proc. IEEE Int. Conf. Robot. Autom. , pp. 2027 \u20132033, Apr. 2018, Accessed: Jan.",
            "ref_ids": [
                "17"
            ],
            "1": "Drones (as in [15] and \n[16]) improve the video taking since i) they provide more flexibility to capture movements, either by fixing the recording point (for example, the front of the knee) although the user moves and turns, and ii) they can direct the point of interest to any part of the body at any time simply by moving the drone around the participant (at a sufficient distance that it does not disturb him o r her) [17].",
            "2": "The dev elopment of KUMITRON [21][22] was inspired by the state of the art: sensors to collect motion related data [13] [23], existing karate systems \n[12][24] and sports [25][26] , drones for dynamic video gathering [15][16][17] , and computer vision algorithms that combine video and inertial information [19] [27]."
        },
        "AIM: Acoustic Inertial Measurement for Indoor Drone Localization and Tracking": {
            "authors": [
                "Yimiao Sun",
                "Weiguo Wang",
                "Luca Mottola",
                "Ruijin Wang",
                "Yuan He"
            ],
            "url": "https://dl.acm.org/doi/pdf/10.1145/3560905.3568499",
            "ref_texts": "[58]Xiaowei Zhou et al .2018. Human motion capture using a drone. In IEEE international conference on robotics and automation. 2027\u20132033.",
            "ref_ids": [
                "58"
            ]
        },
        "KUMITRON: Learning in Pairs Karate related skills with Artificial Intelligence support": {
            "authors": [
                "N E",
                "A S"
            ],
            "url": "https://aied2021.science.uu.nl/file/Kumitron.pdf",
            "ref_texts": "[12] X. Zhou, S. Liu, G. Pavlakos, V. Kumar, and K. Daniilidis, \u201cHuman Motion Capture Using a Drone,\u201d Proc. IEEE Int. Conf. Robot. Autom. , pp. 2027 \u20132033, Apr. 2018, Accessed: Jan. 16, 2021. [Online]. Available: http://arxiv.org/abs/1804.06112. ",
            "ref_ids": [
                "12",
                "Online"
            ],
            "1": "In addition, t he development of the system was inspired by state of the art : sensors to collect motion related data [5], existing karate systems ([6] and [7]) and sports ([8] and [9]), drones for dynamic video ga thering ([10],[11] and [12]), and computer vision algorithms \n([13] and [14]).",
            "2": "[Online].",
            "3": "[12] X.",
            "4": "[Online]."
        },
        "Capturing, Modelling, Analyzing and providing Feedback in Martial Arts with Artificial Intelligence to support Psychomotor Learning Activities": {
            "authors": [
                "O C",
                "S O"
            ],
            "url": "http://e-spacio.uned.es/fez/eserv/bibliuned:master-ETSInformatica-IAA-Acasas/Casas_Ortiz_Alberto_TFM.pdf",
            "ref_texts": ""
        },
        "Automated Human Motion Analysis and Synthesis": {
            "authors": [],
            "url": "https://infoscience.epfl.ch/record/300270/files/EPFL_TH8976.pdf",
            "ref_texts": "2.1.3 Human Motion Capture on Drones Drones can be viewed as flying cameras and are therefore natural applications for active pose estimation methods. One problem, however, is that the drone must keep the person in its field of view. To achieve this, the algorithm of Zhou et al. [196] uses 2D human pose estimation in a monocular video and non-rigid structure from motion to reconstruct the articulated 3D pose of a subject, while that of Naegeli et al. [116] reacts online to the subject\u2019s motion to keep them in view and to optimize for screen-space framing objectives. AirCap [141] calculates trajectories of multiple drones that aim to keep the person in view while simultaneously performing object avoidance. This was extended by Tallamraju et al. [162] so as to optimize multiple MAV trajectories by minimizing the uncertainty of the global human position. In [117], this was integrated into an autonomous system that actively directs a swarm of drones and simultaneously reconstructs 3D human and drone poses from onboard cameras. This strategy implements a pre-defined policy to stay at constant distance to the subject and uses pre-defined view angles (90\u00b1between two drones) to maximize triangulation accuracy. This enables mobile large-scale motion capture, but relies on markers for accurate 2D pose estimation. Xu et al. [177] use three drones for markerless motion capture, using an RGBD video input for tracking the subject. In short, existing methods either optimize for drone placement but for mostly rigid scenes, or estimate 3D human pose but without optimizing the camera placement. Other works such as that of Pirinen et al. [130] perform optimal camera placement for multiple cameras. In Chapter 3, we propose an approach that aims to find the best next drone location for monocular view 10",
            "ref_ids": [
                "196",
                "116",
                "141",
                "162",
                "117",
                "177",
                "130"
            ],
            "1": "[130] optimize multiple cameras poses for triangulation of joints in a dome environment using a self-supervised 9 Chapter 2.",
            "2": "[196] uses 2D human pose estimation in a monocular video and non-rigid structure from motion to reconstruct the articulated 3D pose of a subject, while that of Naegeli et al.",
            "3": "[116] reacts online to the subject\u2019s motion to keep them in view and to optimize for screen-space framing objectives.",
            "4": "AirCap [141] calculates trajectories of multiple drones that aim to keep the person in view while simultaneously performing object avoidance.",
            "5": "[162] so as to optimize multiple MAV trajectories by minimizing the uncertainty of the global human position.",
            "6": "In [117], this was integrated into an autonomous system that actively directs a swarm of drones and simultaneously reconstructs 3D human and drone poses from onboard cameras.",
            "7": "[177] use three drones for markerless motion capture, using an RGBD video input for tracking the subject.",
            "8": "[130] perform optimal camera placement for multiple cameras.",
            "9": "116\u00a70.",
            "10": "Either the human is followed from a constant angle and the angle is set externally by the user [117] or the drone undergoes a constant rotation around the human [196].",
            "11": "3 141.",
            "12": "7 116.",
            "13": "1 116.",
            "14": "3 162.",
            "15": "8 117.",
            "16": "Evaluation diversity \" accuracy \" 1s ave # 1s best # 5s ave # 5s best # TIM-GCN [88] 16 143 143 196 196 HisRep10 [105] 21 116 116 197 197 HisRep125 [105] 20 136 136 191 191 Mix&Match [10] 1002 18 161 156 244 237 DLow [182] 3501 16 136 131 189 171 Ours (0.",
            "17": "1) 6936 34 177 168 208 173 Ours (0.",
            "18": "0) 14995 20 145 116 194 127 Table 5.",
            "19": "Several works have already taken steps in this direction for triangulating human pose [47,130], and monocular 3D pose estimation [13].",
            "20": "[116] T.",
            "21": "[117] T.",
            "22": "[130] A.",
            "23": "[141] N.",
            "24": "[162] R.",
            "25": "Computer Vision and Image Understanding , 104(2-3):157\u2013\n177, 2006.",
            "26": "[177] L.",
            "27": "In International Conference on Computer Vision , pages 11656\u201311665, October 2021.",
            "28": "[196] X.",
            "29": "com /diamondmath+41779455849 PhD candidate working in machine learning and computer vision, with a focus on human motion analysis.",
            "30": "SKILLS Programming Python, PyTorch Languages English (Very fluent, TOEFL score: 116/120), Turkish (Native Fluency), French (Beginner) MISCELLANEOUS\n\u00b7EPIC (CS PhD Student Association) Committee Member (2018): Took part in organizing EDIC Open House \u201919, board game events, lunch talks, etc."
        },
        "On the motion and action prediction using deep graph models": {
            "authors": [],
            "url": "https://repositories.lib.utexas.edu/bitstream/handle/2152/115234/MOHAMED-DISSERTATION-2022.pdf?sequence=1",
            "ref_texts": "[44] Xiaowei Zhou, Sikang Liu, Georgios Pavlakos, Vijay Kumar, and Kostas Daniilidis. Human motion capture using a drone. In 2018 IEEE international conference on robotics and automation (ICRA) , pages 2027\u20132033. IEEE, 2018.",
            "ref_ids": [
                "44"
            ],
            "1": "Where in drones and autonomous driving, 3D pose prediction helps in accurate maneuver and motion planning through the environment [43,44,45,46]."
        },
        "Design and control of DermDrone; a micro-sized quadrotor for dermatology applications with RL-based optimization": {
            "authors": [
                "Library Department"
            ],
            "url": "https://summit.sfu.ca/_flysystem/fedora/2023-01/etd21763.pdf",
            "ref_texts": "\"Tactile Drones\" project to provide immersive tactile feedback in virtual reality for human pose estimation. MoCap is also used in DroneCh i to investigate the application of drone as a somaesthetic HDI. In our case, DermDrone, proposed in Chapter 3 and Chapter 4, falls under recipient user HDI category in which the patien t as the user does not control the scenario and the drone navigates independently without any involve ment from the subject. However, introducing the localisation based on patient\u2019s pose change the category into active user where drone localizes according to the user\u2019s body and the locat ion of drone affects the navigation. The active user HDI works can be categorized according to how they communicate and interact with users. Although many projects like DroneChi and Tactile Drones utilized the MoCap systems, vision-based techniques using the body parts feedback such as hand gesture, facial pose, or body pose are the most natural user inte rface [19][20][21][22]. LightAir [23] suggested an innovative human-drone interaction by leveraging image projection and foot gesture recognition. FlyCap [24] presented a system to capture motions of moving targets using three autonomous drones equipped with depth cameras. The advances in HPE motivated researchers to use it as a tool for human-drone interactions. Zhou et. al. in [25] proposed 2D HPE as a mo tion capture system in a monocular video captured by a drone. A 3D pointing gestures inte rface was developed in [26] to show a target to a drone in firefighting application. AirCap [27] and Flycon [28] are the most related works to our proposed method in Chapter 5 and Chapter 6 in term of using HPE as an HDI. AirCap addresses the online markerless motion capture system in outdoor setting ",
            "ref_ids": [
                "19",
                "20",
                "21",
                "22",
                "23",
                "24",
                "25",
                "26",
                "27",
                "28"
            ],
            "1": "Toosi University of Technology, 2018 Thesis Submitted in Partial Fulfillment of the Requirements for the Degree of Master of Applied Science in the School of Mechatronic Systems Engineering Faculty of Applied Science V\n\u00a9Mojtaba Ahangar Arzati 2021 SIMON FRASER UNIVERSITY Fall 2021 Copyright in this work is held by the a uthor.",
            "2": "21 \n2.",
            "3": "23 \n2.",
            "4": "26 \n2.",
            "5": "26 \n2.",
            "6": "27 Chapter 3.",
            "7": "125 \n6.",
            "8": "125 \n6.",
            "9": "125 \n6.",
            "10": "126 \n6.",
            "11": "126 \n6.",
            "12": "127 \n6.",
            "13": "20 Table 2-5 Specifications of DJI Spark .",
            "14": "22 Table 2-6 Platform rating .",
            "15": "42 Table 3-4 specifications of EMAX RS1108 with GF3025 propeller .",
            "16": "121 Table 6-1 Hyperparameters of the agent .",
            "17": "20 Figure 2-9 Mavic Air .",
            "18": "22 Figure 2-10 Vision module attached to the Mavic Air .",
            "19": "23 Figure 2-11 Workflow of navigation and localization algorithm in setup 1 .",
            "20": "24 Figure 2-12 The graphical user interface designed for MavicAir \u2013based setups .",
            "21": "25 Figure 2-13 Mavic Air with Osmo Pocket as the capturing unit .",
            "22": "26 Figure 2-14 Captured frames via different platforms .",
            "23": "41 Figure 3-10 EMAX RS1108 5200KV .",
            "24": "52 \n xi Figure 3-20 The schematic of the ESC Switching Circuit .",
            "25": "53 Figure 3-21 Power distribution board .",
            "26": "53 Figure 3-22 State machine of PDB .",
            "27": "54 Figure 3-23 CAD model and physical prototype of DermDrone MVP1.",
            "28": "84 Figure 4-20 Flight test room .",
            "29": "85 Figure 4-21 Position error in X-axis .",
            "30": "85 Figure 4-22 Position error in Y-axis .",
            "31": "86 Figure 4-23 Position error in Z-axis .",
            "32": "86 Figure 4-24 Velocity value in X-axis .",
            "33": "87 Figure 4-25 Velocity value in Y-axis .",
            "34": "87 Figure 4-26 (left): poor heading controller (right): acceptable heading controller .",
            "35": "88 Figure 4-27 Heading error along the trajectory in Polar plot mode .",
            "36": "120 Figure 5-8 Comparison of different trained agents with random selection viewpoint and fixed-angle camera .",
            "37": "122 Figure 5-9 Agent's performance in choosing NBV .",
            "38": "122 Figure 5-10 The viewpoint captured by DermDrone with using of Double Dueling DQN (five actions) in the simulation .",
            "39": "124 Figure 6-1 The simulation environment and observation state .",
            "40": "126 Figure 6-2 Proposed architecture model .",
            "41": "Dermatology Melanoma is the most serious type of skin cancer in worldwide, and its prevalence has increased by 44% in the period from 2011 to 2021 in the Unitaed States [1].",
            "42": "020406080100120140160180\n2015 2016 2017 2018 2019 2020 2021NUMBER OF PUBLICATIONS YEARHDI Google Scholar Results \n \n5 DroneNavigator demonstrated the use of drone in visually impaired people navigation while in Moonshot project drones are employed as blind runners\u2019 companions.",
            "43": "Although many projects like DroneChi and Tactile Drones utilized the MoCap systems, vision-based techniques using the body parts feedback such as hand gesture, facial pose, or body pose are the most natural user inte rface [19][20][21][22].",
            "44": "LightAir [23] suggested an innovative human-drone interaction by leveraging image projection and foot gesture recognition.",
            "45": "FlyCap [24] presented a system to capture motions of moving targets using three autonomous drones equipped with depth cameras.",
            "46": "in [25] proposed 2D HPE as a mo tion capture system in a monocular video captured by a drone.",
            "47": "A 3D pointing gestures inte rface was developed in [26] to show a target to a drone in firefighting application.",
            "48": "AirCap [27] and Flycon [28] are the most related works to our proposed method in Chapter 5 and Chapter 6 in term of using HPE as an HDI.",
            "49": "Item Value or Description Camera photo resolution 5 MP (2592\u00d71936) Camera field of view (FOV) 82.",
            "50": "6\u00b0 Video footage resolution HD 720p 30 fps9 Electronic Image stabilization (EIS) Yes Communication network Wi-Fi Weight 87g Dimensions 98\u00d792.",
            "51": "The altitude is fixed at 25 cm during the adjustmen t procedure.",
            "52": "3\u201d CMOS Camera photo resolution 4000\u00d73000 pixels Camera field of view (FOV) 80\u00b0 Video footage resolution 4K Ultra HD: 3840\u00d72160 at 60fps Max Video Bitrate 100 Mbps Supported File Formats FAT32 (\u226432 GB) , exFAT (\u226564 GB) Communication network BLE (the module should be attached) Weight 116g Dimensions 121.",
            "53": "20 Figure 2-8 DJI Spark According to the mentioned points, we selected DJI Spark as our second off-the-shelf drone to evaluate and analyze the task of DermDrone .",
            "54": "3\u201d CMOS Camera photo resolution 3968\u00d72976 Pixels Camera field of view (FOV) 80\u00b0 Video footage resolution FHD: 1920\u00d71080 30p Max Video Bitrate 24 Mbps Supported File Formats FAT32 Gimbal 2-axis mechanical (pitch, roll), Pitch: -85\u00b0 to 0\u00b0 Communication network Wi-Fi 2.",
            "55": "3 m Weight 300g Dimensions 265\u00d7265\u00d755 mm Operating temperature range 0\u00b0-40\u00b0 C (32\u00b0-104\u00b0 F) Battery LiPo 3S 1480 mAh Table 2-4 DJI Spark specifications \n\n \n21 We used the DJI Mobile SDK for iOS developmen t in Swift to implement the localization and navigation algorithms.",
            "56": "Also, the Mavic Ai r offers 21 minutes of flight time, which outperforms the previous platforms.",
            "57": "22 Figure 2-9 Mavic Air \n Item Value or Description Sensor 1/2.",
            "58": "3\u201d CMOS Camera photo resolution 4:3: 4056\u00d73040 \n16:9: 4056\u00d72280 HDR Mode Yes Camera field of view (FOV) 85\u00b0 Video footage resolution 4K Ultra HD: 3840\u00d72160 24/25/30p Max Video Bitrate 100 Mbps Supported File Formats FAT32 Gimbal 3-axis mechanical Pitch: -100\u00b0 to 22\u00b0 Roll: -30\u00b0 to 30\u00b0 Pan: -12\u00b0 to 12\u00b0 Communication network Wi-Fi 2.",
            "59": "8 GHz Max hovering time 20 minutes Hover Accuracy Range Vertical: +/0.",
            "60": "1 m Weight 430g Dimensions 342\u00d7368\u00d764 mm Operating temperature range 0\u00b0-40\u00b0 C (32\u00b0-104\u00b0 F) Battery LiPo 3S 23751 mAh Table 2-5 Specifications of DJI Spark \n\n \n23 2.",
            "61": "2 m \n\n \n24 altitude using the Mavic Air positioning system.",
            "62": "Figure 2-11 Workflow of navigation and localization algorithm in setup 1 \n\n \n25 Figure 2-12 The graphical user interface designed for MavicAir \u2013based setups \n\n \n26 2.",
            "63": "Setup 3: Using the front camera for both localization and image capture As part of our effort to build an integrated platf orm, the third setup is proposed, in which Mavic Air utilizes the front camera for both body part image capture and localization as \n\n \n27 with Spark in section 2.",
            "64": "3 24 15 21 24 21 Mechanical gimbal 7 0 0 4.",
            "65": "21 -7.",
            "66": "88 27.",
            "67": "01 27.",
            "68": "01 22.",
            "69": "5 GHz SoC Broadcom BCM2711 RAM 8GB GPU Broadcom VideoCore VI @ 500 MH \n Multimedia H.",
            "70": "265 (4kp60 decode), H264 (1080p60 decode, \n1080p30 encode) OpenGL ES 3.",
            "71": "38 Components Description Microcontrollers Main Application MCU STM32F405, Cortex-M4, 168MHz, 192kb SRAM, 1Mb flash Radio and Power Management MCU nRF51822, Cortex-M0, 32Mhz, 16kb SRAM, 128kb flash EEPROM 8KB IMU \n3 Axis Gyroscope BMI088 \n3 Axis Accelerometer BMI088 Pressure Sensor BMP388 Interfaces USB 1x Crazyflie Expansion Connectors 2 sets include VCC (3.",
            "72": "FL Connectors 2x ESC Connectors using PWM/OneShot125 4x Supported clients Win, Linux, OSX, Android, iOS Mechanical Specifications PCBA Size (WxHxD) 35x9x42mm (including connectors but not battery wire) Weight 7.",
            "73": "4GHz ISM Radio Amplifier 20 dBm Dual Antenna Support \u0636 Table 3-2 Crazyflie Bolt fli ght controller specifications \n \n \n \n39 3.",
            "74": "Each ToF sensor is capable of measuring accurate distance with mm precision up to 4 meters at 25 Hz with low power consumption.",
            "75": "Motors and Propellers The brushless motor EMAX RS1108 5200KV is used in combination with T3X2.",
            "76": "Figure 3-8 Motor and propeller assembled on the drone \n \n Figure 3-10 EMAX RS1108 5200KV \n Components' Weight and Shape: In the second step, all required components on the DermDrone, including the embedded board, fli ght controller, camera module, camera lens, distance sensors, optical flow deck, and all wiring, were considered to calculate the drone\u2019s payload.",
            "77": "Eventually, the brushless motor EMAX RS1108 5200KV with a combination of T3X2.",
            "78": "The specifications of the selected motor are determined by the GF3025 propeller and two-cell battery, as shown in Table 3-4 .",
            "79": "Voltage (V) Propeller Current \n(A) Thrust \n(G) Power \n(W) Efficiency \n(G/W) Speed \n(RPM) \n \n \n \n \n8 \n \n \n GF3025 1 34 8.",
            "80": "25 14440 \n2 67 16.",
            "81": "19 20060 \n3 98 24.",
            "82": "08 23780 \n4 128 32.",
            "83": "00 27060 \n5 155 40.",
            "84": "60 MAX Table 3-4 specifications of EMAX RS1108 with GF3025 propeller \n3.",
            "85": "HGLRC DShot20A BL Heli_S is the ESC used in this work.",
            "86": "The HGLRC ESC contains EFM8BB21F16G-C as an MCU to interpret the input signal using built-in BLHELI_S firmware.",
            "87": "In Derm Drone, the flight controller communicates with ESC using the Oneshot125 protocol that utilizes eight times shorter pulses (125us250us) than standard PWM (1ms-2ms).",
            "88": "46 Model HGLRC DShot20A BLHeli_S 2-4S 4-in-1 ESC MCU EFM8BB21F16G-C Protocol DSHOT600/300/150/Oneshot125/42, MultiShotDamped mode Input Voltage 2-4S Lipo battery Constant Current 20A-4S Peak Current 25A/10S Firmware BLHELI_S Size 35x38mm (including pad) Weight 8.",
            "89": "2g Table 3-5 Specifications of ESC It is also worth mentioning that although Dshot and MultiShot could offer more advantages than Oneshot125, they couldn\u2019t be used because Crazyflie Bo lt suffers from supporting the Dshot or MultiShot due to hardware limitations .",
            "90": "The circuit is made up of a boost regulator IC, LM2588, as depicted in Figure 3-19.",
            "91": "Both MOSFETs' gates are connected to the ESC_EN pin from the microcont roller, as illustrated in Figure 3-20.",
            "92": "Figure 3-20 The schematic of the ESC Switching Circuit The PDB also connects the source power to the flight controller power using the REG_GND.",
            "93": "The power distribution board is shown in Figure 3-21.",
            "94": "Figure 3-21 Power distribution board \n\n \n54 PDB's State Machine : Four states are defined to improve the performance of DermDrone in terms of safety, power consumption, and user -friendliness.",
            "95": "Figure 3-22 State machine of PDB \n3.",
            "96": "CAD model and physical prototype are presented in Figure 3-23.",
            "97": "Figure 3-23 CAD model and physical prototype of DermDrone MVP1.",
            "98": "There are also four classes for the number of markers contained in each dictionary: 50, 100, 250, or 1000.",
            "99": "Additionally, there is a dictionary called Original with 5x5 grids and 1024 markers.",
            "100": "As a result, 25 predefined dictionaries are defined according to the in ner binary squares and the number of markers contained.",
            "101": "Since there are ten data bits, the maximum number of ArUco with the original dictionary is 1024.",
            "102": "In this project, the maximum and minimum alti tudes of DermDrone are set to 200 cm and \n30cm, respectively.",
            "103": "Therefore, at least one ArUco marker should be detectable at any altitude between 30 cm to 200 cm.",
            "104": "The camera utilized for localization offers a 60-degree horiz ontal FOV with 640\u00d7480 pixels that provides \n230.",
            "105": "20 cm of coverage at an altitude of 200 cm, with each pixel in the captured frame representing 3.",
            "106": "Thus, the maximum gap distance between the two markers is 23.",
            "107": "Additionally, some short paths, including \"O ne Loop,\" \"180 degree,\" \"270 degree,\" and \n\"Two Parts,\" were generated to evaluate the performance of dermatology algorithms with fewer images and smal ler body coverage.",
            "108": "Figure 4-12 Different trajectories used in the project To minimize the trajectory length compared to \"One Loop\", we came with a 270 degree circle/ellipse where the DermDrone starts th e navigation at the angle of -22.",
            "109": "5 degree and ends at 247.",
            "110": "In this work, the position and velocity PID controllers update at 100 Hz, the attitude cont roller updates at 250 Hz, and the attitude rate controller updates at 500 Hz.",
            "111": "0 Thrust base value 40000 Minimum Thrust 20000 Maximum velocity in x-axis and y-axis 0.",
            "112": "20 Table 4-4 Selected controllers\u2019 parameters \n \n83 Mixer Mixer determines the distribution of torque magnit ude in multirotor motors.",
            "113": "Additionally, Figure 4-20 shows the flight test environment.",
            "114": "Figure 4-19 2D and 3D trajectory traveled by DermDrone according to the setpoints and localization feedback \n\n \n85 Figure 4-20 Flight test room The errors between the desired trajectory se tpoints and points along the X-axis and Y-axis are shown in Figure 4-21 and Figure 4-22, re spectively.",
            "115": "Figure 4-21 Position error in X-axis \n\n \n86 Figure 4-22 Position error in Y-axis The altitude controller, as a challenging part of multirotor control, contributes greatly to flight stability as a result of the coupling effect between the altitude controller and controllers in the X and Y axes.",
            "116": "Figure 4-23 shows the position error of DermDrone in the Z-axis.",
            "117": "Figure 4-23 Position error in Z-axis Apart from having a low error position in X-ax is and Y-axis, movement with fixed velocity is the other criteria for an acceptable control ler.",
            "118": "Figure 4-24 and Figure 4-25 show that the magnitude of 2D vector velocity is almost constant along the path, which reduces the motion blur in captured images.",
            "119": "87 Figure 4-24 Velocity value in X-axis \n Figure 4-25 Velocity value in Y-axis The performance of the yaw controller significant ly affects the 3D construction algorithm, which is the main part of the dermatology pi peline.",
            "120": "Figure 4-26 describes an acceptable heading controller and a poor heading controller, with the patient's right hand not covered in the captured frame.",
            "121": "88 Figure 4-26 (left): poor heading controller (right): acceptable heading controller Figure 4-27 and Figure 4-28 show the headi ng error along the trajectory.",
            "122": "Figure 4-27 Heading error along the trajectory in Polar plot mode \n Figure 4-28 Heading error along the trajectory \n\n \n89 Although the plots from the Yaw controller indicate an acceptable performance, the captured images failed to confirm this.",
            "123": "This issue and the error of the heading controller both provide an incorrect heading for DermDrone that results in captured frames with uncompleted body coverage, as shown in Figure 4-26 (a).",
            "124": "101 \n(a) Original Image (b) PixelLib segmentation with xception_pascalvoc model \n \n(c) DeepLab segmentation (d) Overlay version of DeepLab segmentation Figure 4-37 Segmentation comparison between PixelLib and DeepLab \n\n \n102 (a) Original Image (b) PixelLib segmentation with mask_rcnn_coco model \n(c) PixelLib segmentation with deeplabv3_xception65_ade20k model (d) PixelLib segmentation with deeplabv3_xception_tf_dim_ordering model Figure 4-38 Segmentation using PixelLib with different model \n\n \n103 The segmented frames are then fed to a 3D construction pipeline that utilizes the Agisoft Metashape software.",
            "125": "104 \n \n(a) One Loop \u2013 Circle Mode (b) 2 Loops \u2013 Circle Mode \n \n(c) Helical \u2013 Circle Mode (d) 2 Parts \n \n(e) 180 Degrees \u2013 Circle Mode (f) 270 Degrees \u2013 Circle Mode Figure 4-39 The estimated locations of the camera along each path \n\n \n105 Finally, the confidence model is utilized for th e third step.",
            "126": "The Helical, \n\u201cTwo Loops \u201d, and \u201cTwo Parts \u201d saw higher confidence values in this area relative to the \n\u201cOne Loop \u201d, \u201c270 degree \u201d, and \u201c180 degree \u201d modes.",
            "127": "The trajectories with one lap, such as \u201cOne Loop \u201d, \u201c180 degree \u201d, and \u201c270 degree \u201d, couldn\u2019t succeed in building a high-quality mesh because all photos were captured at a fixed altitude with the abdomen in the ce nter of all captured frames.",
            "128": "Accord ing to the results from the 180 Degrees and 270 Degrees paths, a one-lap trajectory is required for an acceptable 3D model.",
            "129": "Similarly, the artifacts present in the mod el's left side in the 270 Degrees path also corrupted the 3D construction.",
            "130": "Head Shoulder Shin and Foot Hands Sides \n Rating Table Very Good 4 Good 3 Fair 2 Poor 1 \n Figure 4-40 Performance of generated models in each path \n\n107(a) One Loop \u2013Circle Mode (b) 2 Loops \u2013Circle Mode (c) Helical \u2013Circle Mode (d) 2 Parts (e) 180 Degrees \u2013Circle Mode (f) 270 Degrees \u2013Circle Mode Figure 4-41 Confidence models of proposed pa ths in front, back, left, and right side \n \n108 \n \n(a) One Loop \u2013 Circle Mode (b) 2 Loops \u2013 Circle Mode \n \n(c) Helical \u2013 Circle Mode (d) 2 Parts \n \n(e) 180 Degrees \u2013 Circle Mode (f) 270 Degrees \u2013 Circle Mode Figure 4-42 Confidence models of proposed paths from different perspectives \n \n \n\n \n109 \n(a) One Loop \u2013 Circle Mode (b) 2 Loops \u2013 Circle Mode \n \n(c) Helical \u2013 Circle Mode (d) 2 Parts \n \n(e) 180 Degrees \u2013 Circle Mode (f) 270 Degrees \u2013 Circle Mode Figure 4-43 Textured models of proposed paths from different perspectives \n \n\n \n110 Chapter 5.",
            "131": "This chapter was published as a paper titled \u201c Viewpoint Selection for DermDrone using Deep Reinforcement Learning \u201d at ICCAS 2021.",
            "132": "Lightweight Open Pose claims 26 fps process on Core i7-6850K CPU.",
            "133": "State Space The state space is a 96\u00d7126 pixels image, including confidence map of joints produced by the HPE network.",
            "134": "04, with NVIDIA GeForce RTX 2080 Ti for a tota l of 251,000 time steps.",
            "135": "The comparison between the agent with the enti re five actions and the agent with only three pitch angle actions indicated that actions in ro ll didn't improve the performance in the same \n\n \n120 step times.",
            "136": "To account for the increased training time requirement by the bigger action space, the number of steps is increased by 2000 episodes; however, no improvements in the final reward were observed.",
            "137": "121 An experiment was conducted to compare the performance of the trained agents.",
            "138": "8% 20.",
            "139": "8% 22.",
            "140": "9% 21.",
            "141": "21 DQN 3.",
            "142": ", -20 penalty expresses Failed Frames and weighted \u2206d corresponds to Localization E rror.",
            "143": "122 Figure 5-8 Comparison of different trained ag ents with random selection viewpoint and fixed-angle camera \n Figure 5-9 Agent's performance in choosing NBV Another interesting conclusion from the result is that all three trained agents tended to choose the upper body as the viewpoint, specif ically when the drone is flying at low altitudes.",
            "144": "That \n\n \n123 means HPE can localize the joints with higher accuracy when DermDrone selects the upper body.",
            "145": "124 Figure 5-10 The viewpoint captured by DermDrone with using of Double Dueling DQN \n(five actions) in the simulation \n\n \n125 Chapter 6.",
            "146": "126 Besides these advantages, the simulator also provides advanced adjustable lighting conditions and an advanced API plugin with various sensory information such as position, attitude angle, and low-level commands for real-time control.",
            "147": "127 During the attempts, DermDrone must avoi d any collisions with the human body.",
            "148": "According to the term, the agent receives positive rewards when the error between the c urrent gimbal\u2019s yaw a ngle and the target yaw angle is lower than 20 degrees.",
            "149": "The concatenated result is applied to four dense layers of a fully connected network with 1024, 512, 128, and 64 hidden units accordingly, and finally, the last fully connected layer outputs one out of the ten discrete actions.",
            "150": "04 with NVIDIA GeForce RTX 2080 Ti for a total of 350,000 time steps.",
            "151": "00025 is used.",
            "152": "00025 learning rate of the optimizer Table 6-1 Hyperparameters of the agent We conducted three training experiments by util izing the hyperparameters discussed in this chapter.",
            "153": "First, we conducted 100 test episodes using the model extracted from the agent at episode \n250.",
            "154": "24.",
            "155": "Third, we selected the model at episode 1200 a nd ran the experiment for the same number of episodes as the two prior.",
            "156": "On the othe r hand, the agent's trajectory at episode 250 shows that the agent couldn\u2019t reach the target position, although it moved toward the target.",
            "157": "The paths a ssociated with models at episodes 750 and 1200 illustrate that the agents learned to meet the target posit ion, although the success rate was not high.",
            "158": "It shows that although both agents from episodes 750 and 1200 succeeded in reaching the goal position, they got stuck there and couldn\u2019t finish the task.",
            "159": "Four conditions must be met for accomplishing the task: (1) The Euclidian distance error between the agent\u2019s position and the target position must be less than 25cm.",
            "160": "25.",
            "161": "Both errors decreased dras tically at episode 750, and the agent started to pass the threshold at episode 1200.",
            "162": "Accordi ng to the graphs, the agent started to exceed the HPE confidence threshold at episode 750, wh ile distance error and yaw error began to pass their thresholds at episode 1200.",
            "163": "Although the agent im proved its gimbal pitch error in episode 750, the error increased unexpectedly in ep isode 1200.",
            "164": "(2021 ).",
            "165": "Cancer Facts and Figures 2021.",
            "166": "org/content/dam/cancerorg/research/cancer-facts-and-statistics/annual-cancer-facts-andfigures/2021/cancer-facts-and-figures-2021.",
            "167": "2012, doi: 10.",
            "168": "2011.",
            "169": "2020, doi: \n10.",
            "170": "(2002).",
            "171": "Clinics in dermatology, 20 3, 297-304 .",
            "172": "2012, doi: 10.",
            "173": "2011.",
            "174": "Binder, \n\u201cFrequency and Characteristics of Enla rging Common Melanocytic Nevi,\u201d 2000.",
            "175": "02, 2021.",
            "176": "3390/ijerph18041726.",
            "177": "2020, doi: 10.",
            "178": "1016/S2589-7500(20)30001-7.",
            "179": "ATKINS, \u201cUNMANNED MOBILE ROBOT AND SOFTWARE FO R CLINICAL EXAMINATION AND TREATMENT,\u201d WO/2020/176969, Oct.",
            "180": "09, 2020 Accessed: Nov.",
            "181": "16, 2021.",
            "182": "jsf?docId=WO2020176969 \n[13] M.",
            "183": "Henze, \u201cDroneNa vigator: Using drones for navigating visually impaired persons,\u201d in ASSETS 2015 Proceedings of the 17th International ACM SIGACCESS Conference on Computers and Accessibility, Oct.",
            "184": "2015, pp.",
            "185": "327\u2013328.",
            "186": "1145/2700648.",
            "187": "2015, doi: \n10.",
            "188": "Fang, \u201cIHuman3D: Intelligent human body 3d reconstructi on using a single flying camera,\u201d in MM 2018 Proceedings of \n \n151 the 2018 ACM Multimedia Conference, Oct.",
            "189": "2018, pp.",
            "190": "1145/3240508.",
            "191": "3240600.",
            "192": "Herdel, \u201cDrone in Love: Emotional Perception of Facial Expres sions on Flying Robots,\u201d May 2021.",
            "193": ", \u201cTactile drones Providing immersive tactile feedback in virtual reality through quadcopters,\u201d in Conference on Human Factors in Computing Systems Proceedings, May 2017, vol.",
            "194": "Part F127655, pp.",
            "195": "1145/3027063.",
            "196": "3050426.",
            "197": "2020.",
            "198": "[19] J.",
            "199": "Gambardella, \u201cHRI in the sky: Controlling UAVs using face poses and hand gestures,\u201d in ACM/IEEE International Conference on Human-Robot Interaction, 2014, pp.",
            "200": "252 \u2013253.",
            "201": "1145/2559636.",
            "202": "2559833.",
            "203": "[20] A.",
            "204": "Manuri, \u201cA kinect based natural interface for quadrotor control,\u201d in Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, \n2012, vol.",
            "205": "1007/978-3-642-30214-5_6.",
            "206": "[21] L.",
            "207": "Cauchard, \u201cD rone & Wo: Cultural influences on humandrone interaction techniques,\u201d in Conference on Human Factors in Computing Systems Proceedings, May 2017, vol.",
            "208": "2017-May, pp.",
            "209": "1145/3025453.",
            "210": "3025755.",
            "211": "[22] J.",
            "212": "Vaughan, \u201cReady Aim Fly! Hands-Free Face-Based HRI for 3D Trajectory Control of UAVs,\u201d in Proceedings 2017 14th Conference on Computer and Robot Vision, CRV 2017, Feb.",
            "213": "2018, vol.",
            "214": "2018-January, pp.",
            "215": "2017.",
            "216": "152 [23] M.",
            "217": "2016.",
            "218": "2932429.",
            "219": "[24] Xu, L.",
            "220": "(2018).",
            "221": "IEEE Transactions on Visualization and Computer Graphics, 24, 2284-2297.",
            "222": "[25] X.",
            "223": "2018, [Online].",
            "224": "06112 \n[26] A.",
            "225": "2021, doi: 10.",
            "226": "1186/s40648-021-00200-w.",
            "227": "[27] N.",
            "228": "2019, vol.",
            "229": "2019-October, pp.",
            "230": "823 \u2013832.",
            "231": "2019.",
            "232": "[28] T.",
            "233": "2018.",
            "234": "1145/3272127.",
            "235": "3275022.",
            "236": "2020, [Online].",
            "237": "org/abs/2008.",
            "238": "12664 \n[30] S.",
            "239": "2019, doi: 10.",
            "240": "2019.",
            "241": "2932570.",
            "242": "2019, \n[Online].",
            "243": "2018, [Online].",
            "244": "(2017).",
            "245": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 5094-5102.",
            "246": "(2019).",
            "247": "2016, doi: 10.",
            "248": "2017.",
            "249": "Manuel Branco Souto, \u201cAutonomous Obstacle Collision A voidance System for UAVs in Rescue Operations,\u201d 2016.",
            "250": "25, no.",
            "251": "2020, doi: 10.",
            "252": "2019.",
            "253": "Siegwart, \u201cGet out of my lab: Large-scale, real-time visualinertial localization,\u201d in Robotics: Science and Systems, 2015, vol.",
            "254": "2015.",
            "255": "2015, [Online].",
            "256": "2018, doi: 10.",
            "257": "2018.",
            "258": "2795643.",
            "259": "2021, doi: \n10.",
            "260": "3390/ai2030023.",
            "261": "2018, doi: \n10.",
            "262": "1207631.",
            "263": "143 \u2013161, May 2019, doi: 10.",
            "264": "2019.",
            "265": "1141 \u20131153, 2013, doi: 10.",
            "266": "2012.",
            "267": "2227719.",
            "268": "2020, pp.",
            "269": "2020.",
            "270": "297 \u2013\n309, May 2020, doi: 10.",
            "271": "146264 \u2013146272, 2019, doi: \n10.",
            "272": "2019.",
            "273": "2943253.",
            "274": "26549 \u201326560, 2020, doi: \n10.",
            "275": "2020.",
            "276": "24, Dec.",
            "277": "2019, doi: 10.",
            "278": "3390/app9245571.",
            "279": "2020, doi: 10.",
            "280": "3390/rs12040640.",
            "281": "(2020a).",
            "282": "(2021).",
            "283": "Karlsson, \u201cVision based control and landing of Micro aerial vehicles,\u201d 2019.",
            "284": "(2021).",
            "285": "(2014, July 7).",
            "286": "io/2014/07/crazyflie-2-0-systemarchitecture/ \n[59] Kimberly.",
            "287": "(2020, May 12).",
            "288": "Greiff, \u201cModelling and Control of the Crazyflie Quadrotor for Aggressive and Autonomous Flight by Optical Flow Driven State Estima tion,\u201d 2017.",
            "289": "(2015, November 14).",
            "290": "(2021, December 9).",
            "291": "(2021, June 12).",
            "292": "2020, 2020, doi: 10.",
            "293": "1155/2020/9689604.",
            "294": "Vitzilaios, \n\u201cExperimental Comparison of Fiducial Markers for Pose Estimation,\u201d 2020.",
            "295": "2016, doi: 10.",
            "296": "1500324.",
            "297": "Balamuralidhar, \u201cRobust Markers for Visual Navigation using ReedSolomon Codes,\u201d 2017.",
            "298": "2018, doi: 10.",
            "299": "2018.",
            "300": "2016, doi: \n10.",
            "301": "2015.",
            "302": "023.",
            "303": "Bouguet, \u201cCamera calibrati on toolbox for matlab,\u201d 2001.",
            "304": "Zhang, \u201cA Flexible New Technique for Camera Calibration; a typo in Appendix B) (last updated on Aug A Flexible New Technique for Camera Calibration,\u201d 2008.",
            "305": "(2021).",
            "306": "(2021).",
            "307": "(2021b).",
            "308": "Zhang, \u201cA Flexible New Te chnique for Camera Calibra tion; a typo in Appendix B) (last updated on Aug A Flexible New Technique for Camera Calibration,\u201d 2008.",
            "309": "(2021).",
            "310": "(2021).",
            "311": "(2021).",
            "312": "(2016).",
            "313": "com/bitcraze/crazyfliefirmware/blob/2020.",
            "314": "(2016).",
            "315": "com/bitcraze/crazyfliefirmware/blob/2020.",
            "316": "(2017).",
            "317": "Journa l of Guidance Control and Dynamics, 40, \n2301-2306.",
            "318": "Mueller, \u201cFusing ultra -wideband range measurements with accelerometers and rate gyroscopes for quadrocopter state estimation,\u201d 2015 IEEE International Conference on Robotics and Automation (ICRA), pp.",
            "319": "1730 \u2013\n1736, 2015.",
            "320": "(2018b).",
            "321": "com/bitcraze/crazyfliefirmware/blob/2020.",
            "322": "(2018b).",
            "323": "com/bitcraze/crazyfliefirmware/blob/2020.",
            "324": "(2016).",
            "325": "(2020).",
            "326": "(2020, December 22).",
            "327": "com/watch?v=LynWtcFB020 \n[88] On-board Real-Time Obstacle Avoidance.",
            "328": "(2020, December 23).",
            "329": "LiangChieh Chen, \u201cEncoder -Decoder with Atrous Separable Convolution for Semantic Image Segmentation,\u201d ECCV, 2018.",
            "330": "2019, \n[Online].",
            "331": "08568 \n[92] 2018 2nd International Conference on Robotics and Automation Sciences (ICRAS).",
            "332": "IEEE, 2018.",
            "333": "[93] IEEE Circuits and Systems Society and Institute of Electrical and Electronics Engineers, 2019 IEEE 62nd International Mi dwest Symposium on Circuits and Systems (MWSCAS).",
            "334": "Bouhamed Omar, \u201cQ -learning based Routing Scheduling For a MultiTask Autonomous Agent,\u201d 2019 IEEE 62nd International Midwest Symposium on Circuits and Systems (MWSCAS), pp.",
            "335": "634 \u2013637, 2019.",
            "336": "(2020).",
            "337": "(2019).",
            "338": "(2018).",
            "339": "2018 International Conference on 3D Vision (3DV), 120-130.",
            "340": "2021, doi: 10.",
            "341": "2019.",
            "342": "2929257.",
            "343": "2018, [Online].",
            "344": "2015, doi: 10.",
            "345": "1038/nature14236.",
            "346": "2015, [Online].",
            "347": "2015, \n[Online].",
            "348": "Kapoor, \u201cAirSim: High -Fidelity Visual and Physical Simulation for Autonomous Vehicles,\u201d May 2017, [Online].",
            "349": "Benchun Zhou, \u201cVision -based Navigation of UAV with Continuous Action Space Using Deep Re inforcement Learning ,\u201d 2019 Chinese Control And Decision Conference (CCDC), pp.",
            "350": "5030 \u20135035, 2019.",
            "351": "(2021, April \n23).",
            "352": "2014, \n[Online].",
            "353": "Specification Sheets of Hardware Components Crazyflie Bolt Schematic \n\n \n163 \n\n \n164 \n\n \n165 Crazyflie Bolt Mechanical Drawing \n \n \n \n \n \n \n\n \n166 Flow Deck Schematic \n \n\n \n167 Flow Deck Mechanical Drawing \n \n Multi-Ranger Deck Mechanical Drawing \n \n\n \n168 Multi-Ranger Deck Schematic \n \n\n \n169 \n\n \n170 Mechanical Drawing of EMAX RS1108 5200KV Brushless Motor \n \n \n\n \n171 Appendix B.",
            "354": "270 Degrees \u2013 Circle Mode \n \n\n \n180 \n \n \n \n\n \n181 6."
        },
        "Development of Vision-Based Human Tracking for Drone's Gimbal": {
            "authors": [],
            "url": "https://jast.ias.ir/article_143637_b17ae55f5c5bd0a8a2f40078f1139a74.pdf",
            "ref_texts": "[12] X. Zhou, S. Liu, G. Pavlakos, V. Kumar, and K. Daniilidis, \u201cHuman Motion Capture Using a Drone,\u201d Proc. IEEE Int. Conf. Robot. Autom. , pp. 2027 \u20132033, ",
            "ref_ids": [
                "12"
            ],
            "1": "Many activities carried out in this field include creating a user interface to help the user control the gimbal and prepare the desired images [9], [10], recording human movement to produce a skeletal model, animation production using a drone [11], \n[12] or three -dimensional positioning of the target movements [13].",
            "2": "[12] X."
        },
        "Cooperative target following and collision avoidance for multiple unmanned aerial vehicles": {
            "authors": [
                "B Ubbink"
            ],
            "url": "https://scholar.sun.ac.za/bitstream/handle/10019.1/109251/ubbink_target_2020.pdf?sequence=1",
            "ref_texts": ""
        }
    }
}