{
    "title": "Low-rank modeling and its applications in image analysis",
    "id": 14,
    "valid_pdf_number": "38/113",
    "matched_pdf_number": "28/38",
    "matched_rate": 0.7368421052631579,
    "citations": {
        "On the applications of robust PCA in image and video processing": {
            "authors": [],
            "url": "https://hal.science/hal-01891028/document",
            "ref_texts": "[343] X. Zhou, C. Yang, H. Zhao, and W. Yu. Low-rank modeling and its applications in image analysis. Preprint , 2014.",
            "ref_ids": [
                "343"
            ],
            "1": ", 2018 3\n\u000fLow-level imaging and analysis: image restoration and denoising [105],[149],[261],[280],[281], texture image denoising [166], hyperspectral image denoising [50],[100],[285], image completion and inpainting [39],[299], image composition for high-dynamic range imaging [21], image decomposition for intrinsic image computation [151],[313] and for structural image decomposition [43], image alignment and rectification [219],[231],[259],[293],[328], image stitching and mosaicking [163], image colorization [306], multi-focus image [277],[278],[325],[326],[327], pansharpening [322], change detection [51], face recognition [185],[289],[320], partial-duplicate image search [302], image saliency detection [147],[160],[161],[222],[228] and image analysis [343],[173].",
            "2": "[343] X."
        },
        "Efficient outlier detection for high-dimensional data": {
            "authors": [
                "Huawen Liu",
                "Xuelong Li",
                "Jiuyong Li",
                "Shichao Zhang"
            ],
            "url": "http://4llab.net/publication/Efficient_Outlier_Detection_for_High-Dimensional_Data.pdf",
            "ref_texts": "[35] M. Nejati, S. Samavi, H. Derksen, and K. Najarian, \u201cDenoising by lowrank and sparse representations,\u201d J. Vis. Commun. Image Represent. , vol. 36, pp. 28\u201339, Apr. 2016.[36] X. Zhou, C. Yang, H. Zhao, and W. Yu, \u201cLow-rank modeling and its applications in image analysis,\u201d ACM Comput. Surveys , vol. 47, no. 2, pp. 1\u201335, 2014.",
            "ref_ids": [
                "35",
                "36"
            ],
            "1": "There are several effective solutions for the optimization problem of (3), such as iterative thresholding, accelerated proximal gradient, augmented Lagrange multipliers, and alter-nating direction methods [35], [36].",
            "2": "(4) For the optimization problem above, the following theorem holds [36].",
            "3": "[35] M.",
            "4": "[36] X."
        },
        "Single image highlight removal with a sparse and low-rank reflection model": {
            "authors": [
                "Jie Guo",
                "Zuojian Zhou",
                "Limin Wang"
            ],
            "url": "http://openaccess.thecvf.com/content_ECCV_2018/papers/Jie_Guo_Single_Image_Highlight_ECCV_2018_paper.pdf",
            "ref_texts": "10. Zhou, X., Yang, C., Zhao, H., Yu, W.: Low-rank modeling an d its applications in image analysis. ACM Computing Surveys 47(2) (2015)",
            "ref_ids": [
                "10"
            ]
        },
        "Low-rank and sparse decomposition based shape model and probabilistic atlas for automatic pathological organ segmentation": {
            "authors": [
                "Changfa Shi"
            ],
            "url": "http://www.nbl-technovator.jp/NBL_Tech/paper/MIA2017.pdf",
            "ref_texts": "16 (1), 265\u2013277 . Zhou, X. , Yang, C. , Zhao, H. , Yu, W. , 2014. Low-rank modeling and its applications in image analysis. ACM Comput. Surv. 47 (2), 36:1\u201336:33 . Zhou, Z. , Li, X. , Wright, J. , Cand\u00e8s, E.J. , Ma, Y. , 2010. Stable principal component pursuit. In: Proceedings of the IEEE International Symposium on Information Theory (ISIT\u201910). Austin, Texas, USA, pp. 1518\u20131522 . Zou, H. , Hastie, T. , 2005. Regularization and variable selection via the elastic net. J. R. Stat. Soc. 67 (2), 301\u2013320 . Zweng, M. , Fallavollita, P. , Demirci, S. , Kowarschik, M. , Navab, N. , Mateus, D. , 2015. Automatic guide-wire detection for neurointerventions using low-rank sparse matrix decomposition and denoising. In: Proceedings of the MICCAI Workshop on Augmented Environments for Computer-Assisted Interventions (AE-CAI). Munich, Germany, pp. 114\u2013123 . "
        },
        "Denoising of hyperspectral images using group low-rank representation": {
            "authors": [],
            "url": "https://discovery.ucl.ac.uk/id/eprint/1477657/1/MengdiWang-JSTARS-2016-UCL.pdf",
            "ref_texts": "[20] X. Zhou, C. Yang, H. Zhao, and W. Yu, \u201cLow-rank modeling and its applications in image analysis,\u201d ACM Comput. Surv. , vol. 47, no. 2, pp.",
            "ref_ids": [
                "20"
            ],
            "1": "[20] X."
        },
        "Robust Infrared Small Target Detection via Jointly Sparse Constraint of l1/2-Metric and Dual-Graph Regularization": {
            "authors": [],
            "url": "https://www.mdpi.com/2072-4292/12/12/1963/pdf",
            "ref_texts": "50. Zhou, X.W.; Yang, C.; Zhao, H.Y.; Yu, W.C. Low-rank modeling and its applications in image analysis. ACM Comput. Surv. (CSUR) 2014 ,47, 1\u201333. [CrossRef]",
            "ref_ids": [
                "50"
            ]
        },
        "Exploring low-rank property in multiple instance learning for whole slide image classification": {
            "authors": [],
            "url": "https://openreview.net/pdf?id=01KmhBsEPFO",
            "ref_texts": "11 Published as a conference paper at ICLR 2023 Kunal Nagpal, Davis Foote, Fraser Tan, Yun Liu, Po-Hsuan Cameron Chen, David F Steiner, Naren Manoj, Niels Olson, Jenny L Smith, Arash Mohtashamian, et al. Development and validation of a deep learning algorithm for gleason grading of prostate cancer from biopsy specimens. JAMA oncology , 6(9):1372\u20131380, 2020. Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh Khandeparkar. A theoretical analysis of contrastive unsupervised representation learning. In International Conference on Machine Learning , pp. 5628\u20135637. PMLR, 2019. Zhuchen Shao, Hao Bian, Yang Chen, Yifeng Wang, Jian Zhang, Xiangyang Ji, et al. Transmil: Transformer based correlated multiple instance learning for whole slide image classification. Advances in Neural Information Processing Systems , 34, 2021. Yash Sharma, Aman Shrivastava, Lubaina Ehsan, Christopher A Moskaluk, Sana Syed, and Donald Brown. Cluster-to-conquer: A framework for end-to-end multi-instance learning for whole slide image classification. In Medical Imaging with Deep Learning , pp. 682\u2013698. PMLR, 2021. Karin Stacke, Jonas Unger, Claes Lundstr \u00a8om, and Gabriel Eilertsen. Learning representations with contrastive self-supervised learning for histopathology applications. arXiv preprint arXiv:2112.05760 , 2021. Peter Str \u00a8om, Kimmo Kartasalo, Henrik Olsson, Leslie Solorzano, Brett Delahunt, Daniel M Berney, David G Bostwick, Andrew J Evans, David J Grignon, Peter A Humphrey, et al. Artificial intelligence for diagnosis and grading of prostate cancer in biopsies: a population-based, diagnostic study. The Lancet Oncology , 21(2):222\u2013232, 2020. Yuri Tolkach, Tilmann Dohmg \u00a8orgen, Marieta Toma, and Glen Kristiansen. High-accuracy prostate cancer pathology using deep learning. Nature Machine Intelligence , 2(7):411\u2013418, 2020. Naofumi Tomita, Behnaz Abdollahi, Jason Wei, Bing Ren, Arief Suriawinata, and Saeed Hassanpour. Attention-based deep neural networks for detection of cancerous and precancerous esophagus tissue on histopathological slides. JAMA network open , 2(11):e1914645\u2013e1914645, 2019. Madeleine Udell, Corinne Horn, Reza Zadeh, Stephen Boyd, et al. Generalized low rank models. Foundations and Trends\u00ae in Machine Learning , 9(1):1\u2013118, 2016. Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768 , 2020a. Yongjun Wang, Baiying Lei, Ahmed Elazab, Ee-Leng Tan, Wei Wang, Fanglin Huang, Xuehao Gong, and Tianfu Wang. Breast cancer image classification via multi-network features and dualnetwork orthogonal low-rank learning. IEEE Access , 8:27779\u201327792, 2020b. John Wright and Yi Ma. High-dimensional data analysis with low-dimensional models: Principles, computation, and applications . Cambridge University Press, 2022. Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. Nystr \u00a8omformer: A nyst \u00a8om-based algorithm for approximating self-attention. In Proceedings of the... AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence , volume 35, pp. 14138. NIH Public Access, 2021. Hongrun Zhang, Yanda Meng, Yitian Zhao, Yihong Qiao, Xiaoyun Yang, Sarah E Coupland, and Yalin Zheng. Dtfd-mil: Double-tier feature distillation multiple instance learning for histopathology whole slide image classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 18802\u201318812, 2022. Yangmuzi Zhang, Zhuolin Jiang, and Larry S Davis. Learning structured low-rank representations for image classification. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 676\u2013683, 2013. Xiaowei Zhou, Can Yang, Hongyu Zhao, and Weichuan Yu. Low-rank modeling and its applications in image analysis. ACM Computing Surveys (CSUR) , 47(2):1\u201333, 2014."
        },
        "Global registration of 3d point sets via lrs decomposition": {
            "authors": [],
            "url": "https://fusiello.github.io/papers/eccv16.pdf",
            "ref_texts": "30. Zhou, X., Yang, C., Zhao, H., Yu, W.: Low-rank modeling and its applications in image analysis. ACM Computing Surveys 47(2) (2014) 36:1{36:33",
            "ref_ids": [
                "30"
            ],
            "1": "A survey of such decompositions and a wide overview of available algorithms can be found in [30]."
        },
        "Robust absolute rotation estimation via low-rank and sparse matrix decomposition": {
            "authors": [],
            "url": "https://re.public.polimi.it/bitstream/11311/1188356/1/07_3dv14.pdf",
            "ref_texts": "[29] X. Zhou, C. Yang, H. Zhao, and W. Yu. Low-rank modeling and its applications in image analysis. CoRR , 2014.",
            "ref_ids": [
                "29"
            ],
            "1": "These two entangled problems of matrix completion andlow-rank recovery in the presence of outliers have been addressed only by a few, recent works [29].",
            "2": "[29] X."
        },
        "Comparison of matrix completion algorithms for background initialization in videos": {
            "authors": [],
            "url": "https://hal.science/hal-01227959/document",
            "ref_texts": "21. X. Zhou, C. Yang, H. Zhao, and W. Yu. Low-rank modeling and its applications in image analysis. ACM Computing Surveys (CSUR) , 47(2):36, 2014.",
            "ref_ids": [
                "21"
            ],
            "1": "Recently, several matrix completion algorithms have been proposed to deal with this challenge, and a complete review can be found in [21].",
            "2": "[21])."
        },
        "Real-time adaptive histogram min-max bucket (HMMB) model for background subtraction": {
            "authors": [],
            "url": "https://www.isical.ac.in/~ash/Sujoy_CSVT_2017.pdf",
            "ref_texts": "[24] X. Zhou, C. Yang, H. Zhao, and W. Yu, \u201cLow-rank modeling and its applications in image analysis,\u201d ACM Comput. Surv. , vol. 47, no. 2, pp. 36:1\u201336:33, Jan. 2014.",
            "ref_ids": [
                "24"
            ],
            "1": "Background models mainly consist of three different approaches: pixel based methods [2]\u2013[5], [13]\u2013[19], small region (consisting ofa few pixels) based methods [20]\u2013[23] and large region (consisting of a frame itself) based methods [8], [9], [24], [25].",
            "2": "In [24], a low-rank model (representing variables of interest as low-rank matrices) is formulated for BS by Zhou et al.",
            "3": "[24] X."
        },
        "Augmented Lagrangian alternating direction method for low-rank minimization via non-convex approximation": {
            "authors": [
                "Yongyong Chen"
            ],
            "url": "https://cyyhit.github.io/files/11-Augmented%20Lagrangian%20alternating%20direction%20method%20for%20low-rank.pdf",
            "ref_texts": "10. Zhou, X., Yang, C., Zhao, H., et al.: Low rank modeling and its applications in image analysis. ACM Comput. Surv. 47(2), 36",
            "ref_ids": [
                "10"
            ],
            "1": "The literature [10] first gave a comprehensive overview to the concept of low-rank modeling and summarized the models and algorithms for low-rank matrix recovery.",
            "2": "Our experiments employ both grayscale and color videos, which are different from the experiments in [1,10,28] and similar to the experiments in [34]."
        },
        "A review on low-rank models in data analysis": {
            "authors": [],
            "url": "https://www.aimspress.com/fileOther/PDF/BDIA/2380-6966_2016_2&3_139.pdf",
            "ref_texts": "[82] X. Zhou, C. Yang, H. Zhao and W. Yu, Low-rank modeling and its applications in image analysis, ACM Computing Surveys ,47(2014), p36.",
            "ref_ids": [
                "82"
            ],
            "1": "[82].",
            "2": "However, my review differs significantly from [82].",
            "3": ", tensor completion and recovery, multi-subsp ace models, and nonlinear models, while [82] mainly focuses on matrix completion and Robust PCA.",
            "4": "[82] X."
        },
        "Low-rank sparse decomposition of graph adjacency matrices for extracting clean clusters": {
            "authors": [],
            "url": "http://www.apsipa.org/proceedings/2018/pdfs/0001153.pdf",
            "ref_texts": "[16] X. Zhou, C. Yang, H. Zhao, and W. Yu, \u201cLow-rank modeling and its applications in image analysis,\u201d ACM Computing Surveys (CSUR) , vol. 47, no. 2, p. 36, 2015.1158Proceedings, APSIPA Annual Summit and Conference 2018 12-15 November 2018, Hawaii [17] Y. Peng, A. Ganesh, J. Wright, W. Xu, and Y. Ma, \u201cRasl: Robust alignment by sparse and low-rank decomposition for linearly correlated images,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence , vol. 34, no. 11, pp. 2233\u20132246, 2012.",
            "ref_ids": [
                "16",
                "17"
            ],
            "1": "[16] X.",
            "2": "1158Proceedings, APSIPA Annual Summit and Conference 2018 12-15 November 2018, Hawaii [17] Y."
        },
        "Reconstruction of undersampled big dynamic MRI data using non-convex low-rank and sparsity constraints": {
            "authors": [],
            "url": "https://www.mdpi.com/1424-8220/17/3/509/pdf",
            "ref_texts": "58. Zhou, X.; Yang, C.; Zhao, H.; Yu, W. Low-rank modeling and its applications in image analysis. ACM Comput. Surv. 2015 ,47, 36. [CrossRef]",
            "ref_ids": [
                "58"
            ],
            "1": "However, if the data are perturbed by high-level noise, it will be impossible to generate satisfactory reconstruction results since the traditional PCA could be easily corrupted by these gross errors [58]."
        },
        "ROMIR: Robust multi-view image re-ranking": {
            "authors": [],
            "url": "https://ses.library.usyd.edu.au/bitstream/handle/2123/27251/romir-robust-multi-9.12.pdf?sequence=1&isAllowed=y",
            "ref_texts": "[34] X. Zhou, C. Yang, H. Zhao, and W. Yu, \u201cLow-rank modeling and its applications in image analysis,\u201d ACM Comput. Surv. , vol. 47, no. 2, pp.",
            "ref_ids": [
                "34"
            ],
            "1": "In real-world applications, high-dimensional data are usually corrupted with redundant noise and approximately drawn from an underlying low-rank subspace [33], [34].",
            "2": "[34] X."
        },
        "System, method and computer-accessible medium for diffusion imaging acquisition and analysis": {
            "authors": [],
            "url": "https://patentimages.storage.googleapis.com/b5/1b/ff/eb6c4154afe152/US10307139.pdf",
            "ref_texts": " [40 ] Smith S . M . , Jenkinson M . , Johansen Berg H . , Rueckert 40 111 : 16574 9 . D . , Nichols T . E . , Mackay C . E . , Watkins K . E . , Ciccarelli [55 ] Lepore N . , Brun C . , Descoteaux M . , Chou Y . Y . , O . , Zaheer Cader M . , Matthes P . M . , Behrens T . E . Zubicaray G . , McMahon K . , Wright M . J . , Martin N . G . , Tract based spatial statistics : Voxel wise analysis of Gee J . C . , Thompson P . M . A Multivariate Group wise multi subject diffusion data NeuroImage . 2006 ; 31 : 1487 Genetic Analysis of White Matter Integrity using Orien 1505 . 45 tation Distribution Functions in Computational Diffusion [41 ] Jbabdi S . , Behrens T . E . , Smith S . M . Crossing fibres in MRI workshop at MICCAI (Beijing , China ) 2010 . tract based spatial statistics NeuroImage . 2010 ; 46 : 249 . [56 ] Zhou X . , Yang C . , Zhao H . , Yu W . Low Rank Modeling 56 . and its Applications in Image Analysis arXiv preprint . [42 ] Zhang H . , Awate S . P . , Das R . S . , Woo J . H . , Melhem 2014 ; arXiv : 1401 . 3409 . E . R . , Gee J . C . , Yushkevich P . A . A tract specific frame 50 [57 ] Lin Z . A review on low rank models in data analysis work for white matter morphometry combining macro Big Data and Inf . Anal . 2016 ; epub ahead of print : "
        },
        "Real-time record sensitive background classifier (RSBC)": {
            "authors": [
                "Sujoy Madhab"
            ],
            "url": "https://www.isical.ac.in/~ash/Sujoy_RBSC_2018.pdf",
            "ref_texts": "411\u2013423 . St-Charles, P.-L. , & Bilodeau, G.-A. (2014). Improving background subtraction using local binary similarity patterns. In Proceedings of the IEEE winter conference on applications of computer vision (WACV) (pp. 509\u2013515). IEEE . St-Charles, P.-L. , Bilodeau, G.-A. , & Bergevin, R. (2015). SuBSENSE: a universal change detection method with local adaptive sensitivity. IEEE Transactions on Image Processing, 24 , 359\u2013373 . Stauffer, C. , & Grimson, W. E. L. (1999). Adaptive background mixture models for real-time tracking. In Proceedings of the IEEE international conference on computer vision and pattern recognition (CVPR): 2 (pp. 246\u2013252). IEEE . Szwoch, G. , Ellwart, D. , & Czy \u02d9zewski, A. (2016). Parallel implementation of background subtraction algorithms for real-time video processing on a supercomputer platform. Journal of Real-Time Image Processing, 11 , 111\u2013125 . Tuzel, O. , Porikli, F. , & Meer, P. (2005). A Bayesian approach to background modeling. In Proceedings of the IEEE computer society conference on computer vision and pattern recognition (CVPR\u201905)-workshops (p. 58). IEEE . Van Droogenbroeck, M. , & Paquot, O. (2012). Background subtraction: experiments and improvements for Vibe. In Proceedings of the IEEE computer society conference on computer vision and pattern recognition workshops (pp. 32\u201337). IEEE . Wang, H. , & Shi, L. (2016). Foreground model for background subtraction with blind updating. In Proceedings of the IEEE international conference on signal and image processing (ICSIP) (pp. 74\u201378). IEEE . Weng, T. L. , Wang, Y. Y. , Ho, Z. Y. , & Sun, Y. N. (2010). Weather-adaptive flying target detection and tracking from infrared video sequences. Expert Systems with Applications, 37 , 1666\u20131675 . Wren, C. , Azarbayejani, A. , Darrell, T. , & Pentland, A. (1997). Pfinder: real-time tracking of the human body. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19 , 780\u2013785 . Zhao, Z. , Bouwmans, T. , Zhang, X. , & Fang, Y. (2012). A fuzzy background modeling approach for motion detection in dynamic backgrounds. In Multimedia and signal processing (pp. 177\u2013185). Springer . Zhong, Z. , Zhang, B. , Lu, G. , Zhao, Y. , & Xu, Y. (2017). An adaptive background modeling method for foreground segmentation. IEEE Transactions on Intelligent Transportation Systems, 18 , 1109\u20131121 . Zhou, X. , Yang, C. , Zhao, H. , & Yu, W. (2014). Low-rank modeling and its applications in image analysis. ACM Computing Surveys, 47 , 36:1\u201336:33 . Zivkovic, Z. , & van der Heijden, F. (2006). Efficient adaptive density estimation per image pixel for the task of background subtraction. Pattern Recognition Letters, "
        },
        "Gamification of EcoDriving Behaviours through Intelligent Management of dynamic car and driver information": {
            "authors": [],
            "url": "https://www.scitepress.org/PublishedPapers/2017/88623/88623.pdf",
            "ref_texts": "60. Zhou, X., Yang, C., Zhao, H., Yu, W.: Low-rank modeling and its applications in image analysis. ACM Computing Surveys (CSUR) 47(2), 36 (2015)123Gamification of EcoDriving Behaviours through Intelligent Management of Dynamic Car and Driver Information 123",
            "ref_ids": [
                "60"
            ],
            "1": "Recently, novel signal processing tools such as matrix completion (MC) have been widely adopted for mitigating similar effects in many research fields such as computer vision [60] or signal reconstruction from partial observations."
        },
        "Background subtraction via truncated nuclear norm minimization": {
            "authors": [
                "A A"
            ],
            "url": "http://www.apsipa.org/proceedings/2017/CONTENTS/papers2017/13DecWednesday/WP-04/WP-04.4.pdf",
            "ref_texts": "[12] Zhou, Xiaowei, et al. \"Low-rank modeling and its applications in image analysis.\" ACM Computing Surveys (CSUR) 47.2 ",
            "ref_ids": [
                "12"
            ],
            "1": "Statistical methods [7, 8], neural network methods [4, 12], sparse methods [9] , robust subspace methods \n[2, 11, 12], etc.",
            "2": "[12] Zhou, Xiaowei, et al."
        },
        "Matrix Completion under Gaussian Models Using MAP and EM Algorithms.": {
            "authors": [],
            "url": "http://www.jocm.us/uploadfile/2017/0331/20170331052046116.pdf",
            "ref_texts": "[1] X. Zhou, C. Yang, H. Zhao, and W. Yu, \u201cLow-rank modeling and its applicati ons in image analysis,\u201d ACM Computing Surveys , vol. 47, no. 2, 2015. ",
            "ref_ids": [
                "1"
            ],
            "1": "[1], [2].",
            "2": "R EFERENCES\n \n[1] X."
        },
        "A model-based approach of foreground region of interest detection for video codecs": {
            "authors": [],
            "url": "https://www.mdpi.com/2076-3417/9/13/2670/pdf",
            "ref_texts": "11. Zhou, X.; Yang, C.; Zhao, H.; Yu, W. Low-Rank Modeling and Its Applications in Image Analysis. CoRR",
            "ref_ids": [
                "11"
            ],
            "1": "Principle Component Analysis (PCA) has been proposed as a state-of-the-art method for low-rank matrix recovery [11].",
            "2": "Another typical method is named as sparse and low-rank decomposition [11], in which moving objects are computed as the sparse matrix."
        },
        "Computational techniques to recover missing gene expression data": {
            "authors": [
                "Tanveer Riaz"
            ],
            "url": "https://soar.wichita.edu/bitstream/handle/10057/15897/Fraidouni_2018.pdf?sequence=1&isAllowed=y",
            "ref_texts": "[17] X. Zhou, C. Yang, H. Zhao and W. Yu, \"Low -Rank Modeling an d Its Applications in Image Analysis,\" ACM Computing Surveys, vol. 47, no. 2, ",
            "ref_ids": [
                "17"
            ],
            "1": "Low rank MC problem can be seen in different practical contexts such as image processing [17], machine learning [18] and bioinformatics \n[19].",
            "2": "[17] X."
        },
        "Estimation and Inference in Generalized Low-Rank Factorization Models for Noisy Matrix Completion": {
            "authors": [],
            "url": "https://escholarship.org/content/qt36k0x6ck/qt36k0x6ck.pdf",
            "ref_texts": "[57] Xiaowei Zhou, Can Yang, Hongyu Zhao, and Weichuan Yu. Low-rank modeling and its applications in image analysis. ACM Comput. Surv. , 47(2), dec 2014.",
            "ref_ids": [
                "57"
            ],
            "1": "We can see numerous examples of this kind of data in recommender systems [34, 44], such as the well-known Netflix problem, sensor network localization [47], traffic monitoring [20], image compression [57], multi-task learning [3, 43], and so forth."
        },
        "Influence of Sorting Measures on Similar Segment Grouping based Denoising Algorithms": {
            "authors": [],
            "url": "https://www.researchsquare.com/article/rs-2406262/latest.pdf",
            "ref_texts": "[9] Zhou, X., Yang, C., Zhao, H., Yu, W.: Lowrank modeling and its applications in image analysis. ACM Computing Surveys, 47(2) 36",
            "ref_ids": [
                "9"
            ],
            "1": "Another approach of the segment based denoising is the low-rank matrix approximation method in which the non-locally distributed similar segments are 1 Springer Nature 2021 L ATEX template identified and grouped to form sub-images with low-rank characteristics [8] [9].",
            "2": "Springer (2011) ISBN: 978-1-4471-5836-3\n[9] Zhou, X."
        },
        "Interference management for Device-to-Device communications in 5G networks": {
            "authors": [],
            "url": "https://scholarworks.aub.edu.lb/bitstream/handle/10938/23083/ed-128.pdf?sequence=1",
            "ref_texts": "[35] X. Zhou, C. Yang, H. Zhao, and W. Yu, \\Low-rank modeling and its applications in image analysis,\" ACM Computing Surveys (CSUR) , vol. 47, no. 2, p. 36, 2015.",
            "ref_ids": [
                "35"
            ],
            "1": "The direct approach to recovering a low-rank matrix is then to minimize the rank of the matrix with certain constraints that make the estimated matrix consistent with the original data [35].",
            "2": "134\n[35] X."
        },
        "Interpolation and fitting on Riemannian manifolds.": {
            "authors": [],
            "url": "https://dial.uclouvain.be/pr/boreal/object/boreal%3A240620/datastream/PDF_01/view",
            "ref_texts": "[ZYZY15] Xiaowei Zhou, Can Yang, Hongyu Zhao, and Weichuan Yu. Low-rank modeling and its applications in image analysis. ACM Computing Surveys (CSUR) , 47(2):36, 2015.",
            "ref_ids": [
                "ZYZY15"
            ],
            "1": "Manopt, in particular, received positive return in many different topics of research, with applications in low-rank modelling in image analysis [ZYZY15], dimensionality reduction [CG15], phase retrieval [SQW17] or even 5G-like MIMO systems [YSZL16]."
        },
        "A Dynamical Low-Rank Algorithm for a Kinetic Model for Gas Mixtures Close to the Compressible Regime": {
            "authors": [],
            "url": "https://ifm.mathematik.uni-wuerzburg.de/~klingen/ewExternalFiles/Masterarbeit%20%20Kai%20Ulrich.pdf",
            "ref_texts": ""
        },
        "Empirical Evaluation of a Dimension-Reduction Method for Time-Series Prediction": {
            "authors": [
                "Mahsa Ghorbani"
            ],
            "url": "https://mountainscholar.org/bitstream/handle/10217/211783/Ghorbani_colostate_0053A_16130.pdf?sequence=1",
            "ref_texts": "[67] Xiaowei Zhou, Can Yang, Hongyu Zhao, and Weichuan Yu. Low-rank modeling and its applications in image analysis. ACM Computing Surveys (CSUR) , 47(2):36, 2015.",
            "ref_ids": [
                "67"
            ],
            "1": "The parameter \u03bbcontrols the rank of X, and the selection of \u03bbshould depend on the noise level [67].",
            "2": "The Proximal Gradient method is very useful to solve norm-regularized maximum-likelihood problems, while the Augmented Lagrangian method provides a powerful framework to solve convex problems with equality constraints [67]."
        },
        "Clutter Suppression in Ultrasound: Performance Evaluation of Low-Rank and Sparse Matrix Decomposition Methods": {
            "authors": [
                "Naiyuan Zhang"
            ],
            "url": "https://spectrum.library.concordia.ca/id/eprint/986837/1/Zhang_MASc_S2020.pdf",
            "ref_texts": "[83] X. Zhou, C. Yang, H. Zhao, and W. Yu, \u201cLow-rank modeling and its applications in image analysis,\u201d ACM Computing Surveys (CSUR) , vol. 47, no. 2, p. 36, 2015.",
            "ref_ids": [
                "83"
            ],
            "1": "Meanwhile, many classifications have been proposed [79, 82, 83, 84] according to linearity, convexity, number of subspaces, or number of addition matrices.",
            "2": "[83] X."
        },
        "Low-rank and Sparse based Representation Methods with the Application of Moving Object Detection": {
            "authors": [],
            "url": "https://era.library.ualberta.ca/items/f28c6a0a-0543-4889-8906-3e1dc6e08476/download/a910ebd7-8965-4b22-bd4a-b2479f06203f",
            "ref_texts": "[156] X. Zhou, C. Yang, H. Zhao, and W. Yu, \\Low-rank modeling and its applications in image analysis,\" ACM Computing Surveys (CSUR) , vol. 47, no. 2, p. 36, 2015. 9",
            "ref_ids": [
                "156"
            ],
            "1": "1 Low-Rank Approximation In the recent years, low-rank matrix recovery, which efiectively separates sparse outliers from corrupted observations, has been successfully applied to a variety of computer vision applications, such as face recognition [31], [84], [133], [136], image classiffcation [39], [53], [149], images alignment [92], image restoration and denoising [64], [73], [81], [148], [156], subspace clustering [79], [112], [128], data compression [21], [56], [146] and background subtraction [10], [11], [156].",
            "2": "16, 21{23, 28, 29, 32, 33, 69, 96, 109\n[156] X."
        },
        "Scaled matrix completion and cell deconvolution with NanoString data": {
            "authors": [],
            "url": "https://digital.lib.washington.edu/researchworks/bitstream/handle/1773/42982/Zhang_washington_0250O_19116.pdf?sequence=1",
            "ref_texts": "[38] X. Zhou, C. Yang, H. Zhao, and W. Yu. Low-Rank Modeling and Its Applications in Image Analysis. ArXiv e-prints , January 2014.",
            "ref_ids": [
                "38"
            ],
            "1": "Other applications of low-rank approximation include, but are not limited to hidden partitioning [35] and image compression [38].",
            "2": "[38] X."
        },
        "Nonparametric Bayesian Models for Signal Processing": {
            "authors": [
                "Caoyuan Li"
            ],
            "url": "https://opus.lib.uts.edu.au/bitstream/10453/142368/2/02whole.pdf",
            "ref_texts": "2.2 Matrix Factorization Approaches Using machine learning methods to ffnd the low-rank and/or sparse approximation of a given data matrix is a fundamental problem in many computer vision applications, for example, background/foreground separation. By casting the problem into the penalization of the regularization term, a number of efiorts have been devoted to applying convex or non-convex optimization methods to obtain the low rank and sparse components [71, 72, 73, 74]. For most of these convex or non-convex methods, one has to manually choose some regularization parameters to properly control the trade-ofi between the data fftting error and the matrix rank when noise is involved. However, due to the lack of noise variance and rank, it is often unrealistic to determine the optimal regularization parameters. Bayesian inference under probabilistic frameworks provides another essential principle to perform matrix factorization. Ding et al. [75] proposed a Bayesian robust principal component analysis (BRPCA) framework which infers an approximate representation for the noise statistics while simultaneously inferring the low rank and sparse components. However, this model is relatively complex, and the 12 intractable posteriors are inferred by Gibbs sampling. Aicher [76] later improved the parameter inference in [75] by using the factorized variational Bayesian (VB) principle. Wang et al. [77] proposed a Bayesian robust matrix factorization model for image and video analysis. The Gaussian noise model is replaced by a Laplace mixture in [77] to enhance model robustness. Similarly, a Bayesian formulation of hierarchical L1norm low-rank matrix factorization is presented in [78]. In addition, Zhao et al. [79] presented a generative robust PCA model under the Bayesian framework with data noise modeled as a mixture of Gaussians (MoG). A common issue of the above models is that the optimal rank of the low rank component has to be manually pre-determined, which potentially either over-ffts or under-ffts the data. Babacan et al. [80] proposed to employ the automatic relevance determination principle in sparse Bayesian learning to determine the optimal rank of the low rank component. Although these methods are successful in many areas including video processing, most of them simply ignore side information, or intrinsically, are not capable of exploiting it. On the other hand, many studies have indicated that kernelized matrix factorization to integrate side information, i.e., prior knowledge or data attributes for speciffc data, can signiffcantly improve the performance of information extraction or prediction [18, 16, 81, 82]. However, the inference of kernelized matrix factorization models using VB is still quite limited. Pork et al. [83] placed Gaussian-Wishart priors on mean vectors and precision matrices of Gaussian user and item factor matrices, such that the mean of each prior distribution is regressed on corresponding side information. They developed a VB algorithm to approximate the posterior distributions over user and item factor matrices with a Bayesian Cramer-Rao bound. Very recently, G\u007f onen and Kaski [84, 85] extended the kernelized matrix factorization with a full VB treatment and with an ability to work with multiple side information sources expressed as difierent kernels. However, this model focused speciffcally on 13 binary output matrices for multi-label classiffcation. Moreover, both models in [84, 85], and [83] lack of robustness, which is required to handle the sparse component or outliers in many real-world applications. In order to incorporate the document labels into the matrix factorization model to improve word representations for the text classiffcation task, Yang et al. [15] constructed two co-occurrence matrices: a word-context matrix and a word-label matrix. They then deffned an objective function which penalised the weighting function related to the latter matrix. Lan et al. [16] proposed a kernel low-rank decomposition formulation which represented the entries using the Nystr\u007f om sampling method. The convex objective function to integrate the side information in [16] is based on the Frobenius norm, the same as in [15], to measure the closeness between two matrices. Narita et al. [17] introduced two regularization approaches using graph Laplacians induced from the side information of relationships among data, one for moderately sparse cases and the other for extremely sparse cases. They presented two kinds of iterative algorithms for approximate solutions: one based on an EM-like algorithm which is stable but not so scalable, and the other based on gradient based optimization which is applicable to large scale datasets. The matrix factorization model for recommendation in social rating networks in [18] incorporates not only trust but also distrust relationships aiming to improve the quality of recommendations and mitigate the data sparsity and cold-start issues. The social relationships are absorbed into the convex optimization problem with a standard gradient descent method to ffnd the latent feature matrices of users and items in an iterative procedure. Fithian and Mazumder [19] explored a general statistical framework for low-rank modeling of a matrix with missing data and side information, based on convex optimization with a generalized nuclear norm penalty. An augmented Lagrange multiplier (ALM) and the alternating direction method of multipliers (ADMM) were employed to perform a robust principal component analysis with side information 14 in [20, 21]. Nguyen and Lee [22] proposed to incorporate prior anatomical information into PET reconstruction using a nonlocal regularization method. To accelerate convergence, they used the complete-data ordered subsets expectation maximization (COSEM) algorithm, which is free from a seriously inconvenient user-speciffc relaxation schedule required in conventional relaxed ordered-subsets (OS) methods. In addition, the stochastic gradient decent (SGD) method was utilized in [23] to learn the latent matrix, where the interactions between user/item and ffeld can be captured. Huang et al. [24] explored an alternating gradient descent (AGD) method to perform matrix completion with side information. As for the matrix completion problem, singular value decomposition is another popular method [86, 87]. While the aforementioned methods incorporated explicit side information in the low-rank matrix factorization setting, Shah et al. [81] designed a method to make use of the implicit information, i.e., via random walks on graphs. They casted the problem as factoring a nonlinear transform of the (partially) observed matrix and developed a coordinate descent based algorithm for the same. Side information can also be presented and utilized in other manners. Choo el al. [88] proposed a weakly supervised nonnegative matrix factorization (NMF) that exibly accommodates diverse forms of prior information via regularization in clustering applications. Some others assumed to know part entries of the factor matrices and used a parameterization scheme to take them into account of the NMF problem. For example, Delmaire et al. [89] presented an informed NMF model in which some entries of a factor matrix are to be provided or bounded by experts and update rules were proposed for that purpose. Dorfier et al. [90] further assumed that the columns of a matrix factor have a sparse decomposition along with a known dictionary. Besides, the idea of a convex NMF in [91] is similar to [89, 90]. However, the update rules are derived by the majorization-minimization algorithm. In another family of sparse representation [91], the kernel matrix is deffned based on sample15 sample similarity, or sample-basis-vector similarity. For most of these convex or non-convex methods to utilize the side information, one has to manually choose some regularization parameters to properly control the tradeofi between the data fftting error and the matrix rank when noise is involved. However, due to the lack of the noise variance and the rank, it is often unrealistic to determine the optimal regularization parameters. Probabilistic frameworks provide another essential principle to perform kernelized matrix factorization. Since the matrix's inner product in probabilistic PCA has an interpretation as a Gaussian process (GP) covariance matrix [81], a number of studies have been devoted to nonlinear probabilistic matrix factorization using GP latent variable models (LVM). The covariance matrix of GP-LVM was replaced by a covariance function of GP containing the side information in [92]. Inspired by this idea, Zhou et al. [82] explicitly proposed the kernelized probabilistic matrix factorization (KPMF) model, which integrated the side information through kernel matrices over rows and columns, respectively. KPMF models a matrix as the product of two latent matrices, which are sampled from two difierent zero-mean Gaussian processes. The covariance functions of the GPs are derived from the side information, and encode the covariance structure across rows and across columns, respectively. Adams et al. [93] extended this framework for incorporating side information by coupling together multiple dependent matrix factorization problems via Gaussian process priors. They replaced scalar latent features with functions that vary over the space of side information. However, GP does not scale with big data due to its cubic time complexity. Le et al. [94] addressed these eflciency issues by proposing local GP kernel functions in the context of modeling road network topology. In order to achieve automatic balance between the matrix rank and the fftting er16 ror, Bayesian methods have been recently employed to learn the KMF model parameters. Porteous et al. [95] introduced a nonparametric mixture model for the prior of the rows and columns of the factored matrices that gives a difierent regularization for each latent class. Besides providing a richer prior, the posterior distribution of mixture assignments inferred by Gibbs sampling reveals the latent classes [95]. This Bayesian approach outperforms other matrix factorization techniques even when using fewer dimensions. Instead of using a nonparametric mixture model for the user and item, Liu et al. [96] proposed two recommendation approaches fusing social relations and item contents with user ratings. One generates user hyperparameters separately for every user vector, while another generates both user hyperparameters and item hyperparameters separately. Xu et al. [97] employed a co-clustering technique to integrate the side information of the user community and item group into the Bayesian matrix factorization. Each community-group pair corresponds to a co-cluster, which is characterized by a rating distribution in exponential family and a topic distribution. Yang and Wang [98] presented a Bayesian hierarchical kernelized probabilistic matrix factorization for matrix-variate normal data with dependent structures induced by rows and columns. The learned the model explicitly captures the underlying correlation among the rows and the columns. The parameters in these models [95, 96, 97, 98] are all inferred using Gibbs sampling. Zakeri et al. [99] extended the Markov Chain Monte Carlo (MCMC) method to factorize a sparsely fflled gene-phenotype matrix with genomic and phenotypic side information, where the objective is to make non-trivial predictions for genes for which no previous disease association is known. In comparison with MCMC sampling methods, variational Bayesian (VB) inference exhibits much lower computational complexity and has been broadly applied to infer the posterior in numerous probabilistic models. However, inference of kernelized matrix factorization models using VB is still quite limited. Pork et al. [83]",
            "ref_ids": [
                "75",
                "76",
                "75",
                "77",
                "77",
                "78",
                "79",
                "80",
                "83",
                "83",
                "15",
                "16",
                "16",
                "15",
                "17",
                "18",
                "19",
                "22",
                "23",
                "24",
                "81",
                "88",
                "89",
                "90",
                "91",
                "91",
                "81",
                "92",
                "82",
                "93",
                "94",
                "95",
                "95",
                "96",
                "97",
                "98",
                "99",
                "83"
            ],
            "1": "UNIVERSITY OF TECHNOLOGY SYDNEY Faculty of Engineering and Information Technology Nonparametric Bayesian Models for Signal Processing by Caoyuan Li A Thesis Submitted in Partial Fulfillment of the Requirements for the Degree Doctor of Philosophy Sydney, Australia 2019 Certiffcate of Authorship/Originality I certify that this thesis has been written by me.",
            "2": "Caoyuan Li Sydney, Australia, 2019.",
            "3": "4899-4911, Oct.",
            "4": "2019 J-2.",
            "5": "The Fifth International Conference on Machine Learning, Optimization, and Data Science , September 10-13, 2019 Contents Certiffcate ii Abstract iii Dedication v Acknowledgments vi List of Publications viii List of Figures xiii List of Tables xvii Notation xviii 1 Introduction 1\n1.",
            "6": "17\n3 Image denoising based on nonlocal Bayesian singular value thresholding and Stein's unbiased risk estimator 23\n3.",
            "7": "23\n3.",
            "8": "76\n5.",
            "9": "79\n5.",
            "10": "79\n5.",
            "11": "81\n5.",
            "12": "82\n6 Bayesian nonnegative matrix factorization with Dirichlet process mixtures 83\n6.",
            "13": "83\n6.",
            "14": "91\n6.",
            "15": "92\n6.",
            "16": "94\n6.",
            "17": "97\n6.",
            "18": "Rows from top to bottom describe the comparison of noise estimation results for low (5 \u0014\u001b\u001415), moderate (45\u0014\u001b\u001455) and severe (90 \u0014\u001b\u0014100) levels of noise, respectively.",
            "19": "(a) Original image, (b) noisy image (PSNR=14 :12 dB), (c) BM3D (PSNR=26 :16 dB), (d) WNNM (PSNR= 26:23 dB), (e) RMMM (PSNR= 25:87 dB), and (f) BSSVT (PSNR= 26:40 dB) .",
            "20": "(a) Original image, (b) noisy image (PSNR= 8 :10 dB), (c) BM3D\n(PSNR=19:85 dB), (d) WNNM (PSNR= 20:82 dB), (e) RMMM\n(PSNR= 20 :35 dB), and (f) BSSVT (PSNR= 21:31 dB) .",
            "21": "(a) BSVT-BM3D (PSNR=20:90 dB), (b) BSVT-WNNM (PSNR=20 :97 dB), (c) BSVT-BM3D (PSNR=17 :81 dB), (d) BSVT-WNNM (PSNR=17:85 dB).",
            "22": "(a) Original image, (b) Noisy image (PSNR= 14:12 dB), (c) BM3D (PSNR= 22:42 dB), (d) WNNM (PSNR= 22:50 dB), (e) BPFA (PSNR= 23:08 dB), and (f) KSBMF (PSNR= 23:11 dB).",
            "23": "(a) Original image, (b) Noisy image (PSNR= 8:10 dB), (c) BM3D (PSNR= 20:00 dB), (d) WNNM (PSNR= 19:05 dB), (e) BPFA (PSNR= 19:70 dB), and (f) KSBMF (PSNR= 20:21 dB).",
            "24": "(c) BPFA\n(PSNR=22:30 dB).",
            "25": "(d) GSR (PSNR=22 :66dB).",
            "26": "(c) BPFA\n(PSNR=29:76 dB).",
            "27": "(d) GSR (PSNR=30 :76dB).",
            "28": "(e) TSLRA (PSNR=34:94 dB).",
            "29": "(e) TSLRA\n(PSNR=32:93 dB).",
            "30": "(d) GSR (PSNR=33 :24 dB).",
            "31": "(e) TSLRA\n(PSNR=22:90 dB).",
            "32": "75\n5.",
            "33": "The video sequence contains 520 frames of size 320 \u0002240 pixels, and the results for frame 260 are shown.",
            "34": "80 xvi 5.",
            "35": "80\n6.",
            "36": "91\n6.",
            "37": "93\n6.",
            "38": "96\n6.",
            "39": "97\n6.",
            "40": "98\n6.",
            "41": "98\n6.",
            "42": "99 List of Tables 3.",
            "43": "81\n6.",
            "44": "94\n6.",
            "45": "94 Nomenclature and Notation Example Description R The set of reals N the set of natural numbers E[\u0001] expection of a random variable h\u0001i expection of a random variable Y2Rm\u0002nBold and capitalized letters denote random matrices u Characters in bold denotes random vectors This notation is also used to denote collections of random variables am\u0001 m-th row of a matrix a\u0001n n-th column of a matrix y Characters in italic denote random scalars diag(x) Diagonal matrix with the values of vector xon the diagonal.",
            "46": ", prior knowledge or data attributes for speciffc data, into the factorization model to improve information extraction or prediction [15, 16, 17, 18, 19, 20, 21, 22, 23, 24].",
            "47": "[75] proposed a Bayesian robust principal component analysis (BRPCA) framework which infers an approximate representation for the noise statistics while simultaneously inferring the low rank and sparse components.",
            "48": "Aicher [76] later improved the parameter inference in [75] by using the factorized variational Bayesian (VB) principle.",
            "49": "[77] proposed a Bayesian robust matrix factorization model for image and video analysis.",
            "50": "The Gaussian noise model is replaced by a Laplace mixture in [77] to enhance model robustness.",
            "51": "Similarly, a Bayesian formulation of hierarchical L1norm low-rank matrix factorization is presented in [78].",
            "52": "[79] presented a generative robust PCA model under the Bayesian framework with data noise modeled as a mixture of Gaussians (MoG).",
            "53": "[80] proposed to employ the automatic relevance determination principle in sparse Bayesian learning to determine the optimal rank of the low rank component.",
            "54": ", prior knowledge or data attributes for speciffc data, can signiffcantly improve the performance of information extraction or prediction [18, 16, 81, 82].",
            "55": "[83] placed Gaussian-Wishart priors on mean vectors and precision matrices of Gaussian user and item factor matrices, such that the mean of each prior distribution is regressed on corresponding side information.",
            "56": "Moreover, both models in [84, 85], and [83] lack of robustness, which is required to handle the sparse component or outliers in many real-world applications.",
            "57": "[15] constructed two co-occurrence matrices: a word-context matrix and a word-label matrix.",
            "58": "[16] proposed a kernel low-rank decomposition formulation which represented the entries using the Nystr\u007f om sampling method.",
            "59": "The convex objective function to integrate the side information in [16] is based on the Frobenius norm, the same as in [15], to measure the closeness between two matrices.",
            "60": "[17] introduced two regularization approaches using graph Laplacians induced from the side information of relationships among data, one for moderately sparse cases and the other for extremely sparse cases.",
            "61": "The matrix factorization model for recommendation in social rating networks in [18] incorporates not only trust but also distrust relationships aiming to improve the quality of recommendations and mitigate the data sparsity and cold-start issues.",
            "62": "Fithian and Mazumder [19] explored a general statistical framework for low-rank modeling of a matrix with missing data and side information, based on convex optimization with a generalized nuclear norm penalty.",
            "63": "Nguyen and Lee [22] proposed to incorporate prior anatomical information into PET reconstruction using a nonlocal regularization method.",
            "64": "In addition, the stochastic gradient decent (SGD) method was utilized in [23] to learn the latent matrix, where the interactions between user/item and ffeld can be captured.",
            "65": "[24] explored an alternating gradient descent (AGD) method to perform matrix completion with side information.",
            "66": "[81] designed a method to make use of the implicit information, i.",
            "67": "[88] proposed a weakly supervised nonnegative matrix factorization (NMF) that exibly accommodates diverse forms of prior information via regularization in clustering applications.",
            "68": "[89] presented an informed NMF model in which some entries of a factor matrix are to be provided or bounded by experts and update rules were proposed for that purpose.",
            "69": "[90] further assumed that the columns of a matrix factor have a sparse decomposition along with a known dictionary.",
            "70": "Besides, the idea of a convex NMF in [91] is similar to [89, 90].",
            "71": "In another family of sparse representation [91], the kernel matrix is deffned based on sample15 sample similarity, or sample-basis-vector similarity.",
            "72": "Since the matrix's inner product in probabilistic PCA has an interpretation as a Gaussian process (GP) covariance matrix [81], a number of studies have been devoted to nonlinear probabilistic matrix factorization using GP latent variable models (LVM).",
            "73": "The covariance matrix of GP-LVM was replaced by a covariance function of GP containing the side information in [92].",
            "74": "[82] explicitly proposed the kernelized probabilistic matrix factorization (KPMF) model, which integrated the side information through kernel matrices over rows and columns, respectively.",
            "75": "[93] extended this framework for incorporating side information by coupling together multiple dependent matrix factorization problems via Gaussian process priors.",
            "76": "[94] addressed these eflciency issues by proposing local GP kernel functions in the context of modeling road network topology.",
            "77": "In order to achieve automatic balance between the matrix rank and the fftting er16 ror, Bayesian methods have been recently employed to learn the KMF model parameters.",
            "78": "[95] introduced a nonparametric mixture model for the prior of the rows and columns of the factored matrices that gives a difierent regularization for each latent class.",
            "79": "Besides providing a richer prior, the posterior distribution of mixture assignments inferred by Gibbs sampling reveals the latent classes [95].",
            "80": "[96] proposed two recommendation approaches fusing social relations and item contents with user ratings.",
            "81": "[97] employed a co-clustering technique to integrate the side information of the user community and item group into the Bayesian matrix factorization.",
            "82": "Yang and Wang [98] presented a Bayesian hierarchical kernelized probabilistic matrix factorization for matrix-variate normal data with dependent structures induced by rows and columns.",
            "83": "The parameters in these models [95, 96, 97, 98] are all inferred using Gibbs sampling.",
            "84": "[99] extended the Markov Chain Monte Carlo (MCMC) method to factorize a sparsely fflled gene-phenotype matrix with genomic and phenotypic side information, where the objective is to make non-trivial predictions for genes for which no previous disease association is known.",
            "85": "[83]\n17 placed Gaussian-Wishart priors on mean vectors and precision matrices of Gaussian user and item factor matrices, such that the mean of each prior distribution is regressed on corresponding side information.",
            "86": "3 Non-Negative Matrix Factorization Methods Following Lee and Seung's seminal paper on NMF published in Nature [103] in 1999, these authors later presented two further algorithms based on multiplicative updates to minimize the cost functions based on the Frobenius norm and generalized KL-divergence, respectively [104].",
            "87": "Several extensions of [104] either modifying the cost function or update rule, or imposing extra constraints, have been proposed to avoid degrading 18 the performance.",
            "88": "To take advantage of conjugate distributions for more straightforward Bayesian inferences under a nonnegative constraint, a Poisson likelihood or noise function 19 accompanied by a Gamma prior for UandVis one of the most popular Bayesian NMF models [112].",
            "89": ", image restoration [112], recommendation systems [113], audio source separation [114], and speech enhancement [115].",
            "90": "Since an exponential distribution can be viewed as a special case of the Gamma distribution, Vincent and Hugo [116] used the former to replace the latter to couple with a Poisson likelihood.",
            "91": "Moreover, as pointed out by Chien and Yang [117], some dependency of the variational lower bound on model parameters was ignored in the original Bayesian Poisson-Gamma NMF model in [112], so the inferred parameters did not reach the true optimum of the variational objective.",
            "92": "[118] developed a variational Bayesian NMF model with Gaussian likelihood, and a truncated normal distribution as the prior of factors by truncating all negative entries and renormalizing the integral to unity.",
            "93": "Since both exponential and Gaussian distributions belong to the exponential family, the truncated normal can be replaced by exponential distribution to formulate a similar Bayesian NMF update formula [119, 120].",
            "94": "In order to integrate prior knowledge about the factor 20 matrices, suitable prior distributions, like Gaussian processes [121] and Gamma chain priors [122], have also been incorporated into the models.",
            "95": "For example, physiological signals including electroencephalogram (EEG) and electromyogram (EMG) are weak bioelectric recordings contaminated by white Gaussian noise, motion artefact, cross-talk, power line interference, as well as spurious background spikes [123, 124].",
            "96": "[95] assumed that there are latent classes for the entities of Yand regularization should be performed per class.",
            "97": "To this end, they used a Dirichlet process mixture to automatically prune the clusters of latent vectors which dominate the posterior in a collaborative ffltering task [95].",
            "98": "However, the nonparametric Bayesian approach works on latent vectors in [95, 132] rather than the noise term in the proposed model.",
            "99": ", Dirichlet process mixtures, is utilized to determine the required number of Gaussian compo22 nents.",
            "100": "23 Chapter 3 Image denoising based on nonlocal Bayesian singular value thresholding and Stein's unbiased risk estimator 3.",
            "101": "The main contributions are summarized as follows: (a) A hybrid nonlocal image 24 blind denoising framework is formed which exploits both Bayesian low-rank approximation and Stein's unbiased risk estimation.",
            "102": "15) q(!jY;r)\u0018Gamma(fi;ff ): (3.",
            "103": "16)\n29 Here,vMF (\u0001) denotes the von Mises-Fisher distribution [141], tN(\u0016;\u001b2Ir;Lr) is the truncated normal distribution with support Lrand Gamma(fi;ff ) denotes the gamma distribution with shape fiand rateff.",
            "104": "16) are given by FU= ^!YcVrcDr; (3.",
            "105": "17) FV= ^!Y>cUrcDr; (3.",
            "106": "18)\n\u0016= diag(cVr>Y>cUr); (3.",
            "107": "19)\n\u001b2= ^!\u00001; (3.",
            "108": "22) whereb!denotes the expectation of !with respect to q(!) and similarly for the other variables.",
            "109": "17)-(3.",
            "110": "22) until convergence, which in turn requires iterative evaluation of the moments of the distributions (3.",
            "111": "16): cUr=UFUG(n;DFU)V> FU; (3.",
            "112": "23) cVr=UFVG(m;DFV)V> FV; (3.",
            "113": "24) c\u0015r=\u0016+\u001b\u0010(\u0016;\u001b ); (3.",
            "114": "17)-(3.",
            "115": "22) and (3.",
            "116": "23)-(3.",
            "117": "setting of patch size and the number of similar patches recommended in previous studies [38, 5] is adopted here: the former is set to 6 \u00026, 7\u00027, 8\u00028 and 9\u00029, and the latter is set to 70, 90, 120 and 140 for \u001b= 20, 50, 70 and 100 respectively.",
            "118": "6 8 10 12 145101520Baboon 6 8 10 12 1451015Cameraman 6 8 10 12 1451015Barbara 46 48 50 52 54455055Estimated \n46 48 50 52 54455055\n46 48 50 52 54455055\n91 93 95 97 999095100\n91 93 95 97 999095100\n91 93 95 97 999095100 Figure 3.",
            "119": "Rows from top to bottom describe the comparison of noise estimation results for low (5 \u0014\u001b\u0014\n15), moderate (45 \u0014\u001b\u001455) and severe (90 \u0014\u001b\u0014100) levels of noise, respectively.",
            "120": "0080.",
            "121": "016 Normalized SUREBaboon =20 SURE MSE\n10-510-100.",
            "122": "24 28.",
            "123": "99 28.",
            "124": "74 22.",
            "125": "42 22.",
            "126": "50 22.",
            "127": "47 22.",
            "128": "83 Cameraman 30.",
            "129": "43 24.",
            "130": "99 25.",
            "131": "16 24.",
            "132": "93 25.",
            "133": "97 31.",
            "134": "19 26.",
            "135": "99 30.",
            "136": "33 24.",
            "137": "23 26.",
            "138": "88 34.",
            "139": "97 Parrot 29.",
            "140": "88 30.",
            "141": "66 24.",
            "142": "76 24.",
            "143": "69 24.",
            "144": "16 26.",
            "145": "23 25.",
            "146": "23 24.",
            "147": "29 24.",
            "148": "41 24.",
            "149": "32 24.",
            "150": "83 22.",
            "151": "15 22.",
            "152": "13 22.",
            "153": "23 31.",
            "154": "24 26.",
            "155": "81\n\u001b 70 100 schemes BM3D WNNM RNNM BSSVT BM3D WNNM RNNM BSSVT Bike 20.",
            "156": "87 18.",
            "157": "38 17.",
            "158": "83 18.",
            "159": "25 18.",
            "160": "63 Cameraman 22.",
            "161": "56 22.",
            "162": "72 22.",
            "163": "59 23.",
            "164": "30 19.",
            "165": "90 Einstein 25.",
            "166": "23 24.",
            "167": "97 24.",
            "168": "56 22.",
            "169": "79 21.",
            "170": "99 22.",
            "171": "68 Flower 23.",
            "172": "20 23.",
            "173": "47 22.",
            "174": "99 23.",
            "175": "77 20.",
            "176": "15 22.",
            "177": "23 24.",
            "178": "80 22.",
            "179": "90 22.",
            "180": "59 22.",
            "181": "34 23.",
            "182": "24 House 26.",
            "183": "98 27.",
            "184": "15 26.",
            "185": "58 23.",
            "186": "71 23.",
            "187": "27 22.",
            "188": "88 24.",
            "189": "15 Monarch 22.",
            "190": "99 23.",
            "191": "40 23.",
            "192": "14 23.",
            "193": "90 19.",
            "194": "82 20.",
            "195": "31 Parrot 22.",
            "196": "15 22.",
            "197": "39 22.",
            "198": "29 22.",
            "199": "87 19.",
            "200": "17 19.",
            "201": "70 19.",
            "202": "45 Peppers 23.",
            "203": "97 23.",
            "204": "63 23.",
            "205": "55 24.",
            "206": "82 21.",
            "207": "63 Starffsh 22.",
            "208": "83 22.",
            "209": "19 22.",
            "210": "00 19.",
            "211": "09 19.",
            "212": "17 19.",
            "213": "39 19.",
            "214": "22 Barbara 24.",
            "215": "56 24.",
            "216": "89 24.",
            "217": "25 23.",
            "218": "34 23.",
            "219": "18 23.",
            "220": "06 23.",
            "221": "98 Average 23.",
            "222": "37 23.",
            "223": "39 23.",
            "224": "16 23.",
            "225": "89 20.",
            "226": "93 20.",
            "227": "887 0.",
            "228": "893 0.",
            "229": "895 0.",
            "230": "896 0.",
            "231": "688 0.",
            "232": "877 0.",
            "233": "875 0.",
            "234": "755 0.",
            "235": "760 Einstein 0.",
            "236": "801 0.",
            "237": "807 0.",
            "238": "806 0.",
            "239": "802 0.",
            "240": "696 0.",
            "241": "699 0.",
            "242": "885 0.",
            "243": "885 0.",
            "244": "880 0.",
            "245": "716 0.",
            "246": "724 0.",
            "247": "876 0.",
            "248": "883 0.",
            "249": "884 0.",
            "250": "767 0.",
            "251": "776 0.",
            "252": "780 House 0.",
            "253": "812 0.",
            "254": "826 0.",
            "255": "828 Monarch 0.",
            "256": "923 0.",
            "257": "930 0.",
            "258": "912 0.",
            "259": "922 0.",
            "260": "824 0.",
            "261": "829 0.",
            "262": "792 0.",
            "263": "831 Parrot 0.",
            "264": "757 0.",
            "265": "750 0.",
            "266": "697 0.",
            "267": "758 Peppers 0.",
            "268": "890 0.",
            "269": "894 0.",
            "270": "878 0.",
            "271": "891 0.",
            "272": "786 0.",
            "273": "788 0.",
            "274": "790 Starffsh 0.",
            "275": "885 0.",
            "276": "887 0.",
            "277": "722 0.",
            "278": "909 0.",
            "279": "915 0.",
            "280": "910 0.",
            "281": "912 0.",
            "282": "762 0.",
            "283": "785 0.",
            "284": "784 0.",
            "285": "785 Average 0.",
            "286": "694 0.",
            "287": "588 0.",
            "288": "598 0.",
            "289": "399 0.",
            "290": "475 0.",
            "291": "495 Cameraman 0.",
            "292": "677 0.",
            "293": "679 0.",
            "294": "583 0.",
            "295": "695 0.",
            "296": "592 0.",
            "297": "617 0.",
            "298": "488 0.",
            "299": "592 0.",
            "300": "591 Flower 0.",
            "301": "623 0.",
            "302": "589 0.",
            "303": "689 0.",
            "304": "683 0.",
            "305": "489 0.",
            "306": "689 House 0.",
            "307": "778 0.",
            "308": "795 0.",
            "309": "791 0.",
            "310": "758 0.",
            "311": "766 0.",
            "312": "699 0.",
            "313": "770 0.",
            "314": "589 0.",
            "315": "695 Parrot 0.",
            "316": "693 0.",
            "317": "617 0.",
            "318": "599 0.",
            "319": "624 0.",
            "320": "524 0.",
            "321": "681 Starffsh 0.",
            "322": "623 0.",
            "323": "619 0.",
            "324": "694 0.",
            "325": "683 0.",
            "326": "683 0.",
            "327": "593 0.",
            "328": "616\n42 soft and hard thresholding.",
            "329": "(a) Original image, (b) noisy image (PSNR=14:12 dB), (c) BM3D (PSNR=26 :16 dB), (d) WNNM (PSNR= 26:23 dB), (e) RMMM (PSNR= 25:87 dB), and (f) BSSVT (PSNR= 26:40 dB)\n(a) Original (b) Noisy (c) BM3D\n (d) WNNM\n (e) RNNM\n (f) BSSVT Figure 3.",
            "330": "(a) Original image, (b) noisy image (PSNR= 8:10 dB), (c) BM3D (PSNR=19 :85 dB), (d) WNNM (PSNR= 20 :82 dB), (e) RMMM (PSNR= 20:35 dB), and (f) BSSVT (PSNR= 21:31 dB)\n44 inspection demonstrate that BSSVT yields better performance in comparison to the state-of-the-art methods.",
            "331": "(a) BSVT-BM3D (PSNR=20 :90 dB), (b) BSVT-WNNM (PSNR=20 :97 dB), (c) BSVT-BM3D (PSNR=17:81 dB), (d) BSVT-WNNM (PSNR=17 :85 dB).",
            "332": "In terms of computational eflciency, a desktop with a recent 2 :2 GHz CPU is employed to execute the code in Matlab 2017b (Mathworks, Massachusetts, US).",
            "333": "Both Bayesian inference and the SURE criterion are able to handle non-Gaussian noise [113, 150, 151, 149].",
            "334": "Many previous studies have indicated both cases result in inferior quality of the recovered data [152, 116].",
            "335": "In comparison with existing kernelized matrix factorization methods particularly the two VB realizations [83, 85], the proposed formulation implicitly estimates the rank of the matrix without requiring the prior knowledge on the rank of the matrix, which frees the user from extensive parameter-tuning and groundless attempts.",
            "336": "Moreover, the proposed model adopts difierent graphical model and priors as in [83].",
            "337": "In regard to the speciffc contribution in image processing, a large number of algorithms have been developed to exploit the nonlocal low-rank and global sparse properties for enhanced image recovery [153].",
            "338": "This sparse Bayesian learning formulation has been applied in compressive sensing and robust PCA [80, 134, 154].",
            "339": "15) where \u0000u\u0001j=hjiIM.",
            "340": "15) it is found that the posterior density of the jth column u\u0001jof 53 Uobeys the multivariate Gaussian distribution: q(u\u0001j) =N(u\u0001jjhu\u0001ji;\u0006u\u0001j); (4.",
            "341": "16) with mean and covariance \u0006u\u0001j= (h\u001bgi\u0001KUKU>+\u0000u\u0001j)\u00001; (4.",
            "342": "17) hu\u0001ji=h\u001bgi\u0001\u0006u\u0001jKUhg\u0001ji: (4.",
            "343": "18) Apparently, the posterior approximation of v\u0001jalso obeys the multivariate Gaussian distribution with the density denoted by q(v\u0001j) =N(v\u0001jjhv\u0001ji;\u0006v\u0001j); (4.",
            "344": "19) and the mean and covariance are given by \u0006v\u0001j= (h\u001bhi\u0001KVKV>+\u0000v\u0001j)\u00001; (4.",
            "345": "22) This is equivalent to q(j)/a\u00001+M+N\n2 jexp(\u00001\n2j(hu>\n\u0001ju\u0001ji+hv>\n\u0001jv\u0001ji+ 2b)): (4.",
            "346": "23) So the posterior distribution of jis a Gamma distribution with mean hji=2a+M+N\n2b+hu>\n\u0001ju\u0001ji+hv>\n\u0001jv\u0001ji: (4.",
            "347": "24)\n54 The required expectations here are found as hu>\n\u0001ju\u0001ji=hu\u0001ji>hu\u0001ji+ tr(\u0006u\u0001j); (4.",
            "348": "So far, graph kernel, difiusion kernel, commute time kernel, and regularized Laplacian kernel have been developed for utilizing the side information in recommender systems [82].",
            "349": "In the area of image processing, the kernel incorporating the local spatial smoothness of an image has been developed to improve image inpaitning [82].",
            "350": "In the past decade, many algorithms based on the nonlocal framework have been proposed for image restoration, most of which signiffcantly outperform methods utilizing image local properties [153].",
            "351": "18);\n12: Update Vusing Eq.",
            "352": "24);\n14: end for 15: Aggregate ^Xito form the denoised image ^x(d)\n16:end for Ensure: denoised image ^x(D)\n60\n4.",
            "353": "It should be noted that a straightforward extension of KSBMF to colour images often introduces perturbing colour artefacts [155, 156].",
            "354": "Due to the nature of the colour transform, the luminance component contains most of the valuable information about primitive image structures and has a higher SNR than the two chroma channels U and V [155, 156].",
            "355": "18);\n7:Update Vusing Eq.",
            "356": "24);\n9:Pre-completed image y(1)\n10:ford= 2 :Ddo 11: foreach patch yiiny(d)do 12: Cluster similar patches to matrix Yi;\n13: Repeat 1\u00008;\n14: end for 15: Aggregate ^Xito form the inpainted image ^x(d)\n16:end for Ensure: Inpainted image ^x(D)\n62 benchmark grayscale images, shown in Fig 3.",
            "357": "The setting of patch size and the number of similar patches recommended in previous studies [38, 5] are adopted: the former is set to 6 \u00026, 7\u00027, 8\u00028 and 9\u00029, and the latter is set to 70, 90, 120 and 140 for \u001b\u001420, 20\u0014\u001b\u001440, 40<\u001b\u001460 and\u001b>60 respectively.",
            "358": "Bayesian robust matrix factorization (BPFA) [157] shares a similar principle to KSBMF in that VB is used to 63 Table 4.",
            "359": "24 28.",
            "360": "89 28.",
            "361": "77 22.",
            "362": "42 22.",
            "363": "50 23.",
            "364": "08 23.",
            "365": "60 24.",
            "366": "99 25.",
            "367": "16 24.",
            "368": "19 26.",
            "369": "99 30.",
            "370": "33 24.",
            "371": "78 25.",
            "372": "92 27.",
            "373": "23 26.",
            "374": "88 34.",
            "375": "23 25.",
            "376": "88 30.",
            "377": "81 24.",
            "378": "76 24.",
            "379": "69 24.",
            "380": "75 25.",
            "381": "22 Peppers 31.",
            "382": "18 31.",
            "383": "16 26.",
            "384": "23 25.",
            "385": "27 24.",
            "386": "29 24.",
            "387": "41 24.",
            "388": "19 24.",
            "389": "83 22.",
            "390": "15 21.",
            "391": "90 22.",
            "392": "23 31.",
            "393": "16 31.",
            "394": "24 26.",
            "395": "77 Average 30.",
            "396": "96 30.",
            "397": "23 25.",
            "398": "89\n\u001b 70 100 schemes BM3D WNNM BPFA KSBMF BM3D WNNM BPFA KSBMF Bike 20.",
            "399": "95 18.",
            "400": "38 17.",
            "401": "83 18.",
            "402": "18 18.",
            "403": "68 Cameraman 22.",
            "404": "56 22.",
            "405": "72 22.",
            "406": "38 23.",
            "407": "27 19.",
            "408": "23 24.",
            "409": "97 24.",
            "410": "48 22.",
            "411": "79 21.",
            "412": "47 22.",
            "413": "73 Flower 23.",
            "414": "20 23.",
            "415": "47 23.",
            "416": "30 23.",
            "417": "82 20.",
            "418": "23 24.",
            "419": "80 25.",
            "420": "79 22.",
            "421": "90 22.",
            "422": "59 22.",
            "423": "43 23.",
            "424": "98 27.",
            "425": "15 26.",
            "426": "68 23.",
            "427": "71 23.",
            "428": "27 23.",
            "429": "00 24.",
            "430": "12 Monarch 22.",
            "431": "99 23.",
            "432": "40 23.",
            "433": "08 23.",
            "434": "98 19.",
            "435": "82 20.",
            "436": "36 Parrot 22.",
            "437": "15 22.",
            "438": "39 22.",
            "439": "35 22.",
            "440": "98 19.",
            "441": "17 19.",
            "442": "70 19.",
            "443": "35 Peppers 23.",
            "444": "97 23.",
            "445": "63 23.",
            "446": "48 24.",
            "447": "82 21.",
            "448": "65 Starffsh 22.",
            "449": "83 22.",
            "450": "28 22.",
            "451": "00 19.",
            "452": "05 19.",
            "453": "15 19.",
            "454": "17 19.",
            "455": "39 19.",
            "456": "16 Barbara 24.",
            "457": "56 24.",
            "458": "89 24.",
            "459": "14 23.",
            "460": "34 23.",
            "461": "18 22.",
            "462": "92 24.",
            "463": "07 Average 23.",
            "464": "37 23.",
            "465": "39 23.",
            "466": "13 23.",
            "467": "93 20.",
            "468": "93 20.",
            "469": "79 21.",
            "470": "887 0.",
            "471": "893 0.",
            "472": "896 0.",
            "473": "898 0.",
            "474": "688 0.",
            "475": "717 Cameraman 0.",
            "476": "877 0.",
            "477": "878 0.",
            "478": "755 0.",
            "479": "683 0.",
            "480": "762 Einstein 0.",
            "481": "801 0.",
            "482": "807 0.",
            "483": "803 0.",
            "484": "807 0.",
            "485": "696 0.",
            "486": "699 0.",
            "487": "885 0.",
            "488": "878 0.",
            "489": "883 0.",
            "490": "716 0.",
            "491": "724 0.",
            "492": "678 0.",
            "493": "876 0.",
            "494": "883 0.",
            "495": "885 0.",
            "496": "767 0.",
            "497": "776 0.",
            "498": "782 House 0.",
            "499": "812 0.",
            "500": "826 0.",
            "501": "830 Monarch 0.",
            "502": "923 0.",
            "503": "930 0.",
            "504": "914 0.",
            "505": "927 0.",
            "506": "824 0.",
            "507": "829 0.",
            "508": "790 0.",
            "509": "832 Parrot 0.",
            "510": "757 0.",
            "511": "750 0.",
            "512": "699 0.",
            "513": "762 Peppers 0.",
            "514": "890 0.",
            "515": "894 0.",
            "516": "876 0.",
            "517": "892 0.",
            "518": "786 0.",
            "519": "788 0.",
            "520": "794 Starffsh 0.",
            "521": "885 0.",
            "522": "890 0.",
            "523": "722 0.",
            "524": "909 0.",
            "525": "915 0.",
            "526": "907 0.",
            "527": "912 0.",
            "528": "762 0.",
            "529": "785 0.",
            "530": "773 0.",
            "531": "786 Average 0.",
            "532": "690 0.",
            "533": "588 0.",
            "534": "618 0.",
            "535": "399 0.",
            "536": "495 Cameraman 0.",
            "537": "677 0.",
            "538": "679 0.",
            "539": "696 0.",
            "540": "592 0.",
            "541": "617 0.",
            "542": "624 Einstein 0.",
            "543": "595 0.",
            "544": "592 0.",
            "545": "592 Flower 0.",
            "546": "623 0.",
            "547": "596 0.",
            "548": "689 0.",
            "549": "683 0.",
            "550": "495 0.",
            "551": "691 House 0.",
            "552": "778 0.",
            "553": "795 0.",
            "554": "794 0.",
            "555": "758 0.",
            "556": "766 0.",
            "557": "695 0.",
            "558": "771 0.",
            "559": "593 0.",
            "560": "695 Parrot 0.",
            "561": "693 0.",
            "562": "599 0.",
            "563": "624 0.",
            "564": "681 Starffsh 0.",
            "565": "623 0.",
            "566": "517 0.",
            "567": "692 0.",
            "568": "683 0.",
            "569": "690 Average 0.",
            "570": "593 0.",
            "571": "594 0.",
            "572": "524 0.",
            "573": "618\n65 infer the factor matrices; however, the former neglects the side information.",
            "574": "(a) Original image, (b) Noisy image (PSNR=\n14:12 dB), (c) BM3D (PSNR= 22:42 dB), (d) WNNM (PSNR= 22:50 dB), (e) BPFA\n(PSNR= 23 :08 dB), and (f) KSBMF (PSNR= 23:11 dB).",
            "575": "(a) Original image, (b) Noisy image (PSNR=\n8:10 dB), (c) BM3D (PSNR= 20:00 dB), (d) WNNM (PSNR= 19 :05 dB), (e) BPFA\n(PSNR= 19 :70 dB), and (f) KSBMF (PSNR= 20:21 dB).",
            "576": "The corresponding three comparative algorithms include BPFA based on Bayesian matrix factorization [157], GSR based on group sparse learning [158], and TSLRA based on nuclear norm minimization [35].",
            "577": "3 : PSNR and SSIM Values by Inpainting Methods on part of Test Images for Difierent Tasks Task BPFA GSR TSLRA KSBMF Random10% 21:77=0:8193 22:36=0:8674 22:13=0:8385 22.",
            "578": "8695\n20% 22:30=0:8794 22:66=0:9004 25:34=0:9194 25.",
            "579": "9208\n30% 26:23=0:9211 28:75=0:9452 27:85=0:9313 28.",
            "580": "89/0.",
            "581": "9488\n40% 29:76=0:9378 30:76=0:9587 30:13=0:9510 30.",
            "582": "9596 TextMask 1 25:53=0:8793 25:62=0:8867 26:09=0:9146 26.",
            "583": "9165 Mask 2 33:72=0:9101 36.",
            "584": "9205 34:94=0:9142 35:73=0:9198 Mask 3 28:02=0:8429 33:29=0:9197 32:93=0:8956 33.",
            "585": "9223 Mask 4 27:70=0:8187 33:24=0:8832 22:90=0:7532 33.",
            "586": "8911 performed on the Barbara image and the latter two are performed on the Monarch image.",
            "587": "(c) BPFA (PSNR=22:30 dB).",
            "588": "(d) GSR (PSNR=22:66dB).",
            "589": "(c) BPFA (PSNR=29:76 dB).",
            "590": "(d) GSR (PSNR=30:76dB).",
            "591": "(e) TSLRA (PSNR=34:94 dB).",
            "592": "(e) TSLRA (PSNR=32:93 dB).",
            "593": "(d) GSR\n(PSNR=33:24 dB).",
            "594": "(e) TSLRA (PSNR=22:90 dB).",
            "595": "To avoid this limitation, the ffrst step of inpainting on the entire image can be replaced by an alternative method, for example, total variation based regularization [159], to pre-complete the whole image for accurate patch matching.",
            "596": "In the area of machine vision and image processing, the KSBMF model can be extended to image or video super-resolution, deblurring, and compressed sensing to integrate other appropriate side information, for example, the statistics of ofisets of similar 72 patches [160].",
            "597": "The proposed model adopts a difierent graphical model and priors as in [83].",
            "598": "1)\n75 where Y2RM\u0002N,U2RM\u0002r,V2RN\u0002r,S2RM\u0002N,E2RM\u0002N, andrthe rank or order of the low-rank term.",
            "599": "3) With the conditional probability and all priors in hand, the joint distribution is 76 given by: p(Y;U;V;S;G;H;\u001bg;\u001bh;;fi;ff)\n=p(YjG;H;S;ff)p(GjU; KU;\u001bg)p(HjV;KV;\u001bh)\n\u0001p(Uj )p(Vj)p(Sjfi)p(\u001b g)p(\u001bh)p()p(fi)p(ff ):(5.",
            "600": "8)\n77 The covariance and mean are denoted as \u0006S ij=1 hffi+hfiiji; (5.",
            "601": "15) with covariance and mean \u0006H= (hffihG>Gi+h\u001bhiIr)\u00001; (5.",
            "602": "16) hhj\u0001i>= \u0006H(h\u001bhihVi>KV\u0001j+hffihGi>(y\u0001j\u0000s\u0001j): (5.",
            "603": "17) The required expectations are expressed as hG>Gi=hGi>hGi+m\u0006G; (5.",
            "604": "18) hH>Hi=hHi>hHi+n\u0006H: (5.",
            "605": "19)\n78 Estimation of ff The posterior probability densities of ff,\u001bgand\u001bhare all found to be Gamma distributed.",
            "606": "22) Estimation of fi Similar toff,\u001bgand\u001bh, the posterior probability density of fiijis also found to be a Gamma distribution with hfiiji=1 hsiji2+ \u0006S ij(5.",
            "607": "23) Each parameter is updated in turn while holding others ffxed.",
            "608": "TheM\u0002Nmatrix Yis constructed by grouping other N\u00001 frames with similar 79 local spatial structures to the underlying one.",
            "609": "Four square matrices with size M=N= 500, 1000, 1500 and 2000 are considered.",
            "610": "80\n(a)\n(b)\n(c)\n(d) Figure 5.",
            "611": "The video sequence contains 520 frames of size 320\u0002240 pixels, and the results for frame 260 are shown.",
            "612": "81 Table 5.",
            "613": "N rank(X)kSk0rank(^X)k^S\u0000SkF kSkFk^Sk0\n500 25 12500 25 3:1\u000210\u0000512498\n1000 50 50000 50 2:5\u000210\u0000550003\n1500 75 112500 75 3:3\u000210\u00005112500\n2000 100 200000 100 2:9\u000210\u00005199990\n5.",
            "614": "Experiments are also conducted using Bayesian robust PCA [75], mixture of Gaussians RPCA [79], and online stochastic tensor decomposition [161], for comparison.",
            "615": "The video sequence contains 520 frames of 320 \u0002240 pixels.",
            "616": "ca/dataset2012/\n82 Gaussian white noise with standard deviation 20 is then added into the video sequence.",
            "617": "uk/rbf/CAVIARDATA1/\n83 Chapter 6 Bayesian nonnegative matrix factorization with Dirichlet process mixtures 6.",
            "618": "15)\n88 where denotes the digamma function.",
            "619": "16) According to Eq.",
            "620": "16), it is clear that q(zmn) follows a multinomial distribution, and its parameters \u001emnkfork= 1, 2;\u0001\u0001\u0001Kcan be represented as: \u001emnk/expf1\n2((\u001ak;1)\u0000log \u001ak;2) +Eqln ffk \u00001\n2\u001ak;1\n\u001ak;2Eqf(ymn\u0000um\u0001v> n\u0001)2g+k\u00001X t=1Eqln(1\u0000fft)g:(6.",
            "621": "17) The approximation to the posterior of entry umrcan be expressed as q(umr)/expfEq[log p(YjU;V;z;\u001c) +log p(umrj\u0015r)]g /s(x)expflog (\u0015rexpf\u0000\u0015rumrg) +EqX n2 mKX k=1\u0012 p(zmn=k)log[r\u001ck 2\u0019expf\u0000\u001ck 2(ymn\u0000umv> n)2g]\u0013 g /expf\u0000u2 mr 2[X n2 mKX k=1\u001emnkh\u001ckihv2 nri] +umr[\u0000h\u0015ri+ X n2 mKX k=1\u001emnkh\u001cki(ymn\u0000X r06=rhumr0ihvnr0i)hvnri]gs(x)\n/expf\u0000\u001cu mr 2(umr\u0000\u0016u mr)2g\u0002s(x)\n/TN (umrj\u0016U mr;\u001cU mr);(6.",
            "622": "18)\n89 whereh\u0001iis the expectation operator, \n is all the entries of the matrix and \n m= fmj(m;n )2 gand n=fnj(m;n )2 g.",
            "623": "18 demonstrates that q(umr) obeys a truncated normal (TN) distribution.",
            "624": "19)\n\u001cU mr=X n2 mKX k=1\u001emnkh\u001ckihv2 nri: (6.",
            "625": "22) Here, Eq[(ymn\u0000umv> n)2] = (ymn\u0000RX r=1humrihvnri)2+RX r=1(hu2 mrihv2 nri\u0000humri2hvnri2): (6.",
            "626": "23) As for precision \u001c, ln q(\u001ck) =Eq[ln p(\u001ckja0;b0)] +const +MY m=1NY n=1q(zmn=k)Eq[ln p(ymnjU;V;zmn=k;\u001c)]\n= (a0+1\n2MX m=1NX n=1q(zmn=k)ln \u001ck\u0000(b0+1\n2MX m=1NX n=1q(zmn=k)Eq[(ymn\u0000um\u0001v> n\u0001)2])\u001ck +const: (6.",
            "627": "24)\n90 Obviously, it is still a Gamma distribution.",
            "628": "23.",
            "629": "30)\n91 The parameters can be updated in turn while holding others ffxed.",
            "630": "3 Results In this Section, the proposed DPNMF model is compared empirically with several state-of-the-art methods including MUNMF in [104], a sparseness-constrained NMF (SCNMF) [29], a Bayesian NMF (PSNMF) [120], and the outlier-robust MahNMF [162].",
            "631": "We set the sparseness level to 0:5 across activation coeflcients so as to be consistent with the neural sparse coding scheme [163].",
            "632": "92\n6.",
            "633": "The two latent matrices are generated from unit mean exponential distributions with three difierent ranks r= 5, 10 and 15.",
            "634": "The DPNMF algorithm performs better than PSNMF over rank 93\n5, although it is slightly inferior to DPNMF over rank 10 and 15.",
            "635": "5 10 15 20 25 Initial Rank00.",
            "636": "01Relative Errorr=5 r=10 r=15\n(a)\n0 50 100 150 200 Initial number of Gaussians00.",
            "637": "05Relative Errorr=5 r=10 r=15 (b)\n0 100 200 300 Initial Number of Gaussians1015202530Remaining Gaussians (c) Figure 6.",
            "638": "3(a) shows the average relative error in terms of the difierent initial rank for r= 5, 10, and 15, respectively.",
            "639": "3(b) shows the efiect of the initial number of Gaussians on the average relative error for r= 5, 10, 15, respectively.",
            "640": "3(a) illustrates the average relative error is almost at for the 94 initial number of Gaussians varying from 10 to 300.",
            "641": "6230 0.",
            "642": "0774 0.",
            "643": "4583 0.",
            "644": "0088\n150.",
            "645": "0075 0.",
            "646": "0945 0.",
            "647": "0075 Sparse50.",
            "648": "6232 0.",
            "649": "0815 0.",
            "650": "0275\n15 0.",
            "651": "0975 0.",
            "652": "1766 0.",
            "653": "0233 Mixture50.",
            "654": "0079 0.",
            "655": "6229 0.",
            "656": "0929 0.",
            "657": "0083 0.",
            "658": "4578 0.",
            "659": "1623 0.",
            "660": "0355\n150.",
            "661": "0154 0.",
            "662": "0965 0.",
            "663": "2 Extraction of muscle synergies In neuroscience, it is supposed that the central neural system controls muscle synergies, or groups of co-activated muscles, rather than individual muscles, to or95 ganize any simple or complex actions and movements.",
            "664": "Since the ground truth synergies are not available, Following the study in [164], this study investigate the classiffcation accuracy of synergies extracted by DPNMF to recognize six wrist motions.",
            "665": "The Ninapro ffrst dataset [165] which consists of EMG recordings for wrist, hand and ffnger movements is utilized.",
            "666": "the rank r, while four is assigned as the best number of synergies for other methods based on previous recommendations [164].",
            "667": "This is more consistent than other methods with the physiological origin of muscles involved in wrist extension [165].",
            "668": "5 is 96\n2 4 6 8 1000.",
            "669": "51Synergy 1 / WC1 2 3 4 5024\n2 4 6 8 1000.",
            "670": "51Synergy 2 / WC1 2 3 4 5024\n2 4 6 8 1000.",
            "671": "51\n1 2 3 4 5024\n2 4 6 8 1000.",
            "672": "51\n1 2 3 4 5024\n2 4 6 8 1000.",
            "673": "51\n1 2 3 4 5024\n2 4 6 8 1000.",
            "674": "51\n1 2 3 4 5024\n2 4 6 8 1000.",
            "675": "51\n1 2 3 4 5024\n2 4 6 8 1000.",
            "676": "51\n1 2 3 4 5024\n2 4 6 8 1000.",
            "677": "51\n1 2 3 4 5024\n2 4 6 8 1000.",
            "678": "51Synergy 3 / WC1 2 3 4 5024\n2 4 6 8 10 Channels00.",
            "679": "51Synergy 4 / WC 1 2 3 4 5 Time(s)024\n(a) DPNMF\n2 4 6 8 1000.",
            "680": "51\n1 2 3 4 5024\n2 4 6 8 10 Channels00.",
            "681": "51\n1 2 3 4 5 Time(s)024 (b) MUNMF\n2 4 6 8 1000.",
            "682": "51\n1 2 3 4 5024\n2 4 6 8 10 Channels00.",
            "683": "51\n1 2 3 4 5 Time(s)024 (c) SCNMF\n2 4 6 8 1000.",
            "684": "51\n1 2 3 4 5024\n2 4 6 8 10 Channels00.",
            "685": "51\n1 2 3 4 5 Time(s)024 (e) PSNMF Figure 6.",
            "686": "the violin graph of the classiffcation accuracy for ffve methods with DPNMF yielding 82:4% accuracy.",
            "687": "A BCI competition data set provided by the Department of Medical Informatics, Institute for Biomedical Engineering, Graz University of Technology, Austria is used [166].",
            "688": "Following up the previous recommendation [166], only signals in channels C3andC4are used since 97Accuracy DPNMF MUNMF SCNMF MahNMF PSNMF0.",
            "689": "80.",
            "690": "91 Figure 6.",
            "691": "One can find the different \u00b5rhythm (8\u00ad12 Hz) and \u03b2rhythm (18\u00ad25 Hz) during two different move\u00ad ments.",
            "692": "For simplicity, most previous studies assumed that the noise distributionof wavelet coefficients subjects to the same Gaussian distribution as in time do\u00admain [167].",
            "693": "With the encoding variable matrices Vand \u02dcV, the decision can be made to have the class label at a single point 98\n3 4.",
            "694": "5 941630Ch 3\n3 4.",
            "695": "5 941630Ch 4\n3 4.",
            "696": "5 9 Time (s)41630Frequency (Hz)\n3 4.",
            "697": "5 9 Time (s)41630Frequency (Hz) Figure 6.",
            "698": "1234523 13 4 23 13 4Frequency\u2026(Hz)\n(a) DPNMF1234523 13 4 23 13 4Frequency\u2026(Hz)\n(b) MUNMF1234523 13 4 23 13 4Frequency\u2026(Hz)\n(c) SCNMF\n1234523 13 4 23 13 4Frequency\u2026(Hz)\n(d) MahNMF1234523 13 4 23 13 4Frequency\u2026(Hz)\n(e) PSNMF Figure 6.",
            "699": "99 in time from the maximal posterior probability.",
            "700": "Readers are referred to [168] for the details of this online Bayesian classiffer.",
            "701": "All basis vectors reveal some useful characteristics, for example, \u0016rhythm,ffrhythm, and sensori-motor rhythm (1216 Hz).",
            "702": "DPNMF reaches the highest accuracy of 83% at 3:8 s while others also rise with the maximum lower than 81% at almost the same time.",
            "703": "750.",
            "704": "80.",
            "705": "75, pp.",
            "706": "118{129, 2016.",
            "707": "16, no.",
            "708": "2080{2095, 2007.",
            "709": "22, no.",
            "710": "687{699, 2013.",
            "711": "59{80, 2019.",
            "712": "19, no.",
            "713": "424, 2017.",
            "714": "Romberg, \\An overview of low-rank matrix recovery from incomplete observations,\" arXiv preprint arXiv:1601.",
            "715": "06422, 2016.",
            "716": "Springer, 2017, pp.",
            "717": "36, 2015.",
            "718": "23, pp.",
            "719": "1{71, 2017.",
            "720": "125{144, 2015.",
            "721": "[15] L.",
            "722": "863{870, 2017.",
            "723": "[16] L.",
            "724": "1{15, 2017.",
            "725": "108\n[17] A.",
            "726": "298{324, 2012.",
            "727": "[18] R.",
            "728": "17, 2014.",
            "729": "[19] W.",
            "730": "238{260, 2018.",
            "731": "Dhillon, \\Robust principal component analysis with side information,\" in International Conference on Machine Learning, 2016, pp.",
            "732": "2291{2299.",
            "733": "Zafeiriou, \\Side information in robust principal component analysis: Algorithms and applications,\" in Proceedings of the IEEE International Conference on Computer Vision, 2017, pp.",
            "734": "4317{4325.",
            "735": "[22] V.",
            "736": "22, no.",
            "737": "3961{3973, 2013.",
            "738": "[23] Z.",
            "739": "45 690{45 698, 2018.",
            "740": "[24] L.",
            "741": "3195{3201, 2017.",
            "742": "76{86, 2014.",
            "743": "564{594, 2009.",
            "744": "1869{1879, 2012.",
            "745": "868{880, 2016.",
            "746": "24, no.",
            "747": "2023{2036, 2018.",
            "748": "19, pp.",
            "749": "22, no.",
            "750": "1477{1484, 2017.",
            "751": "1445{1452, 2017.",
            "752": "815{824, 2016.",
            "753": "4842{4857, 2016.",
            "754": "715{724, 2016.",
            "755": "471{486, 2015.",
            "756": "1{11, 2016.",
            "757": "28{39, 2016.",
            "758": "3050{3061, 2015.",
            "759": "1{17, 2015.",
            "760": "239{252, 2017.",
            "761": "202{219, 2017.",
            "762": "478, pp.",
            "763": "3142{3155, 2017.",
            "764": "5804{5816, 2008.",
            "765": "22, no.",
            "766": "239{243, 2015.",
            "767": "76{82, 2018.",
            "768": "22, no.",
            "769": "80{90, 2013.",
            "770": "4101{4123, 2007.",
            "771": "2675{2685, 2010.",
            "772": "15{21.",
            "773": "Shahbaba, \\Bayesian inference on matrix manifolds for linear dimensionality reduction,\" arXiv preprint arXiv:1606.",
            "774": "04478, 2016.",
            "775": "ACM, 2016, pp.",
            "776": "1743{1752.",
            "777": "15{25, 2014.",
            "778": "1828{1832, 2008.",
            "779": "81, no.",
            "780": "425{455, 1994.",
            "781": "293, pp.",
            "782": "1{11, 2018.",
            "783": "75, no.",
            "784": "2713{2724, 2016.",
            "785": "22, no.",
            "786": "5226{5237, 2013.",
            "787": "22, no.",
            "788": "872{883, 2013.",
            "789": "1122{1142, 2011.",
            "790": "Ye, \\Robust principal component analysis via capped norms,\" in Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.",
            "791": "311{319.",
            "792": "57{81, 2011.",
            "793": "[75] X.",
            "794": "3419{3430, 2011.",
            "795": "115\n[76] C.",
            "796": "[77] N.",
            "797": "1785{1792.",
            "798": "[78] Q.",
            "799": "825{839, 2015.",
            "800": "[79] Q.",
            "801": "[80] S.",
            "802": "3964{3977, 2012.",
            "803": "[81] V.",
            "804": "4, 2017.",
            "805": "[82] T.",
            "806": "[83] S.",
            "807": "1593{1599.",
            "808": "116\n[85] M.",
            "809": "171{184, 2012.",
            "810": "[88] J.",
            "811": "1598{1621, 2015.",
            "812": "[89] G.",
            "813": "253, 2019.",
            "814": "[90] C.",
            "815": "667{682, 2018.",
            "816": "[91] Y.",
            "817": "[92] N.",
            "818": "117\n[93] R.",
            "819": "4944, 2010.",
            "820": "[94] T.",
            "821": "194{207, 2017.",
            "822": "[95] I.",
            "823": "[96] J.",
            "824": "838{850, 2013.",
            "825": "[97] Y.",
            "826": "221{254, 2017.",
            "827": "[98] H.",
            "828": "2528{2540, 2016.",
            "829": "[99] P.",
            "830": "i447{i456, 2018.",
            "831": "868{879, 2019.",
            "832": "118\n[101] Y.",
            "833": "296{308, 2015.",
            "834": "1784{1795, 2015.",
            "835": "6755, p.",
            "836": "788, 1999.",
            "837": "134{170, 2011.",
            "838": "5282{5286.",
            "839": "673{682.",
            "840": "119\n[109] L.",
            "841": "31 089{31 102, 2019.",
            "842": "142{156, 2014.",
            "843": "275{283.",
            "844": "Norouzi, \\Blind audio source separation of stereo mixtures using Bayesian non-negative matrix factorization,\" in 2014\n22nd European Signal Processing Conference (EUSIPCO).",
            "845": "[115] Y.",
            "846": "215{230, 2018.",
            "847": "[116] V.",
            "848": "120\n121{133, 2017.",
            "849": "[117] J.",
            "850": "24, no.",
            "851": "185{195, 2015.",
            "852": "[118] R.",
            "853": "[119] T.",
            "854": "Springer, 2017, pp.",
            "855": "Springer, 2018, pp.",
            "856": "488{498.",
            "857": "[122] T.",
            "858": "1825{1828.",
            "859": "[123] X.",
            "860": "Wang, \\Removal of muscle artifacts from the EEG: A review and recommendations,\" IEEE Sensors Journal, 2019.",
            "861": "121\n[124] S.",
            "862": "1547{1557, 2016.",
            "863": "16, no.",
            "864": "13{29, 1996.",
            "865": "Wiley,, 1985.",
            "866": "195{239, 1984.",
            "867": "193, pp.",
            "868": "394{410, 2019.",
            "869": "15{26, 2018.",
            "870": "1835{1849, 2018.",
            "871": "122\n[133] S.",
            "872": "924{931.",
            "873": "118.",
            "874": "OUP Oxford, 1998.",
            "875": "661{694, 2005.",
            "876": "22, no.",
            "877": "79{86, 1951.",
            "878": "95{106, 1977.",
            "879": "1135{1151, 1981.",
            "880": "1531{1549, 2010.",
            "881": "123\n[144] T.",
            "882": "340{\n352, 2017.",
            "883": "16, no.",
            "884": "973{976, 2009.",
            "885": "17, no.",
            "886": "12{15, 2010.",
            "887": "178, pp.",
            "888": "112{124, 2017.",
            "889": "[150] D.",
            "890": "[151] F.",
            "891": "696{708, 2011.",
            "892": "[152] V.",
            "893": "1592{1605, 2012.",
            "894": "124\n[153] M.",
            "895": "2017, no.",
            "896": "58, 2017.",
            "897": "[154] P.",
            "898": "17, no.",
            "899": "16, pp.",
            "900": "5220{5230, 2017.",
            "901": "[155] M.",
            "902": "1665{1688, 2013.",
            "903": "[156] K.",
            "904": "16, no.",
            "905": "2080{2095, Aug 2007.",
            "906": "[157] M.",
            "907": "[158] J.",
            "908": "23, no.",
            "909": "[159] M.",
            "910": "24, no.",
            "911": "2239{2253, July 2015.",
            "912": "125\n[160] K.",
            "913": "2423{2435, 2014.",
            "914": "[161] A.",
            "915": "Zahzah, \\Online stochastic tensor decomposition for background subtraction in multispectral video sequences,\" in Proceedings of the IEEE International Conference on Computer Vision Workshops, 2015, pp.",
            "916": "[162] N.",
            "917": "[163] M.",
            "918": "15, no.",
            "919": "e1006908, 2019.",
            "920": "[164] A.",
            "921": "51{60, 2018.",
            "922": "[165] M.",
            "923": "23, no.",
            "924": "73{83, 2014.",
            "925": "[166] B.",
            "926": "126\n[167] W.",
            "927": "77, no.",
            "928": "16, pp.",
            "929": "20 863{20 887, 2018.",
            "930": "[168] S.",
            "931": "1077{1080, June 2004."
        },
        "Audio-visual Video Recognition Through Super Descriptor Tensor Decomposition and Low-rank and Sparse Representation": {
            "authors": [
                "Muhammad Rizwan"
            ],
            "url": "https://ro.uow.edu.au/cgi/viewcontent.cgi?article=1394&context=theses1",
            "ref_texts": "[108] X. Zhou, C. Yang, H. Zhao and W. Yu, \u201cLow-rank modeling and its applications in image analysis,\u201d ACM Computing Surveys (CSUR) , vol. 47, no. 2, article no. 36, 2015.",
            "ref_ids": [
                "108"
            ],
            "1": "6) This formation of matrix \bis unique and clearly di fferent from the many approaches in video analysis, where the video frames are arranged as columns of matrix \b[108].",
            "2": "[108] X."
        },
        "\u57fa\u4e8e\u77e9\u9635\u4f4e\u79e9\u4f30\u8ba1\u7684\u53ef\u9760\u591a\u8f7d\u6ce2\u5dee\u5206\u6df7\u6c8c\u952e\u63a7\u63a5\u6536\u673a": {
            "authors": [],
            "url": "https://jeit.ac.cn/article/exportPdf?id=de1d75df-937e-444c-b762-8d7016c29b78",
            "ref_texts": ""
        },
        "The use of sparse plus low-rank decomposition on moving object and change detection in videos": {
            "authors": [
                "Lucas Arrabal"
            ],
            "url": "https://pantheon.ufrj.br/bitstream/11422/9364/2/882133.pdf"
        },
        "Sparse and low-rank models for image restoration": {
            "authors": [],
            "url": "https://theses.lib.polyu.edu.hk/bitstream/200/9079/1/991021962223103411.pdf",
            "ref_texts": "[154] Xiaowei Zhou, Can Yang, Hongyu Zhao, and Weichuan Yu. Low-rank modeling and its applications in image analysis. arXiv preprint arXiv:1401.3409, 2014.",
            "ref_ids": [
                "154"
            ],
            "1": "To address this problem, multiple RPCA models have been proposed to robustify PCA, and have been employed in different applications such as structure from motion, ranking and collaborative filtering, face reconstruction and background subtraction [154]."
        },
        "Applications of compressed sensing to coherent radar imaging": {
            "authors": [
                "Qian Zhu"
            ],
            "url": "https://etda.libraries.psu.edu/files/final_submissions/12600",
            "ref_texts": "79\u2013105. Zhou, X., Yang, C., Zhao, H. and Yu, W., 2015. Low -rank modeling and its applications in image analysis. ACM Computing Surveys (CSUR), 47(2), p.36. Zhu, Q., Dinsmore, R., Gao, B. and Mathews, J.D., 2016. High -resolu tion radar observations of meteoroid fragmentation and flaring at the Jicamarca Radio Observatory. MNRAS , 457(2), 1759 -1769. "
        }
    }
}