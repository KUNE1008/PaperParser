{
    "title": "Motion capture from internet videos",
    "id": 35,
    "valid_pdf_number": "25/35",
    "matched_pdf_number": "18/25",
    "matched_rate": 0.72,
    "citations": {
        "Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans": {
            "authors": [
                "Sida Peng",
                "Yuanqing Zhang",
                "Yinghao Xu",
                "Qianqian Wang",
                "Qing Shuai",
                "Hujun Bao",
                "Xiaowei Zhou"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2021/papers/Peng_Neural_Body_Implicit_Neural_Representations_With_Structured_Latent_Codes_for_CVPR_2021_paper.pdf",
            "ref_texts": "[13] Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. Motion capture from internet videos. In ECCV , 2020. 2",
            "ref_ids": [
                "13"
            ],
            "1": "To obtain the 3D representation at a frame, we first transform the code locations based on the human pose, which can be reliably estimated from sparse camera views [3,13,15]."
        },
        "Animatable neural radiance fields for modeling dynamic human bodies": {
            "authors": [
                "Sida Peng",
                "Junting Dong",
                "Qianqian Wang",
                "Shangzhan Zhang",
                "Qing Shuai",
                "Xiaowei Zhou",
                "Hujun Bao"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Peng_Animatable_Neural_Radiance_Fields_for_Modeling_Dynamic_Human_Bodies_ICCV_2021_paper.pdf",
            "ref_texts": "[13] Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. Motion capture from internet videos. In ECCV , 2020. 2",
            "ref_ids": [
                "13"
            ],
            "1": "Based on SMPL, some works [48, 24, 27, 21, 13] reconstruct an animated human mesh from sparse camera views."
        },
        "Neural human performer: Learning generalizable radiance fields for human performance rendering": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper/2021/file/cf866614b6b18cda13fe699a3a65661b-Paper.pdf",
            "ref_texts": ""
        },
        "SPEC: Seeing people in the wild with an estimated camera": {
            "authors": [
                "Muhammed Kocabas",
                "Hao P. Huang",
                "Joachim Tesch",
                "Lea Muller",
                "Otmar Hilliges",
                "Michael J. Black"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Kocabas_SPEC_Seeing_People_in_the_Wild_With_an_Estimated_Camera_ICCV_2021_paper.pdf",
            "ref_texts": "[10] Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. Motion capture from internet videos. In European Conference on Computer Vision , 2020.",
            "ref_ids": [
                "10"
            ],
            "1": "Closely related to structure-from-motion and bundle adjustment, [2, 10, 37, 67] take videos as input and jointly estimate cameras and reconstruct human bodies; [17, 40] further ground the bodies in 3D scenes."
        },
        "GLAMR: Global occlusion-aware human mesh recovery with dynamic cameras": {
            "authors": [
                "Ye Yuan",
                "Umar Iqbal",
                "Pavlo Molchanov",
                "Kris Kitani",
                "Jan Kautz"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Yuan_GLAMR_Global_Occlusion-Aware_Human_Mesh_Recovery_With_Dynamic_Cameras_CVPR_2022_paper.pdf",
            "ref_texts": "[14] Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. Motion capture from internet videos. In ECCV , 2020. 2",
            "ref_ids": [
                "14"
            ]
        },
        "Learning motion priors for 4d human body capture in 3d scenes": {
            "authors": [
                "Siwei Zhang",
                "Yan Zhang",
                "Federica Bogo",
                "Marc Pollefeys",
                "Siyu Tang"
            ],
            "url": "http://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Learning_Motion_Priors_for_4D_Human_Body_Capture_in_3D_ICCV_2021_paper.pdf",
            "ref_texts": "[12] Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. Motion capture from internet videos. In European Conference on Computer Vision , pages 210\u2013227. Springer, 2020. 2",
            "ref_ids": [
                "12"
            ],
            "1": "SMPL [36]) to obtain complete 3D body meshes from multi-view [12,15,22,25,50,64] or monocular RGB(D) sequences [10,27,31,34,37,55,67]."
        },
        "Human movement datasets: An interdisciplinary scoping review": {
            "authors": [],
            "url": "https://discovery.ucl.ac.uk/id/eprint/10147799/5/Human_Movement_Datasets__An_Interdisciplinary_Scoping_Review%20-%20Supplementary%202.pdf",
            "ref_texts": "[157] Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. 2020. Motion Capture from Internet Videos. In ECCV , Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm (Eds.). Springer International Publishing, Cham, 210\u2013227.",
            "ref_ids": [
                "157"
            ]
        },
        "A-nerf: Articulated neural radiance fields for learning human shape, appearance, and pose": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/65fc9fb4897a89789352e211ca2d398f-Paper.pdf",
            "ref_texts": "[11] J. Dong, Q. Shuai, Y . Zhang, X. Liu, X. Zhou, and H. Bao. Motion capture from internet videos. InECCV , pages 210\u2013227. Springer, 2020.",
            "ref_ids": [
                "11"
            ],
            "1": "It also enables optimization within the bounds of the learned prior [11,16,25] and weak-supervision when integrated in a differentiable form [27] into neural training processes [4,20,23,42,45,65].",
            "2": "[11] J."
        },
        "Reconstructing 3d human pose by watching humans in the mirror": {
            "authors": [
                "Qi Fang",
                "Qing Shuai",
                "Junting Dong",
                "Hujun Bao",
                "Xiaowei Zhou"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Fang_Reconstructing_3D_Human_Pose_by_Watching_Humans_in_the_Mirror_CVPR_2021_paper.pdf",
            "ref_texts": "[11] Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. Motion capture from internet videos. In ECCV , 2020. 2",
            "ref_ids": [
                "11"
            ],
            "1": "Optimization-based methods fit the model using 2D evidence and some human body priors [5,28,42,11]."
        },
        "TotalSelfScan: Learning Full-body Avatars from Self-Portrait Videos of Faces, Hands, and Bodies": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/589c5bd0aa4322e37813e8e41ddf8034-Paper-Conference.pdf",
            "ref_texts": "[17] Junting Dong, Qing Shuai, Jingxiang Sun, Yuanqing Zhang, Hujun Bao, and Xiaowei Zhou. imocap: Motion capture from internet videos. IJCV , 2022.",
            "ref_ids": [
                "17"
            ],
            "1": "Based on the statistical human model, most works [8,25,26,18,17,19] reconstruct the naked body mesh from various inputs and some works further add surface deformation to capture more details [63,27,48,11,61]."
        },
        "Enhancing self-supervised video representation learning via multi-level feature optimization": {
            "authors": [
                "Rui Qian",
                "Yuxi Li",
                "Huabin Liu",
                "John See",
                "Shuangrui Ding",
                "Xian Liu",
                "Dian Li",
                "Weiyao Lin"
            ],
            "url": "http://openaccess.thecvf.com/content/ICCV2021/papers/Qian_Enhancing_Self-Supervised_Video_Representation_Learning_via_Multi-Level_Feature_Optimization_ICCV_2021_paper.pdf",
            "ref_texts": "[13] Marco Cuturi. Sinkhorn distances: lightspeed computation of optimal transport. In NIPS , volume 2, page 4, 2013. 2, 3[14] Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. Motion capture from internet videos. In European Conference on Computer Vision , pages 210\u2013227. Springer, 2020. 1",
            "ref_ids": [
                "13",
                "14"
            ],
            "1": "To expand this pipeline to video domain, diverse spatiotemporal augmentation techniques are proposed to construct contrastive pairs and enhance motion modeling [17, 50, 64, 75, 14].",
            "2": "To better extract the latent semantics in unlabelled data, [10, 2, 3, 51] leveraged Sinkhorn-Knopp algorithm [13] to generate uniformly distributed clusters as pseudo labels for pretraining.",
            "3": "Inspired by [10, 2, 3] (where clustering is regarded as an optimal transport problem), we employ Sinkhorn-Knopp algorithm [13] to transform a set of distributionsfpa 1;pa 2;:::;pa Nginto soft targetsfsa 1;sa 2;:::;sa Ng, where sa i2RK, is uniformly distributed at category level, indicating that there are aroundN Ksamples per category.",
            "4": "1, 2, 7\n[13] Marco Cuturi."
        },
        "Gfpose: Learning 3d human pose prior with gradient fields": {
            "authors": [
                "Hai Ci",
                "Mingdong Wu",
                "Wentao Zhu",
                "Xiaoxuan Ma",
                "Hao Dong",
                "Fangwei Zhong",
                "Yizhou Wang"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Ci_GFPose_Learning_3D_Human_Pose_Prior_With_Gradient_Fields_CVPR_2023_paper.pdf",
            "ref_texts": "[12] Andrey Davydov, Anastasia Remizova, Victor Constantin, Sina Honari, Mathieu Salzmann, and Pascal Fua. Adversarial parametric pose prior. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 10997\u201311005, 2022.[13] Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. Motion capture from internet videos. In ECCV , 2020.",
            "ref_ids": [
                "12",
                "13"
            ],
            "1": "Representative methods include modeling the distribution of plausible poses with GMM [5], V AE [46], GAN [12] or neural implicit functions [61].",
            "2": "GAN-based methods [12] learn adversarial priors by discriminating the generated poses from the real poses.",
            "3": "man pose estimation as a one-to-one mapping fails to express the ambiguity and inevitably suffers from degraded precision [13, 14].",
            "4": "Most previous works [12, 46, 50, 61] learn the pose priors in the SMPL [37] parameter space.",
            "5": "[12] Andrey Davydov, Anastasia Remizova, Victor Constantin, Sina Honari, Mathieu Salzmann, and Pascal Fua."
        },
        "Virtual correspondence: Humans as a cue for extreme-view geometry": {
            "authors": [
                "Chiu Ma",
                "Anqi Joyce",
                "Shenlong Wang",
                "Raquel Urtasun",
                "Antonio Torralba"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Ma_Virtual_Correspondence_Humans_as_a_Cue_for_Extreme-View_Geometry_CVPR_2022_paper.pdf",
            "ref_texts": "[22] Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. Motion capture from internet videos. In ECCV , 2020. 3,4",
            "ref_ids": [
                "22"
            ],
            "1": "To alleviate this issue, researchers have sought to exploit motion patterns [4,5,75] or semantic keypoints of the objects [22,86] to aid the reconstruction.",
            "2": "With the flourishing of deep learning, these methods have made tremendous progress, either from a single image [41,44,47] or multi-view images [21,22,25,61].",
            "3": "More recently, researchers have exploited human keypoints to refine camera poses [22,63], but by virtue of VCs, our method is more flexible and does not require the same keypoints to be co-visible across views.",
            "4": "For instance, researchers have exploited 2D human keypoints to reconstruct 3D joints [15,22]."
        },
        "Human mesh recovery from multiple shots": {
            "authors": [
                "Georgios Pavlakos",
                "Jitendra Malik",
                "Angjoo Kanazawa"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Pavlakos_Human_Mesh_Recovery_From_Multiple_Shots_CVPR_2022_paper.pdf",
            "ref_texts": "[10] Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. Motion capture from internet videos. In ECCV , 2020. 3",
            "ref_ids": [
                "10"
            ],
            "1": ", from multiple views [10,17] or monocular video [3, 24, 46, 50]."
        },
        "Egobody: Human body shape and motion of interacting people from head-mounted devices": {
            "authors": [],
            "url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660176.pdf",
            "ref_texts": "19.Dong, J., Shuai, Q., Zhang, Y., Liu, X., Zhou, X., Bao, H.: Motion capture from internet videos. In: European Conference on Computer Vision. pp. 210\u2013227. Springer (2020) 5",
            "ref_ids": [
                "19"
            ]
        },
        "Learning compositional representation for 4d captures with neural ode": {
            "authors": [
                "Boyan Jiang",
                "Yinda Zhang",
                "Xingkui Wei",
                "Xiangyang Xue",
                "Yanwei Fu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Jiang_Learning_Compositional_Representation_for_4D_Captures_With_Neural_ODE_CVPR_2021_paper.pdf",
            "ref_texts": "[14] Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. Motion capture from internet videos. In European Conference on Computer Vision , pages 210\u2013227. Springer, 2020. 2",
            "ref_ids": [
                "14"
            ],
            "1": "However, most works are developed based on strong assumptions [54,42,53,31,56], demand the costly multi-view inputs [36,52,35,14]."
        },
        "Pose2Sim: an end-to-end workflow for 3D markerless sports kinematics\u2014part 1: robustness": {
            "authors": [
                "David Pagnon",
                "Mathieu Domalain",
                "Lionel Reveret"
            ],
            "url": "https://www.mdpi.com/1424-8220/21/19/6530/pdf",
            "ref_texts": "43. Dong, J.; Shuai, Q.; Zhang, Y.; Liu, X.; Zhou, X.; Bao, H. Motion Capture from Internet Videos. In Computer Vision\u2014ECCV 2020 ; Vedaldi, A., Bischof, H., Brox, T., Frahm, J.-M., Eds.; Lecture Notes in Computer Science; Springer International Publishing: Cham, Switzerland, 2020; Volume 12347, pp. 210\u2013227, ISBN 978-3-030-58535-8.",
            "ref_ids": [
                "43"
            ],
            "1": "[43] recovered 3D human motion from unsynchronized and uncalibrated videos of a repeatable movement found on internet videos (such as a tennis serve performed by a celebrity)."
        },
        "Metapose: Fast 3d pose from multiple views without 3d supervision": {
            "authors": [
                "Ben Usman",
                "Andrea Tagliasacchi",
                "Kate Saenko",
                "Avneesh Sud"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Usman_MetaPose_Fast_3D_Pose_From_Multiple_Views_Without_3D_Supervision_CVPR_2022_paper.pdf",
            "ref_texts": "[14] Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. Motion capture from internet videos. InEuropean Conference on Computer Vision, pages 210\u2013227. Springer, 2020. 13",
            "ref_ids": [
                "14"
            ]
        },
        "Context-aware sequence alignment using 4D skeletal augmentation": {
            "authors": [
                "Taein Kwon",
                "Bugra Tekin",
                "Siyu Tang",
                "Marc Pollefeys"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Kwon_Context-Aware_Sequence_Alignment_Using_4D_Skeletal_Augmentation_CVPR_2022_paper.pdf",
            "ref_texts": "[13] Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. Motion capture from internet videos. InEuropean Conference on Computer Vision , pages 210\u2013227. Springer, 2020. 3",
            "ref_ids": [
                "13"
            ],
            "1": "Closely related to the sequence alignment problem, metrics for assessing human motion similarity have been actively explored by previous studies [4, 9, 13, 33, 34, 37, 52, 54, 54].",
            "2": "Conventional approaches for measuring similarity of human motion sequences are based on estimating the L2 displacement error [13, 34] or DTW [4]."
        },
        "Normalized human pose features for human action video alignment": {
            "authors": [
                "Jingyuan Liu",
                "Mingyi Shi",
                "Qifeng Chen",
                "Hongbo Fu",
                "Lan Tai"
            ],
            "url": "http://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Normalized_Human_Pose_Features_for_Human_Action_Video_Alignment_ICCV_2021_paper.pdf",
            "ref_texts": "[11] Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. Motion capture from internet videos. In European Conference on Computer Vision , pages 210\u2013227. Springer, 2020. 1, 3",
            "ref_ids": [
                "11"
            ],
            "1": "A common approach to the problem of human action video alignment is to first estimate 2D or 3D human poses from two input videos, and then find the alignments by matching with features extracted from joint positions [48, 11], so as to reduce certain interference in video backgrounds and subjects\u2019 clothing.",
            "2": "Alignment of human action videos has been actively explored in recent years for many video analysis tasks, such as action detection in unconstrained videos [16], human reconstruction from 11522\n uncalibrated multiview videos [11], action synchronization [12], few-shot video classification [6], etc."
        },
        "Mocanet: Motion retargeting in-the-wild via canonicalization networks": {
            "authors": [
                "Wentao Zhu",
                "Zhuoqian Yang",
                "Ziang Di",
                "Wayne Wu",
                "Yizhou Wang",
                "Chen Change"
            ],
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/20274/20033",
            "ref_texts": "2020. Toward fast and accurate human pose estimation via soft-gated skip connections. In IEEE FG. Cao, Z.; Hidalgo, G.; Simon, T.; Wei, S.-E.; and Sheikh, Y . 2018. OpenPose: realtime multi-person 2D pose estimation using Part Affinity Fields. In arXiv preprint arXiv:1812.08008. Chan, C.; Ginosar, S.; Zhou, T.; and Efros, A. A. 2019. Everybody Dance Now. In ICCV. Chen, C.-H.; Tyagi, A.; Agrawal, A.; Drover, D.; Stojanov, S.; and Rehg, J. M. 2019a. Unsupervised 3D Pose Estimation with Geometric Self-Supervision. In CVPR. Chen, X.; Duan, Y .; Houthooft, R.; Schulman, J.; Sutskever, I.; and Abbeel, P. 2016. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In NeurIPS. Chen, X.; Lin, K.-Y .; Liu, W.; Qian, C.; and Lin, L. 2019b. Weakly-Supervised Discovery of Geometry-Aware Representation for 3D Human Pose Estimation. In CVPR. Choi, H.; Moon, G.; and Lee, K. M. 2020. Pose2Mesh: Graph Convolutional Network for 3D Human Pose and Mesh Recovery from a 2D Human Pose. In European Conference on Computer Vision (ECCV).Ci, H.; Ma, X.; Wang, C.; and Wang, Y . 2020. Locally Connected Network for Monocular 3D Human Pose Estimation. IEEE TPAMI, 1\u20131. Ci, H.; Wang, C.; Ma, X.; and Wang, Y . 2019. Optimizing Network Structure for 3D Human Pose Estimation. In ICCV. Deng, B.; Lewis, J. P.; Jeruzalski, T.; Pons-Moll, G.; Hinton, G. E.; Norouzi, M.; and Tagliasacchi, A. 2020. NASA Neural Articulated Shape Approximation. In ECCV. Denton, E. L.; and Birodkar, V . 2017. Unsupervised Learning of Disentangled Representations from Video. In NeurIPS. Dong, J.; Shuai, Q.; Zhang, Y .; Liu, X.; Zhou, X.; and Bao, H. 2020. Motion Capture from Internet Videos. In ECCV. Duan, H.; Zhao, Y .; Chen, K.; Shao, D.; Lin, D.; and Dai, B. 2021. Revisiting Skeleton-based Action Recognition. CoRR, abs/2104.13586. Girshick, R.; Radosavovic, I.; Gkioxari, G.; Doll \u00b4ar, P.; and He, K. 2018. Detectron. https://github.com/ facebookresearch/detectron. Gleicher, M. 1998. Retargetting Motion to New Characters. InSIGGRAPH. Higgins, I.; Matthey, L.; Pal, A.; Burgess, C.; Glorot, X.; Botvinick, M.; Mohamed, S.; and Lerchner, A. 2017. betaV AE: Learning Basic Visual Concepts with a Constrained Variational Framework. In ICLR. Hodgins, J. K.; and Pollard, N. S. 1997. Adapting simulated behaviors for new characters. In SIGGRAPH. Hoshyari, S.; Xu, H.; Knoop, E.; Coros, S.; and B \u00a8acher, M. 2019. Vibration-Minimizing Motion Retargeting for Robotic Characters. ACM Trans. Graph., 38. Huang, X.; Liu, M.-Y .; Belongie, S.; and Kautz, J. 2018. Multimodal Unsupervised Image-to-image Translation. In ECCV. Hubert, L.; and Arabie, P. 1985. Comparing partitions. Journal of Classification, 2(1): 193\u2013218. Kang, N.; Bai, J.; Pan, J.; and Qin, H. 2019. Real-time Animation and Motion Retargeting of Virtual Characters Based on Single RGB-D Camera. In IEEE VR. Kim, T.; and Lee, J. H. 2020. C-3PO: Cyclic-Three-Phase Optimization for Human-Robot Motion Retargeting based on Reinforcement Learning. In ICRA. Kim, Y .; Park, H.; Bang, S.; and Lee, S. 2016. Retargeting Human-Object Interaction to Virtual Avatars. IEEE TVCG, 22(11): 2405\u20132412. Kingma, D. P.; Mohamed, S.; Rezende, D. J.; and Welling, M. 2014. Semi-supervised Learning with Deep Generative Models. In NeurIPS. Kocabas, M.; Athanasiou, N.; and Black, M. J. 2020. VIBE: Video Inference for Human Body Pose and Shape Estimation. In CVPR. Koenemann, J.; Burget, F.; and Bennewitz, M. 2014. Realtime imitation of human whole-body motions by humanoids. InICRA. Lee, H.-Y .; Tseng, H.-Y .; Huang, J.-B.; Singh, M. K.; and Yang, M.-H. 2018. Diverse Image-to-Image Translation via Disentangled Representations. In ECCV.",
            "ref_ids": [
                "2020"
            ]
        },
        "NeMo: Learning 3D Neural Motion Fields From Multiple Video Instances of the Same Action": {
            "authors": [
                "Chieh Wang",
                "Zhenzhen Weng",
                "Maria Xenochristou",
                "Joao Pedro",
                "Jeffrey Gu",
                "Karen Liu",
                "Serena Yeung"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_NeMo_Learning_3D_Neural_Motion_Fields_From_Multiple_Video_Instances_CVPR_2023_paper.pdf",
            "ref_texts": "[11] Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. Motion capture from internet videos. In European Conference on Computer Vision , pages 210\u2013227. Springer, 2020. 3",
            "ref_ids": [
                "11"
            ],
            "1": "iMoCap [11] studied a problem similar to ours by curating videos from the internet and also aimed to recover the 3D motion."
        },
        "CAT-NeRF: Constancy-Aware Tx2Former for Dynamic Body Modeling": {
            "authors": [
                "Haidong Zhu",
                "Zhaoheng Zheng",
                "Wanrong Zheng",
                "Ram Nevatia"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023W/DynaVis/papers/Zhu_CAT-NeRF_Constancy-Aware_Tx2Former_for_Dynamic_Body_Modeling_CVPRW_2023_paper.pdf",
            "ref_texts": "[7] Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. Motion capture from internet videos. In ECCV , pages 210\u2013227. Springer, 2020. 2",
            "ref_ids": [
                "7"
            ],
            "1": "Recently, researchers have mainly developed two different methods: statisticbased methods [3, 7, 11, 14, 16, 28, 37, 55, 56] and databased methods [25, 26, 38, 39, 41, 52, 54]."
        },
        "Consistent 3D Human Shape from Repeatable Action": {
            "authors": [
                "Keisuke Shibata",
                "Sangeun Lee",
                "Shohei Nobuhara",
                "Ko Nishino"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2021W/DynaVis/papers/Shibata_Consistent_3D_Human_Shape_From_Repeatable_Action_CVPRW_2021_paper.pdf",
            "ref_texts": "[15] Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. Motion capture from internet videos. In Proc. ECCV , 2020. 3",
            "ref_ids": [
                "15"
            ],
            "1": "synchronize videos of repeated action using 3D pose estimates from a singlecamera, and improves the 3D poses with iterative optimization [15]."
        },
        "\u5f71\u5b50\u8f85\u52a9\u7684\u4e09\u7ef4\u4eba\u4f53\u91cd\u5efa": {
            "authors": [],
            "url": "https://www.jcad.cn/cn/article/pdf/preview/d7b354ea-fad5-46d4-b755-4d349a8f8b64.pdf",
            "ref_texts": ""
        }
    }
}