{
    "title": "LoFTR: Detector-free local feature matching with transformers",
    "id": 5,
    "valid_pdf_number": "263/326",
    "matched_pdf_number": "214/263",
    "matched_rate": 0.8136882129277566,
    "citations": {
        "Transfusion: Robust lidar-camera fusion for 3d object detection with transformers": {
            "authors": [
                "Xuyang Bai",
                "Zeyu Hu",
                "Xinge Zhu",
                "Qingqiu Huang",
                "Yilun Chen",
                "Hongbo Fu",
                "Lan Tai"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Bai_TransFusion_Robust_LiDAR-Camera_Fusion_for_3D_Object_Detection_With_Transformers_CVPR_2022_paper.pdf",
            "ref_texts": "[41] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR , 2021. 4",
            "ref_ids": [
                "41"
            ],
            "1": "Multi-head attention is a popular mechanism to perform information exchange and build a soft association between two sets of inputs, and it has been widely used for the feature matching task [34, 41]."
        },
        "Geometric transformer for fast and robust point cloud registration": {
            "authors": [
                "Zheng Qin",
                "Hao Yu",
                "Changjian Wang",
                "Yulan Guo",
                "Yuxing Peng",
                "Kai Xu"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Qin_Geometric_Transformer_for_Fast_and_Robust_Point_Cloud_Registration_CVPR_2022_paper.pdf",
            "ref_texts": "[25] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR , pages 8922\u20138931, 2021. 1, 3, 4, 5",
            "ref_ids": [
                "25"
            ],
            "1": "Inspired by the recent advances in image matching [22, 25, 39], keypoint-free methods [36] downsample the input point clouds into superpoints and then match them through examining whether their local neighborhood (patch) overlaps.",
            "2": "Global context has proven critical in many computer vision tasks [10, 25, 36].",
            "3": "Besides our powerful hybrid features, we also perform a dual-normalization operation [22,25] on Sto further suppress ambiguous matches, leading to \u0016Swith \u0016si;j=si;j Pj^Qj k=1si;k\u0001si;j Pj^Pj k=1sk;j: (8) We found that this suppression can effectively eliminate wrong matches.",
            "4": "Existing methods [25,36] usually formulate superpoint matching as a multi-label classification problem and adopt a cross-entropy loss with dualsoftmax [25] or optimal transport [23, 36]."
        },
        "Cotr: Correspondence transformer for matching across images": {
            "authors": [
                "Wei Jiang",
                "Eduard Trulls",
                "Jan Hosang",
                "Andrea Tagliasacchi",
                "Kwang Moo"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Jiang_COTR_Correspondence_Transformer_for_Matching_Across_Images_ICCV_2021_paper.pdf",
            "ref_texts": "[63] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-Free Local Feature Matching with Transformers. CVPR , 2021. 2",
            "ref_ids": [
                "63"
            ],
            "1": "1A concurrent relevant work for feature-less image matching was proposed shortly after our work became public [63]."
        },
        "Flowformer: A transformer architecture for optical flow": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2203.16194",
            "ref_texts": "44. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8922\u20138931 (2021)",
            "ref_ids": [
                "44"
            ],
            "1": "Visual correspondence tasks [44,19,7,27,51] is a main stream in computer vision.",
            "2": "Recently, transformers also lead a trend in such tasks [39,44,7,27], which is more related to ours."
        },
        "Gmflow: Learning optical flow via global matching": {
            "authors": [
                "Haofei Xu",
                "Jing Zhang",
                "Jianfei Cai",
                "Hamid Rezatofighi",
                "Dacheng Tao"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_GMFlow_Learning_Optical_Flow_via_Global_Matching_CVPR_2022_paper.pdf",
            "ref_texts": "[38] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR , pages 8922\u20138931, 2021. 1, 2, 3, 4",
            "ref_ids": [
                "38"
            ],
            "1": ", sparse matching between an image pair [34, 38, 43], which usually features a large viewpoint change , moves into a different track.",
            "2": ", SuperGlue [34] and LoFTR [38]) adopt Transformers [41] to reason about the mutual relationship between feature descriptors, and the correspondences are extracted with an explicit matching layer (e.",
            "3": ", Transfromer and the softmax matching layer) with sparse matching [34, 38], our motivation is originated from the development of optical flow methods and the challenges associated with formulating optical flow as a global matching problem are quite different.",
            "4": "LoFTR [38] further improves its performance by removing the feature detection step in the typical pipelines.",
            "5": "Such design philosophies have enabled great achievement in sparse feature matching frameworks [34, 38].",
            "6": "To further consider their mutual dependencies, a natural choice is Transformer [41], which is particularly suitable for modeling the mutual relationship between two sets with the attention mechanism, as demonstrated in sparse matching methods [34,38]."
        },
        "Practical stereo matching via cascaded recurrent network with adaptive correlation": {
            "authors": [
                "Jiankun Li",
                "Peisen Wang",
                "Pengfei Xiong",
                "Tao Cai",
                "Ziwei Yan",
                "Lei Yang",
                "Jiangyu Liu",
                "Haoqiang Fan",
                "Shuaicheng Liu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Practical_Stereo_Matching_via_Cascaded_Recurrent_Network_With_Adaptive_Correlation_CVPR_2022_paper.pdf",
            "ref_texts": "[41] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proc. CVPR , pages 8922\u20138931, 2021.",
            "ref_ids": [
                "41"
            ],
            "1": "In light of LoFTR [41] for sparse feature matching, we add an attention module before correlation computation in the first stage of cascades in order to aggregate global context information in single or cross feature maps.",
            "2": "Following [41], we add positional encoding to the backbone output, which enhances positional dependence of the feature maps."
        },
        "Onepose: One-shot object pose estimation without cad models": {
            "authors": [
                "Jiaming Sun",
                "Zihao Wang",
                "Siyu Zhang",
                "Xingyi He",
                "Hongcheng Zhao",
                "Guofeng Zhang",
                "Xiaowei Zhou"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Sun_OnePose_One-Shot_Object_Pose_Estimation_Without_CAD_Models_CVPR_2022_paper.pdf",
            "ref_texts": "[36] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR , 2021. 2, 3, 4, 5, 6",
            "ref_ids": [
                "36"
            ],
            "1": "To ensure a high success rate of localization, matching to multiple image retrieval candidates is necessary, so that the 2D-2D matching can be expensive especially for learning-based matchers [32, 36].",
            "2": "Recently, learning-based local feature detection, description [10\u201312, 39] and matching [32, 36] surpass these handcrafted methods and have substituted the traditional counterparts in the localization pipeline.",
            "3": "Increasing the number of image pairs to be matched will significantly slow down the localization, especially for learning-based matchers like SuperGlue [32] or LoFTR [36].",
            "4": "Inspired by [32, 36], we further use selfand crossattention layers following the aggregation-attention layers to process and transform the aggregated 3D descriptors and query 2D descriptors.",
            "5": "We follow [36] to use the dual-softmax operator to differentiablly extract match confidence scores P3D.",
            "6": "Linear Attention [14, 41] is used in all the attention layers following [36]."
        },
        "Aspanformer: Detector-free image matching with adaptive span transformer": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2208.14201",
            "ref_texts": "13. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: CVPR. (2021)",
            "ref_ids": [
                "13"
            ],
            "1": "Concurrent with detector-based matching, another line of works focus on generating correspondences directly from raw images [13,14,15,16,17,18,19,20,21],arXiv:2208.",
            "2": "Recently, some works [13,14,21] base their methods on Transformer [23,24] backbone for better modeling of long-range dependencies.",
            "3": "Recently, with the help of deep neural network, possibility is explored to build high performance detector-free matching frameworks based on deep features, which can roughly be classiffed into two categories: cost volume-based methods [16,18,15,22,17,37] and Transformer-based methods [13,14,21].",
            "4": "Due to the high cost of global interaction, the input features are usually downsampled into coarse resolution [17,18,14] or being projected into low rank [13], which to some degree limits the networks' capability for ffned grained feature update.",
            "5": "As validated in Transformer networks [13,30,14], positional encoding is critical in maintaining spatial information for the attened tokens.",
            "6": "Following the same formulation in LoFTR [13], 2D sinusoidal signals in difierent frequencies are used to encode position information and are added to initial features.",
            "7": "5 Matches Determination We inherit the scheme in LoFTR [13] to generate ffnal correspondences, including a coarse matching stage and a sub-pixel reffnement stage.",
            "8": "The coarse matches Mcare further fed into a correlation-based reffnement block, which is the same with LoFTR [13], to obtain the ffnal matching results.",
            "9": "For fair comparison, we follow the same training and testing protocols used by SuperGlue [30] and LoFTR [13], where 230M and 1.",
            "10": "Following previous works [30,13], we train and evaluate our method separately on the two datasets.",
            "11": "We compare the proposed method with 1) detectorbased approaches, including SuperGlue [30] and SGMNet [31] that are equipped with SuperPoint(SP) [9] as local feature extractor, 2) detector-free approaches, including DRC-Net [17], PDC-Net [15,50], LoFTR [13], QuadTree Attention [21], MatchFormer [51] and DKM [52].",
            "12": "1 LoFTR [13] 22.",
            "13": "1 LoFTR [13] 52.",
            "14": "For both datasets, we use pretrained HLoc [57] to retrieve candidate pairs, and recover camera poses with the model trained on MegaDepth dataset following SuperGlue [30] and LoFTR [13].",
            "15": "4 HLoc [57] + LoFTR [13] 47.",
            "16": "0 LoFTR [13] 88.",
            "17": "We evaluate the runtime of proposed method and compare it with LoFTR [13] where both methods apply Transformer backend.",
            "18": "5 LoFTR [13] 42.",
            "19": "728 LoFTR [13] 0."
        },
        "Cofinet: Reliable coarse-to-fine correspondences for robust pointcloud registration": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/c85b2ea9a678e74fdc8bafe5d0707c31-Paper.pdf",
            "ref_texts": "[25] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. arXiv preprint arXiv:2104.00680 , 2021.",
            "ref_ids": [
                "25"
            ],
            "1": "Recently, a coarse-to-fine mechanism is leveraged by our 2D counterparts [23,24,25] to avoid direct keypoint detection, which shows superiority over the state-of-the-art detection-based method [26].",
            "2": "As witnessed in 2D image matching, many recent works [23,24, 25] leverage a coarse-to-fine mechanism to eliminate the inherent repeatability problem in keypoint detection and thus boost the performance.",
            "3": "In a similar coarse-to-fine manner with Patch2Pixel, LoFTR [25] leverages Transformers [33], together with an optimal transport matching layer [26], to match mutual-nearest patches on the coarse level, and then refines the corresponding pixel of the patch center on the finer level."
        },
        "Pixel-perfect structure-from-motion with featuremetric refinement": {
            "authors": [
                "Philipp Lindenberger",
                "Edouard Sarlin",
                "Viktor Larsson",
                "Marc Pollefeys"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Lindenberger_Pixel-Perfect_Structure-From-Motion_With_Featuremetric_Refinement_ICCV_2021_paper.pdf",
            "ref_texts": "[74] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with Transformers. CVPR , 2021. 2",
            "ref_ids": [
                "74"
            ],
            "1": "Differently, dense matching [13, 46, 58, 71, 74, 78, 80] considers all pixels in each image, resulting in denser and more accurate correspondences."
        },
        "Deep vit features as dense visual descriptors": {
            "authors": [],
            "url": "https://dino-vit-features.github.io/paper.pdf",
            "ref_texts": "50. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: LoFTR: Detector-free local feature matching with transformers. CVPR (2021)",
            "ref_ids": [
                "50"
            ],
            "1": "Recent supervised methods employ transformers for dense correspondence in images from the same scene [50, 25]."
        },
        "Transmvsnet: Global context-aware multi-view stereo network with transformers": {
            "authors": [
                "Yikang Ding",
                "Wentao Yuan",
                "Qingtian Zhu",
                "Haotian Zhang",
                "Xiangyue Liu",
                "Yuanjiang Wang",
                "Xiao Liu"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Ding_TransMVSNet_Global_Context-Aware_Multi-View_Stereo_Network_With_Transformers_CVPR_2022_paper.pdf"
        },
        "Dynast: Dynamic sparse transformer for exemplar-guided image generation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2207.06124",
            "ref_texts": "43. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 8922\u20138931 (2021)",
            "ref_ids": [
                "43"
            ],
            "1": "2 Image-wise Matching Given a pair of images, image matching such as [19,27,31,41,49,17,43] aims to find pixel-wise correspondence leveraging local features, which is a fundamental problem in computer vision and is one related field to exemplar-guided image generation in this paper."
        },
        "Lepard: Learning partial point cloud matching in rigid and deformable scenes": {
            "authors": [
                "Yang Li",
                "Tatsuya Harada"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Lepard_Learning_Partial_Point_Cloud_Matching_in_Rigid_and_Deformable_CVPR_2022_paper.pdf",
            "ref_texts": "[61] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8922\u20138931, 2021. 1, 4, 8",
            "ref_ids": [
                "61"
            ],
            "1": "We first build our baseline using the fully convolutional feature extractor KPFCN [62], the concept of Transformer [64] with self and cross attention, and the idea of differentiable matching [55, 61].",
            "2": "We apply softmax on both dimensions (kown as the dual-softmax operation [51, 61]) to convert the scroing matrix to confidence matrixC.",
            "3": "[61, 73].",
            "4": "3\n[61] J."
        },
        "Cats: Cost aggregation transformers for visual correspondence": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/4b6538a44a1dfdc2b83477cd76dee98e-Paper.pdf",
            "ref_texts": "[51] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. arXiv preprint arXiv:2104.00680 , 2021.",
            "ref_ids": [
                "51"
            ],
            "1": "Recent approaches [42,43,45,34,37,39,31,58,47,57,51,35] addressed these challenges by carefully designing deep convolutional neural networks (CNNs)-based models analogously to the classical matching pipeline [48,41], feature extraction, cost aggregation, and flow estimation.",
            "2": "Several works [24,9,37,39,47,51] focused on the feature extraction stage, as it has been proven that the more powerful feature representation the model learns, the more robust matching is obtained [24,9,51].",
            "3": "Recent numerous methods [45,37,39,31,47,51,35] thus have focused on cost aggregation stage to refine the initial matching scores.",
            "4": "Transformer [61], the de facto standard for Natural Language Processing (NLP) tasks, has recently imposed significant impact on various tasks in Computer Vision fields such as image classification [10,55], object detection [3,62], tracking and matching [52,51].",
            "5": "For visual correspondence, LoFTR [51] uses cross and self-attention module to refine the feature maps conditioned on both input images, and formulate the hand-crafted aggregation layer with dual-softmax [45,60] and optimal transport [47] to infer correspondences.",
            "6": "Several works [10,3,62,51] have shown that given images or features as input, Transformers [61] integrate the global information in a flexible manner by learning to find the attention scores for all pairs of tokens."
        },
        "Fs6d: Few-shot 6d pose estimation of novel objects": {
            "authors": [
                "Yisheng He",
                "Yao Wang",
                "Haoqiang Fan",
                "Jian Sun",
                "Qifeng Chen"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/He_FS6D_Few-Shot_6D_Pose_Estimation_of_Novel_Objects_CVPR_2022_paper.pdf",
            "ref_texts": "[46] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR , 2021. 2, 4, 6, 7",
            "ref_ids": [
                "46"
            ],
            "1": "Existing methods can be categorized into detector-based [33\u201335, 42, 43] and detector-free [28, 31, 41, 46].",
            "2": "(1) Gifted with the capability of capturing long-term dependency, the Transformer networks [51, 56] have been successfully applied to aggregate contextual information in the local feature matching [43, 46] and point cloud registration [23] field.",
            "3": "LoFTR [46] is a detector-free deep learning architecture for local image feature matching.",
            "4": "We visualize the results of PREDATOR [23], LoFTR [46] and the proposed FS6D-DPM.",
            "5": "Group ObjectPREDATOR [23] LoFTR [46] TP-UB FS6D-DPM ADDS ADD ADDS ADD ADDS ADD ADDS ADD\n0002 master chef can 73.",
            "6": "GroupPREDATOR [23] LoFTR [46] TP-UB FS6D-DPM ADD-0."
        },
        "Clustergnn: Cluster-based coarse-to-fine graph neural network for efficient feature matching": {
            "authors": [
                "Yan Shi",
                "Xiong Cai",
                "Yoli Shavit",
                "Jiang Mu",
                "Wensen Feng",
                "Kai Zhang"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Shi_ClusterGNN_Cluster-Based_Coarse-To-Fine_Graph_Neural_Network_for_Efficient_Feature_Matching_CVPR_2022_paper.pdf",
            "ref_texts": "[35] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR , pages 8922\u20138931, 2021. 1, 3, 4, 5",
            "ref_ids": [
                "35"
            ],
            "1": "Recent works [5, 30, 35] have proposed to learn the task of feature matching using graph neural networks (GNNs)\n*These authors contributed equally to this work.",
            "2": "Motivation Existing works [30, 35] which learn the feature matching task with attention-based GNNs, use densely connected graphs.",
            "3": "3), based on the dot product of updated feature representations and the Dual-Softmax operator [26, 35].",
            "4": "The hierarchical clustering enables coarseto-fine grouping Dual-softmax operator [26,35] for computing the matching probability matrix P, by applying the log-softmax operator on both the row and column dimensions of eC, as follows: Pi,j=logSoftMax (eCi,\u00b7)j+logSoftMax (eC\u00b7,j)i."
        },
        "Global matching with overlapping attention for optical flow estimation": {
            "authors": [
                "Shiyu Zhao",
                "Long Zhao",
                "Zhixing Zhang",
                "Enyu Zhou",
                "Dimitris Metaxas"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Zhao_Global_Matching_With_Overlapping_Attention_for_Optical_Flow_Estimation_CVPR_2022_paper.pdf",
            "ref_texts": "[40] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In CVPR , pages 8922\u20138931, 2021.",
            "ref_ids": [
                "40"
            ],
            "1": "Furthermore, LoFTR [40] adopted the selfand crossattention to extract better descriptors for feature matching.",
            "2": "As the supervision in feature matching [40], we minimize the negative log-likelihood of Pcin matched regions as, LM=\u22121\n|Mgt c|X\n(\u02c6i,\u02c6j)\u2208Mgt clogPc(\u02c6i,\u02c6j) (8)\n17595\n Optimization loss."
        },
        "Matchformer: Interleaving attention in transformers for feature matching": {
            "authors": [
                "Qing Wang",
                "Jiaming Zhang",
                "Kailun Yang",
                "Kunyu Peng",
                "Rainer Stiefelhagen"
            ],
            "url": "https://openaccess.thecvf.com/content/ACCV2022/papers/Wang_MatchFormer_Interleaving_Attention_in_Transformers_for_Feature_Matching_ACCV_2022_paper.pdf",
            "ref_texts": "36. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: LoFTR: Detector-free local feature matching with transformers. In: CVPR (2021) 2, 3, 4, 7, 8, 9, 10, 11, 12, 13",
            "ref_ids": [
                "36"
            ],
            "1": "Some partial transformer-based methods [36,14] only design an attention-based decoder and remain the extract-to-match pipeline (see Fig.",
            "2": "For instance, while COTR [14] feeds CNN-extracted features into a transformer-based decoder, SuperGlue [32] and LoFTR [36] only apply attention modules atop the decoder.",
            "3": "For example, compared to LoFTR [36] in Fig.",
            "4": "In extract-to-match methods [10,27,36,32,17,39,35,28] de2748\n\n4 Q.",
            "5": "COTR [14], LoFTR [36], and QuadTree [39] follow sequential extract-to-match processing.",
            "6": "For example, SuperGlue [32] and LoFTR [36] applied selfand cross-attention to process the features which were extracted from CNNs.",
            "7": "Finally, the coarse and fine features are passed to perform the coarse-to-fine matching, as introduced in LoFTR [36].",
            "8": "We use 38,300image pairs from 368 scenarios for training, and the same 1,500testing pairs from [36] for evaluation.",
            "9": "4 LoFTR [36] CVPR\u201921 22.",
            "10": "9 LoFTR [36]+QuadTree [39] ICLR\u201922 23.",
            "11": "TocompareLoFTRandMatchFormer at different data scales on outdoor pose estimation task, both use 8 A100 GPUs, otherwise use 64 A100 GPUs following LoFTR [36].",
            "12": "Qualitative visualization of MatchFormer and LoFTR [36].",
            "13": "31 \u2013 LoFTR [36] CVPR\u201921 100% 52.",
            "14": "2K LoFTR [36] CVPR\u201921 4.",
            "15": "3K LoFTR [36] CVPR\u201921 100%0.",
            "16": "5 LoFTR-OT [36] CVPR\u201921 47.",
            "17": "Method SelfCrossPosPEStdPEPose estimation AUC (%)P@5\u00b0@10\u00b0@20\u00b0 LoFTR [36] CVPR\u201921 15.",
            "18": "Method #Params GFLOPs Runtime P LoFTR [36] 11 307 202 87.",
            "19": "9 LoFTR [36]+QuadTree [39] 13 393 234 89."
        },
        "Lamar: Benchmarking localization and mapping for augmented reality": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2210.10770",
            "ref_texts": "75.Sun, J., Shen, Z., Wang, Y ., Bao, H., Zhou, X.: LoFTR: Detector-free local feature matching with transformers. CVPR (2021) 13",
            "ref_ids": [
                "75"
            ],
            "1": "We thus compare 1) and 2) with SuperGlue to 2) with LoFTR [75], a state-of-the-art dense matcher."
        },
        "Learning visual representations via language-guided sampling": {
            "authors": [
                "Mohamed El",
                "Karan Desai",
                "Justin Johnson"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Banani_Learning_Visual_Representations_via_Language-Guided_Sampling_CVPR_2023_paper.pdf",
            "ref_texts": "[90] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2021. 2",
            "ref_ids": [
                "90"
            ],
            "1": "This is commonly done in dense feature learning, where optical flow [36,46, 82,101] or 3D transformations [29,44,83,90,105] provide natural associations between image patches."
        },
        "Onepose++: Keypoint-free one-shot object pose estimation without CAD models": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/e43f900f571de6c96a70d5724a0fb565-Paper-Conference.pdf",
            "ref_texts": "[47] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR , 2021. 1, 2, 4, 6, 7, 8",
            "ref_ids": [
                "47"
            ],
            "1": "Built upon the detector-free feature matching method LoFTR\n[47], we devise a new keypoint-free SfM method to reconstruct a semi-dense point-cloud model for the object.",
            "2": "The keypoint-free semi-dense feature matching method LoFTR [47] achieves outstanding performance on matching image pairs and shows strong capabilities for finding correspondences in low-textured regions.",
            "3": "More specifically, to better adapt LoFTR [47] for SfM, we design a coarse-to-fine scheme for accurate and complete semi-dense object reconstruction.",
            "4": "Note that there are also methods proposed by keypoint-free matchers [47,62] to adapt themselves for SfM.",
            "5": "They either round matches to grid level [47] or merge matches within a grid to the average location [62] to obtain repeatable \u201ckeypoints\u201d for SfM.",
            "6": "Since our method is highly related to the keypoint-free matching method LoFTR [47], we give it a short overview in Section 3.",
            "7": "1 Background Keypoint-Free Feature Matching Method LoFTR [47].",
            "8": "Inspired by [47], we first extract hierarchical feature maps of Iqand then perform matching in a coarse-to-fine manner for efficiency, as illustrated in Fig.",
            "9": "Linear Attention [19] is used in our model to reduce the computational complexity, following [47].",
            "10": "Similar to [47], we crop a local window Wwith a size of w\u00d7waround \u02dcuqin the fine feature map \u02c6F2D\u2208RH\n2\u00d7W\n2\u00d7Cf.",
            "11": "We jointly train the coarse and fine modules of our 2D-3D matching framework with different supervisions, following [47].",
            "12": "We use the LoFTR [47] outdoor model pre-trained on MegaDepth [28].",
            "13": "To be specific, we compare with HLoc combined with different feature matching methods including SuperGlue [41] and LoFTR [47]."
        },
        "Decoupling makes weakly supervised local feature better": {
            "authors": [
                "Kunhong Li",
                "Longguang Wang",
                "Li Liu",
                "Qing Ran",
                "Kai Xu",
                "Yulan Guo"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Li_Decoupling_Makes_Weakly_Supervised_Local_Feature_Better_CVPR_2022_paper.pdf",
            "ref_texts": ""
        },
        "MS2DG-Net: Progressive correspondence learning via multiple sparse semantics dynamic graph": {
            "authors": [
                "Luanyuan Dai",
                "Yizhang Liu",
                "Jiayi Ma",
                "Lifang Wei",
                "Taotao Lai",
                "Changcai Yang",
                "Riqing Chen"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Dai_MS2DG-Net_Progressive_Correspondence_Learning_via_Multiple_Sparse_Semantics_Dynamic_Graph_CVPR_2022_paper.pdf",
            "ref_texts": "[32] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 2",
            "ref_ids": [
                "32"
            ],
            "1": "COTR [10] and LoFTR [32] introduce the idea of Transformer [37] to improve the performance of networks."
        },
        "Meshloc: Mesh-based visual localization": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2207.10762",
            "ref_texts": "80. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021)",
            "ref_ids": [
                "80"
            ],
            "1": "It is common to use stateof-the-art learned local features [22, 24, 57, 62, 80, 100].",
            "2": "We employ state-of-the-art learned local features [22,24,57,62,80,100] and matching strategies [62].",
            "3": "Using the original database images, we evaluate the approach using multiple learned local features [57,62,80,95,100] and 3D models of different levels of detail.",
            "4": "While SG is based on explicitly detecting local features, LoFTR [80] and Patch2Pix [100]\n(P2P) densely match descriptors between pairs of images and extract matches from the resulting correlation volumes.",
            "5": "3D C PA (SG) [62] [80] [100] + SG [100]\n800I 72.",
            "6": "For the retrieval stage, we follow the literature [60,62,80,100] and use the top50 retrieved database images / renderings based on NetVLAD [2] descriptors extracted from the real database and query images.",
            "7": "5 LoFTR [80] 77.",
            "8": "5m and 5\u25e6/ 5m and 10\u25e6of the ground truth pose 2DSuperGlue LoFTR Patch2Pix P2P R2D2 CAP+SP t 3D C PA (SG) [62] [80] (P2P) [100] + SG [100] [57] [96]\n6.",
            "9": "7 LoFTR [80]6.",
            "10": "8 LoFTR [80]6.",
            "11": "2 LoFTR [80]6.",
            "12": "1 LoFTR [80]6."
        },
        "Dfm: A performance baseline for deep feature matching": {
            "authors": [
                "Ufuk Efe",
                "Kutalmis Gokalp",
                "Aydin Alatan"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2021W/IMW/papers/Efe_DFM_A_Performance_Baseline_for_Deep_Feature_Matching_CVPRW_2021_paper.pdf",
            "ref_texts": "[63] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR , 2021. 7",
            "ref_ids": [
                "63"
            ],
            "1": "As various setups had been used for this evaluation [21,73,53,63], it is relatively difficult to make firm conclusions from Table 1."
        },
        "ECO-TR: Efficient correspondences finding via coarse-to-fine refinement": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2209.12213",
            "ref_texts": "39. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 8922\u20138931 (2021)",
            "ref_ids": [
                "39"
            ],
            "1": "LoFTR [39] establishes accurate semi-dense matches with linear transformers in a coarse-to-fine manner."
        },
        "Map-free visual relocalization: Metric pose relative to a single image": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2210.05494.pdf?trk=public_post_comment-text",
            "ref_texts": "66. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: LoFTR: Detector-free local feature matching with transformers. In: CVPR (2021) 3, 4, 6, 8",
            "ref_ids": [
                "66"
            ],
            "1": "We show that a combination of deep feature matching [54,66] and deep single-image depth prediction [48,40] currently achieves highest relative pose accuracy.",
            "2": "This basic formula has been improved by learning better features [41,52,21,74,8], better matching [54,66] and better robust estimators [47,85,49,13,5,6,67], and progress has been measured in wide-baseline feature matching challenges [35] and small overlap regimes.",
            "3": "LoFTR [66].",
            "4": ", for learning-based 2D correspondence methods such as SuperGlue [54] and LoFTR [66]."
        },
        "Transformatcher: Match-to-match attention for semantic correspondence": {
            "authors": [
                "Seungwook Kim",
                "Juhong Min",
                "Minsu Cho"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Kim_TransforMatcher_Match-to-Match_Attention_for_Semantic_Correspondence_CVPR_2022_paper.pdf",
            "ref_texts": "[49] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR) , 2021. 2",
            "ref_ids": [
                "49"
            ],
            "1": "a match) as an individual element for attention, which differs from LoFTR [49] or CoTR [19] which consider the patch-topatch relations within or across 2D feature maps through selfor cross-attention.",
            "2": "LoFTR [49] extends this idea to dense 2D feature maps of the images to match, leveraging selfand cross-attention layers between the feature maps to generate strong features for matching."
        },
        "3D Video Object Detection with Learnable Object-Centric Global Optimization": {
            "authors": [
                "Jiawei He",
                "Yuntao Chen",
                "Naiyan Wang",
                "Zhaoxiang Zhang"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/He_3D_Video_Object_Detection_With_Learnable_Object-Centric_Global_Optimization_CVPR_2023_paper.pdf",
            "ref_texts": "[39] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR , 2021. 3",
            "ref_ids": [
                "39"
            ],
            "1": "Besides, feature correspondence learning [36, 39] has received extensive attention in recent years."
        },
        "Sparsepose: Sparse-view camera pose regression and refinement": {
            "authors": [
                "Samarth Sinha",
                "Jason Y. Zhang",
                "Andrea Tagliasacchi",
                "Igor Gilitschenski",
                "David B. Lindell"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Sinha_SparsePose_Sparse-View_Camera_Pose_Regression_and_Refinement_CVPR_2023_paper.pdf",
            "ref_texts": "[61] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR , 2021. 2",
            "ref_ids": [
                "61"
            ],
            "1": "A variety of techniques estimate camera poses by extracting keypoints and matching their local features across input images [3, 26, 37, 38, 54, 57, 61]."
        },
        "A case for using rotation invariant features in state of the art feature matchers": {
            "authors": [
                "Georg Bokman",
                "Fredrik Kahl"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022W/IMW/papers/Bokman_A_Case_for_Using_Rotation_Invariant_Features_in_State_of_CVPRW_2022_paper.pdf",
            "ref_texts": "[39] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR , 2021. 1, 2, 3, 5, 6, 7",
            "ref_ids": [
                "39"
            ],
            "1": "Left: LoFTR [39] finds good matches between images under illumination and small viewpoint changes, but the performance deteriorates completely under large rotation changes.",
            "2": "This approach has recently been used with great success in [16, 39, 43].",
            "3": "We will base our experiments on LoFTR [39], which uses a CNN for dense feature description and transformer layers for further feature processing and matching.",
            "4": "Models The base model for our experiments is LoFTR [39].",
            "5": "For more details we refer to [39].",
            "6": "We use the dual-softmax version of the LoFTR matching in all experiments as this version was best performing in [39].",
            "7": "Following LoFTR [39] and DISK [44], validation is done on the Sacre Coeur andSt Peter\u2019s Square sets.",
            "8": "1 Metrics The performance on MegaDepth is measured as in [38, 39] in terms of the area under curve (AUC) of the pose accuracy up to a specific threshold.",
            "9": "2 Metrics The AUC@x metric is used as in [39].",
            "10": "All models perform better than DRC-Net [28] and SuperPoint+SuperGlue [13, 38] which are the comparisons in the LoFTR paper [39, Table 3]."
        },
        "Deep patch visual odometry": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2208.04726",
            "ref_texts": "[34] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021.",
            "ref_ids": [
                "34"
            ],
            "1": "Several deep learning approaches [37, 45, 34, 39, 5, 43] have been introduced to address the robustness issue."
        },
        "Particlesfm: Exploiting dense point trajectories for localizing moving cameras in the wild": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2207.09137",
            "ref_texts": "67.Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 8922\u20138931 (2021)",
            "ref_ids": [
                "67"
            ],
            "1": "While the de-facto methods for localization and mapping operate on sparse feature points, dense pixel correspondences [23,59,67] have shown great potential especially on videos, thanks to the rapid developments of optical flow predictors."
        },
        "The 8-point algorithm as an inductive bias for relative pose prediction by vits": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2208.08988",
            "ref_texts": "[60] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR , 2021. 1, 2, 4, 6",
            "ref_ids": [
                "60"
            ],
            "1": "One line of attack [8, 10, 55, 60, 63] has been to follow the classic pipeline and replace classic correspondence methods [2, 32, 54] with learned ones.",
            "2": "We reconcile the Eight-Point Algorithm [19, 31] with ViTs by showing that a ViT forward pass can be made close to [19, 31] by three minor modifications: (1) bilinear attention [24] instead of attention [64]; (2) quadratic position encodings; and (3) dualsoftmax [53, 60, 63] instead of softmax.",
            "3": "For instance, many methods improve detectors and descriptors [8, 10, 63] or correspondence estimation [3, 47, 50, 55, 60, 72].",
            "4": "We note that our components are also often used in correspondence work [60, 63]; here, we use them directly for pose and show a close relationship between ViTs and [31].",
            "5": "These changes include bilinear attention [24], dual-softmaxes [53, 60, 63], and an explicit positional encoding.",
            "6": "While attention makes this impossible to ensure exactly, we help more closely approximate it with a dual-softmax [53, 60, 63], or set norm (Q1K>\n2)to softmax (Q1K>\n2;1)ffsoftmax (Q1K>\n2;2); (3) where softmax (\u0001;k)applies softmax across the k-th axis.",
            "7": "8 LoFTR [60] 0.",
            "8": "In addition, we compare to LoFTR [60].",
            "9": "Methods that are most competitive (LoFTR [60], SuperGlue [55], [22]) require depth supervision in addition to pose, while the best rotation results ([60], [55]) are produced by correspondence-based methods not predicting translation scale."
        },
        "Virtual correspondence: Humans as a cue for extreme-view geometry": {
            "authors": [
                "Chiu Ma",
                "Anqi Joyce",
                "Shenlong Wang",
                "Raquel Urtasun",
                "Antonio Torralba"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Ma_Virtual_Correspondence_Humans_as_a_Cue_for_Extreme-View_Geometry_CVPR_2022_paper.pdf",
            "ref_texts": "[74] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR , 2021. 2,6,8",
            "ref_ids": [
                "74"
            ],
            "1": "While impressive performance has been achieved [66,74], these methods fall short when there is little overlap among input images, as there are hardly any co-visible 3D points.",
            "2": "10LoFTR [74] 5.",
            "3": "46LoFTR [74]+B A[68] 8.",
            "4": "We also compare with LoFTR [74].",
            "5": "We initialize BARF with the poses recovered by our method and LoFTR [74] respectively.",
            "6": "In contrast, LoFTR [74] fails to estimate the relative camera poses among the two sequences correctly (see the green cameras) and the resulting BARF training gets stuck in local minima."
        },
        "Input-level inductive biases for 3D reconstruction": {
            "authors": [
                "Wang Yifan",
                "Carl Doersch",
                "Relja Arandjelovic",
                "Joao Carreira",
                "Andrew Zisserman"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Yifan_Input-Level_Inductive_Biases_for_3D_Reconstruction_CVPR_2022_paper.pdf",
            "ref_texts": "[51] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR , pages 8922\u20138931, 2021. 2",
            "ref_ids": [
                "51"
            ],
            "1": "Transformers have also contributed to improvements in more general scene correspondence [26,51,58], and even using learned correspondence to improve few-shot learning [9], though these transformers are still applied on feature grids with relatively complex mechanisms to represent correspondence explicitly."
        },
        "Is Geometry Enough for Matching in Visual Localization?": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2203.12979",
            "ref_texts": "69. Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 8922\u20138931, 2021.",
            "ref_ids": [
                "69"
            ],
            "1": "We then report the area under the cumulative curve (AUC) of the mean reprojection error up to 1 /5/10px, inspired by the pose error based AUC metric used in [69, 57].",
            "2": "This metric was inspired by the pose error area under the cummulative curve used in [56, 57, 69]."
        },
        "CoVisPose: Co-visibility Pose Transformer for Wide-Baseline Relative Pose Estimation in 360 Indoor Panoramas": {
            "authors": [],
            "url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136920610.pdf",
            "ref_texts": "47. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 8922\u20138931 (June 2021)",
            "ref_ids": [
                "47"
            ],
            "1": "Many works have focused on modeling those components of the classical pipeline which are especially susceptible to failure, such as feature detection [18,14], correspondence estimation [57,40,47], and model fitting [4,39].",
            "2": "LoFTR [47] learns to perform both steps in a detector-free approach and directly outputs dense correspondences.",
            "3": "Similar to [47], we leverage a transformer to attend to inter and intra-image relationships; however, our transformer is applied over image column featuresequences,analogoustosequenceprocessinginNLP,toefficientlyaggregate the full 360\u25e6range.",
            "4": "Under the wall-floor geometry assumptions, wall geometry may be represented by a 1D contour, the position of which is defined for a given image column ias a vertical angle \u03d5i\u2208[0, \u03c0/2]as in [45,47].",
            "5": "While LSTM and CNN architectures have been applied successfully for per-column prediction of layout [45,47], the local inductive biases of these architectures [56,3] makes them ill-suited for the inherently non-local tasksofco-visibilityandcorrespondenceestimationacrosspairsof360\u25e6panoramas.",
            "6": "We use the LoFTR [47] feature matcher as trained in the original paper, exhaustively run on combinations of crops from a panorama pair, and project putative feature matches back to spherical space.",
            "7": "68 LoFTR-OpenMVG [47] 89.",
            "8": "23 LoFTR-OpenMVG [47] 71.",
            "9": "13 LoFTR-OpenMVG [47] 52.",
            "10": "63 LoFTR-OpenMVG [47] 39.",
            "11": ", classic methods like SIFT [31] and learned ones like LoFTR [47], are competitive inthehigh-overlapregime,wherepointfeaturescanbematchedrobustly."
        },
        "Pats: Patch area transportation with subdivision for local feature matching": {
            "authors": [
                "Junjie Ni",
                "Yijin Li",
                "Zhaoyang Huang",
                "Hongsheng Li",
                "Hujun Bao",
                "Zhaopeng Cui",
                "Guofeng Zhang"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Ni_PATS_Patch_Area_Transportation_With_Subdivision_for_Local_Feature_Matching_CVPR_2023_paper.pdf",
            "ref_texts": "[49] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8922\u20138931, 2021.",
            "ref_ids": [
                "49"
            ],
            "1": "Two-view reconstruction results of LoFTR [49], ASpanFormer [7], PDC-Net+ [58] and our approach on MegaDepth dataset [27].",
            "2": "By removing the information bottleneck caused by detectors, LoFTR [49] produces better feature matches.",
            "3": "Here comes the question: how can we find many-to-many patch matches instead of one-to-one [7, 42, 49]? We observe that finding target patches for a source patch can be regarded as transporting the source patch to the target bounding box, where each target patch inside the box occupies a portion of the content.",
            "4": "[49] propose encoding features from both images based on the Transformer [18, 26, 46, 59], which better model long-range dependencies and achieve satisfying performance.",
            "5": "Following [42, 49], we add a dustbin to the patches collection in both the source image and the target image to handle the partial transportation.",
            "6": "Supervision We collect the ground truth correspondences for supervision following LoFTR [49].",
            "7": "5 LoFTR [49] 42.",
            "8": "5 LoFTR [49] 22.",
            "9": "0 LoFTR [49] 35.",
            "10": "1 LoFTR [49] 21.",
            "11": "3 LoFTR [49] 74.",
            "12": "4 LoFTR [49] 47.",
            "13": "0 LoFTR [49] 88.",
            "14": "Following [42, 49], we train the indoor model of PATS on the ScanNet dataset, while training the outdoor model on MegaDepth.",
            "15": "We evaluate on a subset of YFCC100M, which consists of 4 selected image collections of popular landmarks following [42, 49].",
            "16": "Following [7, 42, 49], we use the localization toolbox HLoc [41] with the matches computed by PATS and evaluate on the LTVL benchmark [55]."
        },
        "D2Former: Jointly Learning Hierarchical Detectors and Contextual Descriptors via Agent-Based Transformers": {
            "authors": [
                "Jianfeng He",
                "Yuan Gao",
                "Tianzhu Zhang",
                "Zhe Zhang",
                "Feng Wu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/He_D2Former_Jointly_Learning_Hierarchical_Detectors_and_Contextual_Descriptors_via_Agent-Based_CVPR_2023_paper.pdf",
            "ref_texts": "[42] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 8922\u2013",
            "ref_ids": [
                "42"
            ],
            "1": "To conquer the above challenges, tremendous image matching approaches have been proposed [7,9,12,16,31,34, 42], among which some dense matching methods [7,16,42]\n*Equal Contribution \u2020Corresponding Author (a) Activation areas for different attention mechanism Full attention Agent based attention High level structures \n(object parts)\n(b) Diverse levels of structures in the image Low level structures \n(corners/edges) Figure 1.",
            "2": "Recently, attention mechanisms have been also introduced to the image matching task, where LoFTR [42] and ASpanFormer [7] are representative works.",
            "3": "We follow the evaluation procedure of [12,31,42] to exclude 8 high-resolution sequences, leaving 108 image sequences, where 52 sequences are under strong illumination changes and 56 sequences are under extreme viewpoint variations.",
            "4": "As for the evaluation metric , we use the same definition as in [42], and report the area under the cumulative curve (AUC) of the corner error.",
            "5": "We follow the same procedure as [34, 42] and use 1500 image pairs from [34] to evaluate our method.",
            "6": "And the evaluation metric follows previous work [42], where the AUC of the indoor pose error at thresholds (5\u25e6,10\u25e6,20\u25e6)is reported.",
            "7": "0K LoFTR [42] 65 .",
            "8": "We take the same 1500 image pairs as [42] to evaluate the proposed model.",
            "9": "Here, the evaluation metric we adopt is the same as [42], where the AUC of the pose error at thresholds (5\u25e6,10\u25e6,20\u25e6)is reported.",
            "10": "We compare our model with previous state-of-the-art image matching methods [9, 12, 16, 31, 32, 34, 42, 47].",
            "11": "Compared with LoFTR [42], our D2Former improves by 5.",
            "12": "Specifically, compared with ASpanFormer [42], our method improves by 5.",
            "13": "49 LoFTR [42] 22.",
            "14": "Methods AUC@ 5\u25e6AUC@ 10\u25e6AUC@ 20\u25e6 LoFTR [42] 40.",
            "15": "57% in AUC@ 20\u25e6compared to LoFTR [42].",
            "16": "31 LoFTR [42] 52."
        },
        "Rotation-invariant transformer for point cloud matching": {
            "authors": [
                "Hao Yu",
                "Zheng Qin",
                "Ji Hou",
                "Mahdi Saleh",
                "Dongsheng Li",
                "Benjamin Busam",
                "Slobodan Ilic"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Rotation-Invariant_Transformer_for_Point_Cloud_Matching_CVPR_2023_paper.pdf",
            "ref_texts": "[37] Jiaming Sun, Zehong Shen, Y uang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matchingwith transformers. In CVPR , 2021. 6",
            "ref_ids": [
                "37"
            ],
            "1": "After a dual-normalization [27,29,37] on/tildewideSfor global feature correlation, superpoints associated to the top-kentries are selected as the coarse correspondence setC/prime={(p/prime i,q/prime j)/vextendsingle/vextendsinglep/prime i\u2208P/prime,q/prime j\u2208Q/prime}."
        },
        "Planeformers: From sparse view planes to 3d reconstruction": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2208.04307",
            "ref_texts": "49. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: LoFTR: Detector-free local feature matching with transformers. CVPR (2021) 3, 10, 11",
            "ref_ids": [
                "49"
            ],
            "1": "Additionally, we use self-attention, a powerful concept that has been successfully used in several vision tasks [45,49,31,3,69].",
            "2": "Our approach of using self-attention through transformers [54] is similar to SuperGlue [45] and LoFTR [49] in that it permits joint reasoning over the set of potential correspondences.",
            "3": "8 LoFTR-DS [49] 0.",
            "4": "3D [42]), a previous approach for camera pose estimation; (Dense Correlation Volumes [4]) which uses correlation volumes to predict rotation; SuperGlue [45] and LoFTR [49], which are learned feature matching system.",
            "5": "Since [45] and [49] solve for an essential matrix, their estimate of translation is intrinsically scale-free [17].",
            "6": "For full-evaluation, we report some of the top performing baselines from [24] along with [49].",
            "7": "52 LoFTR-DS-GT Scale [49] 33.",
            "8": "Our approach is competitive with SuperGlue [45] while LoFTR [49] outperforms competing systems in rotation estimation."
        },
        "PMatch: Paired Masked Image Modeling for Dense Geometric Matching": {
            "authors": [
                "Shengjie Zhu",
                "Xiaoming Liu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_PMatch_Paired_Masked_Image_Modeling_for_Dense_Geometric_Matching_CVPR_2023_paper.pdf",
            "ref_texts": "[48] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR , 2021. 1, 2, 4, 5, 6, 7, 8",
            "ref_ids": [
                "48"
            ],
            "1": "The sparse methods [16, 19, 32, 33, 40, 42, 48, 48, 56] only yield correspondence on sparse or semidense locations while the dense methods [20, 54, 55] estimate pixel-wise correspondence.",
            "2": "In geometric matching [9, 48], apart from the encoder encodes source and support frames into feature maps, there exist transformer blocks which correlate two-frame features, e.",
            "3": ", the LoFTR module [48].",
            "4": "LoFTR [48] and ASpanFormer [9] operate all-to-all matching on coarse-scale discrete grid locations.",
            "5": "Then, we follow 21911\n LoFTR [48] in using linear transformer blocks to correlate the source and support frame feature: {\u03c6s=8\n1,\u03c6s=8\n2}=L\u03b8(\u03c6s=8\n1\u2032, \u03c6s=8\n2\u2032).",
            "6": "6 with: Ts=8\n\u2217, Ps=8\n\u2217=D\u03b8\u0010 eC\u00d7M(X)\u0011\n, (7) where M(X)is cosine positional embeddings with learnable tokens [20, 48], projecting the 2D pixel locations to a high dimensional space to avoid ambiguity when multiple similar patches exist.",
            "7": "(14)Global Matching Loss Following [48], we minimize a binary cross-entropy loss over the correlation volume Cafter a dual-softmax operation: ]Cijkl\u2032=softmax (Cij)\u00b7softmax (Ckl), (15) where CijandCklare(H/8)(W/8)\u00d71vectors.",
            "8": "Then, to comprehensively reflect the contributions from both the density and accuracy of geometric matching, we follow [20, 48] in using the two-view relative camera pose estimation performance as the metric.",
            "9": "We follow [48] in sampling the paired images, weighted by the sequence length and overlap ratio.",
            "10": "3 LoFTR [48] CVPR\u201921 52.",
            "11": "Two-View Camera Pose Estimation Evaluation Protocol In the MegaDepth, ScanNet, and Hpatches datasets, we follow the evaluation protocol of [20, 44, 48] in reporting the pose accuracy AUC curve thresholded at 5,10, and 20degrees.",
            "12": ", sparse methods with detector [29, 44], sparse methods without detector [9,32,48,49,59] and dense methods [13, 20, 46, 54, 55, 61].",
            "13": "We follow the training and validation split of [20, 44, 48].",
            "14": "5 LoFTR [48] CVPR\u201921 22.",
            "15": "Generalization to HPatches Following LoFTR [48], we test the MegaDepth dataset trained model on HPatches.",
            "16": "21915\n\n(a) Source Frame I1\n (b) Support Frame I2\n (c) PMatch (Ours)\n (d) DKM [20]\n (e) LoFTR [48] Figure 6.",
            "17": "We conduct the visual comparison against the SoTA dense [20] and sparse [48] methods on the MegaDepth and the ScanNet datasets.",
            "18": "3 Wo/ Detector LoFTR [48] CVPR\u201921 65.",
            "19": "We follow [48] in evaluation protocol.",
            "20": "In Row 1, (c), and (e), compared to LoFTR [48], multi-scale dense refinement improves fine-scale correspondence accuracy.",
            "21": "Running Time Evaluated on an RTX 2080 Ti GPU, we run160ms for an image of 480\u00d7640while LoFTR [48] runs116ms and DKM [20] runs 148ms."
        },
        "Hierarchical Dense Correlation Distillation for Few-Shot Segmentation": {
            "authors": [
                "Bohao Peng",
                "Zhuotao Tian",
                "Xiaoyang Wu",
                "Chengyao Wang",
                "Shu Liu",
                "Jingyong Su",
                "Jiaya Jia"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Peng_Hierarchical_Dense_Correlation_Distillation_for_Few-Shot_Segmentation_CVPR_2023_paper.pdf",
            "ref_texts": "[35] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 2, 3",
            "ref_ids": [
                "35"
            ],
            "1": "Previous transformer-based methods [35, 49] adopt the self-attention layer to parse features and then feed query and support features to the cross-attention layer for pattern matching, as illustrated in Fig.",
            "2": "Recent work explores combining few-shot semantic segmentation and transformer architecture [19,35].",
            "3": "In previous matching-based methods with transformer architecture, the self-attention and crossattention layers are interleaved for multiple times for feature parsing and pattern matching respectively [35, 49] as shown in Fig."
        },
        "Generating Aligned Pseudo-Supervision from Non-Aligned Data for Image Restoration in Under-Display Camera": {
            "authors": [
                "Ruicheng Feng",
                "Chongyi Li",
                "Huaijin Chen",
                "Shuai Li",
                "Jinwei Gu",
                "Chen Change"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_Generating_Aligned_Pseudo-Supervision_From_Non-Aligned_Data_for_Image_Restoration_in_CVPR_2023_paper.pdf",
            "ref_texts": "[37] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In CVPR , 2021. 8",
            "ref_ids": [
                "37"
            ],
            "1": "Thus, we indirectly measure the displacement error with LoFTR [37] serving as a keypoint matcher."
        },
        "Pump: Pyramidal and uniqueness matching priors for unsupervised learning of local descriptors": {
            "authors": [
                "Jerome Revaud",
                "Vincent Leroy",
                "Philippe Weinzaepfel",
                "Boris Chidlovskii"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Revaud_PUMP_Pyramidal_and_Uniqueness_Matching_Priors_for_Unsupervised_Learning_of_CVPR_2022_paper.pdf",
            "ref_texts": "[59] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR , 2021. 3",
            "ref_ids": [
                "59"
            ],
            "1": "To increase the receptive field during feature extraction, LoFTR [59] proposes detector-free local features matching with transformers."
        },
        "tSF: Transformer-Based Semantic Filter for Few-Shot Learning": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2211.00868",
            "ref_texts": "49. Sun, J., Shen, Z., Wang, Y ., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: CVPR (2021)",
            "ref_ids": [
                "49"
            ],
            "1": "Due to its power in learning representation, it has been introduced in many computer vision tasks, such as image classification [8,58,53], detection [76,4,82], segmentation [78,22,66], image matching [49,43] and few-shot learning [6,70,28]."
        },
        "Laser: Latent space rendering for 2d visual localization": {
            "authors": [
                "Zhixiang Min",
                "Naji Khosravan",
                "Zachary Bessinger",
                "Manjunath Narayana",
                "Sing Bing",
                "Enrique Dunn",
                "Ivaylo Boyadzhiev"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Min_LASER_LAtent_SpacE_Rendering_for_2D_Visual_Localization_CVPR_2022_paper.pdf",
            "ref_texts": "[27] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8922\u20138931, 2021. 2",
            "ref_ids": [
                "27"
            ],
            "1": "Related Works The general 6-DoF relocalization methods either explicitly [3,18,22\u201324,27,28,34,35,40] or implicitly [1,8,14]\nfind appearance correspondences between the query image and scene representations (e."
        },
        "Robust image matching via local graph structure consensus": {
            "authors": [
                "Xingyu Jiang"
            ],
            "url": "https://www.ecb.torontomu.ca/~xzhang/publications/PR2022-RobustImageMatching.pdf",
            "ref_texts": "[19] J. Sun , Z. Shen , Y. Wang , H. Bao , X. Zhou , LoFTR: detector-free local feature matching with transformers, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 8922\u20138931 . ",
            "ref_ids": [
                "19"
            ],
            "1": "Although recently proposed end-to-end learning-based methods [18,19] can construct correct correspondences from raw image pairs, wherein the inlier number and inlier rate are both extremely high thus largely outperforming traditional SIFT, an advanced outlier rejection method is still of great significant in real applications.",
            "2": "[19] J."
        },
        "Wt-mvsnet: window-based transformers for multi-view stereo": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/38e511a690709603d4cc3a1c52b4a9fd-Paper-Conference.pdf"
        },
        "A light touch approach to teaching transformers multi-view geometry": {
            "authors": [
                "Yash Bhalgat",
                "Joao F. Henriques",
                "Andrew Zisserman"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Bhalgat_A_Light_Touch_Approach_to_Teaching_Transformers_Multi-View_Geometry_CVPR_2023_paper.pdf",
            "ref_texts": "[72] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 2, 6, 7, 8",
            "ref_ids": [
                "72"
            ],
            "1": "These features along with learning based local matching methods [69, 72, 86] and robust optimization methods [6, 9, 25] form a powerful toolbox for relative geometry estimation.",
            "2": "We use a combination of LoFTR [72] and MAGSAC++ [6] to generate pseudo-geometry information in one of our compared methods.",
            "3": "cal image feature matching method, LoFTR [72], to extract high-quality semi-dense matches between the image pair.",
            "4": "2, we obtain the pseudo groundtruth geometry information using LoFTR [72] for match-Table 3.",
            "5": "We thank the authors of [6, 72, 74] for open-sourcing their code."
        },
        "Cats++: Boosting cost aggregation with convolutions and transformers": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2202.06817",
            "ref_texts": "[27] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detectorfree local feature matching with transformers,\u201d arXiv preprint arXiv:2104.00680 , 2021.",
            "ref_ids": [
                "27"
            ],
            "1": "Recent approaches [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28] addressed these challenges by carefully designing deep Convolutional Neural Networks (CNNs)-based models analogously to the classical matching pipeline [29], [30], namely feature extraction, cost aggregation, and flow estimation.",
            "2": "Several works [21], [22], [25], [27], [31], [32] focused on the feature extraction stage, as it has been demonstrated that the more powerful feature representation the model learns, the more robust matching is obtained [27], [31], [32].",
            "3": "Recent numerous methods [19], [21], [22], [23], [25], [27], [28] thus have focused on the cost aggregation stage to refine the initial matching scores.",
            "4": "3 Transformers in Vision Transformers [41], the de facto standard for Natural Language Processing (NLP) tasks, has recently imposed significant impacton various tasks in Computer Vision fields such as image classification [42], [66], object detection [67], [68], tracking and matching [27], [69].",
            "5": "For those works addressing visual correspondence, LoFTR [27] uses a cross and self-attention module to refine the feature maps conditioned on both input images, and formulate the hand-crafted aggregation layer with dual-softmax [19], [70], and Optimal Transport [25] to infer correspondences.",
            "6": "4 Transformer Aggregator Several works [27], [42], [67], [68] have shown that given images or features as input, transformers [41] integrate the global context information by learning to find the attention scores for all pairs of tokens.",
            "7": "5 Effects of varying the number of the encoders As done in numerous works [27], [39], [42], [67] that utilize transformers, we can also stack more encoders to increase their capacity and validate the effectiveness by varying the number of encoders.",
            "8": "[27] J."
        },
        "Adaptive Spot-Guided Transformer for Consistent Local Feature Matching": {
            "authors": [
                "Jiahuan Yu",
                "Jiahao Chang",
                "Jianfeng He",
                "Tianzhu Zhang",
                "Jiyang Yu",
                "Feng Wu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Adaptive_Spot-Guided_Transformer_for_Consistent_Local_Feature_Matching_CVPR_2023_paper.pdf"
        },
        "Psmnet: Position-aware stereo merging network for room layout estimation": {
            "authors": [
                "Haiyan Wang",
                "Will Hutchcroft",
                "Yuguang Li",
                "Zhiqiang Wan",
                "Ivaylo Boyadzhiev",
                "Yingli Tian",
                "Sing Bing"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_PSMNet_Position-Aware_Stereo_Merging_Network_for_Room_Layout_Estimation_CVPR_2022_paper.pdf",
            "ref_texts": "[33] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR , 2021. 5",
            "ref_ids": [
                "33"
            ],
            "1": "The transformer (inspired by [33]) consists of self-attention and crossattention layers."
        },
        "Adaptive assignment for geometry aware local feature matching": {
            "authors": [
                "Dihe Huang",
                "Ying Chen",
                "Yong Liu",
                "Jianlin Liu",
                "Shang Xu",
                "Wenlong Wu",
                "Yikang Ding",
                "Fan Tang",
                "Chengjie Wang"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Adaptive_Assignment_for_Geometry_Aware_Local_Feature_Matching_CVPR_2023_paper.pdf",
            "ref_texts": "[29] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR , pages 8922\u20138931, 2021. 2, 3, 4, 5, 6, 7, 8",
            "ref_ids": [
                "29"
            ],
            "1": "To achieve efficiency and accurate matching, the SOTA detector-free matching pipelines [6,11,13,29,30] use a coarse-to-fine structure, in which the patch-level matches are first obtained using the mutual nearest neighbor criterion, and then are refined to a sub-pixel level.",
            "2": "Different from dual-softmax or optimal transport in [28, 29] which guarantees one-to-one correspondence, we allow adaptive assignment (including many-to-one and one-to-one) at patch-level matching during training and inference.",
            "3": "Benefiting from the global receptive field and long-range dependencies from Transformers, LoFTR [29] and its variants [6, 30] extend neighborhood consensus to the whole image, setting the SOTA performance for dense feature matching approaches.",
            "4": "The proposed feature interaction structure can be directly applied to LoFTR [29], QuadTree [30] and ASpanFormer [6], i.",
            "5": "For the loss function of the adaptive matching probability matrix P, we use the same Focal Loss [16] as in LoFTR [29]: LM=FL(P,eP), (6) whereePis the ground-truth labels of the adaptive matching probability matrix calculated from the camera poses and depth maps, and Pis the predicted matching probability matrix.",
            "6": "Inspired by LoFTR [29], we use the same loss Lrefine =1\n|Mk gt|P i,j\u20321\n\u03c3(i)2j\u2032\u2212j\u2032 gtfunction for the final predicted matches, where kis the index calculated above.",
            "7": "Mk gtis the ground-truth matches calculated 5429\n from ground-truth depths and camera poses and \u03c32(\u00b7)is the variance of the corresponding heatmap [29].",
            "8": "Implementations We train AdaMatcher on the MegaDepth datasets following [29], without any data augmentation.",
            "9": "We apply AdaMatcher to LoFTR [29] and its variants (QuadTree Attention [30] and ASpanFormer [6]), named AdaMatcherLoFTR, AdaMatcher-Quad and AdaMatcher-ASpan, respectively.",
            "10": "The image feature extractor is a standard ResNet-FPN [10, 15] architecture, which is identical to LoFTR [29].",
            "11": "We split matching methods into \u201dDetector-based\u201d and \u201dDetector-free\u201d as in LoFTR [29].",
            "12": "72 LoFTR-OT [29] 0.",
            "13": "61 LoFTR-DS [29] 0.",
            "14": "We compare AdaMatcher with traditional and current SOTA methods: 1) detectorbased methods including SIFT [19]+HardNet [21], KeyNet+HardNet [21], R2D2 [23], ASLFeat [20], Disk [32], SuperGlue(SG) [28] with SuperPoint(SP) [8] or Disk [32] detector and SuperGlue [28] with OETR [7] for pre-processing, 2) detector-free methods including PDC-Net [31], LoFTR [29], QuadTree Attention [30] and ASpanFormer [6].",
            "15": "39 LoFTR [29] 60.",
            "16": "LoFTR [29] and its variants [6, 30] are trained using ground-truth matches obtained by one-toone assignment, resulting in the inability to learn the geometry consistency of feature matching.",
            "17": "It can be seen that our proposed method achieves significant performance gains when applied on LoFTR [29], ASpanFormer [6] and QuadTree Attention [30].",
            "18": "9 LoFTR-DS [29] 72.",
            "19": "0 LoFTR-OT [29] 88.",
            "20": "The first row is the result of our baseline method LoFTR [29], \u2019CFI\u2019 represents the LoFTR module (four sets of self-cross attention layers) is replaced by ourCo-visible Feature Interaction (Section 3."
        },
        "Semi-supervised learning of semantic correspondence with pseudo-labels": {
            "authors": [
                "Jiwon Kim",
                "Kwangrok Ryoo",
                "Junyoung Seo",
                "Gyuseong Lee",
                "Daehwan Kim",
                "Hansang Cho",
                "Seungryong Kim"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Kim_Semi-Supervised_Learning_of_Semantic_Correspondence_With_Pseudo-Labels_CVPR_2022_paper.pdf",
            "ref_texts": "[59] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8922\u20138931, 2021. 1",
            "ref_ids": [
                "59"
            ],
            "1": "Although formulated in various ways, most recent approaches [9,36,40,41,43,45,48,49,51,54,63,64] addressed these challenges by carefully designing deep neural networks, such as CNNs [36,40,43,45,48,49,51,54,63,64] or Transformers [9,59], based models."
        },
        "Camera Pose Estimation and Localization with Active Audio Sensing": {
            "authors": [],
            "url": "https://openreview.net/pdf?id=AQsqxD1dsN",
            "ref_texts": "86. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: LoFTR: Detector-free local feature matching with transformers. CVPR (2021)",
            "ref_ids": [
                "86"
            ],
            "1": "The most prevalent methods are feature-matching methods that use a pose solver integrated within a RANSAC framework [69], with state-of-the-art approaches using learned methods for feature detection [27, 71, 96, 6], matching [74, 86] and robust model fitting [108, 70, 11, 87]."
        },
        "Mvsformer: Learning robust image representations via transformers and temperature-based depth for multi-view stereo": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2208.02541",
            "ref_texts": "15 Under review as submission to TMLR Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones for object detection. arXiv preprint arXiv:2203.16527 , 2022. Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 2117\u20132125, 2017a. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. InProceedings of the IEEE international conference on computer vision , pp. 2980\u20132988, 2017b. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 10012\u201310022, 2021. Keyang Luo, Tao Guan, Lili Ju, Haipeng Huang, and Yawei Luo. P-mvsnet: Learning patch-wise matching confidence aggregation for multi-view stereo. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 10452\u201310461, 2019. Xinjun Ma, Yue Gong, Qirui Wang, Jingwei Huang, Lei Chen, and Fan Yu. Epp-mvsnet: Epipolar-assembling based depth prediction for multi-view stereo. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 5732\u20135740, 2021. Zhenxing Mi, Di Chang, and Dan Xu. Generalized binary search network for highly-efficient multi-view stereo.arXiv preprint arXiv:2112.02338 , 2021. Rui Peng, Rongjie Wang, Zhenyu Wang, Yawen Lai, and Ronggang Wang. Rethinking depth estimation for multi-view stereo: A unified representation and focal loss. arXiv preprint arXiv:2201.01501 , 2022. Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. arXiv preprint arXiv:1710.05941 , 2017. Joseph Redmon and Ali Farhadi. Yolo9000: better, faster, stronger. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 7263\u20137271, 2017. Johannes L Sch\u00f6nberger, Enliang Zheng, Jan-Michael Frahm, and Marc Pollefeys. Pixelwise view selection for unstructured multi-view stereo. In European Conference on Computer Vision , pp. 501\u2013518. Springer, 2016. Thomas Schops, Johannes L Schonberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, and Andreas Geiger. A multi-view stereo benchmark with high-resolution images and multi-camera videos. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp. 3260\u20133269, 2017. Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202 , 2020. Christian Sormann, Patrick Kn\u00f6belreiter, Andreas Kuhn, Mattia Rossi, Thomas Pock, and Friedrich Fraundorfer. Bp-mvsnet: Belief-propagation-layers for multi-view-stereo. In 2020 International Conference on 3D Vision (3DV) , pp. 394\u2013403. IEEE, 2020. Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 8922\u20138931, 2021. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems , 30, 2017. Fangjinhua Wang, Silvano Galliani, Christoph Vogel, Pablo Speciale, and Marc Pollefeys. Patchmatchnet: Learned multi-view patchmatch stereo. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 14194\u201314203, 2021a."
        },
        "Attention meets geometry: Geometry guided spatial-temporal attention for consistent self-supervised monocular depth estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2110.08192",
            "ref_texts": "[40] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR , 2021.",
            "ref_ids": [
                "40"
            ],
            "1": "Recent works [40] have shown that transformer models with selfand cross-attention can outperform fully convolution networks [27] for the task of finding dense correspondences between image pairs.",
            "2": "Temporal-Attention Layer Inspired by the correlation layer in optical flow [21] and recent dense matching pipelines [40], we formulate a novel temporal attention across frames by exploiting the temporal image sequence input of the self-supervised training scheme."
        },
        "Context-aware sequence alignment using 4D skeletal augmentation": {
            "authors": [
                "Taein Kwon",
                "Bugra Tekin",
                "Siyu Tang",
                "Marc Pollefeys"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Kwon_Context-Aware_Sequence_Alignment_Using_4D_Skeletal_Augmentation_CVPR_2022_paper.pdf",
            "ref_texts": "[53] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8922\u20138931, 2021. 3, 4",
            "ref_ids": [
                "53"
            ],
            "1": "[45,53] proposed Transformers for the task of image alignment.",
            "2": "By contrast to the earlierTransformer-based works [45, 53], our attention module is integrated into a self-supervised learning framework that uses 4D augmentations for sequence matching.",
            "3": "Different from other vision-based tasks [14,53], we only need 1D positional encodings as the order of joints in the skeleton is fixed."
        },
        "Efficient large-scale localization by global instance recognition": {
            "authors": [
                "Fei Xue",
                "Ignas Budvytis",
                "Daniel Olmeda",
                "Roberto Cipolla"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Xue_Efficient_Large-Scale_Localization_by_Global_Instance_Recognition_CVPR_2022_paper.pdf",
            "ref_texts": "[49] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In CVPR , 2021. 2, 5, 6",
            "ref_ids": [
                "49"
            ],
            "1": "These problems can be partially solved by leveraging more powerful local features [3,11,29,35,60,61] or matching networks [13, 14, 25, 36, 38, 49, 65, 67], while their high computational [35, 38] and memory [25, 36, 49] cost impair their efficiency in real applications.",
            "2": "We also compare it with state-of-the-art pipeline HLoc [37] with different local features [10, 11, 28, 33, 35, 50, 61] (H) and those with advanced or dense matching networks [13, 31, 38, 39, 49, 59, 67] (M).",
            "3": "9 LoFTER [49] 88.",
            "4": "Although some works in group M achieve close results to ours by utilizing advanced [38] or dense matching models [49], they sacrifice the time [38] or memory [49] efficiency, and require additional datasets for training."
        },
        "FvOR: Robust joint shape and pose optimization for few-view object reconstruction": {
            "authors": [
                "Zhenpei Yang",
                "Zhile Ren",
                "Miguel Angel",
                "Zaiwei Zhang",
                "Qi Shan",
                "Qixing Huang"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_FvOR_Robust_Joint_Shape_and_Pose_Optimization_for_Few-View_Object_CVPR_2022_paper.pdf",
            "ref_texts": "[51] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 8922\u20138931, 2021. 3",
            "ref_ids": [
                "51"
            ],
            "1": "Inspired by [31,51], the multiimage attention module is composed by alternating between self-attention and cross-attention blocks."
        },
        "Scalenet: A shallow architecture for scale estimation": {
            "authors": [
                "Axel Barroso",
                "Yurun Tian",
                "Krystian Mikolajczyk"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Barroso-Laguna_ScaleNet_A_Shallow_Architecture_for_Scale_Estimation_CVPR_2022_paper.pdf",
            "ref_texts": "[45] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8922\u20138931, 2021. 2, 6",
            "ref_ids": [
                "45"
            ],
            "1": "Besides the added complexity, a multi-scale pyramid is not always a straightforward solution to incorporate in some methods, such as dense correspondence networks [18,45,53].",
            "2": ", that use two input images at the same time to establish the local or dense correspondences [14, 18, 25, 35, 45, 53, 55], but, in that scenario, there is no 12809\n\n512 x 15 x 15 fB 1 x L bins Self Corr.",
            "3": "We use Lowe\u2019s ratio test [23] and MAGSAC [2] to compute camera poses, and, as in [40,45], report the AUC of the pose errors at 5\u25e6,10\u25e6, and 20\u25e6, where the error is calculated as the maximum of the rotation and translation angular errors.",
            "4": "the AUC of the camera pose error at 5\u25e6,10\u25e6, and 20\u25e6 as in [40, 45].",
            "5": "Note that ScaleNet can also be combined with other recent methods [45, 51, 52]."
        },
        "Guide local feature matching by overlap estimation": {
            "authors": [
                "Ying Chen",
                "Dihe Huang",
                "Shang Xu",
                "Jianlin Liu",
                "Yong Liu"
            ],
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/19913/19672",
            "ref_texts": "4947. Sattler, T.; Maddern, W.; Toft, C.; Torii, A.; Hammarstrand, L.; Stenborg, E.; Safari, D.; Okutomi, M.; Pollefeys, M.; Sivic, J.; et al. 2018. Benchmarking 6dof outdoor visual localization in changing conditions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 8601\u20138610. Schonberger, J. L.; and Frahm, J.-M. 2016. Structure-frommotion revisited. In Proceedings of the IEEE conference on computer vision and pattern recognition, 4104\u20134113. Sun, J.; Shen, Z.; Wang, Y .; Bao, H.; and Zhou, X. 2021a. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 8922\u20138931. Sun, P.; Zhang, R.; Jiang, Y .; Kong, T.; Xu, C.; Zhan, W.; Tomizuka, M.; Li, L.; Yuan, Z.; Wang, C.; et al. 2021b. Sparse r-cnn: End-to-end object detection with learnable proposals. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 14454\u201314463. Tian, Z.; Shen, C.; Chen, H.; and He, T. 2019. Fcos: Fully convolutional one-stage object detection. In Proceedings of the IEEE/CVF international conference on computer vision , 9627\u20139636. Tyszkiewicz, M. J.; Fua, P.; and Trulls, E. 2020. DISK: Learning local features with policy gradient. arXiv preprint arXiv:2006.13566. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, \u0141.; and Polosukhin, I. 2017. Attention is all you need. In Advances in neural information processing systems, 5998\u20136008. V oigtlaender, P.; Luiten, J.; Torr, P. H.; and Leibe, B. 2020. Siam r-cnn: Visual tracking by re-detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 6578\u20136588. Wang, W.; Yao, L.; Chen, L.; Cai, D.; He, X.; and Liu, W.",
            "ref_ids": [
                "4947"
            ]
        },
        "3DG-STFM: 3D Geometric Guided Student-Teacher Feature Matching": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2207.02375",
            "ref_texts": "44. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition pp. 8922\u20138931 (2021) 1, 2, 4, 6, 7, 9, 10, 11, 12, 13",
            "ref_ids": [
                "44"
            ],
            "1": "Several recent works [35,36,24,44] attempted to avoid the detection step and established a dense matching by considering all points from a regular grid.",
            "2": "To generate dense ground-truth correspondences as supervision, depth maps, camera intrinsic and extrinsic matrices are used for the calculation of point reprojections from one image to the other [41,44,24].",
            "3": "1: Comparison between dense local feature matching method LoFTR [44] and the proposed method 3DG-STFM.",
            "4": "To address the above problem, detector-free methods [36,24,44,22] proposed pixel-wise dense matching methods.",
            "5": "Recently, LoFTR [44] was proposed to learn global consensus between image correspondences by leveraging Transformers.",
            "6": "Our method is based on the matching strategies mentioned in LoFTR [44] due to their high performances.",
            "7": "2, two transformer-based matching modules, inspired by [44], are adopted in both teacher and student branches of our 3DG-STFM system.",
            "8": "We follow [44] to set a focal loss term, FLwith predicted probability p, to address the imbalance between matching and non-matching pairs.",
            "9": "For direct supervision, we follow the same procedure mentioned in [41,35,44] that uses the camera intrinsic, extrinsic matrices, and depth maps to compute the dense correspondences.",
            "10": "It is worth mentioning that our method is based on LoFTR [44], which provides two version implementations for the outdoor dataset in their official code.",
            "11": "Following the [41,44], we sample 230M image pairs with overlap scores between 0.",
            "12": "84 Detector freeLoFTR [44] 22.",
            "13": "Our student model is compared to LoFTR [44] in indoor and outdoor scenes.",
            "14": "Following [44], we report the AUC of the pose error at thresholds (5\u25e6,10\u25e6,20\u25e6).",
            "15": "31 LoFTR [44] 51.",
            "16": "Following previous work [41,44], we select 108 image sequences under large illumination changes or significant viewpoint variations for evaluation.",
            "17": "0k LoFTR [44] 63."
        },
        "A Unified Transformer Based Tracker for Anti-UAV Tracking": {
            "authors": [
                "Qianjin Yu",
                "Yinchao Ma",
                "Jianfeng He",
                "Dawei Yang",
                "Tianzhu Zhang"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023W/Anti-UAV/papers/Yu_A_Unified_Transformer_Based_Tracker_for_Anti-UAV_Tracking_CVPRW_2023_paper.pdf",
            "ref_texts": "[49] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR , 2021. 5, 6",
            "ref_ids": [
                "49"
            ],
            "1": "As shown in Figure 2, the dense matching algorithm, LoFTR [49], is first applied to find key point pairs.",
            "2": "More details of the dense matching algorithm can refer to LoFTR [49].",
            "3": "We try different models and eventually choose LoFTR [49] model as our dense matching algorithm."
        },
        "Global-aware registration of less-overlap RGB-D scans": {
            "authors": [
                "Che Sun",
                "Yunde Jia",
                "Yi Guo",
                "Yuwei Wu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Sun_Global-Aware_Registration_of_Less-Overlap_RGB-D_Scans_CVPR_2022_paper.pdf",
            "ref_texts": "[25] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 8922\u2013",
            "ref_ids": [
                "25"
            ],
            "1": "The local neighborhood information is often similar with less discriminative [14, 25], especially in blurred and textureless regions."
        },
        "DKM: Dense kernelized feature matching for geometry estimation": {
            "authors": [
                "Johan Edstedt",
                "Ioannis Athanasiadis",
                "Marten Wadenback",
                "Michael Felsberg"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Edstedt_DKM_Dense_Kernelized_Feature_Matching_for_Geometry_Estimation_CVPR_2023_paper.pdf",
            "ref_texts": "[41] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8922\u20138931, 2021. 1, 2, 5, 6, 7",
            "ref_ids": [
                "41"
            ],
            "1": "To tackle this issue, semisparse or detector-free methods such as LoFTR [41] and Patch2Pix [53] were introduced.",
            "2": "This has the benefit of avoiding the detection problem [41].",
            "3": "These methods typ-ically extract matches by (soft-)mutual-nearest neighbours, or optimal transport [32, 41].",
            "4": "[41] use transformers, with additional improvements by later work [7, 44, 50].",
            "5": "This approach has similarities to the approach in LoFTR [41], but they indirectly apply the constraint by finding mutual nearest neighbours.",
            "6": "(10) Like previous semi-sparse [7, 41] and dense works [48] we threshold the estimated certainty.",
            "7": "Specifically, for the warp loss we use the \u21132distance between the predicted and ground truth warp, as in [41].",
            "8": "7 LoFTR [41] CVPR\u201921 65.",
            "9": "State-of-the-Art Comparison Similarly to previous approaches [7, 36, 41, 44], we train and evaluate our approach separately on outdoor andindoor geometry estimation.",
            "10": "Outdoor Training: We train on the real world dataset MegaDepth [22], using the same training and test split as in previous work [7, 41].",
            "11": "Indoor Training: For indoor two-view pose estimation we additionally train on the ScanNet [9] dataset in a similar fashion as previous work [36, 41] and use a resolution of 480\u00d7640.",
            "12": "We follow the evaluation protocol proposed LoFTR [41], resizing the shorter side of the images to 480.",
            "13": "MegaDepth-1500 Pose Estimation: We use the MegaDepth-1500 test set [41] which consists of 1500 pairs from scene 0015 (St.",
            "14": "We follow the protocol in [7, 41] and use a RANSAC threshold of 0.",
            "15": "0 LoFTR [41] CVPR\u201921 52.",
            "16": "8 LoFTR [41] CVPR\u201921 22.",
            "17": "4 LoFTR [41] 47."
        },
        "Transvcl: Attention-enhanced video copy localization network with flexible supervision": {
            "authors": [
                "Sifeng He",
                "Yue He",
                "Minlong Lu",
                "Chen Jiang",
                "Xudong Yang",
                "Feng Qian",
                "Xiaobo Zhang",
                "Lei Yang",
                "Jiandong Zhang"
            ],
            "url": "https://ojs.aaai.org/index.php/AAAI/article/view/25158/24930",
            "ref_texts": "806 Sun, J.; Shen, Z.; Wang, Y .; Bao, H.; and Zhou, X. 2021. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 8922\u20138931. Taherkhani, F.; Dabouei, A.; Soleymani, S.; Dawson, J.; and Nasrabadi, N. M. 2021. Self-supervised wasserstein pseudolabeling for semi-supervised image classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 12267\u201312277. Tan, H.-K.; Ngo, C.-W.; Hong, R.; and Chua, T.-S. 2009. Scalable Detection of Partial Near-Duplicate Videos by Visual-Temporal Consistency. MM \u201909, 145\u2013154. New York, NY , USA: Association for Computing Machinery. ISBN 9781605586083. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, \u0141.; and Polosukhin, I. 2017. Attention is all you need. Advances in neural information processing systems, 30. Yang, J.; Dong, X.; Liu, L.; Zhang, C.; Shen, J.; and Yu, D."
        },
        "SFD2: Semantic-guided Feature Detection and Description": {
            "authors": [
                "Fei Xue",
                "Ignas Budvytis",
                "Roberto Cipolla"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Xue_SFD2_Semantic-Guided_Feature_Detection_and_Description_CVPR_2023_paper.pdf",
            "ref_texts": "[67] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In CVPR , 2021. 1, 3, 6",
            "ref_ids": [
                "67"
            ],
            "1": "Recently, advanced matchers based on sparse keypoints [8, 55, 65] or dense pixels [9, 18, 19, 33, 40, 47, 67, 74] are proposed to enhance keypoint/pixel-wise matching and have obtained remarkable accuracy.",
            "2": "As NN matching is unable to incorporate spatial connections of keypoints for matching, advanced matchers are proposed to enhance the accuracy by leveraging the spatial context of a set of keyppoints [8, 55, 65] or an image patch [9, 18, 33, 52, 67, 84].",
            "3": "Dense matchers [9, 33, 52, 67] compute pixel-wise correspondence from correlation volumes, so they undergo the high time and memory cost as sparse matchers [8, 55, 65].",
            "4": "9 LoFTER [67] 88.",
            "5": ", Superglue (SPG) [55], SGMNet [8], ClusterGNN [65] and ASpanFormer [9], LoFTER [67], Patch2Pix [84], Dual-RCNet [33]."
        },
        "Data Association Between Event Streams and Intensity Frames Under Diverse Baselines": {
            "authors": [],
            "url": "https://ci.idm.pku.edu.cn/Zhang_ECCV22e.pdf",
            "ref_texts": "52. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: Proc. of Conference on Computer Vision and Pattern Recognition (CVPR). pp. 8922\u20138931 (2021)",
            "ref_ids": [
                "52"
            ],
            "1": "2 top), similar to the design of methods [50, 52], as Transformer module can enlarge each feature\u2019s receptive field and thereby include long-range association during matching.",
            "2": "Following DETR [4] and LoFTR [52], we apply the 2D extension of positional encoding in our Transformer module.",
            "3": "We sample a part of the synthetic dataset for training referring to [52, 50] to synthesize event streams by V2E [22].",
            "4": "For evaluation on the synthetic dataset, following [31, 52], we report the area under the cumulative curve (AUC) of the pose errors at different thresholds."
        },
        "Deep kernelized dense geometric matching": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2202.00667",
            "ref_texts": "[40] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8922\u20138931, 2021.",
            "ref_ids": [
                "40"
            ],
            "1": "To tackle this issue, semisparse or detector-free methods such as LoFTR [40] and Patch2Pix [52] were introduced.",
            "2": "This has the benefit of avoiding the detection problem [40].",
            "3": "These methods typically extract matches by (soft-)mutual-nearest neighbours, or optimal transport [32, 40].",
            "4": "[40] use transformers, with additional improvements by later work [6, 43, 49].",
            "5": "This approach has similarities to the approach in LoFTR [40], but they instead indirectly apply the constraint by finding mutual nearest neighbours.",
            "6": "This approach is written as, fxA i;xB igN i=1\u0018^pA!B: (10) Like previous semi-sparse [6, 40] and dense works [47] we threshold the estimated certainty.",
            "7": "Specifically, for the warp loss we use the `2distance between the predicted and ground truth warp, as in [40].",
            "8": "State-of-the-Art Comparison Similarly to previous approaches [6, 35, 40, 43], we train and evaluate our approach separately on outdoor andindoor geometry estimation.",
            "9": "Outdoor Training We train on the real world dataset MegaDepth [21], using the same training and test split as in previous work [6, 40].",
            "10": "Indoor Training For indoor two-view pose estimation we additionally train on the ScanNet [8] dataset in a similar fashion as previous work [35, 40] and use a resolution of 480\u0002640.",
            "11": "We follow the evaluation protocol proposed LoFTR [40], resizing the shorter side of the images to 480.",
            "12": "MegaDepth-1500 Pose Estimation We use the MegaDepth-1500 test set [40] which consists of 1500\n6 Table 1.",
            "13": "7 LoFTR [40] CVPR\u201921 65.",
            "14": "0 LoFTR [40] CVPR\u201921 52.",
            "15": "We follow the protocol in [6, 40] and use a RANSAC threshold of 0.",
            "16": "8 LoFTR [40] CVPR\u201921 22."
        },
        "SIFT matching by context exposed": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2106.09584",
            "ref_texts": "[62] J. Sun, Z. Shen, Y. Wang, H. Bao, and X. Zhou, \u201cLoFTR: Detectorfree local feature matching with transformers,\u201d 2021.",
            "ref_ids": [
                "62"
            ],
            "1": "More recently, the network designed in [62] was able to get keypoint-free correspondences for a coarse-to-fine matching strategy, with initial rough dense correspondences provided by exploiting self and cross attention inferred by transformers.",
            "2": "[62] J."
        },
        "TopicFM: Robust and interpretable topic-assisted feature matching": {
            "authors": [
                "Khang Truong",
                "Soohwan Song",
                "Sungho Jo"
            ],
            "url": "https://ojs.aaai.org/index.php/AAAI/article/view/25341/25113",
            "ref_texts": "2019. From coarse to fine: Robust hierarchical localization at large scale. In CVPR, 12716\u201312725. Sarlin, P.-E.; DeTone, D.; Malisiewicz, T.; and Rabinovich, A. 2020. Superglue: Learning feature matching with graph neural networks. In CVPR, 4938\u20134947. Sattler, T.; Leibe, B.; and Kobbelt, L. 2012. Improving image-based localization by active correspondence search. InECCV, 752\u2013765. Springer. Selvaraju, R. R.; Cogswell, M.; Das, A.; Vedantam, R.; Parikh, D.; and Batra, D. 2017. Grad-cam: Visual explanations from deep networks via gradient-based localization. InICCV, 618\u2013626. Shi, Y.; Cai, J.-X.; Shavit, Y.; Mu, T.-J.; Feng, W.; and Zhang, K. 2022. ClusterGNN: Cluster-based Coarse-to-Fine Graph Neural Network for Efficient Feature Matching. In CVPR, 12517\u201312526. Sivic, J.; and Zisserman, A. 2003. Video Google: A text retrieval approach to object matching in videos. In ICCV, volume 3, 1470\u20131470. IEEE Computer Society. Strudel, R.; Garcia, R.; Laptev, I.; and Schmid, C. 2021. Segmenter: Transformer for semantic segmentation. In ICCV, 7262\u20137272. Sun, J.; Shen, Z.; Wang, Y.; Bao, H.; and Zhou, X. 2021. LoFTR: Detector-free local feature matching with transformers. In CVPR, 8922\u20138931. Taira, H.; Okutomi, M.; Sattler, T.; Cimpoi, M.; Pollefeys, M.; Sivic, J.; Pajdla, T.; and Torii, A. 2018. InLoc: Indoorvisual localization with dense matching and view synthesis. InCVPR, 7199\u20137209. Tyszkiewicz, M.; Fua, P.; and Trulls, E. 2020. DISK: Learning local features with policy gradient. NeurIPS, 33: 14254\u2013",
            "ref_ids": [
                "2019"
            ]
        },
        "End2End multi-view feature matching using differentiable pose optimization": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2205.01694",
            "ref_texts": "[46] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 8918\u2013",
            "ref_ids": [
                "46"
            ],
            "1": "Recent learning-based approaches [26,34,44,46] instead leverage the greater image context to improve the matching, e.",
            "2": "LoFTR [46] and COTR [26] recently proposed detector-free methods that operate on RGB images directly.",
            "3": "Hence, we focus on comparisons to recent matching networks: SuperGlue [44], LoFTR [46], COTR [26], and 3DG-STFM [34].",
            "4": "Two-View Pose Estimation Following prior work [34, 44, 46], we evaluate on the same 1500 image pairs of ScanNet and MegaDepth and compute the area under the curve (AUC) in % at the thresholds[5fl;10fl;20fl]of the pose error, i.",
            "5": "Multi-View Pose Estimation For multi-view evaluation, we sample test images with the same overlap criterion as used by prior work to sample image pairs [34, 44, 46].",
            "6": "8 LoFTR [46] 22.",
            "7": "6 LoFTR [46] 15.",
            "8": "7 LoFTR [46] 24.",
            "9": "7 LoFTR [46] 52.",
            "10": "2 LoFTR [46] 15.",
            "11": "6 LoFTR [46] 25.",
            "12": "7 LoFTR [46] 20.",
            "13": "5 LoFTR [46] 47.",
            "14": "SuperGlue [44] 60ms 371 ms 126 ms 58 ms LoFTR [46] 108 ms 976 ms 148 ms 579 ms COTR [26] 37950 ms 357096 ms 126 ms 47ms 3DG-STFM [34] 130 ms 1176 ms 201 ms 816 ms Ours 60ms 338ms 52ms 57 ms Table 6.",
            "15": "7\n1\n2 Input 5-tuples SuperGlue [44] LoFTR [46] COTR [26] 3DG-STFM [34] Ours1m 0m Figure 4.",
            "16": "1\n2 Input 5-tuples SuperGlue [44] LoFTR [46] COTR [26] 3DG-STFM [34] Ours1\n0 Figure 5.",
            "17": "11\n1\n2\n3\n4 Input 5-tuples SuperGlue [44] LoFTR [46] COTR [26] 3DG-STFM [34] Ours1m 0m Figure 7.",
            "18": "1\n2\n3\n4 Input 5-tuples SuperGlue [44] LoFTR [46] COTR [26] 3DG-STFM [34] Ours1\n0 Figure 8.",
            "19": "Following prior work [34, 44, 46], an overlap range of [0:4;0:8] is used on ScanNet.",
            "20": "On MegaDepth, we follow the data split of prior work [34, 46, 50] using scenes 0015 and 0022 for validation, scenes 0008, 0019, 0021, 0024, 0025, 0032, 0063 and 1589 for testing and the remaining scenes for training.",
            "21": "Scenes with low quality depth maps are filtered out [26, 34, 46, 50].",
            "22": "Baseline Comparison Details In the baseline comparison, we use the network weights provided by the authors of SuperGlue [44], LoFTR [46], COTR [26] and 3DG-STFM [34]."
        },
        "Multi-view stereo with transformer": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2112.00336",
            "ref_texts": "[27] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In Conference on Computer Vision and Pattern Recognition (CVPR) , pages 8922\u20138931, 2021. 2",
            "ref_ids": [
                "27"
            ],
            "1": "Besides, Transformers are utilized for homography estimation, relative pose estimation, and visual localization in [27]."
        },
        "Self-supervised correspondence estimation via multiview registration": {
            "authors": [
                "Mohamed El",
                "Ignacio Rocco",
                "David Novotny",
                "Andrea Vedaldi",
                "Natalia Neverova",
                "Justin Johnson",
                "Ben Graham"
            ],
            "url": "https://openaccess.thecvf.com/content/WACV2023/papers/Banani_Self-Supervised_Correspondence_Estimation_via_Multiview_Registration_WACV_2023_paper.pdf",
            "ref_texts": "[62] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR , 2021. 3, 6, 7",
            "ref_ids": [
                "62"
            ],
            "1": "[62], we generate the feature grid at a lower resolution (1/4) than the input image.",
            "2": "Heuristics such as Lowe\u2019s ratio test [43] prefer unique matches and have been extended to self-supervised pointcloud registration [20, 21] and attention-based matching [27, 57, 62].",
            "3": "We consider three end-to-end approaches: SuperGlue [57], LoFTR [62], and BYOC [21].",
            "4": "4 LoFTR [62] 16.",
            "5": "5 LoFTR [62] + GART 21.",
            "6": "2 LoFTR [62] + RANSAC 75.",
            "7": "2 LoFTR [62] + Ours 84."
        },
        "Self-supervised endoscopic image key-points matching": {
            "authors": [
                "Manel Farhat",
                "Houda Chaabouni",
                "Achraf Ben"
            ],
            "url": "https://arxiv.org/pdf/2208.11424",
            "ref_texts": "2571). Barcelona, Spain. Saha, S., Xiao, D., Frost, S., & Kanagasingam, Y. (2016). A two-step approach for longitudinal registration of retinal images. Journal of Medical Systems , 40. Sarlin, P.-E., DeTone, D., Malisiewicz, T., & Rabinovich, A. (2020). SuperGlue: Learning feature matching with graph neural networks. In CVPR . Schrofi, F., Kalenichenko, D., & Philbin, J. (2015). Facenet: A uniffed embedding for face recognition and clustering. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , . Shan, H., Jia, X., Yan, P., Li3, Y., Paganetti, H., & Wang, G. (2020). Synergizing medical imaging and radiotherapy with deep learning. Machine Learning: Science and Technology ,1. Sharib, A., C., D., Galbrun, E., Guillemin, F., & Blondel, W. (2016). Anisotropic motion estimation on edge preserving riesz wavelets for robust video mosaicing. Pattern Recognition ,51, 425{442. Sharib, A., Daul, C., Weibel, T., & Blondel, W. (2013). Fast mosaicing of cystoscopic images from dense correspondence: Combined surf and tv-l1 optical ow method. In 2013 IEEE Int. Conf. on Image Processing (pp. 1291{1295). Simo-Serra, E., Trulls, E., Ferraz, L., Kokkinos, I., Fua, P., & Moreno-Noguer, F. (2015). Discriminative learning of deep convolutional feature point descriptors. In 2015 IEEE Int. Conf. on Computer Vision (ICCV) (pp. 118{126). Spitzer, H., Kiwitz, K., Amunts, K., Harmeling, S., & Dickscheid, T. (2018). Improving cytoarchitectonic segmentation of human brain areas with self32 supervised siamese networks. In International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. 663{671). Springer. Sun, J., Shen, Z., Wang, Y., Bao, H., & Zhou, X. (2021). LoFTR: Detector-free local feature matching with transformers. CVPR , . Sung, H., Ferlay, J., L., S. R., Laversanne, M., Soerjomataram, I., Jemal, A., & Bray, F. (2020). Global cancer statistics 2020: Globocan estimates of incidence and mortality worldwide for 36 cancers in 185 countries. CA: a cancer journal for clinicians ,0, 1{41. Tajbakhsh, N., Shin, J. Y., Gurudu, S. R., Hurst, R. T., Kendall, C. B., Gotway, M. B., & Liang, J. (2016). Convolutional neural networks for medical image analysis: Full training or ffne tuning? IEEE Transactions on Medical Imaging ,35, 1299{1312. Tian, Y., Barroso Laguna, A., Ng, T., Balntas, V., & Mikolajczyk, K. (2020). Hynet: Learning local descriptor with hybrid similarity measure and triplet loss. In NeurIPS . Tian, Y., Fan, B., & Wu, F. (2017). L2-net: Deep learning of discriminative patch descriptor in euclidean space. In 2017 IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) (pp. 6128{6136). Tian, Y., Yu, X., Fan, B., Wu, F., Heijnen, H., & Balntas, V. (2019). Sosnet: Second order similarity regularization for local descriptor learning. In CVPR . Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., & Polosukhin, I. (2017). Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, & R. Garnett (Eds.), Advances in Neural Information Processing Systems (p."
        },
        "BundleSDF: Neural 6-DoF Tracking and 3D Reconstruction of Unknown Objects": {
            "authors": [
                "Bowen Wen",
                "Jonathan Tremblay",
                "Valts Blukis",
                "Stephen Tyree",
                "Thomas Muller",
                "Alex Evans",
                "Dieter Fox",
                "Jan Kautz",
                "Stan Birchfield"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wen_BundleSDF_Neural_6-DoF_Tracking_and_3D_Reconstruction_of_Unknown_Objects_CVPR_2023_paper.pdf",
            "ref_texts": "[57] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 2",
            "ref_ids": [
                "57"
            ],
            "1": "Feature correspondences in RGB between FtandFt\u22121 are established via a transformer-based feature matching network [57], which was pretrained on a large collection 607\n Coarse\u00a0Pose\u00a0Initialization \u00a0(Sec."
        },
        "Intertrack: Interaction transformer for 3d multi-object tracking": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2208.08041",
            "ref_texts": "[37] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. CVPR , 2021.",
            "ref_ids": [
                "37"
            ],
            "1": "Transformers have seen use in computer vision tasks [13, 6, 37], in which image features are extracted to be consumed by a Transformer as input.",
            "2": "SuperGlue [33] and LoFTR [37] introduce a GNN and Transformer module respectively, as a means to incorporate both local and global image information during feature extraction.",
            "3": "We follow the Transformer design of LoFTR [37] to incorporate global object information in our feature extraction.",
            "4": "We adopt the transformer module from LoFTR [37] and interleave the self and cross attention blocks Nctimes."
        },
        "DynamicStereo: Consistent Dynamic Depth from Stereo Videos": {
            "authors": [
                "Nikita Karaev",
                "Ignacio Rocco",
                "Benjamin Graham",
                "Natalia Neverova",
                "Andrea Vedaldi",
                "Christian Rupprecht"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Karaev_DynamicStereo_Consistent_Dynamic_Depth_From_Stereo_Videos_CVPR_2023_paper.pdf",
            "ref_texts": "[42] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR , 2021. 2, 3, 4, 7",
            "ref_ids": [
                "42"
            ],
            "1": "Transformer architectures have shown that attention can be a powerful and flexible method for pooling information over a range of contexts [6, 8, 9, 42].",
            "2": "LoFTR [42] and SuperGlue [36] use combinations of self and cross-attention layers for sparse feature matching.",
            "3": "However, it is computationally demanding to apply attention jointly even with linear [42] space and stereo attention.",
            "4": "We apply linear attention [42] for space and use standard quadratic attention for time."
        },
        "Joint Appearance and Motion Learning for Efficient Rolling Shutter Correction": {
            "authors": [
                "Bin Fan",
                "Yuxin Mao",
                "Yuchao Dai",
                "Zhexiong Wan",
                "Qi Liu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Fan_Joint_Appearance_and_Motion_Learning_for_Efficient_Rolling_Shutter_Correction_CVPR_2023_paper.pdf",
            "ref_texts": "[46] Jiaming Sun, Zehong Shen, Y uang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: detector-free local feature matching with transformers. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 8922\u20138931, 2021. 3",
            "ref_ids": [
                "46"
            ],
            "1": "As the improvement of vision transformer, there are many works us-ing a transformer, especially cross attention [23,46,53] for cross-view modeling."
        },
        "ObjectMatch: Robust Registration using Canonical Object Correspondences": {
            "authors": [
                "Can Gumeli",
                "Angela Dai",
                "Matthias Niessner"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Gumeli_ObjectMatch_Robust_Registration_Using_Canonical_Object_Correspondences_CVPR_2023_paper.pdf",
            "ref_texts": "[42] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 2",
            "ref_ids": [
                "42"
            ],
            "1": "Methods such as LoFTR [42] introduce more dense and accurate sub-pixel level matching."
        },
        "AutoRecon: Automated 3D Object Discovery and Reconstruction": {
            "authors": [
                "Yuang Wang",
                "Xingyi He",
                "Sida Peng",
                "Haotong Lin",
                "Hujun Bao",
                "Xiaowei Zhou"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_AutoRecon_Automated_3D_Object_Discovery_and_Reconstruction_CVPR_2023_paper.pdf",
            "ref_texts": "[37] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR , 2021. 3",
            "ref_ids": [
                "37"
            ],
            "1": "Specifically, we use the recent semidense image matcher LoFTR [37] for SfM to reconstruct 21384\n\n1."
        },
        "Target-Referenced Reactive Grasping for Dynamic Objects": {
            "authors": [
                "Jirong Liu",
                "Ruo Zhang",
                "Shu Fang",
                "Minghao Gou",
                "Hongjie Fang",
                "Chenxi Wang",
                "Sheng Xu",
                "Hengxu Yan",
                "Cewu Lu"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Target-Referenced_Reactive_Grasping_for_Dynamic_Objects_CVPR_2023_paper.pdf",
            "ref_texts": "[33] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR , 2021. 3",
            "ref_ids": [
                "33"
            ],
            "1": "Different from detector-based methods that extract sparse local features, detector-free methods establish pixel-wise or pointwise dense features [3,13,16,31,33]."
        },
        "Visual correspondence hallucination": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2106.09711",
            "ref_texts": "12 Published as a conference paper at ICLR 2022 Paul-Edouard Sarlin, C. Cadena, R. Siegwart, and M. Dymczyk. From Coarse to Fine: Robust Hierarchical Localization at Large Scale. In Conference on Computer Vision and Pattern Recognition , pp. 12716\u201312725, 2019. Paul-Edouard Sarlin, Daniel Detone, Tomasz Malisiewicz, and Andrew Rabinovich. SuperGlue: Learning Feature Matching with Graph Neural Networks. In Conference on Computer Vision and Pattern Recognition , 2020. Torsten Sattler, W. Maddern, C. Toft, Akihiko Torii, L. Hammarstrand, E. Stenborg, D. Safari, M. Okutomi, Marc Pollefeys, Josef Sivic, F. Kahl, and Tom\u00e1s Pajdla. Benchmarking 6DOF Outdoor Visual Localization in Changing Conditions. In Conference on Computer Vision and Pattern Recognition , 2018. J. L. Sch\u00f6nberger and J.-M. Frahm. Structure-From-Motion Revisited. In Conference on Computer Vision and Pattern Recognition , 2016. Thomas Sch\u00f6ps, Johannes L. Sch\u00f6nberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, and Andreas Geiger. A multi-view stereo benchmark with high-resolution images and multi-camera videos. In Conference on Computer Vision and Pattern Recognition (CVPR) , 2017. Florian Schroff, Dmitry Kalenichenko, and James Philbin. FaceNet: A Unified Embedding for Face Recognition and Clustering. In Computing Research Repository , 2015. Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Learning Local Feature Descriptors Using Convex Optimisation. IEEE Transactions on Pattern Analysis and Machine Intelligence , 36, 2014. Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-Free Local Feature Matching with Transformers. In Conference on Computer Vision and Pattern Recognition , 2021. Weiwei Sun, Wei Jiang, Eduard Trulls, Andrea Tagliasacchi, and Kwang Moo Yi. ACNe: Attentive Context Normalization for Robust Permutation-Equivariant Learning. In Conference on Computer Vision and Pattern Recognition , pp. 11286\u201311295, 2020. Christian Szegedy, V . Vanhoucke, S. Ioffe, Jon Shlens, and Z. Wojna. Rethinking the Inception Architecture for Computer Vision. In Conference on Computer Vision and Pattern Recognition , pp."
        },
        "Long-term Visual Localization with Mobile Sensors": {
            "authors": [
                "Shen Yan",
                "Yu Liu",
                "Long Wang",
                "Zehong Shen",
                "Zhen Peng",
                "Haomin Liu",
                "Maojun Zhang",
                "Guofeng Zhang",
                "Xiaowei Zhou"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Yan_Long-Term_Visual_Localization_With_Mobile_Sensors_CVPR_2023_paper.pdf",
            "ref_texts": "[60] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR , 2021. 2, 4, 5, 8",
            "ref_ids": [
                "60"
            ],
            "1": "Conventionally, most existing methods [40, 50, 51, 60] tackle pose estimation by establishing correspondences between the query image and sparse SfM reconstruction of the scene, typically with hand-crafted local features like SIFT [40].",
            "2": "Recently, many traditional hand-crafted steps have been substituted by learning-based counterparts [16,17,51,60,68].",
            "3": "Similar to [26, 60], we first use positional encoding to augment the features {\u02dcF2D,\u02dcF3D}, resulting in {\u02dcFpe 2D,\u02dcFpe 3D}.",
            "4": "Following [26, 60], we supervise the network by a focal loss [38] for coarse matching and variance-weighted \u21132 loss for fine refinement.",
            "5": "The backbone of our model is fixed with the LoFTR-DS [60] outdoor model, while the other parts are randomly initialized and backpropagated.",
            "6": "We compare our approach with the following baselines in two categories: 1) Feature matching pipelines HLoc [50], using different keypoint descriptors (SIFT [40], SuperPoint [16] and D2-Net [17]), and matchers (Nearest Neighbour and learned SuperGlue [51]), as well as the detector-free matcher LoFTR [60]."
        },
        "Visual Localization using Imperfect 3D Models from the Internet": {
            "authors": [
                "Vojtech Panek",
                "Zuzana Kukelova",
                "Torsten Sattler"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Panek_Visual_Localization_Using_Imperfect_3D_Models_From_the_Internet_CVPR_2023_paper.pdf",
            "ref_texts": "[91] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-Free Local Feature Matching With Transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2021. 1, 3, 7, 8",
            "ref_ids": [
                "91"
            ],
            "1": "As shown in [4, 63, 85, 88, 109], modern local features such as [21,23,91,111] are capable of matching real images with renderings of 3D models, even though they were never explicitly trained for this task.",
            "2": "We are motivated by the observation that modern local features, such as the ones used in state-of-the-art localization pipelines [63,73,75,91], can establish correspondences between a real image and renderings of 3D models [15, 63, 109].",
            "3": "Vitus Cathedral scene, obtained with LoFTR [91].",
            "4": "We evaluate different features and matchers inside MeshLoc: LoFTR [91], SuperGlue [75], and the combination of Patch2Pix [111] and SuperGlue [75].",
            "5": "Vitus C and D\n13181\n method day night CAD LoFTR [91] 0."
        },
        "ASIC: Aligning sparse in-the-wild image collections": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2303.16201",
            "ref_texts": "[71] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 2",
            "ref_ids": [
                "71"
            ],
            "1": "With the availability of labeled datasets, a number of works have performed end-to-end matching with deep networks [28,35,45\u201347,49,53,54,64,67,71,84]."
        },
        "Deep Learning of Partial Graph Matching via Differentiable Top-K": {
            "authors": [
                "Runzhong Wang",
                "Ziao Guo",
                "Shaofei Jiang",
                "Xiaokang Yang",
                "Junchi Yan"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Deep_Learning_of_Partial_Graph_Matching_via_Differentiable_Top-K_CVPR_2023_paper.pdf",
            "ref_texts": "[36] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou. Loftr: Detector-free local feature matching with transformers. In Comput. Vis. Pattern Recog. , 2021. 6",
            "ref_ids": [
                "36"
            ],
            "1": "[30,36].",
            "2": "4\n[36] J."
        },
        "DenseGAP: graph-structured dense correspondence learning with anchor points": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2112.06910",
            "ref_texts": "[4] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR , 2021.",
            "ref_ids": [
                "4"
            ],
            "1": "I NTRODUCTION Image correspondence is the foundation of many computer vision tasks, such as geometric matching [1], [2], [3], pose estimation [4], [5], visual localization [6], and optical flow [1], [2], [3], [7].",
            "2": "Recently, the concurrent works [4], [7] involve global context between matches by using transformers [30] which achieve great success in many NLP and vision tasks [31], [32], [33] using the attention mechanism.",
            "3": "We follow the attentionbased mechanism of Transformer [30] to implement messagepassing layers in the graph network, while Transformer [30] is also used by recent works [4], [7] in a different way.",
            "4": "9K LofTR [4] 1.",
            "5": "66 CAPS [14], SuperGlue [5], LofTR [4] and DualRC-Net [12] and show that our model achieves the best overall performance with a large number of correspondences in Fig."
        },
        "ETR: An Efficient Transformer for Re-ranking in Visual Place Recognition": {
            "authors": [
                "Hao Zhang",
                "Xin Chen",
                "Heming Jing",
                "Yingbin Zheng",
                "Yuan Wu",
                "Cheng Jin"
            ],
            "url": "https://openaccess.thecvf.com/content/WACV2023/papers/Zhang_ETR_An_Efficient_Transformer_for_Re-Ranking_in_Visual_Place_Recognition_WACV_2023_paper.pdf",
            "ref_texts": "[33] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR , pages 8922\u20138931, 2021.",
            "ref_ids": [
                "33"
            ],
            "1": "Inspired by the success of Transformers [39] and seminal work such as SuperGlue [31] and LoFTR [33], we use Transformers to process local descriptors extracted from pre-trained CNN models.",
            "2": "Several algorithms such as SuperGlue [31] and LoFTR [33] have been developed to optimize this process."
        },
        "Learning Rotation-Equivariant Features for Visual Correspondence": {
            "authors": [
                "Jongmin Lee",
                "Byungjin Kim",
                "Seungwook Kim",
                "Minsu Cho"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Learning_Rotation-Equivariant_Features_for_Visual_Correspondence_CVPR_2023_paper.pdf",
            "ref_texts": ""
        },
        "Recurrent Homography Estimation Using Homography-Guided Image Warping and Focus Transformer": {
            "authors": [
                "Yuan Cao",
                "Runmin Zhang",
                "Lun Luo",
                "Beinan Yu",
                "Zehua Sheng",
                "Junwei Li",
                "Liang Shen"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Cao_Recurrent_Homography_Estimation_Using_Homography-Guided_Image_Warping_and_Focus_Transformer_CVPR_2023_paper.pdf",
            "ref_texts": "[36] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 4",
            "ref_ids": [
                "36"
            ],
            "1": "Following previous works [23, 34, 36, 40], we interleave the self-attention layer and the cross-attention layer, which is shown in Fig."
        },
        "Two-View Geometry Scoring Without Correspondences": {
            "authors": [
                "Axel Barroso",
                "Eric Brachmann",
                "Victor Adrian",
                "Gabriel J. Brostow",
                "Daniyar Turmukhambetov"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.pdf",
            "ref_texts": "[50] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 1, 2, 3, 4, 5, 6, 8",
            "ref_ids": [
                "50"
            ],
            "1": "Although the latest matching pipelines produce very accurate and robust correspondences [19, 46, 50], correspondence-based scoring methods are still sensitive to the ratio of inliers, number of correspondences, or the accuracy of the keypoints This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.",
            "2": "Inspired by the success of Vision Transformers [61], and detectorfree matchers [31, 50], we define an architecture that incorporates the epipolar geometry into an attention layer, and hence the quality of the fundamental matrix hypothesis conditions the coherence of the computed features.",
            "3": "We see that the failures center on the scoring function, it is especially difficult to score hypotheses in the indoor scenario (ScanNet), where local features suffer more [50].",
            "4": ", detector-free matchers [31, 50, 60], or direct relative-pose regressor networks [1, 11, 42, 44, 68].",
            "5": "Transformers are getting attention in the vision community, and have been applied successfully to image matching [31, 50], multi-view stereo [20, 29, 63], or depth estimation [27, 34], among others [21, 44].",
            "6": "We then mine image pairs with low overlapping views from validation splits of ScanNet [18] and MegaDepth [35] datasets as in [46, 50] and study MAGSAC++\u2019s behavior for 500 iterations, i.",
            "7": "Feature Extractor The feature extractor is a standard convolutional architecture as in [50], it follows the Unet-style network [45] design with skip and residual connections [28] and computes feature maps at 1/4of the input resolution.",
            "8": "Transformer We use an Lmulti-head attention transformer architecture following [50], and alternate between self and crossattention blocks to exploit the similarities within and across the feature maps.",
            "9": "To limit the computational complexity of our transformer block, we use a Linear Transformer [33] as in [50].",
            "10": "For the indoor model, we use the training splits proposed in [50].",
            "11": "For our outdoor model, we use the MegaDepth training and validation scenes proposed in [50], except for scenes in the test set of the CVPR IMW 2019 PhotoTourism dataset [32, 52] as in [46].",
            "12": "For the feature extractor, we use a ResNet-18 [28] as in [50].",
            "13": "[50]."
        },
        "NeuMap: Neural Coordinate Mapping by Auto-Transdecoder for Camera Localization": {
            "authors": [
                "Shitao Tang",
                "Sicong Tang",
                "Andrea Tagliasacchi",
                "Ping Tan",
                "Yasutaka Furukawa"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_NeuMap_Neural_Coordinate_Mapping_by_Auto-Transdecoder_for_Camera_Localization_CVPR_2023_paper.pdf",
            "ref_texts": "[42] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 2, 5",
            "ref_ids": [
                "42"
            ],
            "1": "This approach exploits learning-based feature extraction and correspondence matching methods [13, 35, 42, 44] to achieve robust localization.",
            "2": "FMbased localization has achieved state-of-the-art performance [13,14,32,34,35,37,42,49].",
            "3": "This pipeline has demonstrated significant improvements, with most followup works focusing on enhancing feature matching capability [35, 42, 49, 50].",
            "4": "LoFTR [42] and its variants [9,44] propose dense matching frameworks without key-points.",
            "5": "For Aachen, with images that have significant view and illumination differences, we use a feature extractor from LoFTR [42], pretrained with images containing such variations."
        },
        "3D Line Mapping Revisited": {
            "authors": [
                "Shaohui Liu",
                "Yifan Yu",
                "Remi Pautrat",
                "Marc Pollefeys",
                "Viktor Larsson"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_3D_Line_Mapping_Revisited_CVPR_2023_paper.pdf",
            "ref_texts": "[70] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR , 2021. 2",
            "ref_ids": [
                "70"
            ],
            "1": "Some recent methods also exploit point-line [14, 15] and line-junction-line structures [38, 39] to improve matching results, yet still not reaching the reliability level of advanced point matchers [58, 70]."
        },
        "Are Local Features All You Need for Cross-Domain Visual Place Recognition?": {
            "authors": [
                "Giovanni Barbarani",
                "Mohamad Mostafa",
                "Hajali Bayramov",
                "Gabriele Trivigno",
                "Gabriele Berton",
                "Carlo Masone",
                "Barbara Caputo"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023W/IMW/papers/Barbarani_Are_Local_Features_All_You_Need_for_Cross-Domain_Visual_Place_CVPRW_2023_paper.pdf",
            "ref_texts": "[49] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR , 2021. 1, 2, 3, 4, 5",
            "ref_ids": [
                "49"
            ],
            "1": "To improve results and address these issues, a number of previous works noted that local features [9, 24, 33, 55] and image matching [15, 42, 43, 49] methods are inherently more robust to domain shifts , and that these can be used to re-rank a set of candidates (usually through spatial verification) provided through image retrieval methods, leading to large improvements in results [18, 55].",
            "2": "Local features for spatial verification Local features-based spatial verification represents an established paradigm that has been applied to several computer vision tasks, ranging from structure from motion (SfM) [26, 28, 46], simultaneous localization and mapping (SLAM) [11, 18, 43] and visual localization [26, 44, 49, 53].",
            "3": "In recent years, we have witnessed a flourishing literature on learnable detectors and descriptors exploiting local features for pose and homography estimation [12, 14, 15, 21, 42, 43, 49].",
            "4": "This approach has been subsequently generalized by LoFTR [49], which removes the dependence from an underlying detector exploiting cross-attention transformers for directly selecting keypoints matches among an image pair.",
            "5": "Benchmark Methodology To further motivate our benchmark, we point out that the application of spatial verification methods to the VPR task is not straightforward in light of the fact that many of them [43,49] are trained on 3D models from SfM [26], thus having access to accurate matching labels.",
            "6": "Specifically, we use SuperGlue [43]\n(which uses SuperPoint [12] local features), D2-Net [15], R2D2 [42], DELG [9], Reranking Transformers (RRT) [50]\n(which uses DELG local features), Patch-NetVLAD [18] with both its RANSAC and Rapid Scoring implementation, TransVPR [55], LoFTR [49] and CVNet [24]."
        },
        "PanoPoint: Self-Supervised Feature Points Detection and Description for 360deg Panorama": {
            "authors": [
                "Hengzhi Zhang",
                "Hong Yi",
                "Haijing Jia",
                "Wei Wang",
                "Makoto Odamaki"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023W/OmniCV/papers/Zhang_PanoPoint_Self-Supervised_Feature_Points_Detection_and_Description_for_360deg_Panorama_CVPRW_2023_paper.pdf",
            "ref_texts": "[47] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 7",
            "ref_ids": [
                "47"
            ],
            "1": "In addition to the methods mentioned in the previous section, we add two transformer-based matchers [47, 51]: the detect-based method SuperGlue and the detect-free method LoFTR."
        },
        "Evit: Privacy-preserving image retrieval via encrypted vision transformer in cloud computing": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2208.14657",
            "ref_texts": "[71] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. 2021. LoFTR: Detector-Free Local Feature Matching With Transformers. InIEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021 . Computer Vision Foundation / IEEE, 8922\u20138931.",
            "ref_ids": [
                "71"
            ],
            "1": "Then many researchers explore ViT and find ViT can obtain excellent performance in many computer vision tasks [16,41,44,48,64,71], even surpass the CNN model in performance [4,31,51,78,94].",
            "2": "In this paper, we use ViT as backbone in our retrieval model, and the reasons are as follows: a) ViT is popular and makes excellent performance in recent many computer vision tasks [4,31,41,44,48,51,64,71,78,94]."
        },
        "Privacy-Preserving Representations are not Enough: Recovering Scene Content from Camera Poses": {
            "authors": [
                "Kunal Chelani",
                "Torsten Sattler",
                "Fredrik Kahl",
                "Zuzana Kukelova"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Chelani_Privacy-Preserving_Representations_Are_Not_Enough_Recovering_Scene_Content_From_Camera_CVPR_2023_paper.pdf",
            "ref_texts": "[43] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR , 2021. 2",
            "ref_ids": [
                "43"
            ],
            "1": "The 3D model can either be stored explicitly [19\u201321, 27, 31\u201333, 43], e.",
            "2": "The robustness is a direct consequence of recent advances in local features [10, 13, 30] and effective feature matchers [32,43,48,53]."
        },
        "An Intelligent Weighted Object Detector for Feature Extraction to Enrich Global Image Information": {
            "authors": [
                "Lingyu Yan",
                "Ke Li",
                "Rong Gao",
                "Chunzhi Wang",
                "Neal Xiong"
            ],
            "url": "https://www.mdpi.com/2076-3417/12/15/7825/pdf",
            "ref_texts": "34. Sun, J.; Shen, Z.; Wang, Y.; Bao, H.; Zhou, X. LoFTR: Detector-Free Local Feature Matching with Transformers. arXiv 2021 , arXiv:2104.00680.",
            "ref_ids": [
                "34"
            ],
            "1": "Considering the importance of the receiver domain in the feature extraction network, LoFTR [34] borrows the self-attention and mutual-attention layers from a transformer to obtain feature descriptions between images and proposes a new local image feature matching method, which successively performs coarse-grained and fine-grained feature detection as well as matching of image features, making the obtained global receiver domain capable of producing dense feature matching results in regions with small texture information."
        },
        "FeatureBooster: Boosting Feature Descriptors with a Lightweight Neural Network": {
            "authors": [
                "Xinjiang Wang",
                "Zeyu Liu",
                "Yu Hu",
                "Wei Xi",
                "Wenxian Yu",
                "Danping Zou"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Wang_FeatureBooster_Boosting_Feature_Descriptors_With_a_Lightweight_Neural_Network_CVPR_2023_paper.pdf",
            "ref_texts": "[41] Jiaming Sun, Zehong Shen, Y uang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In CVPR , pages 8922\u20138931, 2021. 2",
            "ref_ids": [
                "41"
            ],
            "1": "The core idea of our approach, motivated by recent work [25,36,41], is integrating the visual and geometric information of all the keypoints into individual descriptors by a Transformer."
        },
        "Effect of parameter optimization on classical and learning-based image matching methods": {
            "authors": [
                "Ufuk Efe",
                "Kutalmis Gokalp",
                "Aydin Alatan"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021W/TradiCV/papers/Efe_Effect_of_Parameter_Optimization_on_Classical_and_Learning-Based_Image_Matching_ICCVW_2021_paper.pdf",
            "ref_texts": "[33] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR , 2021.",
            "ref_ids": [
                "33"
            ],
            "1": "2 Homography Estimation Accuracy (HEA) Homography Estimation Accuracy (HEA) is another widely used metric for image matching evaluation and used in [11, 35, 33, 14] as a performance metric.",
            "2": "[33] J."
        },
        "Check and Link: Pairwise Lesion Correspondence Guides Mammogram Mass Detection": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2209.05809",
            "ref_texts": "32. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: LoFTR: Detector-free local feature matching with transformers. In: CVPR (2021) 5",
            "ref_ids": [
                "32"
            ],
            "1": "3 Learnable Image Matching The well-known image matching in computer vision aims to establish dense correspondences across images for camera pose recovery and scene structure estimation in geometric vision tasks, such as Structure-from-Motion (SfM) and Simultaneous Localization and Mapping (SLAM) [8,10,23,38,26,2,30,32]."
        },
        "POPE: 6-DoF Promptable Pose Estimation of Any Object, in Any Scene, with One Reference": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2305.15727",
            "ref_texts": "[53] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 4, 6, 7, 8, 9",
            "ref_ids": [
                "53"
            ],
            "1": "SIFT [50], and SURF [51], or using learned features [52,53,54,55,56,57].",
            "2": "Different from prior works establishing image-level correspondence (matching the support image with the entire target image) [52,53], we propose a coarse-to-fine paradigm.",
            "3": "We then establish image correspondences using a transformer-based local feature estimation framework [53] when using natural RGB images as prompt .",
            "4": "We compared our proposed POPE method with two other approaches: LoFTR [53], an image-matching based method that directly performs correspondence matching for pose estimation, and Gen6D [4], which utilizes a correlation-based network to discover object boxes, find pose initialization, and refine the relative object pose.",
            "5": "In POPE, we utilize pre-trained models for different tasks: the Segment Anything model [24] with a ViT-H architecture for object mask generation, the DINO-v2 model [22] pre-trained with ViT-S/14 for object proposal generation, and the LoFTR model [53] pre-trained with indoor scenes for natural image-based image matching.",
            "6": "LoFTR [53] fails to provide accurate matches when handling clustered scenes with object occlusions, resulting in inaccurate box predictions.",
            "7": "We utilize the semi-dense correspondences from LOFTR [53] to reconstruct a semi-dense point cloud using COLMAP [16].",
            "8": "970 LoFTR [53] 33.",
            "9": "597 LoFTR [53] 19.",
            "10": "867 LoFTR [53] 9.",
            "11": "317 LoFTR [53] 4."
        },
        "MIPI 2023 Challenge on RGB+ ToF Depth Completion: Methods and Results": {
            "authors": [
                "Qingpeng Zhu",
                "Wenxiu Sun",
                "Yuekun Dai",
                "Chongyi Li",
                "Shangchen Zhou",
                "Ruicheng Feng",
                "Qianhui Sun",
                "Chen Change",
                "Jinwei Gu",
                "Yi Yu",
                "Yangke Huang",
                "Kang Zhang",
                "Meiya Chen",
                "Yu Wang",
                "Yongchao Li",
                "Hao Jiang",
                "Amrit Kumar",
                "Vikash Kumar",
                "Kunal Swami",
                "Pankaj Kumar",
                "Yunchao Ma",
                "Jiajun Xiao",
                "Zhi Ling"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023W/MIPI/papers/Zhu_MIPI_2023_Challenge_on_RGBToF_Depth_Completion_Methods_and_Results_CVPRW_2023_paper.pdf",
            "ref_texts": "[21] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 8922\u20138931, 2021. 4",
            "ref_ids": [
                "21"
            ],
            "1": "The self-attention module of light LoFTR[21] is introduced to aggregate more sparse depth points."
        },
        "Revisiting the receptive field of conv-gru in droid-slam": {
            "authors": [
                "Antyanta Bangunharcana",
                "Soohyun Kim",
                "Soo Kim"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022W/VOCVALC/papers/Bangunharcana_Revisiting_the_Receptive_Field_of_Conv-GRU_in_DROID-SLAM_CVPRW_2022_paper.pdf",
            "ref_texts": "[44] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021.",
            "ref_ids": [
                "44"
            ],
            "1": "The recent surge in the adoption of Transformers [50] based methods towards vision tasks [6,12,27] have also sparked recent research which uses Graph Neural Networks [39] and Transformers [44, 56] for correspondence search.",
            "2": "Self-attention Conv-GRU Inspired by the success of recent correspondence search works by using Transformers to attend to global features [44, 56], we investigate attention-based updates of the ConvGRU."
        },
        "PEAL: Prior-Embedded Explicit Attention Learning for Low-Overlap Point Cloud Registration": {
            "authors": [
                "Junle Yu",
                "Luwei Ren",
                "Yu Zhang",
                "Wenhui Zhou",
                "Lili Lin",
                "Guojun Dai"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_PEAL_Prior-Embedded_Explicit_Attention_Learning_for_Low-Overlap_Point_Cloud_Registration_CVPR_2023_paper.pdf",
            "ref_texts": "[30] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR , pages 8922\u20138931, 2021. 2",
            "ref_ids": [
                "30"
            ],
            "1": "Image matching [21, 26, 28, 30] is a basic and important technology in computer vision.",
            "2": "Learning-based methods [28, 30] can significantly improve the performance of local features under large viewpoints and illumination changes."
        },
        "Digging into self-supervised learning of feature descriptors": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2110.04773",
            "ref_texts": "[66] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and ZhouXiaowei. LoFTR:Detector-freelocalfeaturematching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 8922\u20138931, 2021. 2",
            "ref_ids": [
                "66"
            ],
            "1": "Although, those approaches have demonstrated improved performance over classical hand-crafted methods on challenging benchmarks such as vision localization and image matching, most of them require ground-truth pixel correspondencesbetweentwoviews[20,30,57,66].",
            "2": "2\n[66] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and ZhouXiaowei."
        },
        "Visualizing Skiers' Trajectories in Monocular Videos": {
            "authors": [
                "Matteo Dunnhofer",
                "Luca Sordi",
                "Christian Micheloni"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023W/CVSports/papers/Dunnhofer_Visualizing_Skiers_Trajectories_in_Monocular_Videos_CVPRW_2023_paper.pdf",
            "ref_texts": "[57] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR , 2021. 3, 5, 6, 14",
            "ref_ids": [
                "57"
            ],
            "1": "For this motivation, in this work, we explore the usage of generic key-point correspondence algorithms [39, 51, 53, 57] to compute the homography that enables the computation of the skier trajectory.",
            "2": "We executed the state-of-the-art LOFTR image matching algorithm [57] to detect and match key-points of consecutive frames of SkiTB\u2019s monocular test videos.",
            "3": "8 Resolution, frame-rate 720p, 30 FPS\n\u03c4tlength in # points (min, avg, max) 1, 25, 229\n# clips for alpine skiing sub-discipline (SL, GS, SG, DH) 35, 55, 71, 72\n# athletes (men, women) 23 (13, 10)\n# locations 29\n# weather conditions (clear, overcast, fog, snowing) 200, 30, 2, 1\n[57]."
        },
        "Multi-Sensor Data Fusion for 3D Reconstruction of Complex Structures: A Case Study on a Real High Formwork Project": {
            "authors": [
                "Linlin Zhao",
                "Huirong Zhang",
                "Jasper Mbachu"
            ],
            "url": "https://www.mdpi.com/2072-4292/15/5/1264/pdf",
            "ref_texts": "92. Sun, J.; Shen, Z.; Wang, Y.; Bao, H.; Zhou, X. LoFTR: Detector-Free Local Feature Matching with Transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Nashville, TN, USA, 20\u201325 June 2021.",
            "ref_ids": [
                "92"
            ],
            "1": "Image Feature Matching Motivated by the above statement, this study uses local feature matching with transformers (LoFTR) proposed by the study [92], a novel detector -free method for local feature matching.",
            "2": "Image Feature Matching Motivated by the above statement, this study uses local feature matching with transformers (LoFTR) proposed by the study [92], a novel detector-free method for local feature matching."
        },
        "The EuroSDR TIME benchmark for historical aerial images": {
            "authors": [],
            "url": "https://cris.fbk.eu/bitstream/11582/332647/1/isprs-archives-XLIII-B2-2022-1175-2022_compressed.pdf",
            "ref_texts": "(PFG), Vol. 89(5), pp. 461 -473. Seo, D.K., Kim, Y.H., Eo, Y.D. and Park, W.Y., 2018. Learning based colorization of grayscale aerial images using random forest regression. Applied Science s, 8(8), p.1269. Sevara, C. 2013. Top secret topographies: recovering two and three dimensional archaeological information from historic reconnaissance datasets using image -based modelling techniques. Int. J. Heritage in the Digital Era , Vol. 2, pp. 395-418. Sevara, C., Verhoeven, G., Doneus, M., Draganits, E., 2018. Surfaces from the visual past: recovering high -resolution terrain data from historic aerial imagery for multitemporal landscape analysis. J ournal of Archaeological Method and Theory , Vol. 25, pp. 611642. Siok, K. and Ewiak, I., 2020. The simulation approach to the interpretation of archival aerial photographs. Open Geosciences , 12(1), pp.1 -10. Sonnemann, T., Saurbier, M., Remondino, F., Schoretter, G., 2006. Reality -based 3D modeling of the Angkorian temples using aerial images. Proc. of 2nd International Conference on \"Remote Sensing in Archaeology\" , pp. 573 -579. Sun, J., Shen, Z., Wang, Y., Bao, Y., Zhou, X., 2021. LoFTR : Detector -free local feature matching with Transformers. Proc. CVPR. Valjavec, M.B., Zorn, M. and Ribeiro, D., 2018. Mapping war geoheritage: Recognising geomorphological traces of war. Open Geosciences , 10(1), pp. 385-394. Verhoeven, G., Doneus, M., Brie se, C., & Vermeulen, F., 2012. Mapping by matching a computer visionbased approach to fast and accurate georeferencing of archaeological aerial photographs. Journal of Archaeological Science , 39(7), pp. 20602070. Verhoeven, G., Sevara, C., Karel, W., Ressl, C., Doneus, M., & Briese, C., 2013. Undistorting the past: new techniques for orthorectification of archaeological aerial frame imagery . Good practice in archaeological diagnostics: Natural Science in Archaeology , pp. 31 -67. Walstra , J., Chandler, J., Dixon, N. and Dijkstra, T., 2004. Time for changequantifying landslide evolution using historical aerial photographs and modern photogrammetric methods. Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci. , 35(B4), pp. 475 \u2013480. Wiede mann, A., Hemmleb, M., Albertz, J., 2000. Reconstruction of historical buildings based on images from the Meydenbauer archives. Int. Arch. Photogramm. Remote Sens. , Vol. 33(B5/2), p. 887 -893. Zambanini, S. and Sablatnig, R., 2017. A Hough voting strategy f or registering historical aerial images to present -day satellite imagery. Proc. ICIAP , pp. 595-605, Springer. Zhang, L., Rupnik, E. and Pierrot -Deseilligny, M., 2021. Feature matching for multi -epoch historical aerial images. ISPRS Journal of Photogrammetry and Remote Sensing , 182, pp.176 -189. The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, Volume XLIII-B2-2022 XXIV ISPRS Congress (2022 edition), 6\u201311 June 2022, Nice, France This contribution has been peer-reviewed. https://doi.org/10.5194/isprs-archives-XLIII-B2-2022-1175-2022 | \u00a9 Author(s) 2022. CC BY 4.0 License."
        },
        "Learning co-segmentation by segment swapping for retrieval and discovery": {
            "authors": [
                "Xi Shen",
                "Alexei A. Efros",
                "Armand Joulin",
                "Mathieu Aubry"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022W/IMW/papers/Shen_Learning_Co-Segmentation_by_Segment_Swapping_for_Retrieval_and_Discovery_CVPRW_2022_paper.pdf",
            "ref_texts": "[58] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. arXiv , 2021. 2",
            "ref_ids": [
                "58"
            ],
            "1": "Finally, LoFTR [58] adopts a coarseto-fine approach to matching with a transformer encoder."
        },
        "Rethinking Optical Flow from Geometric Matching Consistent Perspective": {
            "authors": [
                "Qiaole Dong",
                "Chenjie Cao",
                "Yanwei Fu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Dong_Rethinking_Optical_Flow_From_Geometric_Matching_Consistent_Perspective_CVPR_2023_paper.pdf",
            "ref_texts": "[46] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 2, 3, 4",
            "ref_ids": [
                "46"
            ],
            "1": "Empirically, the static scene under viewpoint changes of GIM [46,49], can also be taken as one special type of optical flow estimation, which is even much simpler than FlyingChair.",
            "2": "Among recent detector-free matching methods [11, 12, 37, 46, 49], Tang et al.",
            "3": "[49] propose QuadTree attention to capture both fine-level details and long-range dependencies, which outperforms the Linear attention [26] used in [46].",
            "4": "(3) We also use additional l2loss in 1/2-resolution for finegrained supervision [46].",
            "5": "We utilize MegaDepth [27] as our matching dataset and train the feature encoder with randomly sampled 36,800 image pairs per epoch for a total of 30 epochs [46]."
        },
        "LFM: a lightweight lcd algorithm based on feature matching between similar key frames": {
            "authors": [
                "Zuojun Zhu",
                "Xiangrong Xu",
                "Xuefei Liu",
                "Yanglin Jiang"
            ],
            "url": "https://www.mdpi.com/1424-8220/21/13/4499/pdf",
            "ref_texts": "45. Sun, J.; Shen, Z.; Wang, Y.; Bao, H.; Zhou, X. LoFTR: Detector-Free Local Feature Matching with Transformers. arXiv 2021 , arXiv:2104.00680.",
            "ref_ids": [
                "45"
            ],
            "1": "proposed the LoFTR [45] algorithm.",
            "2": "These include: a lightweight CNN based on Fish Convolution Block; object detection based on YOLOv4 with our lightweight CNN instead of CSPDarkNet [46]; a classification tree with a structure that is similar to the BoW dictionary; an improved feature matching network of LOFTR [45].",
            "3": "Feature Matching Based on LoFTR Compared with other feature matching algorithms, such as ORB [15] and SuperPoint [40], LoFTR [45] can solve the repeatability problem of feature detector.",
            "4": "Feature Matching between Similar Key Frames A mass of experimental data in [45] demonstrates the accuracy and superiority of LOFTR for feature matching in low-texture regions as well as symmetric and repeated regions, and the performance of relative pose estimation and visual positioning on multiple data sets reaches the most advanced level."
        },
        "Unsupervised Semantic Correspondence Using Stable Diffusion": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2305.15581",
            "ref_texts": "[18] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detectorfree local feature matching with transformers. Conference on Computer Vision and Pattern Recognition , 2021.",
            "ref_ids": [
                "18"
            ],
            "1": "While learning-based methods provide superior performance [16,15,17,18] often these methods need to be trained on large supervised datasets [17,19].",
            "2": "While a large body of work exists, including those that focus more on finding geometric correspondences that rely on local features [4, 6,7] and matching them [41,42,43,8] or directly via deep networks [44,45,17,18], here we focus only on semantic correspondences [11,16,15,46], that is, the task of finding corresponding locations in images that are of the same \u201csemantic\u201d \u2013 e."
        },
        "OpenGlue: Open source graph neural net based pipeline for image matching": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2204.08870",
            "ref_texts": "[45] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8922\u20138931, 2021. 6",
            "ref_ids": [
                "45"
            ],
            "1": "We compare OpenGlue against Mutual Nearest Neighbors matcher with second nearest check (SMNN [20]), SuperGlue [40] and LoFTR [45]."
        },
        "Deepmatcher: a deep transformer-based network for robust and accurate local feature matching": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2301.02993",
            "ref_texts": "[2] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detectorfree local feature matching with transformers,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2021, pp. 8922\u20138931.",
            "ref_ids": [
                "2"
            ],
            "1": "Parallel to the detector-based matching, another stream of research seeks to establish correspondences directly from original images by extracting visual descriptors on dense grids across an image, thus assuring that substantial repeating keypoints could well be captured [2], [21]\u2013[28].",
            "2": "On the basis of this insight, various studies base their modelling of long-range relationships on Transformer backbone [2], [26]\u2013[28].",
            "3": "As a representative work, LoFTR [2] updates features by repeatedly interleaving the selfand cross-attention layers, and replace vanilla Transformer with linear Transformer [30] to achieve manageable computation cost.",
            "4": "To handle this issue, LoFTR [2], the pioneering detector-free GNN method, utlizes Transformer to realize global context information exchange and extracts matches in a coarse-to-fine manner.",
            "5": "Previous works [2], [3], [16], [26] attach a distinctive absolute positional embedding to each keypoint, thus alleviating such ambiguity.",
            "6": "Compared to sinusoidal encoding [2], [3], [26], rotary positional embedding has two advantages: (i) \u0002(\u0001)is an orthogonal function, the encoding only changes the feature\u2019s direction but not the feature\u2019s length, which could stabilize the learning process.",
            "7": "Following [2], we calculate the index Egt of the ground truth matches, which are utilized in conjunction with soft assignment matrix Gto calculate matching loss Lm defined as focal loss [50].",
            "8": "23 Detector-free Methods \u2014\u2014LoFTR [2] 22.",
            "9": "In accordance with [2], [3], we report the area under the cumulative curve (AUC) of pose errors at the thresholds (5fl;10fl;20fl), where pose errors are defined as the maximum of translational and rotational errors between ground-truth poses and predicted poses by DeepMatcher.",
            "10": "31 LoFTR [2] 52.",
            "11": "Following [2], [14], we select 100 image pairs each scene for training and 1500 image pairs for testing.",
            "12": "76 LoFTR [2] 0.",
            "13": "Methods Params GFLOPs Runtime TC LoFTR [2] 11.",
            "14": "[2] J."
        },
        "SIM2E: Benchmarking the Group Equivariant Capability of Correspondence Matching Algorithms": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2208.09896",
            "ref_texts": "25. Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922{",
            "ref_ids": [
                "25"
            ],
            "1": "LoFTR [25] is a detector-free and end-to-end architecture.",
            "2": "{ LoFTR [25] is trained with the same experimental setup as SuperGlue.",
            "3": "893 1278 LoFTR [25] 0.",
            "4": "607 1226 LoFTR [25] 0.",
            "5": "584 802 LoFTR [25] 0.",
            "6": "570 758 LoFTR [25] 0."
        },
        "CTrGAN: Cycle Transformers GAN for Gait Transfer": {
            "authors": [
                "Shahar Mahpod",
                "Noam Gaash",
                "Hay Hoffman",
                "Gil Ben"
            ],
            "url": "https://openaccess.thecvf.com/content/WACV2023/papers/Mahpod_CTrGAN_Cycle_Transformers_GAN_for_Gait_Transfer_WACV_2023_paper.pdf",
            "ref_texts": ""
        },
        "Graph-CoVis: GNN-based Multi-view Panorama Global Pose Estimation": {
            "authors": [
                "Negar Nejatishahidin",
                "Will Hutchcroft",
                "Manjunath Narayana",
                "Ivaylo Boyadzhiev",
                "Yuguang Li",
                "Naji Khosravan",
                "Jana Kosecka",
                "Sing Bing"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023W/OmniCV/papers/Nejatishahidin_Graph-CoVis_GNN-Based_Multi-View_Panorama_Global_Pose_Estimation_CVPRW_2023_paper.pdf",
            "ref_texts": "[24] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 8918\u2013",
            "ref_ids": [
                "24"
            ],
            "1": "Learned models have been proposed for each of the steps [12, 21, 27], combination of steps [7, 24], and the endto-end pipeline [3, 9, 18, 19].",
            "2": "Lofter [24] achieves detector-free matching across images by learning feature descriptors starting from a dense pixelwise sampling and refining them for high quality fine-level matching."
        },
        "Find My Astronaut Photo: Automated Localization and Georectification of Astronaut Photography": {
            "authors": [
                "Alex Stoken",
                "Kenton Fisher"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023W/IMW/papers/Stoken_Find_My_Astronaut_Photo_Automated_Localization_and_Georectification_of_Astronaut_CVPRW_2023_paper.pdf",
            "ref_texts": "[33] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR , 2021. 3, 6",
            "ref_ids": [
                "33"
            ],
            "1": "These matchers follow the style of the Local Feature TRansformer (LoFTR) [33], leveraging the benefits of self and cross attention between the images themselves or their features [5,9,17,34,35].",
            "2": "We divide the models into four categories (1) detector-based local feature matchers (SIFT [22], SuperPoint+SuperGlue [29], D2-Net [13]) (2) detector-free local feature matchers (LoFTR [33], SE2-LoFTR [5], Aspanformer [9], Matchformer [35], Patch2Pix [39]) (3) pretrained global embeddings (NetVLAD [2], GeM-Net [24, 25], SwA V [7], DINO [8], Barlow Twins [38]) and (4) dense similarity/overlap predictors (DKM [14])."
        },
        "RWT-SLAM: Robust visual SLAM for highly weak-textured environments": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2207.03539",
            "ref_texts": ""
        },
        "Rendernet: Visual relocalization using virtual viewpoints in large-scale indoor environments": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2207.12579",
            "ref_texts": "[30] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. arXiv preprint arXiv:2104.00680 , 2021.",
            "ref_ids": [
                "30"
            ],
            "1": "9 LoFTR[30] 47.",
            "2": "5m, 5 \u00b0) / (5m, 10 \u00b0), such as sparse image matching based methods [8], [11], [9], [38] and dense image matching based methods [23], [22], [30].",
            "3": "Compared with very recent state-of-the-art LoFTR [30] which is a transformer-based dense image matching method, we achieve improvements on most metrics, which are 11."
        },
        "Pose refinement with joint optimization of visual points and lines": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2110.03940",
            "ref_texts": "[7] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detector-free local feature matching with transformers,\u201d CoRR , vol. abs/2104.00680, 2021.",
            "ref_ids": [
                "7"
            ],
            "1": "A large number of visual point extractors [4]\n[5], matching methods [6] [7] and point-based 3D mapping approaches [8] [9] have been proposed.",
            "2": "2 LoFTR [7] 47.",
            "3": "[7] J."
        },
        "Visual localization via few-shot scene region classification": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2208.06933",
            "ref_texts": "[42] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021.",
            "ref_ids": [
                "42"
            ],
            "1": "Introducing pre-learned visual feature extractors [13, 15, 32, 42] is not new in the visual localization community.",
            "2": "Since recently learned descriptors [13, 15, 32, 42] achieve promising visual feature embedding ability under viewpoint changes and cross multiple scenes, we make use of the off-the-shelf SuperPoint [13] (without detector) as our feature extractor, so that we can obtain robust semi-dense feature maps from an input image."
        },
        "Structured Epipolar Matcher for Local Feature Matching": {
            "authors": [
                "Jiahao Chang",
                "Jiahuan Yu",
                "Tianzhu Zhang"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023W/IMW/papers/Chang_Structured_Epipolar_Matcher_for_Local_Feature_Matching_CVPRW_2023_paper.pdf"
        },
        "HarrisZ+: Harris corner selection for next-gen image matching pipelines": {
            "authors": [
                "Fabio Bellavia",
                "Dmytro Mishkin"
            ],
            "url": "https://arxiv.org/pdf/2109.12925",
            "ref_texts": "Alcantarilla, P.F., Bartoli, A., Davison, A.J., 2012. KAZE features, in: Proceedings of the European Conference on Computer Vision (ECCV). Barroso-Laguna, A., Riba, E., Ponsa, D., Mikolajczyk, K., 2019. Key.Net: Keypoint detection by handcrafted and learned CNN filters, in: Proceedings of the International Conference on Computer Vision (ICCV). Bay, H., Ess, A., Tuytelaars, T., Gool, L.V ., 2008. SURF: Speeded up robust features. Computer Vision and Image Understanding 110. Beaudet, P., 1978. Rotationally invariant image operators, in: International Joint Conference on Pattern Recognition. Bellavia, F., 2022. SIFT matching by context exposed. IEEE Transactions on Pattern Analysis and Machine Intelligence . Bellavia, F., Tegolo, D., Valenti, C., 2011. Improving Harris corner selection strategy. IET Computer Vision . Brown, M., Szeliski, R., Winder, S., 2005. Multi-image matching using multiscale oriented patches, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Cavalli, L., Larsson, V ., Oswald, M.R., Sattler, T., Pollefeys, M., 2020. AdaLAM: Revisiting handcrafted outlier detection, in: Proceedings of the European Conference on Computer Vision (ECCV). Chum, O., Werner, T., Matas, J., 2005. Two-View Geometry Estimation Unafiected by a Dominant Plane, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). DeTone, D., Malisiewicz, T., Rabinovich, A., 2018. SuperPoint: Selfsupervised interest point detection and description, in: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) Workshops. Dusmanu, M., Rocco, I., Pajdla, T., Pollefeys, M., Sivic, J., Torii, A., Sattler, T., 2019. D2-Net: A Trainable CNN for Joint Detection and Description of Local Features, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Fischler, M., Bolles, R., 1981. Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM 24.F\u00a8orstner, W., 1986. A feature based correspondence algorithm for image matching. International Archives of Photogrammetry and Remote Sensing 26. Gonzalez, R.C., Woods, R.E., 2008. Digital image processing. Prentice Hall. Harris, C., Stephens, M., 1988. A combined corner and edge detector, in: Proceedings of the Alvey Vision Conference. Jiang, W., Trulls, E., Hosang, J., Tagliasacchi, A., Yi, K.M., 2021. COTR: Correspondence transformer for matching across images, in: Proceedings of the IEEE International Conference on Computer Vision (ICCV). Jin, Y ., Mishkin, D., Mishchuk, A., Matas, J., Fua, P., Yi, K.M., Trulls, E., 2021. Image matching across wide baselines: From paper to practice. International Journal of Computer Vision 129. Lecca, M., Torresani, A., F.Remondino, 2020. Comprehensive evaluation of image enhancement for unsupervised image description and matching. IET Image Processing . Lindeberg, T., 1994. Scale-Space Theory in Computer Vision. Kluwer Academic Publishers. Lowe, D., 2004. Distinctive image features from scale-invariant keypoints. International Journal of Computer Vision 60. Matas, J., Chum, O., Urban, M., Pajdla, T., 2002. Robust wide baseline stereo from maximally stable extremal regions, in: Proceeding of the British Machine Vision Conference (BMVC). Mikolajczyk, K., Schmid, C., 2004. Scale & a flne invariant interest point detectors. International Journal of Computer Vision 60. Mishchuk, A., Mishkin, D., Radenovic, F., Matas, J., 2017. Working Hard to Know Your Neighbor\u2019s Margins: Local Descriptor Learning Loss, in: Proc. of the Conf. on Neural Information Processing Systems (NeurIPS). Mishkin, D., Matas, J., Perdoch, M., 2015. MODS: Fast and robust method for two-view matching. Computer Vision and Image Understanding . Mishkin, D., Radenovic, F., Matas, J., 2018. Repeatability is Not Enough: Learning A flne Regions via Discriminability, in: Proceedings of the European Conference on Computer Vision (ECCV). Morel, J., Yu, G., 2009. ASIFT: A new framework for fully a flne invariant image comparison. SIAM Journal on Imaging Sciences 2. Mur-Artal, R., Montiel, J., Tardos, J., 2015. ORB-SLAM: a versatile and accurate monocular slam system. IEEE Transactions on Robotics 31. Pultar, M., 2020. Improving the HardNet descriptor. arXiv ePrint 2007.09699 . Riba, E., Mishkin, D., Ponsa, D., Rublee, E., Bradski, G., 2020. Kornia: an open source di fierentiable computer vision library for pytorch, in: Proc. of the IEEE /CVF Winter Conf. on Applications of Computer Vision (WACV). Rosten, E., Porter, R., Drummond, T., 2010. Faster and better: A machine learning approach to corner detection. IEEE Transactions on Pattern Analysis and Machine Intelligence 32. Sarlin, P.E., DeTone, D., Malisiewicz, T., Rabinovich, A., 2020. SuperGlue: Learning feature matching with graph neural networks, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Sch\u00a8onberger, J.L., Frahm, J.M., 2016. Structure-from-Motion revisited, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Shi, J., Tomasi, C., 1994. Good features to track, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Sun, J., Shen, Z., Wang, Y ., Bao, H., Zhou, X., 2021. LoFTR: Detector-free local feature matching with transformers, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Szeliski, R., 2021. Computer Vision: Algorithms and Applications. Springer. Tian, Y ., Balntas, V ., Ng, T., Laguna, A.B., Demiris, Y ., Mikolajczyk, K., 2020. D2D: Keypoint extraction with describe to detect approach, in: Proceedings of the Asian Conference on Computer Vision (ACCV). Truong, P., Danelljan, M., Gool, L.V ., Timofte, R., 2021. PDC-Net: Learning accurate dense correspondences and when to trust them, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Tuytelaars, T., Mikolajczyk, K., 2008. Local invariant feature detectors: a survey. Foundations and Trends in Computer Graphics and Vision 3. Tyszkiewicz, M.J., Fua, P., Trulls, E., 2020. DISK: Learning local features with policy gradient, in: Proceedings of the Conference on Neural Information Processing Systems (NeurIPS). Verdie, Y ., Yi, K.M., Fua, P., Lepetit, V ., 2015. TILDE: A temporally invariant learned detector., in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Yi, K.M., Trulls, E., Lepetit, V ., Fua, P., 2016. LIFT: Learned invariant feature transform, in: Proc. of the European Conf. on Computer Vision (ECCV). Zhang, W., Sun, C., 2020. Corner detection using multi-directional structure tensor with multiple scales. International Journal of Computer Vision ."
        },
        "Semi-supervised Keypoint Detector and Descriptor for Retinal Image Matching": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2207.07932",
            "ref_texts": "29. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: LoFTR: Detector-free local feature matching with transformers. In: CVPR (2021) 4",
            "ref_ids": [
                "29"
            ],
            "1": "In contrast to RIM, a number of end-to-end methods exist for natural image matching, including SuperPoint [7], R2D2 [21], SuperGlue [25], NCNet [23], LoFTR [29], COTR [10], PDC-Net [32], etc."
        },
        "Earth Movers in The Big Data Era: A Review of Optimal Transport in Machine Learning": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2305.05080",
            "ref_texts": "[6] J. Sun, Z. Shen, Y. Wang, H. Bao, and X. Zhou, \u201cLoftr: Detectorfree local feature matching with transformers,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 8922\u20138931, 2021.",
            "ref_ids": [
                "6"
            ],
            "1": "This seemingly abstract concept has found applications in problems including domain adaptation [1], object detection [2], reinforcement learning [3], graph representation and matching [4], [5], feature matching [6], analyzing deep learning generalization [7], generative modeling [8], knowledge distillation [9], fairness [10], [11] and many others.",
            "2": "[6] J."
        },
        "Learning to Localize in Unseen Scenes with Relative Pose Regressors": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2303.02717",
            "ref_texts": "[40] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. Proceedings of the IEEE/CVF",
            "ref_ids": [
                "40"
            ],
            "1": "In the context of localization, Transformers were shown to achieve state-of-the-art performance for image retrieval [16], object detection[11], feature detection, extraction and matching [22, 40] and for multiscene absolute pose regression [37]."
        },
        "Uncertainty-Driven Dense Two-View Structure From Motion": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2302.00523",
            "ref_texts": "[17] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detector-free local feature matching with transformers,\u201d in CVPR , 2021, pp. 8922\u2013",
            "ref_ids": [
                "17"
            ],
            "1": "Their key motivation is to learn suitable feature representations and reliable matching from large-scale datasets [14]\n[15] [16] [17].",
            "2": "Following this, several works focus on improving correspondences through learning-based methods [15] [16] [17] [36] [37] [38].",
            "3": "70 LoFTR-DT [17] 42.",
            "4": "84 LoFTR-OT [17] \u2020 16.",
            "5": "62 LoFTR-OT [17] 21.",
            "6": "96 LoFTR-DT [17] 22.",
            "7": ", SuperPoint [40] + SuperGlue [39] and LoFTR [17], which have been commonly used by many recent 3D reconstruction and localization systems [4] [56].",
            "8": "[17] J."
        },
        "C-3PO: Towards Rotation Equivariant Feature Detection and Description": {
            "authors": [],
            "url": "https://openreview.net/pdf?id=dXouQ9ubkPJ",
            "ref_texts": "26. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 8922\u20138931 (2021) 4",
            "ref_ids": [
                "26"
            ],
            "1": "Several recent approaches also leverage transformer-based attention models for feature matching [12,22,26].",
            "2": "Notably, [5] replaces the CNN backbone of LoFTR [26] with steerable CNNs based on discrete groups."
        },
        "A2B: Anchor to Barycentric Coordinate for Robust Correspondence": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2306.02760",
            "ref_texts": "348 Sun D, Roth S, Lewis J, Black MJ (2008) Learning optical flow. In: Proc. Eur. Conf. Comput. Vis., Springer, pp 83\u201397 Sun J, Shen Z, Wang Y , Bao H, Zhou X (2021) Loftr: Detector-free local feature matching with transformers. A2B: Anchor to Barycentric Coordinate for Robust Correspondence 25 In: Proc. IEEE Conf. Comput. Vis. Pattern Recogn., pp 8922\u20138931 Sun W, Jiang W, Trulls E, Tagliasacchi A, Yi KM"
        },
        "MVSFormer: Multi-View Stereo by Learning Robust Image Features and Temperature-based Depth": {
            "authors": [],
            "url": "https://openreview.net/pdf?id=2VWR6JfwNo",
            "ref_texts": "15 Published in Transactions on Machine Learning Research (12/2022) Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics , 36(4), 2017. Jae Yong Lee, Joseph DeGol, Chuhang Zou, and Derek Hoiem. Patchmatch-rl: Deep mvs with pixelwise depth, normal, and visibility. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 6158\u20136167, 2021. Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones for object detection. arXiv preprint arXiv:2203.16527 , 2022. Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 2117\u20132125, 2017a. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. InProceedings of the IEEE international conference on computer vision , pp. 2980\u20132988, 2017b. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 10012\u201310022, 2021. Keyang Luo, Tao Guan, Lili Ju, Haipeng Huang, and Yawei Luo. P-mvsnet: Learning patch-wise matching confidence aggregation for multi-view stereo. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 10452\u201310461, 2019. Xinjun Ma, Yue Gong, Qirui Wang, Jingwei Huang, Lei Chen, and Fan Yu. Epp-mvsnet: Epipolar-assembling based depth prediction for multi-view stereo. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 5732\u20135740, 2021. Zhenxing Mi, Di Chang, and Dan Xu. Generalized binary search network for highly-efficient multi-view stereo.arXiv preprint arXiv:2112.02338 , 2021. Rui Peng, Rongjie Wang, Zhenyu Wang, Yawen Lai, and Ronggang Wang. Rethinking depth estimation for multi-view stereo: A unified representation and focal loss. arXiv preprint arXiv:2201.01501 , 2022. Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. arXiv preprint arXiv:1710.05941 , 2017. Joseph Redmon and Ali Farhadi. Yolo9000: better, faster, stronger. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 7263\u20137271, 2017. Johannes L Sch\u00f6nberger, Enliang Zheng, Jan-Michael Frahm, and Marc Pollefeys. Pixelwise view selection for unstructured multi-view stereo. In European Conference on Computer Vision , pp. 501\u2013518. Springer, 2016. Thomas Schops, Johannes L Schonberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, and Andreas Geiger. A multi-view stereo benchmark with high-resolution images and multi-camera videos. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp. 3260\u20133269, 2017. Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202 , 2020. Christian Sormann, Patrick Kn\u00f6belreiter, Andreas Kuhn, Mattia Rossi, Thomas Pock, and Friedrich Fraundorfer. Bp-mvsnet: Belief-propagation-layers for multi-view-stereo. In 2020 International Conference on 3D Vision (3DV) , pp. 394\u2013403. IEEE, 2020. Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 8922\u20138931, 2021. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems , 30, 2017."
        },
        "Robust point-cloud registration based on dense point matching and probabilistic modeling": {
            "authors": [],
            "url": "https://www.lume.ufrgs.br/bitstream/handle/10183/252056/001154434.pdf?sequence=1"
        },
        "Learning soft estimator of keypoint scale and orientation with probabilistic covariant loss": {
            "authors": [
                "Pei Yan",
                "Yihua Tan",
                "Shengzhou Xiong",
                "Yuan Tai",
                "Yansheng Li"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Yan_Learning_Soft_Estimator_of_Keypoint_Scale_and_Orientation_With_Probabilistic_CVPR_2022_paper.pdf",
            "ref_texts": "[37] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the Conference on Computer Vision and Pattern Recognition , pages 8922\u2013",
            "ref_ids": [
                "37"
            ],
            "1": "Following the existing work [37], the \u201cSacre Coeur\u201d and \u201cSt.",
            "2": "5, three methods, LIFT [40], D2Net [12] and LoFTR [37]9, are added as comparison methods."
        },
        "FeatureNeRF: Learning Generalizable NeRFs by Distilling Foundation Models": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2303.12786",
            "ref_texts": "[51] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 8918\u2013",
            "ref_ids": [
                "51"
            ],
            "1": "Several supervised [13, 24, 51] and self-supervised [1, 11, 28] methods have been proposed to resolve this task in 2D and 3D domain respectively."
        },
        "SuperPoint features in endoscopy": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2203.04302",
            "ref_texts": "[28] Sun, J., Shen, Z., Wang, Y ., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: CVPR. IEEE (2021)",
            "ref_ids": [
                "28"
            ],
            "1": "Recent image matching trends propose dense matching as an intermediate step to local matching [34] and incorporating attention for the matching stages [23,9,28].",
            "2": "IEEE (2005)\n[28] Sun, J."
        },
        "A Fully-automatic Side-scan Sonar SLAM Framework": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2304.01854",
            "ref_texts": "[38] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detectorfree local feature matching with transformers,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , 2021, pp. 8922\u20138931.",
            "ref_ids": [
                "38"
            ],
            "1": "This could be possibly solved by data-driven methods [37] [38], which would be an interesting future direction, while to achieve that, more annotated keypoints are needed as training data.",
            "2": "[38] J."
        },
        "Residual aligner network": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2203.04290",
            "ref_texts": "25. Sun, J., Shen, Z., Wang, Y ., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8922\u20138931 (2021) 2, 10",
            "ref_ids": [
                "25"
            ],
            "1": "Alternatively, Cross Attention (CA) mechanism [27] is used in [14,25,8] to obtain the global receptive field and use so-called indicator matrices to quantify the relationship between each pair of pixels from two images, and the usage of multiple indicator matrices is called multi-head.",
            "2": "1) with cross attention (Attn) [25] to compare the performance at module-level."
        },
        "Level set-based camera pose estimation from multiple 2D/3D ellipse-ellipsoid correspondences": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2207.07953",
            "ref_texts": "[9] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detector-free local feature matching with transformers,\u201d in CVPR , 2021.",
            "ref_ids": [
                "9"
            ],
            "1": "Recently, detector-free matching, which directly produces dense matches without the detection phase, have also shown promising results [9].",
            "2": "[9] J."
        },
        "ImTooth: Neural Implicit Tooth for Dental Augmented Reality": {
            "authors": [
                "Hai Li",
                "Hongjia Zhai",
                "Xingrui Yang",
                "Zhirong Wu",
                "Yihao Zheng",
                "Haofan Wang",
                "Jianchao Wu",
                "Hujun Bao",
                "Guofeng Zhang"
            ],
            "url": "http://www.cad.zju.edu.cn/home/gfzhang/papers/VR-TVCG-2023-ImTooth/ImTooth.pdf",
            "ref_texts": "[49] J. Shotton, B. Glocker, C. Zach, S. Izadi, A. Criminisi, and A. W. Fitzgibbon. Scene coordinate regression forests for camera relocalization in RGB-D images. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 2930\u20132937, 2013.[50] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou. LoFTR: Detector-free local feature matching with transformers. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 8922\u20138931, 2021.",
            "ref_ids": [
                "49",
                "50"
            ],
            "1": "The coordinate regression methods [7, 8, 49] aim to estimate the 3d coordinates of the pixel in the camera view and apply Perspective-n-Point (PnP) to solve the pose of the query image.",
            "2": "Finally, the feature-based visual localization methods [18, 44, 52] usually consist of two steps, global image retrieval [2, 3, 13, 16, 20, 22, 38, 39], and local feature matching [12,23,41,50,53].",
            "3": "[49] J.",
            "4": "[50] J."
        },
        "Learning Probabilistic Coordinate Fields for Robust Correspondences": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2306.04231",
            "ref_texts": "[18] J. Sun, Z. Shen, Y. Wang, H. Bao, and X. Zhou, \u201cLoftr: Detectorfree local feature matching with transformers,\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recogn. , 2021, pp. 8922\u20138931.",
            "ref_ids": [
                "18"
            ],
            "1": "Recently, several works established dense matches in an end-to-end manner [16], [17], [18], [19].",
            "2": "Most recently, it was pointed out in LoFTR [18] that inappropriate positional encoding can even significantly degrade performance.",
            "3": "We have evaluated our approach on indoor (SUN3D [24] and ScanNet [25]) and outdoor (YFCC100M [26], PhotoTourism [27], and MegaDepth [28]) datasets on top of various state-of-the-art matching approaches (SuperGlue [14], LoFTR [18], and OANet [2]).",
            "4": "Furthermore, despite detectorfree approaches [16], [18], [30], [31] that work directly on dense feature maps, the limited resolution of feature maps can still cause information loss.",
            "5": "[18], [33] also implement similar encoding strategies to address object detection and local feature matching.",
            "6": "1 Background: Barycentric Coordinates Fields For explicit representation of position, coordinates are widely used within convolutional networks [6], [8], [27] or with multifrequency sine/cosine functions [18], [32].",
            "7": "We choose the state-of-the-art method LoFTR [18] as our baseline.",
            "8": "Training and testing indices are provided by [18].",
            "9": "Note that the AUC metric adopts the approximate AUC as in [2], [18].",
            "10": "Particularly, SuperGlue with PCFs and the SuperPoint descriptor achieves competitive results with the detector-free method LoFTR [18].",
            "11": "We choose the state-of-the-art detector-free network LoFTR [18] as our baseline.",
            "12": "To further isolate the contribution of barycentric coordinates and reliable region prediction, we conduct the experiment on the MegaDepth dataset using LoFTR-OT [18] as a baseline.",
            "13": "For dense correspondence tasks [18], [42], the extracted image feature maps can be shared with these methods to further save computational cost.",
            "14": "[18] J."
        },
        "Global multi-modal 2D/3D registration via local descriptors learning": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2205.03439",
            "ref_texts": "23. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: LoFTR: Detector-free local feature matching with transformers. CVPR (2021)",
            "ref_ids": [
                "23"
            ],
            "1": "In recent years learned keypoint detectors and descriptors [23,3,2,22] have been successfully applied to computer vision problems involving natural images, replacing hand-crafted approaches such as SIFT [10] and ORB [18].",
            "2": "1 Challenges of local feature extraction for medical images Our method is based on the LoFTR algorithm proposed in [23].",
            "3": "Similarly to [20,23], we deffne the similarity between two descriptors as their dot product, which allows the networks to encode the quality of a keypoint in the norm of its descriptor."
        },
        "Semi-supervised Deep Large-Baseline Homography Estimation with Progressive Equivalence Constraint": {
            "authors": [
                "Hai Jiang",
                "Haipeng Li",
                "Yuhang Lu",
                "Songchen Han",
                "Shuaicheng Liu"
            ],
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/25183/24955",
            "ref_texts": "2020. BEBLID: Boosted efficient binary local image descriptor. Pattern Recognition Letters, 133: 366\u2013372. Sun, J.; Shen, Z.; Wang, Y .; Bao, H.; and Zhou, X. 2021. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 8922\u20138931. Tian, Y .; Yu, X.; Fan, B.; Wu, F.; Heijnen, H.; and Balntas, V . 2019. Sosnet: Second order similarity regularization for local descriptor learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 11016\u201311025. Truong, P.; Danelljan, M.; Gool, L. V .; and Timofte, R. 2020. GOCor: Bringing globally optimized correspondence volumes into your neural network. Advances in Neural Information Processing Systems, 33: 14278\u201314290. Truong, P.; Danelljan, M.; and Timofte, R. 2020. GLU-Net: Global-Local Universal Network for Dense Flow and Correspondences. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. Truong, P.; Danelljan, M.; Van Gool, L.; and Timofte, R.",
            "ref_ids": [
                "2020"
            ]
        },
        "Shallow-Guided Transformer for Semantic Segmentation of Hyperspectral Remote Sensing Imagery": {
            "authors": [
                "Yuhan Chen",
                "Pengyuan Liu",
                "Jiechen Zhao",
                "Kaijian Huang",
                "Qingyun Yan"
            ],
            "url": "https://www.mdpi.com/2072-4292/15/13/3366/pdf",
            "ref_texts": "31. Sun, J.; Shen, Z.; Wang, Y.; Bao, H.; Zhou, X. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Nashville, TN, USA, 20\u201325 June 2021; pp. 8922\u20138931.",
            "ref_ids": [
                "31"
            ],
            "1": "First, it enhances the model\u2019s inductive bias, which is crucial for learning and generalization [31,32]."
        },
        "SNAP: Self-Supervised Neural Maps for Visual Positioning and Semantic Understanding": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2306.05407",
            "ref_texts": "[85] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: DetectorFree Local Feature Matching with Transformers. In CVPR , 2021. 5",
            "ref_ids": [
                "85"
            ],
            "1": "Past works have thus focused on learning specific components like feature extraction [104,52,20, 21,92,62,23,94,98,49], matching [105,108,68,85,36,110,100], and pose [97,69] or point cloud refinement [47]."
        },
        "One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2306.16928",
            "ref_texts": "[69] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021.",
            "ref_ids": [
                "69"
            ],
            "1": "Given a set of four predicted nearby views, we perform feature matching to identify corresponding keypoints across each pair of images (a total of six pairs) using an off-the-shelf module LoFTR [69]."
        },
        "FlowFormer: A Transformer Architecture and Its Masked Cost Volume Autoencoding for Optical Flow": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2306.05442",
            "ref_texts": "[54] J. Sun, Z. Shen, Y. Wang, H. Bao, and X. Zhou, \u201cLoftr: Detectorfree local feature matching with transformers,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2021, pp. 8922\u20138931.",
            "ref_ids": [
                "54"
            ],
            "1": "Visual correspondence tasks [54], [55], [56], [57] is a main stream in computer vision.",
            "2": "LoFTR [54] utilized transformer to remove feature detector and further improved performance.",
            "3": "[54] J."
        },
        "Learning a Task-Specific Descriptor for Robust Matching of 3D Point Clouds": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2210.14899",
            "ref_texts": "[28] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 8922\u20138931, 2021. 2",
            "ref_ids": [
                "28"
            ],
            "1": "It not only learns the local geometry of each point in the current point cloud by convolution but also exploits the repetitive structure in the paired point cloud by using the Transformer [26]\u2013[28]."
        },
        "Learning by Aligning 2D Skeleton Sequences in Time": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2305.19480",
            "ref_texts": "[66] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 5",
            "ref_ids": [
                "66"
            ],
            "1": "Following [41,66], we add a cross-attention module to learn interactions between HandH\u2032(i."
        },
        "AMatFormer: Efficient Feature Matching via Anchor Matching Transformer": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2305.19205",
            "ref_texts": "[25] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detectorfree local feature matching with transformers,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , 2021, pp. 8922\u20138931.",
            "ref_ids": [
                "25"
            ],
            "1": "[25] propose a local image feature matching method based on pixel-level dense matching at coarse-level and then accurate matching at fine-level.",
            "2": "[25] J."
        },
        "Rethinking Low-Level Features for Interest Point Detection and Description": {
            "authors": [
                "Changhao Wang",
                "Guanwen Zhang",
                "Zhengyun Cheng",
                "Wei Zhou"
            ],
            "url": "https://openaccess.thecvf.com/content/ACCV2022/papers/Wang_Rethinking_Low-level_Features_for_Interest_Point_Detection_and_Description_ACCV_2022_paper.pdf",
            "ref_texts": "33. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: CVPR (2021)",
            "ref_ids": [
                "33"
            ],
            "1": "However, building correspondences on the basis of descriptors simply may cause a mass of outliers [3,27,33,41,42].",
            "2": "Inspired by SuperGlue, LoFTR [33], a detector-free approach, proposes to obtain high-quality matches with transformers [39].",
            "3": "In recent years, several studies calculate the similarity matrix of descriptors between input pairs and use a differentiable matching layer to learn good correspondences [25,27,33]."
        },
        "A Large-Scale Invariant Matching Method Based on DeepSpace-ScaleNet for Small Celestial Body Exploration": {
            "authors": [
                "Mingrui Fan",
                "Wenlong Lu",
                "Wenlong Niu",
                "Xiaodong Peng",
                "Zhen Yang"
            ],
            "url": "https://www.mdpi.com/2072-4292/14/24/6339/pdf",
            "ref_texts": "30. Sun, J.; Shen, Z.; Wang, Y.; Bao, H.; Zhou, X. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Nashville, TN, USA, 21\u201324 June 2021; pp. 8922\u20138931.",
            "ref_ids": [
                "30"
            ],
            "1": "LOFTR proposes a coarse-to-fine local dense feature matching method that achieves better results in weakly textured regions [30].",
            "2": "The AUC of the pose error is the maximum of the angular errors in rotation and translation, which is often used in existing positional error estimates [13,30]."
        },
        "Refinement for Absolute Pose Regression with Neural Feature Synthesis": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2303.10087",
            "ref_texts": "[52] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR , 2021. 1, 3",
            "ref_ids": [
                "52"
            ],
            "1": "Classical methods for solving this problem rely on geometry-based techniques [4,5,6,26,39,41,42,43] that require explicit feature correspondence search [15, 16, 24, 40, 52, 53], which is a challenging task in itself.",
            "2": "Different from the common feature matching literatures\u2019 [24, 52] convention, our features are normalized along the spatial direction instead of the channel direction to ensure the consistency of the neighboring pixels."
        },
        "HM3D-ABO: A Photo-realistic Dataset for Object-centric Multi-view 3D Reconstruction": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2206.12356",
            "ref_texts": "[34] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 8922\u20138931, 2021. 2",
            "ref_ids": [
                "34"
            ]
        },
        "Local feature extraction from salient regions by feature map transformation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2301.10413",
            "ref_texts": "[38] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021.",
            "ref_ids": [
                "38"
            ],
            "1": "Detector-free local feature matcher [38, 45] and a decoupled pipeline for a detection and description module were also studied [16].",
            "2": "Nighttime images are challenging due to the uncertainties of light and structure [38].",
            "3": "Our network performed fairly well when compared with matcher methods [38, 45].",
            "4": "9 LoFTR\u0003[38] 72."
        },
        "A Fast and Robust Heterologous Image Matching Method for Visual Geo-Localization of Low-Altitude UAVs": {
            "authors": [
                "Haigang Sui",
                "Jiajie Li",
                "Junfeng Lei",
                "Chang Liu",
                "Guohua Gou"
            ],
            "url": "https://www.mdpi.com/2072-4292/14/22/5879/pdf",
            "ref_texts": "35. Sun, J.; Shen, Z.; Wang, Y.; Bao, H.; Zhou, X. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR); IEEE Xplore: Washington, DC, USA, 2021; pp. 8922\u20138931.",
            "ref_ids": [
                "35"
            ],
            "1": "[35] tried to integrate the Transformer into the detector-free matching method.",
            "2": "Secondly, the matching strategy from coarse to fine is beneficial to the matching efficiency and precision [32,35].",
            "3": "Secondly, the matching strategy from coarse to fine is beneficial to the matching efficiency and precision [32,35].",
            "4": "Hereinafter referred to as the SuperGlue method), the detector-free matching method LoFTR [35] and Patch2Pix [54]."
        },
        "Tracking Everything Everywhere All at Once": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2306.05422.pdf?trk=public_post_comment-text",
            "ref_texts": "[58] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proc. Computer Vision and Pattern Recognition (CVPR) , pages 8922\u20138931, 2021. 2",
            "ref_ids": [
                "58"
            ],
            "1": "Several methods learn such correspondences in a selfor weakly-supervised manner [5, 9, 12, 34, 49, 67, 69, 73] using cues like cycle consistency [26,70,76], while others [16,28,58,64,65] use stronger supervision signals such as geometric correspondences generated from 3D reconstruction pipelines [36, 52]."
        },
        "PhotoMatch: An Open-Source Tool for Multi-View and Multi-Modal Feature-Based Image Matching": {
            "authors": [],
            "url": "https://www.mdpi.com/2076-3417/13/9/5467/pdf",
            "ref_texts": "38. Sun, J.; Shen, Z.; Wang, Y.; Bao, H.; Zhou, X. LoFTR: Detector-Free Local Feature Matching With Transformers. In Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Nashville, TN, USA, 20\u201325 June 2021; pp.",
            "ref_ids": [
                "38"
            ],
            "1": "For instance, in the last Image Matching Challenge (IMC) (Image Matching Challenge\u20142022 edition ) [34], the best-performing algorithms were ASpanFormer [35], and combinations of SuperGlue [36], SuperPoint [37], LoFTR [38], DKM [39] and DISK [40]."
        },
        "Jigsaw: Learning to Assemble Multiple Fractured Objects": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2305.17975",
            "ref_texts": "[20] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR , 2021.",
            "ref_ids": [
                "20"
            ],
            "1": "Methods utilizing CNN, GNN, and Transformer have found successful applications in various domains, such as image registration [19,20] and graph matching on images [21,22,23,24,25]."
        },
        "SimSC: A Simple Framework for Semantic Correspondence with Temperature Learning": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2305.02385",
            "ref_texts": "[38] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR , 2021. 2",
            "ref_ids": [
                "38"
            ],
            "1": "Recently, dense feature maps extracted by deep neural networks [15, 37] have shown great success in many computer vision tasks [8,10,26,36,38,39]."
        },
        "HSCNet++: Hierarchical Scene Coordinate Classification and Regression for Visual Localization with Transformer": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2305.03595",
            "ref_texts": ""
        },
        "FetReg2021: A Challenge on Placental Vessel Segmentation and Registration in Fetoscopy": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2206.12512",
            "ref_texts": "28 Sadda, P., Imamoglu, M., Dombrowski, M., Papademetris, X., Bahtiyar, M.O., Onofrey, J., 2019. Deep-learned placental vessel segmentation for intraoperative video enhancement in fetoscopic surgery. Sadda, P., Onofrey, J.A., Bahtiyar, M.O., Papademetris, X., 2018. Better feature matching for placental panorama construction, in: Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), Springer Verlag. pp. 128\u2013137. Sarlin, P.E., DeTone, D., Malisiewicz, T., Rabinovich, A., 2020. Superglue: Learning feature matching with graph neural networks, in: IEEE Conference on Computer Vision and Pattern Recognition, pp. 4938\u20134947. Senat, M.V ., Deprest, J., Boulvain, M., Paupe, A., Winer, N., Ville, Y ., 2004. Endoscopic Laser Surgery versus Serial Amnioreduction for Severe Twin-to-Twin Transfusion Syndrome. New England Journal of Medicine 351, 136\u2013144. Sun, J., Shen, Z., Wang, Y ., Bao, H., Zhou, X., 2021. LoFTR: Detector-free local feature matching with transformers, in: IEEE Conference on Computer Vision and Pattern Recognition, pp. 8922\u20138931. Tan, M., Le, Q., 2019. E flcientnet: Rethinking model scaling for convolutional neural networks, in: International Conference on Machine Learning, PMLR. pp."
        },
        "Tracking Growth and Decay of Plant Roots in Minirhizotron Images": {
            "authors": [
                "Alexander Gillert",
                "Bo Peters",
                "Uwe Freiherr",
                "Jurgen Kreyling",
                "Gesche Blume"
            ],
            "url": "https://openaccess.thecvf.com/content/WACV2023/papers/Gillert_Tracking_Growth_and_Decay_of_Plant_Roots_in_Minirhizotron_Images_WACV_2023_paper.pdf",
            "ref_texts": "[17] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 8918\u2013",
            "ref_ids": [
                "17"
            ],
            "1": "Neural network based feature matching methods such as SuperGlue [15], LoFTR [17] or COTR [10] promise better performance, however contrary to our method, they require ground truth annotations which are expensive to obtain with our images."
        },
        "Feature Correlation Transformer for Estimating Ambiguous Optical Flow": {
            "authors": [],
            "url": "https://www.researchsquare.com/article/rs-2253481/latest.pdf",
            "ref_texts": "[16] Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free loc al feature matching with transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8922\u20138931",
            "ref_ids": [
                "16"
            ],
            "1": "4938\u20134947 (2020)\n[16] Sun, J."
        },
        "Fully Differentiable RANSAC": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2212.13185",
            "ref_texts": "[62] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR , 2021. 1, 2, 6, 8",
            "ref_ids": [
                "62"
            ],
            "1": "In recent years, neural networks (NNs) have been employed to estimate tentative matches, including their coordinates and confidences [20,62,63,73\u201375].",
            "2": "\u2022 To demonstrate its potential to unlock the end-to-end training of geometric pipelines, r-RANSAC is incorporated into an end-to-end feature matcher, LoFTR\n[62], to improve the predicted matches and confidence.",
            "3": ", LoFTR [62], to improve matching prediction with reliable confidence scores (Section 4.",
            "4": "Learning Feature Matching with r-RANSAC In this section, we tune an end-to-end feature matcher, LoFTR [62], on the epipolar error using r-RANSAC.",
            "5": "If RANSAC is non-differentiable, these metrics cannot be directly usedInference Protocol LoFTR [62] F1 score (%) avg.",
            "6": "To demonstrate its potential in unlocking the training of geometric pipelines, we train r-RANSAC together with a recent detector-free feature matcher, LoFTR [62], with which 8 we achieve improved confidence prediction and accurate robust estimation."
        },
        "Visual Localization and Target Perception Based on Panoptic Segmentation": {
            "authors": [
                "Kefeng Lv",
                "Yongsheng Zhang",
                "Ying Yu",
                "Zhenchao Zhang",
                "Lei Li"
            ],
            "url": "https://www.mdpi.com/2072-4292/14/16/3983/pdf",
            "ref_texts": "23. Sun, J.; Shen, Z.; Wang, Y.; Bao, H.; Zhou, X. LoFTR: Detector-Free Local Feature Matching with Transformers. In Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Nashville, TN, USA, 20\u201325 June 2021; pp. 8918\u20138927.",
            "ref_ids": [
                "23"
            ],
            "1": "To overcome this difficulty, the methods developed in [23,24] generate feature descriptors with stronger stability.",
            "2": "Unlike the methods [21,23] (wherein the 3D points are projected in the target image to verify the semantics), the proposed method maps the positions of the feature points in the rendered image to the truth image.",
            "3": "The RobotCar Season dataset is a subset of the University of Oxford RobotCar dataset [23].",
            "4": "Evaluation Measures A standard practice [23] was followed for the public dataset, wherein the pose accuracy of a method was measured by the deviation between the estimated and the ground truth pose."
        },
        "ALIKED: A Lighter Keypoint and Descriptor Extraction Network via Deformable Transformation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2304.03608",
            "ref_texts": "[53] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detectorfree local feature matching with transformers,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , 2021, pp. 8922\u20138931.",
            "ref_ids": [
                "53"
            ],
            "1": "Neither the learned matcher [51], [52] nor the direct image matcher [53]\u2013[55] are included, as they are beyond the scope of our study and are not comparable to the proposed method.",
            "2": "[53] J."
        },
        "DGC-GNN: Descriptor-free Geometric-Color Graph Neural Network for 2D-3D Matching": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2306.12547",
            "ref_texts": "[46] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2021.",
            "ref_ids": [
                "46"
            ],
            "1": "2 Global Geometric Guidance Global context guidance has demonstrated its effectiveness in various computer vision tasks [46,22, 53,33].",
            "2": "However, most existing methods [46,33] consider the outputs from different encoding layers as global and local features.",
            "3": "We then adopt linear attention [20,46] as a cross-attention mechanism, which allows each keypoint in one modality to interact with all keypoints from another modality.",
            "4": "The co-visible images are obtained using the co-visible scores provided by LOFTR [46]."
        },
        "LightGlue: Local Feature Matching at Light Speed": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2306.13643",
            "ref_texts": "[68] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with Transformers. CVPR , 2021. 2, 6, 11, 12, 14, 15",
            "ref_ids": [
                "68"
            ],
            "1": "Conversely, dense matchers like LoFTR [68] and followups [9,78] match points distributed on dense grids rather than sparse locations.",
            "2": "[68] to avoid training on scenes included in the Image Matching Challenge [31].",
            "3": "Following best practices in benchmarking [31] and unlike past works [56,68], we use a state-of-the-art robust estimator [3] and extensively tune the inlier threshold for each method separately.",
            "4": "Baselines: We follow the setup of [68] and resize all images such that their smaller dimension is equal to 480 pixels.",
            "5": "For reference, we also evaluate the dense matcher LoFTR [68], selecting only the top 1024 predicted matches for the sake of fairness.",
            "6": "Setup: We use image pairs from the MegaDepth-1500 test set following the evaluation of [68].",
            "7": "We also evaluate the recent, dense deep matchers LoFTR [68], MatchFormer [78], and ASpanFormer [9].",
            "8": "Furthermore, we compare our best scoring method on IMC 2020, DISK+LightGlue, with tuned versions of DISK [73], SuperPoint+SuperGlue [16,56] as well as the SfM implementation of the dense matcher LoFTR [68].",
            "9": "In contrast to the split used by previous works [38, 68], this set of test images avoids training overlap with SuperGlue [56].",
            "10": "works [9,68,78].",
            "11": "[68], we bin each pair by its covisibility score [19], into ranges [0.",
            "12": "We follow the setup introduced in LoFTR [68] and resize images to a maximum edge length of 480.",
            "13": "Our reasoning behind this decision, which is in contrast to previous works in feature matching [56,68] which fix the RANSAC parameters, is that we mainly use RANSAC as a tool to evaluate the low-level matches on a downstream task, and we want to minimize the variations introduced by its hyperparameters in order to obtain fair and representative evaluations."
        },
        "Automatic Production of Deep Learning Benchmark Dataset for Affine-Invariant Feature Matching": {
            "authors": [
                "Guobiao Yao",
                "Jin Zhang",
                "Jianya Gong",
                "Fengxiang Jin"
            ],
            "url": "https://www.mdpi.com/2220-9964/12/2/33/pdf",
            "ref_texts": "23. Sun, J.; Shen, Z.; Wang, Y.; Bao, H.; Zhou, X. LoFTR: Detector-free local feature matching with transformers. arXiv 2021 , arXiv:2104.00680. [CrossRef]",
            "ref_ids": [
                "23"
            ],
            "1": "introduced the position encoding and attention mechanism in the transformer [22] network and constructed a model named the local feature transformer (LoFTR) [23] with a texture enhancement function; this method significantly improves the matching performance in regions with weak texture.",
            "2": "First, the complementary algorithms, ASIFT [15] and LoFTR [23], are integrated to extract quasi-dense corresponding features across wide-baseline oblique stereo images (Yellow part of Figure 1).",
            "3": "The extraction of type-II matches was performed by LoFTR [23] with projection transformation.",
            "4": "The extraction of type-II matches was performed by LoFTR [23] with projection transformation."
        },
        "DAC: Detector-Agnostic Spatial Covariances for Deep Local Features": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2305.12250",
            "ref_texts": "[54] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 2",
            "ref_ids": [
                "54"
            ],
            "1": "Finally, recent works [24, 54, 56] exploit attention mechanisms to match pixels without explicitly using detectors."
        },
        "Detector-Free Structure from Motion": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2306.15669",
            "ref_texts": "[48] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. CVPR , 2021. 2, 3, 6, 8, 12, 13",
            "ref_ids": [
                "48"
            ],
            "1": "Recently, detector-free matchers [48,10,58] achieve state-of-the-art performance on the image-matching task.",
            "2": ", LoFTR [48].",
            "3": "In recent years, many methods directly match image pairs in a dense [52] or semi-dense manner [39,27,48,50,10, 58], avoiding feature detection.",
            "4": "With the help of Transformer [54], some semi-dense matching methods [48,10,58] achieve higher accuracy compared with detector-based baselines and show strong capabilities in building correspondences on low-textured regions.",
            "5": "Note that the coarse-level correspondences output by some detector-free [48,58,10] matchers are typically at 1/8image resolution, which can directly be used as quantized matches.",
            "6": "Our method with detector-free matcher LoFTR [48] is qualitatively compared with the detector-based baseline SP + SG + PixSfM on multiple scenes.",
            "7": "2) Detector-free SfM baseline LoFTR [48]\n+ PixSfM [30], where PixSfM is fed with LoFTR\u2019s matches, which are quantized by the same strategy as in our pipeline.",
            "8": "Implementation Details Our detector-free SfM framework is implemented with multiple detector-free matchers, including LoFTR [48], MatchFormer [58] and AspanTransformer [10], to demonstrate the compatibility of our pipeline.",
            "9": "We compare our method with PixSfM that uses the same LoFTR [48] coarse matches and the same number of CPU cores as ours, where its cost Texture-Poor ObjectsDense ReconstructionBenefitsOur Detector-Free SfM Figure 8.",
            "10": "On the scene with 2000 images, the detector-free matching [48] and coarse SfM takes 4.",
            "11": "Following [59,48], we make our loss uncertainty weighted with a variance term \u03c32(x): L=1 NX j\u2208ntX k\u2208nv1\n\u03c32(x)\u2225x\u2212xgt\u22252, where ntis the number of feature tracks, nvis the number of query views in a track, and Nis the total number of refined keypoints.",
            "12": "6 demonstrate that our framework canETH3D Dataset IMC (Mount Rushmore ) AUC@1\u25e6AUC@3\u25e6AUC@5\u25e6AUC@3\u25e6AUC@5\u25e6AUC@10\u25e6 LoFTR [48]No Refine 30."
        },
        "Learning to Estimate 6DoF Pose from Limited Data: A Few-Shot, Generalizable Approach using RGB Images": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2306.07598",
            "ref_texts": "[91] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 3, 4",
            "ref_ids": [
                "91"
            ],
            "1": "For 6D pose estimation, the few-shot pose estimator usually employs local image feature matching [59], establishing correspondences between two images in a detector-based [61, 62, 82, 83] or detector-free [49, 56, 81, 91] fashion.",
            "2": "Both Wand\u2113can be applied to either key points or whole images [91]."
        },
        "LU-NeRF: Scene and Pose Estimation by Synchronizing Local Unposed NeRFs": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2306.05410",
            "ref_texts": "[52] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR , 2021. 2, 7[53] Rick Szeliski and Sing Bing Kang. Shape ambiguities in structure from motion. IEEE Transactions on Pattern Analysis and Machine Intelligence , 1997. 4",
            "ref_ids": [
                "52",
                "53"
            ],
            "1": "These techniques are largely built upon local features [32, 45, 22, 52] and require accurate detection and matching across images.",
            "2": "Mirror-symmetry ambiguity The ambiguities and degeneracies encountered when estimating 3D structure have been extensively studied [53, 7, 17].",
            "3": "COLMAP-LoFTR improves COLMAP with LoFTR [52], a detector-free feature matcher.",
            "4": "COLMAP can be improved with modern feature extraction and matching algorithms [47] such as SuperPoint [22] and SuperGLUE [48] (denoted COLMAP-SPSG), or LoFTR [52] (denoted COLMAPLoFTR), but these still struggle in scenes with little or repeated texture.",
            "5": "2, 7[53] Rick Szeliski and Sing Bing Kang."
        },
        "SiLK--Simple Learned Keypoints": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2304.06194",
            "ref_texts": "[47] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings oftheIEEE/CVF conference oncomputer vision and pattern recognition, pages 8922\u20138931, 2021. 2, 4, 5, 6, 8, 13",
            "ref_ids": [
                "47"
            ],
            "1": "Similar to [50, 47], SiLK adopts a probabilistic approach by modeling the matching probabilities in a doublesoftmax, cycle-consistency setting and optimizes the log likelihood.",
            "2": "Implemented as a GNN[43] in [42], or as a Transformer[51] in [47], CA\u2019s predictions are conditional on all descriptors from the pair of images.",
            "3": "SiLK does not use CA, but outperforms [42] and performs competitively with [47].",
            "4": "LoFTR\n[47] leveraged OT as well, but found little difference between OT and the simpler approach of mutual nearest neighbor (MNN) in some benchmarks.",
            "5": "Descriptors Define Matching Probability Similar to [47, 50], we model the cycle matching probability using a double softmax (i.",
            "6": "4 Descriptor and Keypoint Losses Similar to [47, 50], the descriptor loss is the negative loglikelihood loss applied to the matching probabilities for the positive round-trips (i.",
            "7": "HPatches Homography Estimation Following [16, 42, 39, 47], we evaluate homography estimation on HPatches [3].",
            "8": "We follow LoFTR [47], (currently SOTA on HPatches), and and scale the shorter image edge to 480 at inference time.",
            "9": "Finally, we use OpenCV RANSAC algorithm to compute homography based on matching results and evaluate Homography Estimation Accuracy [16] and Homography Estimation AUC [42, 47].",
            "10": "GNN [42] or transformer [47]) to adapt the features to a specific pair of images, and then finds matches.",
            "11": "While SiLK does not employ CA, we still include comparisons to the SOTA detector-free LoFTR [47].",
            "12": "Relative camera pose estimation has been used in multiple previous works [52, 53, 6, 42, 47].",
            "13": "2 LoFTR(MegaDepth) [47] 91.",
            "14": "5 23 FPN [47] 0.",
            "15": "For example, LoFTR[47], trained on indoor ScanNet data, drops significantly vs LoFTR trained on outdoor MegaDepth data (Tab.",
            "16": "One can also observe large performance gain (except on MMA) versus LoFTR [47](dense keypoint method) in the low resolution regime.",
            "17": "More recently[47], a similar idea has emerged from the probabilistic formulation of the matching problem: Filtering out low-probability matches seems like a natural way to reduce false positive matches."
        },
        "Learning-based Relational Object Matching Across Views": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2305.02398",
            "ref_texts": "[32] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoFTR: Detector-free local feature matching with transformers,\u201d in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2021.",
            "ref_ids": [
                "32"
            ],
            "1": "Different to object associations, keypoint matching methods yield local image correspondences between pairs of images for moderate view point changes [26], [32], [10].",
            "2": "[32] J."
        },
        "Identifying Historic Buildings over Time through Image Matching": {
            "authors": [
                "Kyriaki A. Tychola",
                "Stamatis Chatzistamatis",
                "Eleni Vrochidou",
                "George E. Tsekouras",
                "George A. Papakostas"
            ],
            "url": "https://www.mdpi.com/2227-7080/11/1/32/pdf",
            "ref_texts": "60. Sun, J.; Shen, Z.; Wang, Y.; Bao, H.; Zhou, X. LoFTR: Detector-Free Local Feature Matching with Transformers. Comput. Vis. Pattern Recognit. 2021 , 8922\u20138931. [CrossRef]",
            "ref_ids": [
                "60"
            ],
            "1": "Nevertheless, feature extraction may not extract an adequate number of points of interest because of the bad texture of repeated patterns, different viewing angles, lighting , and blur \n[59,60].",
            "2": "Therefore, a high dimensional vector represents features detected, descriptors are derived independently for each image , and the matching of the two im ages is achieved by searching the neighborhood [55,60].",
            "3": "Nevertheless, feature extraction may not extract an adequate number of points of interest because of the bad texture of repeated patterns, different viewing angles, lighting, and blur [59,60].",
            "4": "Therefore, a high-dimensional vector represents features detected, descriptors are derived independently for each image, and the matching of the two images is achieved by searching the neighborhood [55,60]."
        },
        "Single-Stage Visual Query Localization in Egocentric Videos": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2306.09324",
            "ref_texts": "[42] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. CVPR , pages 8918\u20138927, 2021.",
            "ref_ids": [
                "42"
            ]
        },
        "RoMa: Revisiting Robust Losses for Dense Feature Matching": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2305.15404",
            "ref_texts": "[48] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8922\u20138931, 2021.",
            "ref_ids": [
                "48"
            ],
            "1": "Recently, the detector-free approach [48,7,50,10] replaces the keypoint detection with dense matching on a coarse scale, followed by mutual nearest 2 DKM warp \u2299 certaintyRoMa warp \u2299 certaintyFigure 2: Qualitative comparison.",
            "2": "LoFTR [48] optimize the model log-likelihood of mutual nearest neighbors, followed by L2regression-based refinement.",
            "3": "7 LoFTR [48] CVPR\u201921 65.",
            "4": "0 LoFTR [48] CVPR\u201921 52.",
            "5": "We follow the evaluation protocol proposed LoFTR [48].",
            "6": "MegaDepth-1500 Pose Estimation: We use the MegaDepth-1500 test set [48] which consists of 1500 pairs from scene 0015 (St.",
            "7": "We follow the protocol in [48,10] and use a RANSAC threshold of 0.",
            "8": "4 LoFTR [48] 78.",
            "9": "In practice, this is approximated using the trapezoidal rule, as in previous work [14, 57, 48]."
        },
        "A Comprehensive Survey on Applications of Transformers for Deep Learning Tasks": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2306.07303",
            "ref_texts": "4505\u20134514). Association for Computational Linguistics. Su, W., Zhu, X., Cao, Y ., Li, B., Lu, L., Wei, F., & Dai, J. (2020). VL-BERT: pre-training of generic visual-linguistic representations. In 8th International Conference on Learning Representations, ICLR, Addis Ababa, Ethiopia, April 26-30 . OpenReview.net. Subakan, C., Ravanelli, M., Cornell, S., Bronzi, M., & Zhong, J. (2021). Attention is all you need in speech separation. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP, Toronto, ON, Canada, June 6-11 (pp. 21\u201325). IEEE. Subramanyam, K., Rajasekharan, A., & Sangeetha, S. (2021a). Ammu: A survey of transformer-based biomedical pretrained language models. arXiv e-prints , (pp. arXiv\u20132105). Subramanyam, K., Rajasekharan, A., & Sangeetha, S. (2021b). AMMUS : A survey of transformer-based pretrained models in natural language processing. CoRR ,abs/2108.05542 . URL: https://arxiv.org/abs/2108.05542 .arXiv:2108.05542 . Sun, H., Chen, X., Shi, Q., Hong, M., Fu, X., & Sidiropoulos, N. D. (2017). Learning to optimize: Training deep neural networks for wireless resource management. In 18th IEEE International Workshop on Signal Processing Advances in Wireless Communications, SPAWC, Sapporo, Japan, July 3-6 (pp. 1\u20136). IEEE. Sun, J., Shen, Z., Wang, Y ., Bao, H., & Zhou, X. (2021a). Loftr: Detector-free local feature matching with transformers. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR virtual, June 19-25 (pp. 8922\u20138931). Computer Vision Foundation / IEEE. Sun, Q., Fang, N., Liu, Z., Zhao, L., Wen, Y ., Lin, H. et al. (2021b). Hybridctrm: Bridging cnn and transformer for multimodal brain image segmentation. Journal of Healthcare Engineering ,2021 . Suzuki, M., & Matsuo, Y . (2022). A survey of multimodal deep generative models. Adv. Robotics ,36, 261\u2013278. Szummer, M., & Picard, R. W. (1998). Indoor-outdoor image classification. In 1998 International Workshop on Content-Based Access of Image and Video Databases, CAIVD 1998, Bombay, India, January 3, 1998 (pp. 42\u201351). IEEE Computer Society. Tan, H., & Bansal, M. (2019). LXMERT: learning cross-modality encoder representations from transformers. In K. Inui, J. Jiang, V . Ng, & X. Wan (Eds.), Proceedings of the Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP, Hong Kong, China, November 3-7 (pp. 5099\u20135110). Association for Computational Linguistics. Tas, O., & Kiyani, F. (2007). A survey automatic text summarization. PressAcademia Procedia ,5, 205\u2013213. Tay, Y ., Dehghani, M., Bahri, D., & Metzler, D. (2023). Efficient transformers: A survey. ACM Comput. Surv. ,55, 109:1\u2013109:28. Tay, Y ., Tran, V . Q., Ruder, S., Gupta, J. P., Chung, H. W., Bahri, D., Qin, Z., Baumgartner, S., Yu, C., & Metzler, D. (2022). Charformer: Fast character transformers via gradient-based subword tokenization. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29 . OpenReview.net. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., & J \u00b4egou, H. (2021). Training data-efficient image transformers & distillation through attention. In M. Meila, & T. Zhang (Eds.), Proceedings of the 38th International Conference on Machine Learning, ICML, 18-24 July, Virtual Event (pp. 10347\u201310357). PMLR volume 139 of Proceedings of Machine Learning Research . Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). Attention is all you need. In I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V . N. Vishwanathan, & R. Garnett (Eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems, December 4-9, Long Beach, CA, USA (pp. 5998\u20136008). Vig, J., Madani, A., Varshney, L. R., Xiong, C., Socher, R., & Rajani, N. F. (2021). Bertology meets biology: Interpreting attention in protein language models. In 9th International Conference on Learning Representations, ICLR, Virtual Event, Austria, May 3-7 . OpenReview.net. Wang, D., & Chen, J. (2018). Supervised speech separation based on deep learning: An overview. IEEE ACM Trans. Audio Speech Lang. Process. ,26, 1702\u20131726. Wang, G., Smetannikov, I., & Man, T. (2020a). Survey on automatic text summarization and transformer models applicability. In CCRIS: International Conference on Control, Robotics and Intelligent System, Xiamen, China, October 27-29 (pp. 176\u2013184). ACM. Wang, J., Yang, Z., Hu, X., Li, L., Lin, K., Gan, Z., Liu, Z., Liu, C., & Wang, L. (2022a). GIT: A generative image-to-text transformer for vision and language. CoRR ,abs/2205.14100 . URL: https://doi.org/10.48550/arXiv.2205.14100 . doi:10.48550/ arXiv.2205.14100 .arXiv:2205.14100 . Wang, P., Cheng, Y ., & Dong, B. (2021a). Augmented convolutional neural networks with transformer for wireless interference identification. In IEEE Global Communications Conference, GLOBECOM, Madrid, Spain, December 7-11 (pp. 1\u20136). IEEE. Wang, S., Bi, S., & Zhang, Y .-J. A. (2022b). Deep reinforcement learning with communication transformer for adaptive live streaming in wireless edge networks. IEEE Journal on Selected Areas in Communications ,40, 308\u2013322. Wang, T., Lai, Z., & Kong, H. (2021b). Tfnet: Transformer fusion network for ultrasound image segmentation. In C. Wallraven, Q. Liu, & H. Nagahara (Eds.), Pattern Recognition 6th Asian Conference, ACPR, Jeju Island, South Korea, November 9-12, Revised Selected Papers, Part I (pp. 314\u2013325). Springer volume 13188 of Lecture Notes in Computer Science . Wang, T., Lan, J., Han, Z., Hu, Z., Huang, Y ., Deng, Y ., Zhang, H., Wang, J., Chen, M., Jiang, H. et al. (2022c). O-net: a novel framework with deep fusion of cnn and transformer for simultaneous segmentation and classification. Frontiers in Neuroscience ,16. Wang, W., Liang, D., Chen, Q., Iwamoto, Y ., Han, X.-H., Zhang, Q., Hu, H., Lin, L., & Chen, Y .-W. (2020b). Medical image classification using deep learning. Deep learning in healthcare: paradigms and applications , (pp. 33\u201351). Wang, Z., Ma, Y ., Liu, Z., & Tang, J. (2019). R-transformer: Recurrent neural network enhanced transformer. CoRR ,abs/1907.05572 . URL: http://arxiv.org/abs/1907.05572 .arXiv:1907.05572 . Wang, Z., Yu, J., Yu, A. W., Dai, Z., Tsvetkov, Y ., & Cao, Y . (2022d). Simvlm: Simple visual language model pretraining with weak supervision. In The Tenth International Conference on Learning Representations, ICLR, Virtual Event, April 25-29 . OpenReview.net. Wu, D., Pigou, L., Kindermans, P., Le, N. D., Shao, L., Dambre, J., & Odobez, J. (2016). Deep dynamic neural networks for multimodal gesture segmentation and recognition. IEEE Trans. Pattern Anal. Mach. Intell. ,38, 1583\u20131597. Wu, Y ., Wang, G., Wang, Z., Wang, H., & Li, Y . (2022). Di-unet: Dimensional interaction self-attention for medical image segmentation. Biomed. Signal Process. Control. ,78, 103896. Xie, W., Zou, J., Xiao, J., Li, M., & Peng, X. (2022). Quan-transformer based channel feedback for ris-aided wireless communication systems. IEEE Commun. Lett. ,26, 2631\u20132635. Xing, Y ., Shi, Z., Meng, Z., Lakemeyer, G., Ma, Y ., & Wattenhofer, R. (2021). KM-BART: knowledge enhanced multimodal BART for visual commonsense generation. In C. Zong, F. Xia, W. Li, & R. Navigli (Eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP, (Volume 1: Long Papers), Virtual Event, August 1-6 (pp. 525\u2013535). Association for Computational Linguistics. Xu, Y ., Wei, H., Lin, M., Deng, Y ., Sheng, K., Zhang, M., Tang, F., Dong, W., Huang, F., & Xu, C. (2022). Transformers in computational visual media: A survey. Computational Visual Media ,8, 33\u201362. Xu, Y ., & Zhao, J. (2022). Actor-critic with transformer for cloud computing resource three stage job scheduling. In 7th International Conference on Cloud Computing and Big Data Analytics (ICCCBDA), Chengdu, China, 22-24 April (pp. 33\u201337). Yan, J., Li, J., Xu, H., Yu, Y ., & Xu, T. (2022a). Seizure prediction based on transformer using scalp electroencephalogram. Applied Sciences ,12, 4158. Yan, S., Wang, C., Chen, W., & Lyu, J. (2022b). Swin transformer-based GAN for multi-modal medical image translation. Frontiers in Oncology ,12. Yan, W., Zhang, Y ., Abbeel, P., & Srinivas, A. (2021). Videogpt: Video generation using VQ-V AE and transformers. CoRR , abs/2104.10157 . URL: https://arxiv.org/abs/2104.10157 .arXiv:2104.10157 . Yang, H., & Yang, D. (2023). Cswin-pnet: A cnn-swin transformer combined pyramid network for breast lesion segmentation in ultrasound images. Expert Syst. Appl. ,Volume 213, Part B , 119024. Yang, M., Lee, D., & Park, S. (2022). Automated diagnosis of atrial fibrillation using ECG component-aware transformer. Comput. Biol. Medicine ,150, 106115. Yeh, C., Mahadeokar, J., Kalgaonkar, K., Wang, Y ., Le, D., Jain, M., Schubert, K., Fuegen, C., & Seltzer, M. L. (2019). Transformertransducer: End-to-end speech recognition with self-attention. CoRR ,abs/1910.12977 . URL: http://arxiv.org/abs/1910."
        },
        "PoseMatcher: One-shot 6D Object Pose Estimation by Deep Feature Matching": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2304.01382",
            "ref_texts": "[37] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR , 2021. 3, 4, 5, 6",
            "ref_ids": [
                "37"
            ],
            "1": "The closest work to our own is OnePose++, an improved version of OnePose[38], where the feature extraction is replaced by the detector free feature matcher LoFTR [37].",
            "2": "In order to optimize through the matching task we apply the differentiable dual-softmax operator as proposed by LoFTR [37] and subsequently used by both OnePose and OnePose++ [38, 12].",
            "3": "We flatten both feature maps and apply self and cross attention layers following other feature matching papers [34, 37, 38, 12, 44] to generate more easily separable features on each set.",
            "4": "(3) The coarse loss Lcused to optimize Cuses focal loss [24] as suggested by LoFTR [37].",
            "5": "IO-Layer: An object-image attention layer Image to image feature matching has seen its effectiveness increase with the introduction of self and cross attention mechanisms [44, 34, 37, 26].",
            "6": "We improve on the attention modules used by LoFTR and OnePose++ [37, 38].",
            "7": "Pose Refinement We add a fine level 2D-refinement post-processing step as described by LoFTR [37] and OnePose++[12].",
            "8": "More details about 2D refinement are available in LoFTR [37].",
            "9": "Moreover, in order to supervise the refinement step, LoFTR [37] proposes computing the total variance of the matching heatmap in order to penalize more confident but erroneous estimations.",
            "10": "OnePose and OnePose++ [38, 12] are applied to grayscale images because they make use of pre-trained descriptor models trained solely on grayscale, SuperPoint [6] and LoFTR respectively [37]."
        },
        "Modality-Invariant Representation for Infrared and Visible Image Registration": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2304.05646",
            "ref_texts": "[33] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8922\u20138931, 2021.",
            "ref_ids": [
                "33"
            ],
            "1": "In the first round estimation, we incorporate a cross attention module to aggregate the global context information into the cross feature maps [33]."
        },
        "LFM-3D: Learnable Feature Matching Across Wide Baselines Using 3D Signals": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2303.12779",
            "ref_texts": ""
        },
        "GRelPose: Generalizable End-to-End Relative Camera Pose Regression": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2211.14950",
            "ref_texts": "[44] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR , 2021. 1, 2, 4, 6, 10",
            "ref_ids": [
                "44"
            ],
            "1": "Our architecture includes three modules, a dense feature description module based on a pretrained LoFTR architecture [44], a matching and warping module, and a pose regressor.",
            "2": "The recently proposed works LoFTR [44] and COTR [23] utilize selfand cross-attention layers and multiscale analysis to produce semi-dense, near pixel-wise correspondences.",
            "3": "For this module we use a sub-network of the LoFTR model [44], pretrained on the Scannet dataset [13], depicting indoor scenes.",
            "4": "We use the same train/test split used by LoFTR [44].",
            "5": "For (1) we extract the feature maps from different stages of LoFTR [44].",
            "6": "In particular, (1) we evaluate the role of the features obtained by [44], and (2) we analyze the impact of the warping module.",
            "7": "As demonstrated in Table 5, replacing the LoFTR [44] features with feature maps extracted from the CNN block significantly degrades the accuracy.",
            "8": "Therefore, the feature pyramid network [54] in LoFTR [44] outputs a 256\u000232\u000242 feature map per image."
        },
        "CostFormer: Cost Transformer for Cost Aggregation in Multi-view Stereo": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2305.10320",
            "ref_texts": "[Sunet al. , 2021 ]Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021.",
            "ref_ids": [
                "Sunet al\\. , 2021 "
            ]
        },
        "Graph Self-Supervised Learning for Endoscopic Image Matching": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2306.11141",
            "ref_texts": "[48] J. Sun, Z. Shen, Y. Wang, H. Bao, X. Zhou, LoFTR: Detector-free local feature matching with transformers, CVPR.",
            "ref_ids": [
                "48"
            ],
            "1": "Many computer vision tasks can be described as graph matching problems such as image classification [44, 45], object detection [46, 47], and image matching [18, 48].",
            "2": "Similar but different to SuperGlue, LoFTR (Local Feature TRansformer) [48] used self and cross-attention layers based on a linear transformer to optimize computational complexity.",
            "3": "[48] J."
        },
        "Homography Augmented Momentum Contrastive Learning for SAR Image Retrieval": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2109.10329",
            "ref_texts": "[34] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. arXiv preprint arXiv:2104.00680 , 2021.",
            "ref_ids": [
                "34"
            ],
            "1": "Thus, replacing these matching techniques with scalable DNN-based methods is in an active research area where many approachessuchasSuperGlue[33]andLoFTR[34]havebeenproposed."
        },
        "GMSF: Global Matching Scene Flow": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2305.17432",
            "ref_texts": "[36] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021.",
            "ref_ids": [
                "36"
            ],
            "1": "Each of the blocks includes self-attention followed by cross-attention [31, 36, 39, 47]."
        },
        "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2212.04575",
            "ref_texts": "[30] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021.",
            "ref_ids": [
                "30"
            ],
            "1": "Conversely, learned dense feature descriptors [15, 22, 14, 30, 34] defined over the whole image instead of only local visual information and their corresponding feature matching methods [25, 24, 23, 12, 39] estimate the best correspondences based on these dense features.",
            "2": "Methods such as [25, 30] are supervised by ground truth matches estimated from poses and depth (e."
        },
        "RelMobNet: End-to-end relative camera pose estimation using a robust two-stage training": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2202.12838",
            "ref_texts": "33. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 8922\u20138931 (2021)",
            "ref_ids": [
                "33"
            ],
            "1": "Detector-free local feature matching with transformers (LoFTR) employs a cross attention layer to obtain feature descriptors conditioned on both images to obtain dense matches in low texture areas where traditional methods struggle to produce repeatable points [33]."
        },
        "Danish airs and grounds: A dataset for aerial-to-street-level place recognition and localization": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2202.01821",
            "ref_texts": "[26] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoFTR: Detector-free local feature matching with transformers,\u201d CVPR , 2021.",
            "ref_ids": [
                "26"
            ],
            "1": "LOFTR [26], on the other hand, takes a pair of images as input and via a ViT [27]-based transformer architecture estimates both keypoints and matches simultaneously.",
            "2": "[26] J."
        },
        "TopicFM+: Boosting Accuracy and Efficiency of Topic-Assisted Feature Matching": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2307.00485",
            "ref_texts": "[20] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detectorfree local feature matching with transformers,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , 2021, pp. 8922\u20138931.",
            "ref_ids": [
                "20"
            ],
            "1": "In particular, some studies [20]\u2013[23] have utilized Transformers [24] to learn robust feature representations.",
            "2": "Some approaches [20], [21], [23] have attempted to implicitly capture global contextual information using Transformers.",
            "3": "(iii) Existing detector-free methods [20], [23], [25] also demand substantial computational resources and extensive memory for dense matching.",
            "4": "(iv) Several studies [20], [21], [26] have adopted a coarse-to-fine strategy to handle high-resolution images.",
            "5": "In the fine-level matching stage, we improve upon coarse matches by determining an adaptive feature location rather than relying on a fixed center location [20], [23].",
            "6": "As a result, our coarse-to-fine method produces high-quality matching results while requiring less computation compared to state-of-the-art transformer-based methods [20], [23], [32].",
            "7": "To address this issue, researchers have introduced detectorfree methods [20], [21], [23], [26].",
            "8": "Similarly, several coarse-to-fine methods [20]\u2013[23] utilize transformers to learn robust and distinctive features.",
            "9": "LoFTR [20] employs a linear transformer [46] to enhance the representation ability of visual descriptors by incorporating global context information.",
            "10": "However, these existing methods [20], [21], [23], [32] face inefficiencies when propagating global context information across the entire image region.",
            "11": "On the other hand, existing end-to-end methods rely on CNNs [26] or transformers to extract dense feature maps using either local or global context [20], [21].",
            "12": "In fact, most existing Transformer-based feature-matching methods [20], [23], [25], [53] still rely on a deep CNN backbone to extract the initial features.",
            "13": "Unlike existing methods [20], [21], [23] that use a fixed center location for each patch, our approach identifies more reliable keypoint locations by estimating a score map for each patch.",
            "14": "Previous methods [20], [43] have directly employed attention-based networks on all feature tokens to estimate the distribution ofM, followed by the use of Dual-Softmax [20] or optimal transport layers [43], [57].",
            "15": "Although the featureaugmentation algorithm above is computationally efficient compared to Transformer-based methods [20], [21], [23], [25], it encounters a bottleneck when most of the features are assigned to a single topic.",
            "16": "The dual-softmax operation [20] was applied to compute the matching probabilities as follows: P(mij|z(s) ij=k, Fc) =DualSoft\u0010\n\u27e8\u02c6FA c,i,\u02c6FB c,j\u27e9\u0011\n(15) Using Eq.",
            "17": "Existing methods [20], [23], [32] usually fix the first keypoint (i.",
            "18": "W E PROVIDE TWO EXPERIMENTS FOLLOWING THE SETUPS IN LOFTR [20] (LEFT ) AND PATCH 2PIX[26] (RIGHT ).",
            "19": "0K LoFTR\u22c6[20] 65.",
            "20": "In contrast to recent transformer-based models such as LoFTR [20], [21], [23], which require approximately 24GB of memory per GPU and a day of training using 16 GPUs, our training approach is more efficient and well-suited for limited 9 Fig.",
            "21": "Qualitative results of our method (TopicFM-fast) compared to LoFTR [20] and QuadTree [32] on the ScanNet dataset.",
            "22": "Previous image-matching methods have also conducted experiments on the HPatches dataset, although with different setups [20], [26].",
            "23": "We performed two versions of the evaluation, one based on LoFTR [20] and the other on Patch2Pix [26].",
            "24": "1 LoFTR\u22c6[20], [61] 52.",
            "25": "As shown in Table I (left), both models outperformed the transformer-based method LoFTR\n[20] and the recent dense matching method PDC-Net+ [59] by a substantial margin.",
            "26": "Following the approach in [20], [43], we measured the Area Under the Curve (AUC) of pose estimation error at thresholds of{5o,10o,20o}.",
            "27": "The proposed method, TopicFM+, outperformed the robust detector-based method SP\n[16] + SuperGlue [43], as well as recent Transformer-baseddetector-free methods, including LoFTR [20], MatchFormer [21], QuadTree [32], and ASTR [53], as shown in Tables II and III.",
            "28": "In line with previous work [20], [26], we utilized the visual localization pipeline HLoc [7] for evaluation.",
            "29": "2 LoFTR [20] 88.",
            "30": "7 LoFTR [20] 47.",
            "31": "728 LoFTR [20] 0.",
            "32": "Efficiency Evaluation We compared our method with recent Transformer-based methods, including LoFTR [20], QuadTree [32], and AspanFormer [23].",
            "33": "For the fine-level matching stage, ModelA adopted the fixed center-keypoint approach, similar to LoFTR [20].",
            "34": "[20] J."
        },
        "Context-TAP: Tracking Any Point Demands Spatial Context Features": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2306.02000",
            "ref_texts": "[26] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8922\u20138931, 2021.",
            "ref_ids": [
                "26"
            ],
            "1": "Since FlowNet [6,15], learning optical flow with neural networks presents superior performance over traditional optimization-based methods and is fast progressing with more training data obtained by the renderer and better network architecture [6,15,19, 25,26,13,14,32]."
        },
        "FindView: Precise Target View Localization Task for Look Around Agents": {
            "authors": [
                "Haruya Ishikawa",
                "Yoshimitsu Aoki"
            ],
            "url": "https://arxiv.org/pdf/2303.09054",
            "ref_texts": "(2):91\u2013110, 2004. 2, 7, 14, 15 Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary Bradski. Orb: An efficient alternative to sift or surf. In 2011 International conference on computer vision , pages 2564\u20132571. Ieee, 2011. 2, 7, 14 Torsten Sattler, Will Maddern, Carl Toft, Akihiko Torii, Lars Hammarstrand, Erik Stenborg, Daniel Safari, Masatoshi Okutomi, Marc Pollefeys, Josef Sivic, et al. Benchmarking 6dof outdoor visual localization in changing conditions. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 8601\u20138610, 2018. 2 Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue: Learning feature matching with graph neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 4938\u20134947, 2020. 2 Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8922\u20138931, 2021. 2 Emilio Parisotto, Devendra Singh Chaplot, Jian Zhang, and Ruslan Salakhutdinov. Global pose estimation with an attention-based recurrent network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops , pages 237\u2013246, 2018. 2 Dinesh Jayaraman and Kristen Grauman. Learning to look around: Intelligently exploring unseen environments for unknown tasks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 1238\u20131247, 2018. 3 Santhosh K Ramakrishnan and Kristen Grauman. Sidekick policy learning for active visual exploration. In Proceedings of the European Conference on Computer Vision (ECCV) , pages 413\u2013430, 2018. 3 Santhosh K Ramakrishnan, Dinesh Jayaraman, and Kristen Grauman. Emergence of exploratory look-around behaviors through active observation completion. Science Robotics , 4(30), 2019. 3 Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, et al. On evaluation of embodied navigation agents. arXiv preprint arXiv:1807.06757 , 2018. 4, 9 Jianxiong Xiao, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Recognizing scene viewpoint using panoramic place representation. In 2012 IEEE Conference on Computer Vision and Pattern Recognition , pages 2695\u20132702. IEEE, 2012. 4, 16 Shih-Han Chou, Cheng Sun, Wen-Yen Chang, Wan-Ting Hsu, Min Sun, and Jianlong Fu. 360-indoor: towards learning real-world objects in 360deg indoor equirectangular images. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision , pages 845\u2013853, 2020. 4, 16 Claudio Michaelis, Benjamin Mitzkus, Robert Geirhos, Evgenia Rusak, Oliver Bringmann, Alexander S Ecker, Matthias Bethge, and Wieland Brendel. Benchmarking robustness in object detection: Autonomous driving when winter is coming. arXiv preprint arXiv:1907.07484 , 2019. 5, 9, 12 Richard S. Sutton and Andrew G. Barto. Introduction to Reinforcement Learning . MIT Press, Cambridge, MA, USA, 1st edition, 1998. ISBN 0262193981. 5, 13 Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Proceedings of the 12th International Conference on Neural Information Processing Systems , NIPS\u201999, pages 1057\u20131063, Cambridge, MA, USA, 1999. MIT Press. URL http://dl.acm. org/citation.cfm?id=3009657.3009806 . 6, 13 Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E Taylor, and Peter Stone. Curriculum learning for reinforcement learning domains: A framework and survey. arXiv preprint arXiv:2003.04960 , 2020. 6 John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR , abs/1707.06347, 2017. URL http://arxiv.org/abs/1707.06347 . 6, 13 Christoph Kamann and Carsten Rother. Benchmarking the robustness of semantic segmentation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8828\u20138838, 2020. 9"
        },
        "Fusing Visual Appearance and Geometry for Multi-modality 6DoF Object Tracking": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2302.11458",
            "ref_texts": "[41] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoFTR: Detectorfree local feature matching with transformers,\u201d in IEEE/CVF Conf. Comput. Vis. Pattern Recog. , 2021, pp. 8922\u20138931.",
            "ref_ids": [
                "41"
            ],
            "1": "Prominent examples are SuperPoint [39], D2-Net [40], or LoFTR [41].",
            "2": "[41] J."
        },
        "Learning Feature Matching via Matchable Keypoint-Assisted Graph Neural Network": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2307.01447",
            "ref_texts": "[39] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detectorfree local feature matching with transformers,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2021, pp. 8922\u20138931.",
            "ref_ids": [
                "39"
            ],
            "1": "In spite of its remarkable performance on a plethora of visual tasks [36]\u2013[39], the major drawback of Transformer is the quadratic complexity of the attention mechanism w.",
            "2": "[39] J."
        },
        "3D reconstruction of spherical images: A review of techniques, applications, and prospects": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2302.04495",
            "ref_texts": "[159] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detectorfree local feature matching with transformers,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , 2021, pp. 8922\u20138931.",
            "ref_ids": [
                "159"
            ],
            "1": "[159] J."
        },
        "SuperGF: Unifying Local and Global Features for Visual Localization": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2212.13105",
            "ref_texts": "[40] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 3",
            "ref_ids": [
                "40"
            ],
            "1": ", image retrieval [26, 47, 49] and image matching [34, 40]."
        },
        "Robust-DefReg: A Robust Deformable Point Cloud Registration Method based on Graph Convolutional Neural Networks": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2306.04701",
            "ref_texts": "21. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 8922\u20138931 (2021)",
            "ref_ids": [
                "21"
            ],
            "1": "Studies such as DRC-Net [15], Patch2Pix [31], and LoFTR [21] utilize a method that gradually refines a coarse-to-fine mechanism to address the inherent issue of repeatability in keypoint detection."
        },
        "JigsawPlan: Room Layout Jigsaw Puzzle Extreme Structure from Motion using Diffusion Models": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2211.13785",
            "ref_texts": "13 Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. 2021. LoFTR: DetectorFree Local Feature Matching with Transformers. CVPR (2021). Corey Toler-Franklin, Benedict Brown, Tim Weyrich, Thomas Funkhouser, and Szymon Rusinkiewicz."
        },
        "OAMatcher: An Overlapping Areas-based Network for Accurate Local Feature Matching": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2302.05846",
            "ref_texts": "[31] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detectorfree local feature matching with transformers,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2021, pp. 8922\u20138931.",
            "ref_ids": [
                "31"
            ],
            "1": "Concurrent with the detector-based matching methods, the detector-free approaches [25]\u2013[30], [30]\u2013[35] focus on extracting visual descriptors on dense grids across images directly, therefore, the repeatable keypoints in image pairs can be captured [31].",
            "2": "Recently, Transformer [36] has been successfully applied for establishing precise matches due to its powerful capability of modelling long-range global context information [14], [31].",
            "3": "As a pioneering work, LoFTR [31] views keypoints as nodes to construct a graphy neural network (GNN), in which the Linear Transformer [37] are leveraged to aggregate global message intra/inter images.",
            "4": "Nevertheless, being detector-based methods, they have the fundamental drawback of being unable to detect repeatable keypoints when handling image pairs with extreme appearance changes [31].",
            "5": "Upon witnessing the great success of SuperGlue [14], LoFTR [31] innovatively design a Transformer-based detectorfree architecture, achieving outstanding matching performance.",
            "6": "For LAL, we follow LoFTR [31] and utilize self/cross linear attention [37] to perform long-range global context aggregation intra/inter images.",
            "7": "The Transformer Encoder Layer of other works [14], [15], [17], [31] only focuses on propagating global context information, while ignoring extracting local feature representations.",
            "8": "76 LoFTR [31] 0.",
            "9": "23 Detector-free Methods \u2014\u2014DRCNet [31] 7.",
            "10": "49 LoFTR [31] 22.",
            "11": "Following [14], [31], we report the area under the cumulative curve (AUC) of the pose errors at thresholds (5fl;10fl;20fl).",
            "12": "33 Detector-free Methods \u2014\u2014DRCNet [31] 27.",
            "13": "31 LoFTR [31] 52.",
            "14": "Following [13], [31], we select 100 image pairs each scene for training and 1500 image pairs for testing.",
            "15": "Following [31], we use the same evaluation metrics AUC@(5fl, 10fl, 20fl) as the indoor pose estimation task.",
            "16": "Methods Params GFLOPs Runtime LoFTR [31] 11.",
            "17": "[31] J."
        },
        "Improving Transformer-based Image Matching by Cascaded Capturing Spatially Informative Keypoints": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2303.02885",
            "ref_texts": "[43] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 1, 2, 3, 4, 5, 6, 7, 12, 13, 14",
            "ref_ids": [
                "43"
            ],
            "1": "But such detector-based CNNs suffer from limited receptive fields and search space, as noticed in [43].",
            "2": "To solve this issue, transformer-based detector-free methods have emerged as more robust alternatives, demonstrating impressive matching abilities in texture-less regions [43,18,47,57,4].",
            "3": "Coarse MatchingFeature Extraction + Pre-training transformer backbone + Parameter and Memory -efficient Tuning Existing steps New contributions Cascade MatchingRegressive RefinementNMS detection + Replace linear attention to vanilla attention+ Efficient self/cross Attention for high -resolution+ Complementary post-process Figure 2: Illustration of CasMTR pipeline; and our novelties compared against the existing steps from detector-free matching methods [43, 47, 4] are highlighted in red.",
            "4": "To address these challenges, we improve the existing transformer-based matching pipeline [43,4] by efficiently capturing spatially informative keypoints in a cascaded manner.",
            "5": "It makes a significant contribution by enabling pure transformer-based models to conduct dense matching by cascaded capturing spatially informative keypoints without relying on the expensive learning of huge 4D correlations as merely extended from [43].",
            "6": "However, these methods still suffer from limited interest points in indistinctive regions [43].",
            "7": "Detector-free methods enjoy an end-to-end pipeline to achieve the matching directly without an explicit keypoint detection phase [24,5,43].",
            "8": "Learningbased detector-free methods can be generally categorized into Convolutional Neural Network (CNN) based methods [35,34,20,52,54,12] and transformer or attentionbased ones [43,18,47,57,41,4,46].",
            "9": "Some transformer-based manners [43,47,57,4], led by LoFTR [43], largely enlarge the receptive fields with interlacing self/cross attention modules, and enjoy better performance in texture-less regions.",
            "10": "We briefly review the transformer-based matching baseline in the example of LoFTR [43].",
            "11": "We first follow [43] and use FPN to produce coarse-to-fine features Fs A,Fs Bfor the image pair IA,IB, where s\u2208 {1\n2,1\n4,1\n8}indicate the image scale.",
            "12": "For each stage, we first add sinusoidal position encoding as other methods [43,47,4].",
            "13": "Following [43], the Focal binary cross-entropy Loss (FL) [23] is used to optimize the cascade matching as L\u02dcs FL=\u2212EM\u02dcs[(1\u2212P\u02dcs)\u03b3log(P\u02dcs)], (2) where \u03b3= 2;M\u02dcsindicates matching queries which satisfy the cycle-consistent and have one ground truth target in k candidates.",
            "14": "Local Regressive Refinement The patch-wise refinement module in LoFTR [43] is also incorporated in our work for sub-pixel matching.",
            "15": "Confidence based Detection with NMS Different from detector-based methods [11,9,38], latest attention based methods [43,47,4,57] achieve good results even without detector.",
            "16": "Ground-truth matching pairs are from COLMAP [40] computed depth maps, Following [43], for one epoch, we randomly sample 200 pairs from each scene for the training, and 1500 pairs from independent two scenes are selected as the test set.",
            "17": "1 LoFTR [43] 52.",
            "18": "Relative Pose Estimation As in [38,43], we evaluate the relative pose estimation with AUC of pose errors at thresholds (5\u25e6,10\u25e6,20\u25e6), while the pose error is defined as the maximum angular error of rotation and translation.",
            "19": "CasMTR can outperform all competitors especially in AUC 5\u25e6and AUC 10\u25e6, which include both transformerbased [43,47,57,4] and CNN-based [12] manners.",
            "20": "Following [38,43], we report the AUC of corner error up to thresholds 3, 5, and 10 pixels in Tab.",
            "21": "To ensure fairness, we MegaDepthScanNet(a)LoFTR(b)QuadTree(c)CasMTR(dense)(d)CasMTR(NMS) Figure 6: Qualitative outdoor and indoor matching results compared with LoFTR [43], QuadTree [47], CasMTR-4c (ScanNet), CasMTR-2c (MegaDepth), and our NMS detected results.",
            "22": "1 LoFTR [43] 22.",
            "23": "6k LoFTR [43] 64.",
            "24": "Since no official codes are provided from [43], we re-implement the visual localization and report results in Tab.",
            "25": "* means our implementation of LoFTR; note that our re-implementations are better on DUC1 and worse on DUC2 compared with [43].",
            "26": "5m,5\u25e6)/(1m,10\u25e6) HLoc [37]+LoFTR [43]* 49.",
            "27": "5m,5\u25e6) / (1m,10\u25e6) HLoc [37]+LoFTR [43] 88.",
            "28": "3, the straightforward way to achieve dense matching with detector-free methods [43,47] is to make all patchwise features in the refinement module produce matching results as much as possible.",
            "29": "It is also used as the standard attention in LoFTR [43].",
            "30": "* means our implementation of LoFTR; note that results of our implementation are better on DUC1 and worse on DUC2 than those reported in [43].",
            "31": "5m,5\u25e6) / (1m,10\u25e6) HLoc [37]+LoFTR [43]* 49.",
            "32": "10, CasMTR-2c outperforms LoFTR [43] and QuadTree [47], while NMS can further improve the results."
        },
        "Object-Guided Day-Night Visual localization in Urban Scenes": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2202.04445",
            "ref_texts": "[46] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8922\u20138931, 2021. 2, 4, 6",
            "ref_ids": [
                "46"
            ],
            "1": "Recently, Patch2Pix [60] learns to regress a single pixel correspondence, while Loftr [46] learns a dense matching of the pixels between corresponding receptive fields.",
            "2": "1 Experimental Setup OGuL is evaluated against other feature matching methods: the default Nearest-Neighbor (NN) approach, the coarse-to-fine Patch2Pix [60] and LOFTR [46], the graph-based approach SuperGlue [39], and the filtering method AdaLAM [7].",
            "3": "0 \u0000 LOFTRy[46] \u0000 72."
        },
        "Tame a Wild Camera: In-the-Wild Monocular Camera Calibration": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2306.10988",
            "ref_texts": "[50] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR , 2021. 9",
            "ref_ids": [
                "50"
            ],
            "1": "3 LoFTR [50] CVPR\u201921 \u2714 22."
        },
        "Learnable Graph Matching: A Practical Paradigm for Data Association": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2303.15414",
            "ref_texts": "[45] J. Sun, Z. Shen, Y. Wang, H. Bao, and X. Zhou, \u201cLoFTR: Detectorfree local feature matching with transformers,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2021, pp. 8922\u20138931.",
            "ref_ids": [
                "45"
            ],
            "1": "LoFTR [45] is a kind of dense matching algorithm, from patch matching to pixel matching in coarse-to-fine style.",
            "2": "[45] J."
        },
        "A Lightweight Domain Adaptive Absolute Pose Regressor Using Barlow Twins Objective": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2211.10963",
            "ref_texts": "[59] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021. 16",
            "ref_ids": [
                "59"
            ],
            "1": "Transformer-based methods like LoFTR [59] and TransforMatcher [27] carry out feature matching as well as semantic correspondence matching."
        },
        "NeRF-Loc: Visual Localization with Conditional Neural Radiance Field": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2304.07979",
            "ref_texts": "[29] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. InProceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021.",
            "ref_ids": [
                "29"
            ],
            "1": "To handle texture-less scene, detector-free 2D matching methods [29][11][6] has been proposed, which shows promising results.",
            "2": "Inspired by LoFTR[29], a coarse-to-fine matching scheme is adopted to directly estimate 3D-2D correspondences.",
            "3": "For more detail, please refer to [29]."
        },
        "A Hybrid Deep Feature-Based Deformable Image Registration Method for Pathological Images": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2208.07655",
            "ref_texts": "[19] Sun, J., Shen, Z., Wang, Y., Bao, H. & Zhou, X. LoFTR: Detector-free local feature matching with transformers. Proceedings Of The IEEE/CVF Conference On Computer Vision And Pattern Recognition . pp. 8922-8931",
            "ref_ids": [
                "19"
            ],
            "1": "There are two types of deep learning techniques: detector-based and detector-free approaches [19].",
            "2": "Detector-free techniques, such as correspondence transformer-based image matching network [22], and LOFTR\n[19], employ the designed network for end-to-end matching without requiring a dedicated detector to identify interest sites.",
            "3": "2564-2571 (2011)\n[19] Sun, J."
        },
        "COLMAP-SLAM: A FRAMEWORK FOR VISUAL ODOMETRY": {
            "authors": [],
            "url": "https://isprs-archives.copernicus.org/articles/XLVIII-1-W1-2023/317/2023/isprs-archives-XLVIII-1-W1-2023-317-2023.pdf",
            "ref_texts": "[tutorial]. IEEE robotics & automation magazine , 18(4), pp.80 92. Schonberger, J.L. and Frahm, J.M., 2016. Structure -from -motion revisited. Proc. CVPR , pp. 4104 -4113. Singandhupe, A. and La, H.M., 2019, February. A review of slam techniques and security in autonomous driving. Proc. IEEE IRC, pp. 602 -607. Sumikura, S., Shibuya, M. and Sakurada, K., 2019, October. OpenVSLAM: A versatile visual SLAM framework. Proc. 27th ACM International Conference on Multimedia , pp. 2292 -2295. Sun, J., Shen, Z., Wang, Y., Bao, H. and Zhou, X., 2021. LoFTR: Detector -free local feature matching with transformers. Proc. CVPR, pp. 8922 -8931. Verdie, Y., Yi, K., Fua, P. and Lepetit, V., 2015. Tilde: A temporally invariant learned detector. Proc. CVPR , pp. 5279 5288. Younes, G., Asmar, D., Shammas, E. and Zelek, J., 2017. Keyframe -based monocular SLAM: design, survey, and future directions. Robotics and Autonomous Systems , 98, pp.67 -88. Zhao, X., Wu, X., Miao, J., Chen, W., Chen, P.C. and Li, Z., ",
            "ref_ids": [
                "tutorial"
            ],
            "1": "Visual odometry \n[tutorial]."
        },
        "A Real-Time Fusion Framework for Long-term Visual Localization": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2210.09757",
            "ref_texts": "[14] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detectorfree local feature matching with transformers,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 8922\u20138931, 2021.",
            "ref_ids": [
                "14"
            ],
            "1": "[14] depends on Transformer networks to exceed the limits of local features in order to improve the accuracy of 2D-3D correspondences and localization performance.",
            "2": "[14] J."
        },
        "DRKF: Distilled Rotated Kernel Fusion for Efficiently Boosting Rotation Invariance in Image Matching": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2209.10907",
            "ref_texts": "[22] P.-E. Sarlin, D. DeTone, T. Malisiewicz, and A. Rabinovich, \u201cSuperglue: Learning feature matching with graph neural networks,\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , pp. 4938\u20134947, 2020.[23] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoftr: Detectorfree local feature matching with transformers,\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , pp. 8922\u20138931, 2021.",
            "ref_ids": [
                "22",
                "23"
            ],
            "1": "The same problem still exists for recent local feature matching techniques like SuperGlue [22] and LoFTR [23].",
            "2": "3103 SuperPoint [9]+SuperGlue [22] \" 0.",
            "3": "0809 LoFTER [23] % 0.",
            "4": "SuperGlue [22] and LoFTR [23].",
            "5": "For SIFT, we use itsTABLE III INFERENCE TIME ON NVIDIA TX2 Methods Inference Time (ms) SuperPoint [9] + SuperGlue [22] 110 RoRD [17] 229 ASLFeat [12] 56 MOFA 207 Base 58 DRKF 213 DRKF (*Rep) 57\n*Rep refers to reparameterization.",
            "6": "In this part, we also involve state-of-the-art feature matching methods like SuperGlue [22] and LoFTR [23].",
            "7": "We compare our DRKF with SIFT and learning-based methods, including SuperPoint [9] + SuperGlue [22], D2-Net [10], ASLFeat [12], LoFTR [23], GIFT [18], and RoRD [17] in Fig.",
            "8": "[22] P.",
            "9": "[23] J."
        },
        "Pentagon-Match (PMatch): Identification of View-Invariant Planar Feature for Local Feature Matching-Based Homography Estimation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2305.17463",
            "ref_texts": ""
        },
        "Reuse your features: unifying retrieval and feature-metric alignment": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2204.06292",
            "ref_texts": "[28] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoFTR: Detector-free local feature matching with transformers,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2021, pp. 8922\u20138931.",
            "ref_ids": [
                "28"
            ],
            "1": "Differently, image matching extracts local features [24\u201326] and performs data association by descriptor distance or learned matchers [27, 28].",
            "2": "[28] J."
        },
        "Searching from Area to Point: A Hierarchical Framework for Semantic-Geometric Combined Feature Matching": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2305.00194",
            "ref_texts": "[39] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR , 2021.",
            "ref_ids": [
                "39"
            ],
            "1": "As the technology evolves, learning-based approaches such as Convolution Neural Network (CNN) and Transformer [44] are used in feature matching either to replace some separate phases in the detectorbased matching framework [10], [28], or to establish point matches directly from the image pair [33], [39], i.",
            "2": "At the same time, in contrast to CNN with restricted receptive field [32], [33], [55], Transformer is more suitable for this framework and achieves state-of-the-art performance [7], [15]\u2013[17], [21], [39], [41], [50], as attention layers endow perception of global context.",
            "3": "Effective coarse-to-fine module is adopted [39] to reduce this redundant search space.",
            "4": "In order to avoid detection failure, detector-free framework is proposed [16], [33], [39], which aims at jointly trainable feature detection, matching and outlier rejection to establish point matches directly from image pairs.",
            "5": "Owing to limited receptive field [39] of CNN, recent detector-free methods [16], [39] adopt Transformer [44] to process dense feature extracted by CNN, attaining state-ofthe-art performance.",
            "6": "In LoFTR [39], patch-level correspondences are first established through feature attention in the coarse level, which is set as the initial search space for the fine level, significantly reducing the time consumption compared to the vanilla Transformer [16].",
            "7": "The Transformer-based matcher [16], [39] is embedded in GR to acquire precise point matches.",
            "8": "For A2PM framework, we combine SGAM with present Transformer-based detector-free methods: SGAM ASpan [7], SGAM QT [41], SGAM LoFTR [39] and SGAM COTR\n[16]).",
            "9": "76% LoFTR [39] 30.",
            "10": "For detector-free matching, we choose the original ASpan [7], QT [41], LoFTR [39] and COTR [16].",
            "11": "a) Evaluation protocol: Following [35], [39], we report the pose estimation AUC.",
            "12": "65% LoFTR [39] 67."
        },
        "HWR200: New open access dataset of handwritten texts images in Russian": {
            "authors": [
                "Potyashin I"
            ],
            "url": "https://www.dialog-21.ru/media/5925/potyashiniplusetal048.pdf",
            "ref_texts": "524. Denis Coquenet, Cl\u00e9ment Chatelain, and Thierry Paquet. 2023. Dan: a segmentation-free document attention network for handwritten document recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence. Sharon Fogel, Hadar Averbuch-Elor, Sarel Cohen, Shai Mazor, and Roee Litman. 2020. Scrabblegan: Semisupervised varying length handwritten text generation. // The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June. Basilis Gatos, Georgios Louloudis, Tim Causer, Kris Grint, Ver\u00f3nica Romero, Joan Andreu S\u00e1nchez, Alejandro H. Toselli, and Enrique Vidal. 2014. Ground-truth production in the transcriptorium project. // 2014 11th IAPR International Workshop on Document Analysis Systems, P 237\u2013241. Idp-forms (2021). Available at: https://github.com/ai-forever/htr_datasets/tree/main/ IDP-forms. Jeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. 2017. Billion-scale similarity search with gpus. IEEE Transactions on Big Data, 7:535\u2013547. U.-V. Marti. 2002. The iam-database: an english sentence database for offline handwriting recognition. International Journal on Document Analysis and Recognition, 28(1):114\u2013133. Minesh Mathew, Dimosthenis Karatzas, R. Manmatha, and C. Jawahar. 2020. Docvqa: A dataset for vqa on document images. 07. Daniyar Nurseitov, Kairat Bostanbekov, Daniyar Kurmankhojayev, Anel Alimova, Abdelrahman Abdallah, and Rassul Tolegenov. 2021. Handwritten kazakh and russian (hkr) database for text recognition. Multimedia Tools and Applications, P 1\u201323. M. B. Potanin, Denis Dimitrov, A. Shonenkov, Vladimir Bataev, Denis Karachev, and Maxim Novopoltsev. 2021. Digital peter: New dataset, competition and handwriting recognition methods. The 6th International Workshop on Historical Document Imaging and Processing. School_notebooks_ru (2021). Available at: https://github.com/ai-forever/htr_datasets/tree/main/ school_notebooks. Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. 2021. LoFTR: Detector-free local feature matching with transformers. CVPR. Curtis Wigington, Chris Tensmeyer, Brian Davis, William Barrett, Brian Price, and Scott Cohen. 2018. Start, follow, read: End-to-end full-page handwriting recognition. // Computer Vision \u2013 ECCV 2018\", P 372\u2013388, Cham. Springer International Publishing. Stuart Wrigley. 2019. Avoiding \u2018de-plagiarism\u2019: Exploring the affordances of handwriting in the essay-writing process. Active Learning in Higher Education, 20(2):167\u2013179.Potyashin I., Kaprielova M., Chekhovich Y., Kildyakov A., Seil T., Finogeev E., Grabovoy A.",
            "ref_ids": [
                "524"
            ]
        },
        "Lightweight Monocular Depth Estimation with an Edge Guided Network": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2209.14829",
            "ref_texts": "[25] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoFTR: Detectorfree local feature matching with transformers,\u201d in CVPR , 2021, pp.",
            "ref_ids": [
                "25"
            ],
            "1": "Inspired by [25], we adopt the linear transformer encoder layer to capture the long-range dependencies (or global context) between the edge and context features through crossattention in two directions.",
            "2": "[25] J."
        },
        "Closed-loop feedback registration for consecutive images of moving flexible targets": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2110.10772"
        },
        "The Case for an LMS Camera": {
            "authors": [
                "Tripurari Singh",
                "Mritunjay Singh",
                "Image Algorithmics",
                "United States"
            ],
            "url": "https://library.imaging.org/admin/apis/public/api/ist/website/downloadArticle/ei/35/15/COLOR-201"
        },
        "Online visual tracking with one-shot context-aware domain adaptation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2008.09891",
            "ref_texts": "[31] J. Sun, Z. Shen, Y. Wang, H. Bao, X. Zhou, LoFTR: Detector-free local feature matching with transformers, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2021.",
            "ref_ids": [
                "31"
            ],
            "1": "Transformer architectures have increasingly expanded to deal with non-sequential problems after replacing recurrent neural networks in several sequential tasks such as natural language processing [27], voice processing [28], and computer vision [29, 30, 31, 32].",
            "2": "[31] J."
        },
        "Nonlinear Intensity Sonar Image Matching based on Deep Convolution Features": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2111.15514",
            "ref_texts": "[20] J. Sun, Z. Shen, Y. Wang, H. Bao, and X. Zhou, \"LoF TR: Detector-Free Local Feature Matching with Transformers,\" 2021. ",
            "ref_ids": [
                "20"
            ],
            "1": "Comparative approaches In the subsequent experiments, we introduced image matching approaches SIFT, ORB, BRISK [18], SuperPoi nt [19] and LoFTR [20] for comparison.",
            "2": "[20] J."
        },
        "CS 6384 Computer Vision Project Proposal Description": {
            "authors": [],
            "url": "https://yuxng.github.io/Courses/CS6384Spring2022/CS_6384_Project_Proposal.pdf",
            "ref_texts": "[48] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021.",
            "ref_ids": [
                "48"
            ],
            "1": "For this topic, students can explore methods for detecting keypoint features [9], edges [57,35], lines [59] or contours [43] as well matching these features [41, 48]."
        },
        "Real-time Local Feature with Global Visual Information Enhancement": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2211.10981",
            "ref_texts": "[24] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou, \u201cLoFTR: Detector-free local feature matching with transformers,\u201d Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2021.",
            "ref_ids": [
                "24"
            ],
            "1": "Thus, we adopt the 2-dimensional extension of absolute sinusoidal positional embedding [24] before Non-local block [23]: PEi x;y:=8\n>><\n>>:sin(!k\u0001x); i= 4k cos(!k\u0001x); i= 4k+ 1 sin(!k\u0001y); i= 4k+ 2 cos(!k\u0001y); i= 4k+ 3(3) wherePEi x;yis the positional embedding for ithchannel of the feature on (x;y).",
            "2": "[24] J."
        },
        "Global-Local Bayesian Transformer for Semantic Correspondence": {
            "authors": [],
            "url": "https://openreview.net/pdf?id=aGkxJtOxKx",
            "ref_texts": "11 Under review as a conference paper at ICLR 2023 I. Rocco, R. Arandjelovi \u00b4c, and J. Sivic. Convolutional neural network architecture for geometric matching. In CVPR , 2017. I. Rocco, R. Arandjelovi \u00b4c, and J. Sivic. End-to-end weakly-supervised semantic alignment. In ECCV , 2018a. I. Rocco, M. Cimpoi, R. Arandjelovi \u00b4c, A. Torii, T. Pajdla, and J. Sivic. Neighbourhood consensus networks. In NIPS , 2018b. I. Rocco, R. Arandjelovi \u00b4c, and J. Sivic. Efficient neighbourhood consensus networks via submanifold sparse convolutions. In ECCV , 2020. Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary Bradski. Orb: An efficient alternative to sift or surf. In ICCV , 2011. Paul Hongsuck Seo, Jongmin Lee, Deunsol Jung, Bohyung Han, and Minsu Cho. Attentive semantic alignment with offset-aware correlation kernels. In ECCV , 2018. Parikshit Shah, Nikhil Rao, and Gongguo Tang. Sparse and low-rank tensor decomposition. In NIPS , 2015. Kumar Shridhar, Felix Laumann, and Marcus Liwicki. A comprehensive guide to bayesian convolutional neural network with variational inference. arXiv , 2019. Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In CVPR , 2021. Tatsunori Taniai, Sudipta N. Sinha, and Yoichi Sato. Joint recovery of dense correspondence and cosegmentation in two images. In CVPR , 2016. Engin Tola, Vincent Lepetit, and Pascal Fua. Daisy: An efficient dense descriptor applied to wide-baseline stereo. TPAMI , 2010. Prune Truong, Martin Danelljan, Luc V Gool, and Radu Timofte. Gocor: Bringing globally optimized correspondence volumes into your neural network. In NIPS , 2020a. Prune Truong, Martin Danelljan, and Radu Timofte. Glu-net: Global-local universal network for dense flow and correspondences. In CVPR , 2020b. Prune Truong, Martin Danelljan, Fisher Yu, and Luc Van Gool. Probabilistic warp consistency for weakly-supervised semantic correspondences. In CVPR , 2022. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS , 2017. Guo-Sen Xie, Huan Xiong, Jie Liu, Yazhou Yao, and Ling Shao. Few-shot semantic segmentation with cyclic memory network. In ICCV , 2021. Gengshan Yang and Deva Ramanan. V olumetric correspondence networks for optical flow. In NIPS , 2019. Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu. Incorporating convolution designs into visual transformers. In ICCV , 2021. Shujian Zhang, Xinjie Fan, Bo Chen, and Mingyuan Zhou. Bayesian attention belief networks. In ICML , 2021. Dongyang Zhao, Ziyang Song, Zhenghao Ji, Gangming Zhao, Weifeng Ge, and Yizhou Yu. Multiscale matching networks for semantic correspondence. In ICCV , 2021."
        },
        "Skin feature point tracking using deep feature encodings": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2112.14159",
            "ref_texts": "1180, 2015. Joseph Redmon and Ali Farhadi. Yolo9000: better, faster, stronger. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 7263{7271, 2017. Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv preprint , 2018. Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Uniffed, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 779{788, 2016. Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems , pages 91{99, 2015. Rasmus Rothe, Radu Timofte, and Luc Van Gool. Deep expectation of real and apparent age from a single image without facial landmarks. Int. J. of Comp. Vis. , 126(2-4):144{157, 2018. Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue: Learning feature matching with graph neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 4938{4947, 2020. Gulbadan Sikander and Shahzad Anwar. Driver fatigue detection systems: A review. IEEE Transactions on Intelligent Transportation Systems , 20(6):2339{2352, 2018. Edgar Simo-Serra, Eduard Trulls, Luis Ferraz, Iasonas Kokkinos, Pascal Fua, and Francesc Moreno-Noguer. Discriminative learning of deep convolutional feature point descriptors. In Proceedings of the IEEE international conference on computer vision , pages 118{126, 2015. Marzuraikah Mohd Stofa, Mohd Asyraf Zulkiey, and Muhammad Ammirrul Atiqi Mohd Zainuri. Skin lesions classiffcation and segmentation: A review. International Journal of Advanced Computer Science and Applications , 12(10), 2021. Peifeng Su, Daizhi Liu, Xihai Li, and Zhigang Liu. A saliencybased band selection approach for hyperspectral imagery inspired by scale selection. IEEE Geoscience and Remote Sensing Letters , 15(4):572{576, 2018. Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922{"
        },
        "Efficient and Accurate Co-Visible Region Localization with Matching Key-Points Crop (MKPC): A Two-Stage Pipeline for Enhancing Image Matching Performance": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2303.13794",
            "ref_texts": "[5]Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou, \u201cLoftr: Detector-free local feature matching with transformers,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , 2021, pp. 8922\u20138931.",
            "ref_ids": [
                "5"
            ]
        },
        "Beyond the CLS Token: Image Reranking using Pretrained Vision Transformers": {
            "authors": [],
            "url": "https://bmvc2022.mpi-inf.mpg.de/0080.pdf",
            "ref_texts": "[27] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021.",
            "ref_ids": [
                "27"
            ],
            "1": "On the one hand, recent works [24, 27] exploring feature correspondence have adopted a hybrid model with modular design.",
            "2": "LoFTR [27] proposes a twostage method using coarse and fine level features with optimal transport."
        },
        "Keypoint Matching via Random Network Consensus": {
            "authors": [],
            "url": "https://openreview.net/pdf?id=WhbWzFg8cZ",
            "ref_texts": "12 Under review as a conference paper at ICLR 2023 Torsten Sattler, Bastian Leibe, and Leif Kobbelt. Efficient & effective prioritized matching for largescale image-based localization. IEEE transactions on pattern analysis and machine intelligence , 39(9):1744\u20131756, 2016. Torsten Sattler, Will Maddern, Carl Toft, Akihiko Torii, Lars Hammarstrand, Erik Stenborg, Daniel Safari, Masatoshi Okutomi, Marc Pollefeys, Josef Sivic, et al. Benchmarking 6dof outdoor visual localization in changing conditions. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 8601\u20138610, 2018. Nikolay Savinov, Lubor Ladicky, and Marc Pollefeys. Matching neural paths: transfer from recognition to correspondence search. Advances in Neural Information Processing Systems , 30, 2017a. Nikolay Savinov, Akihito Seki, Lubor Ladicky, Torsten Sattler, and Marc Pollefeys. Quad-networks: unsupervised learning to rank for interest point detection. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 1822\u20131830, 2017b. Johannes Lutz Sch \u00a8onberger and Jan-Michael Frahm. Structure-from-motion revisited. In Conference on Computer Vision and Pattern Recognition (CVPR) , 2016. Jianbo Shi et al. Good features to track. In 1994 Proceedings of IEEE conference on computer vision and pattern recognition , pp. 593\u2013600. IEEE, 1994. Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, and Andrew Fitzgibbon. Scene coordinate regression forests for camera relocalization in rgb-d images. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 2930\u20132937, 2013. Edgar Simo-Serra, Eduard Trulls, Luis Ferraz, Iasonas Kokkinos, Pascal Fua, and Francesc MorenoNoguer. Discriminative learning of deep convolutional feature point descriptors. In Proceedings of the IEEE international conference on computer vision , pp. 118\u2013126, 2015. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 , 2014. Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Learning local feature descriptors using convex optimisation. IEEE Transactions on Pattern Analysis and Machine Intelligence , 36(8): 1573\u20131585, 2014. Noah Snavely, Steven M Seitz, and Richard Szeliski. Photo tourism: exploring photo collections in 3d. In ACM siggraph 2006 papers , pp. 835\u2013846. 2006. Henrik Stewenius, Christopher Engels, and David Nist \u00b4er. Recent developments on direct relative orientation. ISPRS Journal of Photogrammetry and Remote Sensing , 60(4):284\u2013294, 2006. Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 8922\u20138931, 2021. Supasorn Suwajanakorn, Noah Snavely, Jonathan J Tompson, and Mohammad Norouzi. Discovery of latent 3d keypoints via end-to-end geometric reasoning. Advances in neural information processing systems , 31, 2018. Hajime Taira, Masatoshi Okutomi, Torsten Sattler, Mircea Cimpoi, Marc Pollefeys, Josef Sivic, Tomas Pajdla, and Akihiko Torii. Inloc: Indoor visual localization with dense matching and view synthesis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp. 7199\u20137209, 2018. Shitao Tang, Chengzhou Tang, Rui Huang, Siyu Zhu, and Ping Tan. Learning camera localization via dense scene matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 1831\u20131841, 2021. Micha\u0142 Tyszkiewicz, Pascal Fua, and Eduard Trulls. Disk: Learning local features with policy gradient. Advances in Neural Information Processing Systems , 33:14254\u201314265, 2020."
        },
        "Recurrent Image Registration using Mutual Attention based Network": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2206.01863",
            "ref_texts": "19. Sun, J., Shen, Z., Wang, Y ., Bao, H., Zhou, X.: Loftr: Detector-free local feature matching with transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8922\u20138931 (2021)",
            "ref_ids": [
                "19"
            ],
            "1": "Local feature matching can also benefit from self and cross attention, because transformer networks are proved to obtain feature descriptors that are conditioned on both images [19].",
            "2": "3 Mutual Attention Similar to the idea from [13,19,7,26], Mutual Attention (MA) mechanism [21] is used in the RMAn to obtain the global receptive field and use so-called indicator matrices to quantify the relationship between each pair of pixels from two images, and the usage of multiple indicator matrices is called multi-head."
        },
        "Modularizing deep learning for geometry-aware registration and reconstruction": {
            "authors": [
                "Wei Jiang"
            ],
            "url": "https://open.library.ubc.ca/media/download/pdf/24/1.0427395/4",
            "ref_texts": "[246] J. Sun, Z. Shen, Y . Wang, H. Bao, and X. Zhou. LoFTR: Detector-Free Local Feature Matching with Transformers. CVPR , 2021.!page 58",
            "ref_ids": [
                "246"
            ],
            "1": "While 1A concurrent relevant work for feature-less image matching [246].",
            "2": "!page 1\n[246] J."
        },
        "Monocular 3D Object Detection and 3D Multi-Object Tracking for Autonomous Vehicles": {
            "authors": [],
            "url": "https://tspace.library.utoronto.ca/bitstream/1807/110712/4/Reading_Cody_Ariel_202203_MAS_thesis.pdf",
            "ref_texts": "[94] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2021.",
            "ref_ids": [
                "94"
            ],
            "1": "6: An example of image feature matching output from LoFTR [94].",
            "2": "LoFTR [94] is capable of ffnding correspondences on the texture-less wall and table.",
            "3": "Recently, Transformers have seen use in computer vision tasks such as image classiffcation [26], object detection [8], semantic segmentation [98], and feature matching [94].",
            "4": "Recently, many methods have turned to a learning-based approach which typically performs image feature extraction using a convolutional neural network (CNN) [19, 72, 94, 85].",
            "5": "To mitigate the receptive ffeld limitation, SuperGlue [85] and LoFTR [94] introduce a graph neural-network (GNN) and transformer module into their respective pipelines to allow for aggregation of both local and global information during feature extraction.",
            "6": "To extract track and detection features, we follow the Transformer design of LoFTR [94] to incorporate global object information.",
            "7": "We adopt the transformer module from LoFTR [94] and interleave the self and cross attention blocks Nctimes."
        },
        "Multimodal Image Local Registration by Attention Gauge Fields with Robust Adaptive Probability Distributions": {
            "authors": [],
            "url": "https://www.techrxiv.org/ndownloader/files/39647491/1",
            "ref_texts": "[17] J. Sun, Z. Shen, Y. Wang, H. Bao, and X. Zhou, \u201cLoftr: Detectorfree local feature matching with transformers,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 8922\u20138931.",
            "ref_ids": [
                "17"
            ],
            "1": "Although the popular multimodal image feature matching algorithm can solve the global image feature matching problem well [15], [16], [17],but through a large number of experimental observations, we found that: (i) The feature matching of non-target areas in the image will bring errors to the registration of target areas: Firstly, because a single traditional homologous transformation is represented by only one matrix parameters, it is unable to satisfy the multi-space transformation parameters with many non-rigid distortions.",
            "2": "281 Loftr [17] 0.",
            "3": "062 Loftr [17]MSE 22.",
            "4": "536 Loftr [17] 0.",
            "5": "009 Loftr [17] 0.",
            "6": "176 Loftr [17] 0.",
            "7": "[17] J."
        },
        "A Detector-oblivious Multi-arm Network for Keypoint Matching": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2104.00947",
            "ref_texts": ""
        },
        "PA-LoFTR: Local Feature Matching with 3D Position-Aware Transformer": {
            "authors": [],
            "url": "https://openreview.net/pdf?id=U8MtHLRK06q"
        },
        "Video-based 3D Object Detection with Learnable Object-Centric Global Optimization": {
            "authors": [],
            "url": "https://openreview.net/pdf?id=G7_LoXdE2Oe",
            "ref_texts": "11 Under review as a conference paper at ICLR 2023 Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 8922\u20138931, 2021. Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 2446\u20132454, 2020. Chengzhou Tang and Ping Tan. BA-net: Dense bundle adjustment networks. In International Conference on Learning Representations , 2019. Bill Triggs, Philip F McLauchlan, Richard I Hartley, and Andrew W Fitzgibbon. Bundle adjustment\u2014a modern synthesis. In International workshop on vision algorithms , pp. 298\u2013372. Springer, 1999. Li Wang, Li Zhang, Yi Zhu, Zhi Zhang, Tong He, Mu Li, and Xiangyang Xue. Progressive coordinate transforms for monocular 3d object detection. Advances in Neural Information Processing Systems , 34, 2021a. Qitai Wang, Yuntao Chen, Ziqi Pang, Naiyan Wang, and Zhaoxiang Zhang. Immortal tracker: Tracklet never dies. arXiv preprint arXiv:2111.13672 , 2021b. Tai Wang, Jiangmiao Pang, and Dahua Lin. Monocular 3d object detection with depth from motion. InEuropean Conference on Computer Vision (ECCV) , 2022a. Zengran Wang, Chen Min, Zheng Ge, Yinhao Li, Zeming Li, Hongyu Yang, and Di Huang. Sts: Surround-view temporal stereo for multi-view 3d detection. arXiv preprint arXiv:2208.10145 , 2022b. Haiping Wu, Yuntao Chen, Naiyan Wang, and Zhaoxiang Zhang. Sequence level semantics aggregation for video object detection. In Proceedings of the IEEE International Conference on Computer Vision , pp. 9217\u20139225, 2019. Linjie Yang, Yuchen Fan, and Ning Xu. Video instance segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 5188\u20135197, 2019. Shichao Yang and Sebastian Scherer. Cubeslam: Monocular 3-d object slam. IEEE Transactions on Robotics , 35(4):925\u2013938, 2019. Tianwei Yin, Xingyi Zhou, and Philipp Krahenbuhl. Center-based 3d object detection and tracking. InProceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp."
        },
        "Video Motion Analysis with Limited Labeled Training Data": {
            "authors": [],
            "url": "https://dukespace.lib.duke.edu/dspace/bitstream/handle/10161/27587/Yu_duke_0066D_17121.pdf?sequence=1&isAllowed=y",
            "ref_texts": "145 Stone, A., Maurer, D., Ayvaci, A., Angelova, A., and Jonschkowski, R. (2021), \u201cSMURF: Self-teaching multi-frame unsupervised RAFT with full-image warping,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 3887\u20133896. Su, H., Jampani, V., Sun, D., Gallo, O., Learned-Miller, E., and Kautz, J. (2019), \u201cPixel-adaptive convolutional neural networks,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 11166\u201311175. Sui, X., Li, S., Geng, X., Wu, Y., Xu, X., Liu, Y., Goh, R., and Zhu, H. (2022), \u201cCRAFT: Cross-Attentional Flow Transformer for Robust Optical Flow,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 17602\u201317611. Sun, D., Yang, X., Liu, M.-Y., and Kautz, J. (2018a), \u201cPWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume,\u201d in Conference on Computer Vision and Pattern Recognition . Sun, D., Yang, X., Liu, M.-Y., and Kautz, J. (2018b), \u201cPWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume,\u201d in The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . Sun, J., Shen, Z., Wang, Y., Bao, H., and Zhou, X. (2021), \u201cLoFTR: Detectorfree local feature matching with transformers,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 8922\u20138931. Sun, S., Akhtar, N., Song, H., Mian, A., and Shah, M. (2019), \u201cDeep affinity network for multiple object tracking,\u201d IEEE transactions on pattern analysis and machine intelligence , 43, 104\u2013119. Sundberg, P., Brox, T., Maire, M., Arbel\u00b4 aez, P., and Malik, J. (2011), \u201cOcclusion boundary detection and figure/ground assignment from optical flow,\u201d in CVPR"
        },
        "\u4e09\u7ef4\u573a\u666f\u70b9\u4e91\u7406\u89e3\u4e0e\u91cd\u5efa\u6280\u672f": {
            "authors": [],
            "url": "http://www.cjig.cn/jig/ch/reader/create_pdf.aspx?file_no=230004&year_id=2023&quarter_id=6&falg=1",
            "ref_texts": "6848 -6853 [DOI: 10.1109 /IROS 47612 .2022 .9981829 ] Sun J M , Shen Z H , Wang Y , Bao H J and Zhou X W . 2021 . LoFTR : detector -free local feature matching with transformers//Proceedings of 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition . Nashville , USA : IEEE : 8918 -8927 [DOI: 10.1109 / CVPR 46437 .2021 .00881 ] Sun P , Kretzschmar H , Dotiwalla X , Chouard A , Patnaik V , Tsui P , Guo J , Zhou Y , Chai Y , Caine B , Vasudevan V , Han W , Ngiam J, Zhao H , Timofeev A , Ettinger S , Krivokon M , Gao A , Joshi A, Zhang Y , Shlens J , Chen Z F and Anguelov D . 2020 . Scalabil \u2043 ity in perception for autonomous driving : waymo open dataset//Pro \u2043 ceedings of 2020 IEEE/CVF Conference on Computer Vision and 1762"
        },
        "Tech details for loftr in the imw challenge": {
            "authors": [],
            "url": "https://zju3dv.github.io/loftr/files/LoFTR_IMC21.pdf",
            "ref_texts": "[6] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In CVPR , 2021.[7] Prune Truong, Martin Danelljan, Luc Van Gool, and Radu Timofte. Learning accurate dense correspondences and when to trust them. In CVPR , 2021.",
            "ref_ids": [
                "6",
                "7"
            ],
            "1": "Method and Technical Details Our method is based on LoFTR [6], a detector-free local feature matching method with Transformers [8].",
            "2": "Similar to other detector-free feature matching or correspondences estimation methods such as NCNet [5], [7], LoFTR does not depend on pre-extracted local features, thus lacks consistent features within a single image.",
            "3": "Dataset and Pre-trained Models We use the MegaDepth [4] dataset to train our models, following the same setup as in [6].",
            "4": "Among scenes kept, we enumerate all image pairs with covisible scores in a range of [0:1;0:7]and further split each scene into sub-scenes with covisible scores in ranges [0:1;0:3];[0:3;0:5];[0:5;0:7]respectively.",
            "5": "[7] Prune Truong, Martin Danelljan, Luc Van Gool, and Radu Timofte."
        },
        "A Spacel Information Extraction Method based on Multimodal Social Media Data: A Case Study on Urban Inundation": {
            "authors": [],
            "url": "https://www.preprints.org/manuscript/202305.1205/download/final_file"
        },
        "Deep Structured Models for Spatial Intelligence": {
            "authors": [],
            "url": "https://openreview.net/pdf?id=NO06lRxBSOLP",
            "ref_texts": "1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149) , volume 1, pages 125\u2013131. IEEE, 1999. D. G. Lowe. Object recognition from local scale-invariant features. In ICCV ,1 9 9 9 . Z. Lu and K. Grauman. Story-driven summarization for egocentric video. In CVPR ,2 0 1 3 . W. Luo, A. G. Schwing, and R. Urtasun. Efficient deep learning for stereo matching. In CVPR , 2016. Z. Lv, F. Dellaert, J. M. Rehg, and A. Geiger. Taking a deeper look at the inverse compositional algorithm. In CVPR ,2 0 1 9 . BIBLIOGRAPHY. 156 W.-C. Ma, S. Wang, M. A. Brubaker, S. Fidler, and R. Urtasun. Find your way by observing the sun and other semantic cues. In ICRA ,2 0 1 7 . W.-C. Ma, S. Wang, R. Hu, Y. Xiong, and R. Urtasun. Deep rigid instance scene flow. In CVPR , 2019. W. Maddern, G. Pascoe, C. Linegar, and P. Newman. 1 year, 1000km: The oxford robotcar dataset. IJRR ,2 0 1 6 . J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman. Non-local sparse models for image restoration. In ICCV ,2 0 0 9 . D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. InICCV ,2 0 0 1 . R. Martin-Brualla, Y. He, B. C. Russell, and S. M. Seitz. The 3D Jigsaw Puzzle: Mapping Large Indoor Spaces. In ECCV ,2 0 1 4 . G. M\u00e1ttyus, S. Wang, S. Fidler, and R. Urtasun. Enhancing road maps by parsing aerial images around the world. In ICCV ,2 0 1 5 . G. M\u00e1ttyus, S. Wang, S. Fidler, and R. Urtasun. Hd maps: Fine-grained road segmentation by parsing ground and aerial images. In CVPR ,2 0 1 6 . D. Maturana and S. Scherer. Voxnet: A 3d convolutional neural network for real-time object recognition. In IROS ,2 0 1 5 . K. Matzen and N. Snavely. Nyc3dcars: A dataset of 3d vehicles in geographic context. In ICCV , 2013. N. Mayer, E. Ilg, P. H\u00e4usser, P. Fischer, D. Cremers, A. Dosovitskiy, and T. Brox. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. arXiv ,2 0 1 5 . T. Meinhardt, M. Moller, C. Hazirbas, and D. Cremers. Learning proximal operators: Using denoising networks for regularizing inverse imaging problems. In ICCV ,2 0 1 7 . A. Mensch and M. Blondel. Differentiable dynamic programming for structured prediction and attention. In ICML ,2 0 1 8 . M. Menze, C. Heipke, and A. Geiger. Discrete optimization for optical flow. In GCPR ,2 0 1 5 . L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and A. Geiger. Occupancy networks: Learning 3d reconstruction in function space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4460\u20134470, 2019. BIBLIOGRAPHY. 157 B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European Conference on Computer Vision , pages 405\u2013421. Springer, 2020. F. Monti, D. Boscaini, J. Masci, E. Rodol\u00e0, J. Svoboda, and M. M. Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. CVPR ,2 0 1 7 . R. Mur-Artal, J. M. M. Montiel, and J. D. Tardos. Orb-slam: a versatile and accurate monocular slam system. IEEE Transactions on Robotics ,2 0 1 5 . K. P. Murphy, Y. Weiss, and M. I. Jordan. Loopy belief propagation for approximate inference: an empirical study. In UAI,1 9 9 9 . R. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux, D. Kim, A. Davison, P. Kohi, J. Shotton, S. Hodges, and A. Fitzgibbon. Kinectfusion: Real-time dense surface mapping and tracking. InISMAR ,2 0 1 1 a . R. A. Newcombe, S. J. Lovegrove, and A. J. Davison. Dtam: Dense tracking and mapping in real-time. In CVPR ,2 0 1 1 b . F. Nex, M. Gerke, F. Remondino, H. Przybilla, M. B\u00e4umker, and A. Zurhorst. Isprs benchmark for multi-platform photogrammetry. ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences ,2 0 1 5 . J. Niemeyer, F. Rottensteiner, and U. Soergel. Contextual classification of lidar data and building object detection in urban areas. ISPRS journal of photogrammetry and remote sensing ,2 0 1 4 . D. Nist\u00e9r. An efficient solution to the five-point relative pose problem. IEEE transactions on pattern analysis and machine intelligence ,2 6 (6 ) : 7 5 6 \u2013 7 7 0 ,2 0 0 4 . N.Mayer, E.Ilg, P.H\u00e4usser, P.Fischer, D.Cremers, A.Dosovitskiy, and T.Brox. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In CVPR , 2016. J. Nocedal and S. J. Wright. Numerical optimization 2ed. Springer-Verlag ,2 0 0 6 . N. Noorshams and M. J. Wainwright. Belief propagation for continuous state spaces: Stochastic message-passing with quantitative guarantees. JMLR ,2 0 1 3 . S. Nowozin and C. Lampert. Structured learning and prediction in computer vision. Foundations and Trends \u00aein Computer Graphics and Vision ,2 0 1 1 . Y. Ono, E. Trulls, P. Fua, and K. M. Yi. Lf-net: Learning local features from images. arXiv preprint arXiv:1805.09662 ,2 0 1 8 . A. Papachristodoulou, J. Anderson, G. Valmorbida, S. Prajna, P. Seiler, and P. Parrilo. Sostools version 3.00 sum of squares optimization toolbox for matlab. arXiv ,2 0 1 3 . BIBLIOGRAPHY. 158 N. Parikh and S. Boyd. Proximal algorithms. Foundations and Trends in optimization ,1 (3 ) : 127\u2013239, 2014a. N. Parikh and S. Boyd. Proximal algorithms. Foundations and Trends in optimization ,2 0 1 4 b . J. J. Park, P. Florence, J. Straub, R. Newcombe, and S. Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 165\u2013174, 2019. P. A. Parrilo. Structured semidefinite programs and semialgebraic geometry methods in robustness and optimization . PhD thesis, 2000. J. Pearl. Fusion, propagation, and structuring in belief networks. Artificial intelligence ,2 9 (3 ) : 241\u2013288, 1986. J. Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference .1 9 8 8 . J. Peng, T. Hazan, D. McAllester, and R. Urtasun. Convex max-product algorithms for continuous mrfs with applications to protein folding. In ICML ,2 0 1 1 . M. Pollefeys, R. Koch, and L. Van Gool. Self-calibration and metric reconstruction inspite of varying and unknown intrinsic camera parameters. International Journal of Computer Vision , 32(1):7\u201325, 1999. E. Prados and O. Faugeras. Shape from shading. 2006. P. Purkait, T.-J. Chin, and I. Reid. Neurora: Neural robust rotation averaging. In ECCV ,2 0 2 0 . P. Putzky and M. Welling. Recurrent inference machines for solving inverse problems. arXiv , 2017. C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. 2016a. C. R. Qi, H. Su, M. Nie\u00dfner, A. Dai, M. Yan, and L. J. Guibas. Volumetric and multi-view cnns for object classification on 3d data. In CVPR ,2 0 1 6 b . C. R. Qi, L. Yi, H. Su, and L. J. Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. 2017a. X. Qi, R. Liao, J. Jia, S. Fidler, and R. Urtasun. 3d graph neural networks for rgbd semantic segmentation. In CVPR ,2 0 1 7 b . A. Quattoni and A. Torralba. Recognizing indoor scenes. In CVPR ,2 0 0 9 . M. Quigley, K. Conley, B. Gerkey, J. Faust, T. Foote, J. Leibs, R. Wheeler, and A. Y. Ng. Ros: an open-source robot operating system. In ICRA workshop ,2 0 0 9 . BIBLIOGRAPHY. 159 N. Radwan, A. Valada, and W. Burgard. VLocNet++: Deep Multitask Learning for Semantic Visual Localization and Odometry. arXiv ,2 0 1 8 . U. Ramer. An iterative procedure for the polygonal approximation of plane curves. Computer graphics and image processing ,1 9 7 2 . R. Ranftl and V. Koltun. Deep fundamental matrix estimation. In Proceedings of the European conference on computer vision (ECCV) , pages 284\u2013299, 2018. N. Ravi, P. Shankar, A. Frankel, A. Elgammal, and L. Iftode. Indoor localization using camera phones. In WMCSA ,2 0 0 6 . J. Rehder, J. Nikolic, T. Schneider, T. Hinzmann, and R. Siegwart. Extending kalibr: Calibrating the extrinsics of multiple imus and of individual axes. In 2016 IEEE International Conference on Robotics and Automation (ICRA) , pages 4304\u20134311. IEEE, 2016. X. Ren, L. Bo, and D. Fox. Rgb-(d) scene labeling: Features and algorithms. In CVPR ,2 0 1 2 . J. Revaud, P. Weinzaepfel, Z. Harchaoui, and C. Schmid. Epicflow: Edge-preserving interpolation of correspondences for optical flow. In CVPR ,2 0 1 5 . J. Revaud, P. Weinzaepfel, C. De Souza, N. Pion, G. Csurka, Y. Cabon, and M. Humenberger. R2d2: repeatable and reliable detector and descriptor. arXiv preprint arXiv:1906.06195 ,2 0 1 9 . S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing for data: Ground truth from computer games. In ECCV ,2 0 1 6 . G. Riegler, A. O. Ulusoys, and A. Geiger. Octnet: Learning deep 3d representations at high resolutions. 2017. G. Ros, S. Ramos, M. Granados, A. Bakhtiary, D. Vazquez, and A. Lopez. Vision-based offline-online perception paradigm for autonomous driving. In WACV ,2 0 1 5 . G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M. Lopez. The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. In CVPR ,2 0 1 6 . S. Ross, D. Munoz, M. Hebert, and J. Bagnell. Learning message-passing inference machines for structured prediction. In CVPR ,2 0 1 1 . S. Roth and M. Black. Fields of experts: A framework for learning image priors. In CVPR ,2 0 0 5 . S. Roth and M. J. Black. On the spatial statistics of optical flow. In IJCV ,2 0 0 7 . S. Roth and M. J. Black. Fields of experts. IJCV ,2 0 0 9 . F. Rottensteiner, G. Sohn, M. Gerke, and J. D. Wegner. Isprs test project on urban classification and 3d building reconstruction. 2013. BIBLIOGRAPHY. 160 S. Rusinkiewicz and M. Levoy. Efficient variants of the icp algorithm. In 3DIM ,2 0 0 1 . S. Saito, Z. Huang, R. Natsume, S. Morishima, A. Kanazawa, and H. Li. Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 2304\u20132314, 2019. R. Salakhutdinov, S. Roweis, and Z. Ghahramani. On the convergence of bound optimization algorithms. In UAI,2 0 0 2 . M. Salzmann. Continuous inference in graphical models with polynomial energies. In CVPR , 2013. P.-E. Sarlin, D. DeTone, T. Malisiewicz, and A. Rabinovich. Superglue: Learning feature matching with graph neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 4938\u20134947, 2020. T. Sattler, B. Leibe, and L. Kobbelt. Fast image-based localization using direct 2d-to-3d matching. InICCV ,2 0 1 1 . T. Sattler, A. Torii, J. Sivic, M. Pollefeys, H. Taira, M. Okutomi, T. Pajdla, T. Sattler, A. Torii, J. Sivic, M. Pollefeys, H. Taira, and A. Large-scale. Are Large-Scale 3D Models Really Necessary for Accurate Visual Localization? 2017. A. Saxena, M. Sun, and A. Y. Ng. Make3d: Learning 3d scene structure from a single still image. PAMI ,2 0 0 9 . F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural network model. TNN ,2 0 0 9 . G. Schindler, M. Brown, and R. Szeliski. City-scale location recognition. In CVPR ,2 0 0 7 . U. Schmidt and S. Roth. Shrinkage fields for effective image restoration. In CVPR ,2 0 1 4 . U. Schmidt, J. Jancsary, S. Nowozin, S. Roth, and C. Rother. Cascades of regression tree fields for image restoration. PAMI ,2 0 1 3 . H. Schneiderman and T. Kanade. A statistical method for 3d object detection applied to faces and cars. In CVPR ,2 0 0 0 . M. Schreiber, C. Kn\u00f6ppel, and U. Franke. Laneloc: Lane marking based localization using highly accurate maps. In IV,2 0 1 3 . K. T. Sch\u00fctt, P. Kindermans, H. Sauceda, S. Chmiela, A. Tkatchenko, and K. M\u00fcller. Schnet: A continuous-filter convolutional neural network for modeling quantum interactions. arXiv ,2 0 1 7 . A. Schwing and R. Urtasun. Fully connected deep structured networks. arXiv ,2 0 1 5 . BIBLIOGRAPHY. 161 A. Schwing, T. Hazan, and R. Urtasun. Efficient structured prediction for 3d indoor scene understanding. In ECCV ,2 0 1 2 a . A. Schwing, S. Fidler, M. Pollefeys, and R. Urtasun. Box in the box: Joint 3d layout and object reasoning from single images. In ICCV ,2 0 1 3 a . A. G. Schwing, T. Hazan, M. Pollefeys, and R. Urtasun. Distributed Message Passing for Large Scale Graphical Models. In CVPR ,2 0 1 1 . A. G. Schwing, T. Hazan, M. Pollefeys, and R. Urtasun. Efficient Structured Prediction with Latent Variables for General Graphical Models. In ICML ,2 0 1 2 b . A. G. Schwing, S. Fidler, M. Pollefeys, and R. Urtasun. Box in the box: Joint 3d layout and object reasoning from single images. In ICCV ,2 0 1 3 b . A. Shafaei, J. J. Little, and M. Schmidt. Play and learn: using video games to train computer vision models. arXiv ,2 0 1 6 . J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A. Kipman, and A. Blake. Real-time human pose recognition in parts from single depth images. In CVPR ,2 0 1 1 . J. Shotton, B. Glocker, C. Zach, S. Izadi, A. Criminisi, and A. Fitzgibbon. Scene coordinate regression forests for camera relocalization in rgb-d images. In CVPR ,2 0 1 3 . N. Silberman, D. Hoiem, P. Kohli, and R. Fergus. Indoor segmentation and support inference from rgbd images. In ECCV .2 0 1 2 . E. Simo-Serra, E. Trulls, L. Ferraz, I. Kokkinos, P. Fua, and F. M. Noguer. Discriminative learning of deep convolutional feature point descriptors. In ICCV ,2 0 1 5 . M. Simonovsky and N. Komodakis. Dynamic edge-conditioned filters in convolutional neural networks on graphs. CVPR ,2 0 1 7 . K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv ,2 0 1 4 . K. Simonyan, A. Vedaldi, and A. Zisserman. Learning local feature descriptors using convex optimisation. In PAMI ,2 0 1 4 . S. Singh, A. Gupta, and A. Efros. Unsupervised discovery of mid-level discriminative patches. InECCV ,2 0 1 2 . V. Sitzmann, E. R. Chan, R. Tucker, N. Snavely, and G. Wetzstein. Metasdf: Meta-learning signed distance functions. arXiv preprint arXiv:2006.09662 ,2 0 2 0 a . V. Sitzmann, J. N. Martel, A. W. Bergman, D. B. Lindell, and G. Wetzstein. Implicit neural representations with periodic activation functions. arXiv preprint arXiv:2006.09661 ,2 0 2 0 b . BIBLIOGRAPHY. 162 A. Smola, S. Vishwanathan, and T. Hofmann. Kernel methods for missing variables. AISTATS , 2005. N. Snavely, S. M. Seitz, and R. Szeliski. Modeling the world from internet photo collections. IJCV ,2 0 0 8 . L. Song, A. Gretton, D. Bickson, Y. Low, and C. Guestrin. Kernel belief propagation. In AISTATS ,2 0 1 1 . D. Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and Y. Weiss. Tightening lp relaxations for map using message passing. In UAI,2 0 0 8 . B. Sriperumbudur and G. Lanckriet. On the convergence of the concave-convex procedure. In NIPS ,\u2019 0 9 . B. Sriperumbudur, D. Torres, and G. Lanckriet. Sparse eigen methods by dc programming. In ICML ,\u2019 0 7 . C. Strecha, A. M. Bronstein, M. M. Bronstein, and P. Fua. Ldahash: Improved matching with smaller descriptors. In PAMI ,2 0 1 2 . E. Sudderth, A. Ihler, M. Isard, W. Freeman, and A. Willsky. Nonparametric belief propagation. Communications of the ACM ,2 0 1 0 a . E. B. Sudderth, A. T. Ihler, M. Isard, W. T. Freeman, and A. S. Willsky. Nonparametric belief propagation. Communications of the ACM ,2 0 1 0 b . D. Sun, S. Roth, and M. J. Black. Secrets of optical flow estimation and their principles. In CVPR ,2 0 1 0 . J. Sun, Z. Shen, Y. Wang, H. Bao, and X. Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8922\u20138931, 2021. I. Sutskever, O. Vinyals, and Q. Le. Sequence to sequence learning with neural networks. In NIPS ,2 0 1 4 . C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In CVPR ,2 0 1 5 . R. Szeliski. Computer vision: algorithms and applications .2 0 1 0 . C. Tang and P. Tan. Ba-net: Dense bundle adjustment network. arXiv ,2 0 1 8 . L. P. Tchapmi, C. B. Choy, I. Armeni, J. Gwak, and S. Savarese. Segcloud: Semantic segmentation of 3d point clouds. arXiv ,2 0 1 7 . BIBLIOGRAPHY. 163 Z. Teed and J. Deng. Raft: Recurrent all-pairs field transforms for optical flow. In ECCV ,2 0 2 0 . R. Templeman, M. Korayem, D. Crandall, and A. Kapadia. Placeavoider: Steering first-person cameras away from sensitive spaces. In Network and Distributed System Security Symposium (NDSS) ,2 0 1 4 . H. Thomas, C. R. Qi, J.-E. Deschaud, B. Marcotegui, F. Goulette, and L. J. Guibas. Kpconv: Flexible and deformable convolution for point clouds. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 6411\u20136420, 2019. S. Thrun and M. Montemerlo. The graph slam algorithm with applications to large-scale mapping of urban structures. IJRR ,2 0 0 6 . J. Tighe and S. Lazebnik. Finding things: Image parsing with regions and per-exemplar detectors. InCVPR ,2 0 1 3 . E. Tola, V. Lepetit, and P. Fua. Daisy: An efficient dense descriptor applied to wide-baseline stereo. In PAMI ,2 0 1 0 . A. Torii, J. Sivic, T. Pajdla, and M. Okutomi. Visual place recognition with repetitive structures. InCVPR ,2 0 1 3 . A. Torralba, K. P. Murphy, and W. T. Freeman. Sharing visual features for multiclass and multiview object detection. PAMI ,2 0 0 7 . S. Treuillet and E. Royer. Outdoor/indoor vision based localization for blind pedestrian navigation assistance. Intl. J. of Image and Graphics ,2 0 1 0 . B. Triggs, P. F. McLauchlan, R. I. Hartley, and A. W. Fitzgibbon. Bundle adjustment\u2014a modern synthesis. In International workshop on vision algorithms , pages 298\u2013372. Springer, 1999. T. Trzcinski, M. Christoudias, P. Fua, and V. Lepetit. Boosting binary keypoint descriptors. In CVPR ,2 0 1 3 . I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. Support vector machine learning for interdependent and structured output spaces. In ICML ,2 0 0 4 . D. J. Uherka and A. M. Sergott. On the continuous dependence of the roots of a polynomial on its coefficients. American Mathematical Monthly ,1 9 7 7 . D. Ulyanov, A. Vedaldi, and V. S. Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv ,2 0 1 6 . B. Ummenhofer, H. Zhou, J. Uhrig, N. Mayer, E. Ilg, A. Dosovitskiy, and T. Brox. Demon: Depth and motion network for learning monocular stereo. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 5038\u20135047, 2017. BIBLIOGRAPHY. 164 B. Ummenhofer, L. Prantl, N. Thuerey, and V. Koltun. Lagrangian fluid simulation with continuous convolutions. In International Conference on Learning Representations ,2 0 1 9 . A. Varol, M. Salzmann, P. Fua, and R. Urtasun. A constrained latent variable model. In CVPR , 2012. S. Vicente and L. Agapito. Soft inextensibility constraints for template-free non-rigid reconstruction. In ECCV ,2 0 1 2 . A. Viterbi. Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. IEEE transactions on Information Theory ,1 9 6 7 . L. von Stumberg, P. Wenzel, Q. Khan, and D. Cremers. Gn-net: The gauss-newton loss for multi-weather relocalization. IEEE Robotics and Automation Letters ,2 0 2 0 . M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference . Now Publishers Inc, 2008. R. Walters, J. Li, and R. Yu. Trajectory prediction using equivariant continuous convolution. arXiv preprint arXiv:2010.11344 ,2 0 2 0 . G. Wan, X. Yang, R. Cai, H. Li, H. Wang, and S. Song. Robust and Precise Vehicle Localization based on Multi-sensor Fusion in Diverse City Scenes. arXiv ,2 0 1 7 . Q. Wang, X. Zhou, B. Hariharan, and N. Snavely. Learning feature descriptors using camera pose supervision. In European Conference on Computer Vision , pages 757\u2013774. Springer, 2020. S. Wang, A. Schwing, and R. Urtasun. Efficient inference of continuous markov random fields with polynomial potentials. In NIPS ,2 0 1 4 a . S. Wang, A. Schwing, and R. Urtasun. Efficient inference of continuous markov random fields with polynomial potentials. In NeurIPS ,2 0 1 4 b . S. Wang, S. Fidler, and R. Urtasun. Lost shopping! monocular localization in large indoor spaces. In ICCV ,2 0 1 5 a . S. Wang, S. Fidler, and R. Urtasun. Holistic 3d scene understanding from a single geo-tagged image. In CVPR ,2 0 1 5 b . S. Wang, S. Fidler, and R. Urtasun. Holistic 3d scene understanding from a single geo-tagged image. In CVPR , 2015c. S. Wang, S. Fidler, and R. Urtasun. Lost shopping! monocular localization in large indoor spaces. In ICCV ,2 0 1 5 d . S. Wang, M. Bai, G. Mattyus, H. Chu, W. Luo, B. Yang, J. Liang, J. Cheverie, S. Fidler, and R. Urtasun. Torontocity: Seeing the world with a million eyes. arXiv ,2 0 1 6 a . BIBLIOGRAPHY. 165 S. Wang, S. Fidler, and R. Urtasun. Proximal deep structured models. In NeurIPS ,2 0 1 6 b . S. Wang, R. Clark, H. Wen, and N. Trigoni. Deepvo: Towards end-to-end visual odometry with deep recurrent convolutional neural networks. In ICRA ,2 0 1 7 . S. Wang, S. Suo, W.-C. Ma, A. Pokrovsky, and R. Urtasun. Deep parametric continuous convolutional neural networks. In CVPR ,2 0 1 8 . X. Wang, A. Jabri, and A. A. Efros. Learning correspondence from the cycle-consistency of time. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2566\u20132576, 2019a. Z. Wang, L. Chen, S. Rathore, D. Shin, and C. Fowlkes. Geometric pose affordance: 3d human pose with scene constraints. arXiv ,2 0 1 9 b . R. M. Webb, D. Lubinski, and C. P. Benbow. Spatial ability: A neglected dimension in talent searches for intellectually precocious youth. Journal of Educational Psychology ,2 0 0 7 . J. D. Wegner, S. Branson, D. Hall, K. Schindler, and P. Perona. Cataloging public objects using aerial and street-level images-urban trees. In CVPR ,2 0 1 6 . K. Wei, A. Aviles-Rivero, J. Liang, Y. Fu, C.-B. Sch\u00f6nlieb, and H. Huang. Tuning-free plugand-play proximal algorithm for inverse imaging problems. In International Conference on Machine Learning ,2 0 2 0 a . X. Wei, Y. Zhang, Z. Li, Y. Fu, and X. Xue. Deepsfm: Structure from motion via deep bundle adjustment. In ECCV ,2 0 2 0 b . P. Weinzaepfel, J. Revaud, Z. Harchaoui, and C. Schmid. Deepflow: Large displacement optical flow with deep matching. In ICCV ,2 0 1 3 . Y. Weiss and W. Freeman. Correctness of belief propagation in gaussian graphical models of arbitrary topology. Neural computation ,2 0 0 1 a . Y. Weiss and W. T. Freeman. Correctness of belief propagation in gaussian graphical models of arbitrary topology. Neural computation ,2 0 0 1 b . Y. Weiss and W. T. Freeman. Correctness of belief propagation in gaussian graphical models of arbitrary topology. Neural computation , 2001c. R. W. Wolcott and R. M. Eustice. Visual localization within lidar maps for automated urban driving. In IROS ,2 0 1 4 . R. W. Wolcott and R. M. Eustice. Fast LIDAR localization using multiresolution Gaussian mixture maps. In ICRA ,2 0 1 5 . BIBLIOGRAPHY. 166 O. Woodman and R. Harle. Pedestrian localisation for indoor environments. In International Conference on Ubiquitous Computing ,2 0 0 8 a . O. Woodman and R. Harle. Pedestrian localisation for indoor environments. In International Conference on Ubiquitous Computing ,2 0 0 8 b . W. Wu, Z. Qi, and L. Fuxin. Pointconv: Deep convolutional networks on 3d point clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9621\u20139630, 2019. Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao. 3d shapenets: A deep representation for volumetric shapes. In CVPR ,2 0 1 5 . X. Wei, I. A. B\u00e2rsan, S. Wang, J. Martinez, and R. Urtasun. Learning to localize through compressed binary maps. In CVPR ,2 0 1 9 . Y. Xiang and S. Savarese. Estimating the aspect layout of object categories. In CVPR ,2 0 1 2 . J. Xiao and Y. Furukawa. Reconstructing the World\u2019s Museums. In ECCV ,2 0 1 2 . Y. Xiong, M. Ren, R. Liao, K. Wong, and R. Urtasun. Deformable filter convolution for point cloud reasoning. arXiv preprint arXiv:1907.13079 ,2 0 1 9 . J. Xu, R. Ranftl, and V. Koltun. Accurate optical flow via direct cost volume processing. CVPR , 2017. L. Xu, J. Jia, and Y. Matsushita. Motion detail preserving optical flow estimation. In PAMI , 2012. Q. Xu, W. Wang, D. Ceylan, R. Mech, and U. Neumann. Disn: Deep implicit surface network for high-quality single-view 3d reconstruction. arXiv preprint arXiv:1905.10711 ,2 0 1 9 . B. Yang, M. Liang, and R. Urtasun. Hdnet: Exploiting hd maps for 3d object detection. In CoRL ,2 0 1 8 . N. Yang, L. v. Stumberg, R. Wang, and D. Cremers. D3vo: Deep depth, deep pose and deep uncertainty for monocular visual odometry. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1281\u20131292, 2020. Z. Yang, O. Litany, T. Birdal, S. Sridhar, and L. Guibas. Continuous geodesic convolutions for learning on 3d shapes. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision , pages 134\u2013144, 2021a. Z. Yang, S. Wang, S. Manivasagam, Z. Huang, W.-C. Ma, X. Yan, E. Yumer, and R. Urtasun. S3: Neural shape, skeleton, and skinning fields for 3d human modeling. arXiv preprint arXiv:2101.06571 ,2 0 2 1 b . BIBLIOGRAPHY. 167 J. Yao, S. Fidler, and R. Urtasun. Describing the scene as a whole: Joint object detection, scene classification and semantic segmentation. In CVPR ,2 0 1 2 . L. Yi, H. Su, X. Guo, and L. Guibas. Syncspeccnn: Synchronized spectral cnn for 3d shape segmentation. CVPR ,2 0 1 7 . K. Yoneda, H. Tehrani, T. Ogawa, N. Hukuyama, and S. Mita. Lidar scan feature for localization with highly precise 3-d map. In IV,2 0 1 4 . C. N. Yu and T. Joachims. Learning structural svms with latent variables. In ICML ,2 0 0 9 . Z. Yu and S. Gao. Fast-mvsnet: Sparse-to-dense multi-view stereo with learned propagation and gauss-newton refinement. In CVPR ,2 0 2 0 . J. Yuan and A. M. Cheriyadat. Combining maps and street level images for building height and facade estimation. In SIGSPATIAL ,2 0 1 6 . A. L. Yuille and A. Rangarajan. The concave-convex procedure. Neural Computation ,2 0 0 3 . C. Zach and P. Kohli. A convex discrete-continuous approach for markov random fields. In ECCV .2 0 1 2 . S. Zagoruyko and N. Komodakis. Learning to compare image patches via convolutional neural networks. In CVPR ,2 0 1 5 a . S. Zagoruyko and N. Komodakis. Learning to compare image patches via convolutional neural networks. In CVPR ,2 0 1 5 b . A. R. Zamir and M. Shah. Accurate image localization based on google maps street view. In ECCV .2 0 1 0 . A. R. Zamir, T. Wekel, P. Agrawal, C. Wei, J. Malik, and S. Savarese. Generic 3d representation via pose estimation and matching. In ECCV ,2 0 1 6 . J. \u017dbontar and Y. LeCun. Computing the stereo matching cost with a convolutional neural network. In CVPR ,2 0 1 5 . J. Zbontar and Y. LeCun. Computing the stereo matching cost with a convolutional neural network. In CVPR ,2 0 1 5 a . J. Zbontar and Y. LeCun. Computing the stereo matching cost with a convolutional neural network. In CVPR ,2 0 1 5 b . A. Zeng, S. Song, M. Nie\u00dfner, M. Fisher, J. Xiao, and T. Funkhouser. 3dmatch: Learning local geometric descriptors from rgb-d reconstructions. In CVPR ,2 0 1 7 . BIBLIOGRAPHY. 168 X. Zeng, R. Liao, L. Gu, Y. Xiong, S. Fidler, and R. Urtasun. Dmm-net: Differentiable mask-matching network for video object segmentation. In ICCV ,2 0 1 9 . J. Zhang and B. Ghanem. Ista-net: Iterative shrinkage-thresholding algorithm inspired deep network for image compressive sensing. 2017. J. Zhang and B. Ghanem. Ista-net: Interpretable optimization-inspired deep network for image compressive sensing. In CVPR ,2 0 1 8 . J. Zhang and S. Singh. Loam: Lidar odometry and mapping in real-time. In RSS,2 0 1 4 . K. Zhang, G. Riegler, N. Snavely, and V. Koltun. Nerf++: Analyzing and improving neural radiance fields. arXiv preprint arXiv:2010.07492 ,2 0 2 0 . Z. Zhang. A flexible new technique for camera calibration. IEEE Transactions on pattern analysis and machine intelligence ,2 2 (1 1 ) : 1 3 3 0 \u2013 1 3 3 4 ,2 0 0 0 . H. Zhao, L. Jiang, C.-W. Fu, and J. Jia. Pointweb: Enhancing local neighborhood features for point cloud processing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5565\u20135573, 2019. H. Zhao, L. Jiang, J. Jia, P. Torr, and V. Koltun. Point transformer. arXiv preprint arXiv:2012.09164 ,2 0 2 0 . S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet, Z. Su, D. Du, C. Huang, and P. Torr. Conditional random fields as recurrent neural networks. In ICCV ,2 0 1 5 . T. Zhou, M. Brown, N. Snavely, and D. G. Lowe. Unsupervised learning of depth and ego-motion from video. arXiv ,2 0 1 7 . M. Z. Zia, M. Stark, K. Schindler, and R. Vision. Are cars just 3d boxes?\u2013jointly estimating the 3d shape of multiple objects. In CVPR ,2 0 1 4 . J. Ziegler, H. Lategahn, M. Schreiber, C. G. Keller, C. Knoppel, J. Hipp, M. Haueis, and C. Stiller. Video based localization for bertha. In IV,2 0 1 4 . D. Zoran and Y. Weiss. From learning models of natural image patches to whole image restoration. InICCV ,2 0 1 1 ."
        },
        "NeuroGlue: attentional graph based mosaicking in neurosurgery": {
            "authors": [],
            "url": "https://www.politesi.polimi.it/bitstream/10589/188467/4/2022_4_De_Luca_01.pdf"
        },
        "Learning Rotation-Equivariant Features for Visual Correspondence-supplementary material": {
            "authors": [
                "N L"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/supplemental/Lee_Learning_Rotation-Equivariant_Features_CVPR_2023_supplemental.pdf",
            "ref_texts": "[1]Vassileios Balntas, Karel Lenc, Andrea Vedaldi, and Krys-tian Mikolajczyk. Hpatches: A benchmark and evaluationof handcrafted and learned local descriptors. InProceed-ings of the IEEE Conference on Computer Vision and PatternRecognition, pages 5173\u20135182, 2017.1,5[2]Taco Cohen and Max Welling. Group equivariant convo-lutional networks. InInternational conference on machinelearning, pages 2990\u20132999. PMLR, 2016.1[3]Taco S Cohen, Mario Geiger, and Maurice Weiler. A gen-eral theory of equivariant cnns on homogeneous spaces. InProceedings of the 33rd International Conference on NeuralInformation Processing Systems, pages 9145\u20139156, 2019.1[4]Taco S Cohen and Max Welling. Steerable cnns.arXivpreprint arXiv:1612.08498, 2016.1[5]Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-novich. Superpoint: Self-supervised interest point detectionand description. InCVPR Deep Learning for Visual SLAMWorkshop, 2018.2,3,4,7[6]Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Polle-feys, Josef Sivic, Akihiko Torii, and Torsten Sattler. D2-net: A trainable cnn for joint description and detection oflocal features. InProceedings of the ieee/cvf conference oncomputer vision and pattern recognition, pages 8092\u20138101,2019.2,3[7]Martin A Fischler and Robert C Bolles. Random sampleconsensus: a paradigm for model fitting with applications toimage analysis and automated cartography.Communicationsof the ACM, 24(6):381\u2013395, 1981.2[8]Yuhe Jin, Dmytro Mishkin, Anastasiia Mishchuk, Jiri Matas,Pascal Fua, Kwang Moo Yi, and Eduard Trulls. Imagematching across wide baselines: From paper to practice.International Journal of Computer Vision, 129(2):517\u2013547,2021.1,3[9]Axel Barroso Laguna and Krystian Mikolajczyk. Key. net:Keypoint detection by handcrafted and learned cnn filters re-visited.IEEE Transactions on Pattern Analysis and MachineIntelligence, 2022.2[10]Jongmin Lee, Yoonwoo Jeong, and Minsu Cho. Self-supervised learning of image scale and orientation. In31stBritish Machine Vision Conference 2021, BMVC 2021, Vir-tual Event, UK. BMV A Press, 2021.5[11]Jongmin Lee, Byungjin Kim, and Minsu Cho. Self-supervised equivariant learning for oriented keypoint detec-tion. InProceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 4847\u20134857,2022.5[12]Kunhong Li, Longguang Wang, Li Liu, Qing Ran, Kai Xu,and Yulan Guo. Decoupling makes weakly supervised localfeature better. InProceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 15838\u201315848, 2022.3[13]Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,Pietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C LawrenceZitnick. Microsoft coco: Common objects in context. InEuropean conference on computer vision, pages 740\u2013755.Springer, 2014.3[14]Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao,and Xiaowei Zhou. Gift: Learning transformation-invariantdense visual descriptors via group cnns.Advances in NeuralInformation Processing Systems, 32:6992\u20137003, 2019.1,2,3,4,5,9[15]David G Lowe. Distinctive image features from scale-invariant keypoints.International journal of computer vi-sion, 60(2):91\u2013110, 2004.2,4[16]Anastasiia Mishchuk, Dmytro Mishkin, Filip Radenovic,and Jiri Matas. Working hard to know your neighbor\u2019s mar-gins: Local descriptor learning loss. InAdvances in NeuralInformation Processing Systems, pages 4826\u20134837, 2017.3[17]Yuki Ono, Eduard Trulls, Pascal Fua, and Kwang Moo Yi.Lf-net: learning local features from images. InAdvancesin neural information processing systems, pages 6234\u20136244,2018.3,5,7[18]R\u00b4emi Pautrat, Viktor Larsson, Martin R Oswald, and MarcPollefeys. Online invariance selection for local feature de-scriptors. InEuropean Conference on Computer Vision,pages 707\u2013724. Springer, 2020.1,2,3,6[19]Jerome Revaud, Cesar De Souza, Martin Humenberger, andPhilippe Weinzaepfel. R2d2: Reliable and repeatable detec-tor and descriptor.Advances in neural information process-ing systems, 32:12405\u201312415, 2019.2,3[20]Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,Aditya Khosla, Michael Bernstein, et al. Imagenet largescale visual recognition challenge.International journal ofcomputer vision, 115(3):211\u2013252, 2015.4[21]Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz,and Andrew Rabinovich. Superglue: Learning featurematching with graph neural networks. InProceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 4938\u20134947, 2020.4[22]Xuelun Shen, Cheng Wang, Xin Li, Zenglei Yu, JonathanLi, Chenglu Wen, Ming Cheng, and Zijian He. Rf-net: Anend-to-end image matching network based on receptive field.InProceedings of the IEEE Conference on Computer Visionand Pattern Recognition, pages 8132\u20138140, 2019.3,5,7[23]Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, andXiaowei Zhou. Loftr: Detector-free local feature matchingwith transformers. InProceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages8922\u20138931, 2021.4[24]Yurun Tian, Axel Barroso Laguna, Tony Ng, Vassileios Bal-ntas, and Krystian Mikolajczyk. Hynet: Learning local de-scriptor with hybrid similarity measure and triplet loss.Ad-vances in Neural Information Processing Systems, 33:7401\u20137412, 2020.2[25]Yurun Tian, Axel Barroso Laguna, Tony Ng, Vassileios Bal-ntas, and Krystian Mikolajczyk. Hynet: Learning local de-scriptor with hybrid similarity measure and triplet loss. InNeurIPS, 2020.3[26]Maurice Weiler and Gabriele Cesa. General e (2)-equivariantsteerable cnns.Advances in Neural Information ProcessingSystems, 32:14334\u201314345, 2019.1,3,5[27]Maurice Weiler, Fred A Hamprecht, and Martin Storath.Learning steerable filters for rotation equivariant cnns. In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition, pages 849\u2013858, 2018.1[28]Hao Zhou, Torsten Sattler, and David W Jacobs. Evaluatinglocal features for day-night matching. InEuropean Confer-ence on Computer Vision, pages 724\u2013736. Springer, 2016.1,2,6",
            "ref_ids": [
                "1",
                "2",
                "3",
                "4",
                "5",
                "6",
                "7",
                "8",
                "9",
                "10",
                "11",
                "12",
                "13",
                "14",
                "15",
                "16",
                "17",
                "18",
                "19",
                "20",
                "21",
                "22",
                "23",
                "24",
                "25",
                "26",
                "27",
                "28"
            ],
            "1": "Section2evaluates the matchingquality of our proposed method under rotation and illu-mination variations on the day/night image pairs, with de-tails about the benchmark generation.",
            "2": "Section3shows theresults of realistic downstream task on the IMC2021 [8]dataset.",
            "3": "Section4shows the comparisons of computationaloverhead and the number of parameters.",
            "4": "Section5showsdifferent strategies of multiple descriptor extraction usingdominant orientation candidates.",
            "5": "Section6evaluates theexisting feature matching methods in the Roto-360 dataset.",
            "6": "Section7shows the re-training results of GIFT with cyclicrotation augmentation.",
            "7": "Section8shows the matching re-sults with increasing the number of samples of the Roto-360dataset.",
            "8": "Section9presents additional qualitative results tovisualize the consistency of dominant orientation estima-tion, the similarity maps under in-plane rotations of images,and predicted matches on the HPatches and extreme rota-tion (ER) datasets [1,14].",
            "9": "Group equivarianceA feature extractor\u0000is said to be equivariant to a geo-metric transformationTgif transforming an inputx2XbyTgand then passing it through\u0000gives the same result asfirst passingxthrough\u0000and then transforming the result-ing feature map byT0g.",
            "10": "Formally, the equivariance can beexpressed for transformation groupGand\u0000:X!Yas\u0000[Tg(x)] =T0g[\u0000(x)],(1)whereTgandT0grepresent transformations on each space ofa group actiong2G.",
            "11": "IfTtis a translation group(R2,+),andfis a feature mapping functionZ2!RKgiven convo-lution filter weights 2R2\u21e5K, the translation equivarianceof a convolutional operation can be expressed as follows:[Ttf]\u21e4 (x)=[Tt[f\u21e4 ]](x),(2)where\u21e4indicates the convolution operation.",
            "12": "Recent studies [2\u20134,26,27] propose convolutional neuralnetworks that are equivariant to symmetry groups of trans-lation, rotation, and reflection.",
            "13": "The groupGcan be defined byG\u21e0=(R2,+)oHas thesemidirect product of the translation group(R2,+)with therotation groupH.",
            "14": "Then, the rotation-equivariant convolu-tion on groupGcan be defined as:[Tgf]\u21e4 (g)=[Tg[f\u21e4 ]](g),(3)by replacingt2(R2,+)withg2Gin Eq.",
            "15": "2.",
            "16": "Formally, let\u0000={Li|i2{1,2,3,.",
            "17": "For one layerLi2\u0000, the transformationTgis defined asLi[Tg(g)] =Tg[Li(g)],(4)which indicates that the output is preserved afterLiaboutTg.",
            "18": "(5)2.",
            "19": "Experiments inextremerotated day-nightimage matching (ERDNIM)To show the robustness of our method under both ge-ometric and illumination changes, we evaluate the match-ing performance of our method in theextremerotated Day-Night Image Matching (ERDNIM) dataset, which rotatesthe reference images of the RDNIM dataset [18], which isoriginally from the DNIM dataset [28].",
            "20": "1 SIFT SuperPoint D2Net R2D2KeyNet+HyNetGIFT LISRD ours ours*DayHEstimation 0.",
            "21": "064 0.",
            "22": "073 0.",
            "23": "044 0.",
            "24": "085 0.",
            "25": "108 0.",
            "26": "228 0.",
            "27": "2320.",
            "28": "272MMA 0.",
            "29": "049 0.",
            "30": "082 0.",
            "31": "024 0.",
            "32": "054 0.",
            "33": "068 0.",
            "34": "123 0.",
            "35": "2700.",
            "36": "2450.",
            "37": "277NightHEstimation 0.",
            "38": "108 0.",
            "39": "092 0.",
            "40": "002 0.",
            "41": "062 0.",
            "42": "097 0.",
            "43": "151 0.",
            "44": "291 0.",
            "45": "3160.",
            "46": "364MMA 0.",
            "47": "082 0.",
            "48": "111 0.",
            "49": "033 0.",
            "50": "076 0.",
            "51": "093 0.",
            "52": "177 0.",
            "53": "358 0.",
            "54": "3620.",
            "55": "404Table 1.",
            "56": "We use two evaluation metrics: homography estimation accuracy(HEstimation), and mean matching accuracy (MMA) at 3 pixel thresholds.",
            "57": "We usek=4in this experiment.",
            "58": "2.",
            "59": "Data generationThe source dataset DNIM [28] consists of 1722 imagesfrom 17 sequences of a fixed webcam taking pictures at reg-ular time spans over 48 hours.",
            "60": "Therefore, 1,722 image pairs areobtained for each of the day benchmark and night bench-mark, where the day benchmark is composed of day-dayand day-night image pairs, and the night benchmark is com-posed of night-day and night-night image pairs.",
            "61": "To evalu-ate the robustness under geometric transformation, the RD-NIM [18] dataset is generated by warping the target imageof each pair with homographies as in SuperPoint [5] gen-erated with random translations, rotations, scales, and per-spective distortions.",
            "62": "We randomly rotate the refer-ence images in the range[0\u0000,360\u0000).",
            "63": "The number of imagepairs for evaluation remains the same as RDNIM [18].",
            "64": "Fig-ure2shows some examples of ERDNIM image pairs.",
            "65": "2.",
            "66": "2.",
            "67": "Examples of ERDNIM image pairs2.",
            "68": "3.",
            "69": "Evaluation metricsWe use two evaluation metrics, HEstimation andmean matching accuracy (MMA), following LISRD [18].",
            "70": "We measure the homography estimation score [5] usingRANSAC [7] to fit the homography using the predictedmatches.",
            "71": "The predicted homography is considered to becorrect if the average distance between the four corners isless than a threshold: HEstimation=14P4i=1||\u02c6ci\u0000ci||2\uf8ff\u270f, where we use\u270f=3.",
            "72": "MMA [6,19] is the percentage ofthe correct matches over all the predicted matches, wherewe also use 3 pixels as the threshold to determine the cor-rectness of matches.",
            "73": "2.",
            "74": "4.",
            "75": "We compare the descriptor baselines SIFT [15],SuperPoint [5], D2-Net [6], R2D2 [19], KeyNet+HyNet [9,24], GIFT [14], and LISRD [18].",
            "76": "Our proposed model withthe rotation-equivariant network (ReResNet-18) achievesstate-of-the-art performance in terms of homography esti-mation.",
            "77": "GIFT [14], an existing rotation-invariant descriptor, shows a comparatively lower performance on this extremelyrotated benchmark with varying illumination.",
            "78": "Note thatwe use the same dataset generation scheme with the samesource dataset [13] to GIFT [14].",
            "79": "LISRD [18], which selectsviewpoint and illumination invariance online, demonstratesbetter MMA than ours on theDaybenchmark, but ours*which extracts top-kcandidate descriptors shows the bestMMA and homography estimation on bothDayandNightbenchmarks.",
            "80": "3.",
            "81": "Stereo track# kptsmAA 5\u0000mAA 10\u0000# inliersSuperPoint10240.",
            "82": "259 0.",
            "83": "348 61.",
            "84": "9GIFT10240.",
            "85": "2920.",
            "86": "39470.",
            "87": "8ours10240.",
            "88": "305 0.",
            "89": "404 99.",
            "90": "8SuperPoint20480.",
            "91": "263 0.",
            "92": "358 73.",
            "93": "9GIFT20480.",
            "94": "313 0.",
            "95": "42098.",
            "96": "6ours20480.",
            "97": "2960.",
            "98": "403118.",
            "99": "5Table 2.",
            "100": "Results of the downstream task in IMC2021 [8].",
            "101": "In Table2, we evaluate on IMC 2021 stereo track [8]using the validation set of PhotoTourism and PragueParksto show the results on a realistic downstream task.",
            "102": "Ourdescriptor consistently performs better than SuperPoint [5]descriptors under varying number of keypoints, and obtainscomparable results with GIFT [14] descriptors.",
            "103": "This showsthat our method performs similarly for the general and non-planar transformations, while it significantly outperformsexisting methods on Roto-360 and RDNIM datasets withextreme rotation transformations.",
            "104": "Note that it is also possi-ble to use image pairs with GT annotations of intrinsic andextrinsic parameters byapproximatingthe 2D relative ori-entation for our training1, and we leave this for future.",
            "105": "4.",
            "106": "Computational overhead and the number ofparameters4.",
            "107": "Computational overheadTable3compares an average of the inference time andGPU usage with other descriptor extraction methods above.",
            "108": "1The details of obtaining the rotation from a homography can be foundin Section 2 of \u201cDeeper understanding of the homography decomposition(Malis and Vargas, 2007)\u201d.",
            "109": "methodspeed (ms) GPU usage (GB)ours147.",
            "110": "45.",
            "111": "21 GBours\u2020206.",
            "112": "4 4.",
            "113": "83 GBSuperPoint [5]66.",
            "114": "0 2.",
            "115": "35 GBGIFT [14]198.",
            "116": "8 2.",
            "117": "93 GBLISRD [18]781.",
            "118": "0 2.",
            "119": "85 GBPosFeat [12]208.",
            "120": "8 4.",
            "121": "67 GBTable 3.",
            "122": "(Section 2.",
            "123": "8 of [26])4.",
            "124": "2.",
            "125": "6Mours\u20202.",
            "126": "6MGIFT [14]0.",
            "127": "4MLISRD [18]3.",
            "128": "7MPosFeat [12]21.",
            "129": "1MHardNet [16]9.",
            "130": "0MHyNet [25]1.",
            "131": "3MSuperPoint [5]1.",
            "132": "3MLF-Net [17]2.",
            "133": "6MRF-Net [22]1.",
            "134": "4MD2-Net [6]7.",
            "135": "6MR2D2 [19]0.",
            "136": "5MThe right table shows thenumber of parameters in mil-lions, where the first group(top) are descriptor-only mod-els and the second group(bottom) are joint detectionand description models.",
            "137": "When usingour model with the deeperbackbone denoted denoted byours\u2020, the number of model parameters increases, but itdoes not increase significantly compared to other compar-ison groups, where is still similar to that of LF-Net [17].",
            "138": "5.",
            "139": "Elaboration of multiple descriptor extrac-tionIn this section, we show the results of different configu-rations of the multiple descriptor extraction scheme whichwas mentioned in Section 4.",
            "140": "3, Table 3, Table 4, and Table 6of the main paper.",
            "141": "Table4shows the results with different strategies formultiple descriptor extraction on the Roto-360 dataset.",
            "142": "6 selects mul-tiple candidates dynamically, where the total number ofcandidates is similar to using top-2 candidates, but theMMA@5px is as high as using top-3 candidates which cand.",
            "143": "Roto-360@5px @3pxpred.",
            "144": "top191.",
            "145": "35 90.",
            "146": "18688 1161top292.",
            "147": "31 91.",
            "148": "191315 2322top392.",
            "149": "82 91.",
            "150": "692012 34830.",
            "151": "892.",
            "152": "25 91.",
            "153": "13951 16600.",
            "154": "692.",
            "155": "82 91.",
            "156": "691333 2340Table 4.",
            "157": "Note that this multi-ple descriptor extraction scheme is largely inspired by theclassical method based on an orientation histogram such asSIFT [15].",
            "158": "6.",
            "159": "Comparison with feature matching methodsmethodRoto-360@5px @3pxpred.",
            "160": "ours+NN91.",
            "161": "4 90.",
            "162": "2688.",
            "163": "3SP+SG [5,21]30.",
            "164": "1 29.",
            "165": "8874.",
            "166": "1LoFTR [23]18.",
            "167": "8 15.",
            "168": "9509.",
            "169": "4Table 5.",
            "170": "Comparison with keypoint matching methods on theRoto-360 dataset.",
            "171": "Table5compares the feature matching methods toour descriptors with simple nearest neighbour matching(NN) algorithm.",
            "172": "We evaluate our local feature with near-est neighbour matching (ours+NN) and compare it withSuperGlue [21](i.",
            "173": ", SuperPoint+SuperGlue [5,21]) andLoFTR [23].",
            "174": "The results with the simple matching algo-rithm of ours+NN clearly outperforms the two other meth-ods on the extremely rotated examples of the Roto-360dataset.",
            "175": "Note, however, that both SuperGlue [21] andLoFTR [23] are for featurematchingand thus are not di-rectly comparable to our method for featureextraction.",
            "176": "7.",
            "177": "Changing the rotation range of the GIFTTable6shows that GIFT* does not improve perfor-mance on the Roto-360 dataset because the bilinear pool-ing of GIFT does not guarantee invariance for rotation.",
            "178": "This is because our group aligning computes invariant fea-tures without breaking any equivariance, in contrast toGIFT [14] whose bilinear pooling violates group equivari-ance due to their inter-group interaction from the3\u21e53con-methodRoto-3605px 3pxpred.",
            "179": "ours91.",
            "180": "35 90.",
            "181": "18688.",
            "182": "3GIFT42.",
            "183": "05 41.",
            "184": "59589.",
            "185": "2GIFT*40.",
            "186": "71 40.",
            "187": "27564.",
            "188": "2Table 6.",
            "189": "The result of re-training the GIFT [14] model by re-placing the rotation group with 360-degree cyclic.",
            "190": "GIFT* de-notes a retrained model by extending the rotation sampling intervalfrom -180\u0000to 180\u0000.",
            "191": "8.",
            "192": "The number of sampled images for Roto-360# sample10 100 1KAlign91.",
            "193": "4 80.",
            "194": "0 89.",
            "195": "9Avg82.",
            "196": "1 72.",
            "197": "3 80.",
            "198": "7Max78.",
            "199": "0 69.",
            "200": "3 79.",
            "201": "2None18.",
            "202": "8 16.",
            "203": "4 20.",
            "204": "5Bilinear41.",
            "205": "0 28.",
            "206": "5 43.",
            "207": "7Table 7.",
            "208": "Results on Roto-360 constructed using a differentnumber of source images.",
            "209": "Table7shows the mean matching accuracy (MMA) at 5pixels threshold when increasing the number of source im-ages to 100 images (3,600 pairs) and 1,000 images (36,000pairs).",
            "210": "Therefore, we use 10 samples as they are sufficient to mea-sure the relative rotation robustness of the local features.",
            "211": "9.",
            "212": "Additional qualitative results9.",
            "213": "Visualization of the consistency of orientationestimationWe provide more examples for Figure 5 of the main pa-per, which visualize the consistency of orientation estima-tion.",
            "214": "To visualize Figure3,we create a sequence of480\u21e5640images augmented byrandom in-plane rotation with Gaussian noise sourced byILSVRC2012 [20].",
            "215": "Figure3shows the qualitative com-parison of the estimated orientation consistency.",
            "216": ",the dominant orientation es-timation is consistent if the difference with the ground-truthrotation is within a30\u0000threshold.",
            "217": "Our rotation-equivariantmodel trained with the orientation alignment loss inspiredby [10,11] consistently estimates more correct keypoint ori-entations than LF-Net [17] and RF-Net [22].",
            "218": "9.",
            "219": "2.",
            "220": "Visualization of the similarity maps of a key-point under varying rotationsFigure4shows the similarity maps with respect to a key-point under varying rotations of images with a resolutionof180\u21e5180, with uniform rotation intervals of45\u0000.",
            "221": "We vi-sualize 5 locations with the highest similarity scores withthe query keypoint for better visibility.",
            "222": "Our descriptor lo-calizes the correct keypoint locations more precisely com-pared to GIFT [14] and LF-Net [17].",
            "223": "Specifically, althoughGIFT [14] uses group-equivariant features constructed us-ing rotation augmentation, their descriptor fails to locatethe corresponding keypoints accurately in rotated imageswhich shows that the explicit rotation-equivariant net-works [26] yield better rotation-invariant features than con-structing the group-equivariance features with image aug-mentation [14].",
            "224": "9.",
            "225": "3.",
            "226": "Visualization of the predicted matches on theextreme rotationFigure5visualize the predicted matches on the ERdataset [14].",
            "227": "We extract a maximum of 1,500 keypointsfrom each image and find matches using the mutual near-est neighbor algorithm.",
            "228": "The results show that our methodconsistently finds matches more accurately compared toGIFT [14] and LF-Net [17].",
            "229": "9.",
            "230": "4.",
            "231": "Visualization of the predicted matches on theHPatches viewpointFigure6visualize the predicted matches on theHPatches [1] viewpoint variations We extract a maximumof 1,500 keypoints from each image and find matches usingthe mutual nearest neighbor algorithm.",
            "232": "The results showthat our method consistently finds matches more accuratelycompared to GIFT [14] and LF-Net [17].",
            "233": "ReferenceTargetReferenceTargetFigure 2.",
            "234": "Example of ERDNIM image pairs augmented from [18,28].",
            "235": "The reference image of a pair is augmented with random rotation in the range[0\u0000,360\u0000), and the target image is augmented byhomographies generated with random translation, rotation, scale, perspective distortion.",
            "236": "SourceTargetAligned viewSourceTargetAligned viewLF-NetRF-NetoursLF-NetRF-NetoursFigure 3.",
            "237": "We extract the source keypoints using SuperPoint [5] andobtain the target keypoints using GT homography.",
            "238": "We evaluate the consistency of orientation estimation by comparing the relative angledifference and the ground-truth angle at a threshold of30\u0000.",
            "239": "Our methodpredicts more consistent orientations of keypoints compared to LF-Net [17] and RF-Net [22].",
            "240": "0\u00ba45\u00ba90\u00ba135\u00ba180\u00ba225\u00ba270\u00ba315\u00ba oursGIFTLF-NetInputoursGIFTLF-NetInputoursGIFTLF-NetInputFigure 4.",
            "241": "Forbetter visibility, we visualize the top 5 pixels with the highest similarity to the keypoints.",
            "242": "(a) ours(b) GIFT(c) LF-Net Figure 5.",
            "243": "Visualization of predicted matches in the ER dataset [14].",
            "244": "We use a maximum of 1,500 keypoints for matching by the mutualnearest neighbor algorithm.",
            "245": "(a) ours(b) GIFT(c) LF-Net Figure 6.",
            "246": "We use a maximum of 1,500 keypoints, the mutualnearest neighbor matcher, and a three-pixel threshold for correctness.",
            "247": "In this experiment, we use the rotation-equivariant WideResNet16-8(ReWRN) backbone, which is \u2018ours\u2020\u2019 in table 4 of the main paper.",
            "248": "[1]Vassileios Balntas, Karel Lenc, Andrea Vedaldi, and Krys-tian Mikolajczyk.",
            "249": "InProceed-ings of the IEEE Conference on Computer Vision and PatternRecognition, pages 5173\u20135182, 2017.",
            "250": "1,5[2]Taco Cohen and Max Welling.",
            "251": "InInternational conference on machinelearning, pages 2990\u20132999.",
            "252": "PMLR, 2016.",
            "253": "1[3]Taco S Cohen, Mario Geiger, and Maurice Weiler.",
            "254": "InProceedings of the 33rd International Conference on NeuralInformation Processing Systems, pages 9145\u20139156, 2019.",
            "255": "1[4]Taco S Cohen and Max Welling.",
            "256": "arXivpreprint arXiv:1612.",
            "257": "08498, 2016.",
            "258": "1[5]Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-novich.",
            "259": "InCVPR Deep Learning for Visual SLAMWorkshop, 2018.",
            "260": "2,3,4,7[6]Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Polle-feys, Josef Sivic, Akihiko Torii, and Torsten Sattler.",
            "261": "D2-net: A trainable cnn for joint description and detection oflocal features.",
            "262": "InProceedings of the ieee/cvf conference oncomputer vision and pattern recognition, pages 8092\u20138101,2019.",
            "263": "2,3[7]Martin A Fischler and Robert C Bolles.",
            "264": "Communicationsof the ACM, 24(6):381\u2013395, 1981.",
            "265": "2[8]Yuhe Jin, Dmytro Mishkin, Anastasiia Mishchuk, Jiri Matas,Pascal Fua, Kwang Moo Yi, and Eduard Trulls.",
            "266": "International Journal of Computer Vision, 129(2):517\u2013547,2021.",
            "267": "1,3[9]Axel Barroso Laguna and Krystian Mikolajczyk.",
            "268": "IEEE Transactions on Pattern Analysis and MachineIntelligence, 2022.",
            "269": "2[10]Jongmin Lee, Yoonwoo Jeong, and Minsu Cho.",
            "270": "In31stBritish Machine Vision Conference 2021, BMVC 2021, Vir-tual Event, UK.",
            "271": "BMV A Press, 2021.",
            "272": "5[11]Jongmin Lee, Byungjin Kim, and Minsu Cho.",
            "273": "InProceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 4847\u20134857,2022.",
            "274": "5[12]Kunhong Li, Longguang Wang, Li Liu, Qing Ran, Kai Xu,and Yulan Guo.",
            "275": "InProceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 15838\u201315848, 2022.",
            "276": "3[13]Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,Pietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C LawrenceZitnick.",
            "277": "InEuropean conference on computer vision, pages 740\u2013755.",
            "278": "Springer, 2014.",
            "279": "Advances in NeuralInformation Processing Systems, 32:6992\u20137003, 2019.",
            "280": "1,2,3,4,5,9[15]David G Lowe.",
            "281": "International journal of computer vi-sion, 60(2):91\u2013110, 2004.",
            "282": "2,4[16]Anastasiia Mishchuk, Dmytro Mishkin, Filip Radenovic,and Jiri Matas.",
            "283": "InAdvances in NeuralInformation Processing Systems, pages 4826\u20134837, 2017.",
            "284": "3[17]Yuki Ono, Eduard Trulls, Pascal Fua, and Kwang Moo Yi.",
            "285": "InAdvancesin neural information processing systems, pages 6234\u20136244,2018.",
            "286": "3,5,7[18]R\u00b4emi Pautrat, Viktor Larsson, Martin R Oswald, and MarcPollefeys.",
            "287": "InEuropean Conference on Computer Vision,pages 707\u2013724.",
            "288": "Springer, 2020.",
            "289": "1,2,3,6[19]Jerome Revaud, Cesar De Souza, Martin Humenberger, andPhilippe Weinzaepfel.",
            "290": "R2d2: Reliable and repeatable detec-tor and descriptor.",
            "291": "Advances in neural information process-ing systems, 32:12405\u201312415, 2019.",
            "292": "2,3[20]Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,Aditya Khosla, Michael Bernstein, et al.",
            "293": "International journal ofcomputer vision, 115(3):211\u2013252, 2015.",
            "294": "4[21]Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz,and Andrew Rabinovich.",
            "295": "InProceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 4938\u20134947, 2020.",
            "296": "4[22]Xuelun Shen, Cheng Wang, Xin Li, Zenglei Yu, JonathanLi, Chenglu Wen, Ming Cheng, and Zijian He.",
            "297": "InProceedings of the IEEE Conference on Computer Visionand Pattern Recognition, pages 8132\u20138140, 2019.",
            "298": "InProceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages8922\u20138931, 2021.",
            "299": "4[24]Yurun Tian, Axel Barroso Laguna, Tony Ng, Vassileios Bal-ntas, and Krystian Mikolajczyk.",
            "300": "Ad-vances in Neural Information Processing Systems, 33:7401\u20137412, 2020.",
            "301": "2[25]Yurun Tian, Axel Barroso Laguna, Tony Ng, Vassileios Bal-ntas, and Krystian Mikolajczyk.",
            "302": "InNeurIPS, 2020.",
            "303": "3[26]Maurice Weiler and Gabriele Cesa.",
            "304": "General e (2)-equivariantsteerable cnns.",
            "305": "Advances in Neural Information ProcessingSystems, 32:14334\u201314345, 2019.",
            "306": "1,3,5[27]Maurice Weiler, Fred A Hamprecht, and Martin Storath.",
            "307": "In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition, pages 849\u2013858, 2018.",
            "308": "1[28]Hao Zhou, Torsten Sattler, and David W Jacobs.",
            "309": "InEuropean Confer-ence on Computer Vision, pages 724\u2013736.",
            "310": "Springer, 2016.",
            "311": "1,2,6\n."
        },
        "Generating Aligned Pseudo-Supervision from Non-Aligned Data for Image Restoration in Under-Display Camera-Supplementary Material": {
            "authors": [],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/supplemental/Feng_Generating_Aligned_Pseudo-Supervision_CVPR_2023_supplemental.pdf",
            "ref_texts": "[12] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In CVPR , 2021. 4",
            "ref_ids": [
                "12"
            ],
            "1": "Thus, we indirectly measure the displacement error with LoFTR [12] that serves as a keypoint matcher."
        },
        "Supplementary Material for Adaptive Assignment for Geometry Aware Local Feature Matching": {
            "authors": [],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/supplemental/Huang_Adaptive_Assignment_for_CVPR_2023_supplemental.pdf",
            "ref_texts": "[13] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR , pages 8922\u20138931, 2021. 2",
            "ref_ids": [
                "13"
            ],
            "1": "830 LoFTR-OT [13] 0.",
            "2": "836 LoFTR-DS [13] 0.",
            "3": "MethodsPose Estimation AUC\n@5\u25e6@10\u25e6@20\u25e6 LoFTR-DS [13] 43.",
            "4": "We use the test split with 1500 image pairs following the experimental setting of [4, 12, 13].",
            "5": "To align with the existing methods [4,13,14], we resized all test images to 480\u00d7640.",
            "6": "3, AdaMatcher has a significant performance improvement on different baselines [4, 13, 14].",
            "7": "Computational Costs of Feature Interaction We evaluate the computation and parameters between LoFTR\u2019s feature interaction module [13] and our CFI mod-MethodsPose Estimation AUC\n@5\u25e6@10\u25e6@20\u25e6 LoFTR-OT [13] 15.",
            "8": "87 LoFTR-DS [13] 17."
        },
        "GPR-Net: Multi-view Layout Estimation via a Geometry-aware Panorama Registration Network Supplemental Material": {
            "authors": [],
            "url": "https://openaccess.thecvf.com/content/CVPR2023W/OmniCV/supplemental/Su_GPR-Net_Multi-View_Layout_CVPRW_2023_supplemental.pdf",
            "ref_texts": "[5] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR, 2021. 1",
            "ref_ids": [
                "5"
            ],
            "1": "More baselines comparison on the panorama registration In Table 1, we report more competing baselines on the panorama registration, including DirectionNet [1] and LoFTR [5]\n+ OpenMVG [4] following the setting in the CoVisPose [2]."
        },
        "DynamicStereo: Consistent Dynamic Depth from Stereo Videos Supplementary Material": {
            "authors": [],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/supplemental/Karaev_DynamicStereo_Consistent_Dynamic_CVPR_2023_supplemental.pdf",
            "ref_texts": "[4] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. CVPR , 2021. 1",
            "ref_ids": [
                "4"
            ],
            "1": "Then we apply linear selfattention [4] across space to both tensors and cross attention across space between left and right tensors."
        },
        "PATS: Patch Area Transportation with Subdivision for Local Feature Matching\u2013Supplementary Material\u2013": {
            "authors": [],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/supplemental/Ni_PATS_Patch_Area_CVPR_2023_supplemental.pdf",
            "ref_texts": "[10] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8922\u20138931, 2021. 3",
            "ref_ids": [
                "10"
            ],
            "1": "However, our method and other detector-free methods [1, 10] directly regress matches from a pair of images and thus do not obtain repeatable keypoints across multiple images.",
            "2": "Specifically, To address the problem, previous works [10,13] either takes average among a patch or takes the most plausible point positions as the keypoints."
        },
        "Adaptive Spot-Guided Transformer for Consistent Local Feature Matching Supplementary Material": {
            "authors": [],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/supplemental/Yu_Adaptive_Spot-Guided_Transformer_CVPR_2023_supplemental.pdf"
        },
        "Detector-Free Dense Feature Matching for Fetoscopic Mosaicking": {
            "authors": [],
            "url": "https://discovery.ucl.ac.uk/id/eprint/10157679/1/HSMR_LOFTR_SB.pdf",
            "ref_texts": "[4] J. Sun, Z. Shen, Y. Wang, H. Bao, and X. Zhou, \u201cLoFTR: Detector-free local feature matching with transformers,\u201d in Conference on Computer Vision and Pattern Recognition , 2021, pp.",
            "ref_ids": [
                "4"
            ],
            "1": "In the paper, we propose the use of transformer-based detector-free local feature matching (LoFTR) method [4] as a dense feature matching technique for creating reliable mosaics with minimal drifting error.",
            "2": "Detector-Free Feature Representation The recently proposed LoFTR [4] method first establish pixel-wise dense matches at a coarse level and later refine the good matches at a fine level.",
            "3": "For more detail, please refer to [4], in which it is shown that LoFTR produces high-quality matches even in regions having low-textures and are affected by motion blur or repetitive patterns; making it an ideal matching module for fetoscopic mosaicking.",
            "4": "The LoFTR matching model, pretrained on the ScanNet dataset [4], is used for obtaining the fine-level matches between two consecutive frames.",
            "5": "CONCLUSIONS We propose a fetoscopic video mosaicking method that benefited from the detector-free feature matching with transformers (LoFTR) [4] method, resulting in generating reliable virtual expanded field-of-view image of the intraoperative fetoscopic environment.",
            "6": "[4] J."
        },
        "Supplementary Material for Is Geometry Enough for Matching in Visual Localization?": {
            "authors": [],
            "url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700402-supp.pdf",
            "ref_texts": "24. Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 8922\u20138931, 2021.",
            "ref_ids": [
                "24"
            ],
            "1": "This metric was inspired by the pose error area under the cummulative curve used in [20, 21, 24]."
        },
        "\u57fa\u4e8e\u6539\u8fdb\u7684\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u7684\u53ef\u89c1\u5149\u7ea2\u5916\u56fe\u50cf\u8f6c\u6362\u7b97\u6cd5": {
            "authors": [],
            "url": "https://www.researching.cn/ArticlePdf/m00009/2023/52/4/0410003.pdf"
        },
        "Re-ranking methods for visual geo-localization with domain shift": {
            "authors": [],
            "url": "https://webthesis.biblio.polito.it/secure/26774/1/tesi.pdf",
            "ref_texts": "[70] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. \u00abLoFTR: Detector-Free Local Feature Matching with Transformers\u00bb. In: CVPR(2021).",
            "ref_ids": [
                "70"
            ],
            "1": "2 LoFTR LoFTR[70]isamethodthatconsistsoflocalfeatureextractionandmatching while skipping the detection phase that existing methods like [71, 72] use.",
            "2": "Image from [70]\n4."
        },
        "Supplementary Materials CTrGAN: Cycle Transformers GAN for Gait Transfer": {
            "authors": [],
            "url": "https://openaccess.thecvf.com/content/WACV2023/supplemental/Mahpod_CTrGAN_Cycle_Transformers_WACV_2023_supplemental.pdf",
            "ref_texts": "[2] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021 , pages 8922\u20138931. Computer Vision Foundation / IEEE, 2021.",
            "ref_ids": [
                "2"
            ],
            "1": "In order to reduce processing power, we use linear attention [1, 2] instead of Transformer\u2019s original attention."
        },
        "Learning Soft Estimator of Keypoint Scale and Orientation with Probabilistic Covariant Loss-Supplementary Material": {
            "authors": [],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/supplemental/Yan_Learning_Soft_Estimator_CVPR_2022_supplemental.pdf",
            "ref_texts": "[15] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the Conference on Computer Vision and Pattern Recognition , pages 8922\u2013",
            "ref_ids": [
                "15"
            ]
        },
        "Sym\u00e9trie U (1) et brisure de sym\u00e9trie dans les couches d'activation de r\u00e9seaux de neurones convolutifs profonds": {
            "authors": [],
            "url": "https://espace.etsmtl.ca/id/eprint/3094/1/BOUCHARD_Louis_Fran%C3%A7ois.pdf",
            "ref_texts": "86 Sun, J., Shen, Z., Wang, Y., Bao, H. & Zhou, X. (2021). LoFTR : Detector-Free Local Feature Matching with Transformers. arXiv preprint arXiv :2104.00680 . Sung, F., Yang, Y., Zhang, L., Xiang, T., Torr, P. H. & Hospedales, T. M. (2018). Learning to compare : Relation network for few-shot learning. Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 1199\u20131208. Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J. & Wojna, Z. (2016). Rethinking the inception architecture for computer vision. Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 2818\u20132826. Szegedy, C., Ioffe, S., Vanhoucke, V. & Alemi, A. A. (2017). Inception-v4, inception-resnet and the impact of residual connections on learning. Thirty-first AAAI conference on artificial intelligence . Tan, M. & Le, Q. V. (2019). EfficientNet : Rethinking Model Scaling for Convolutional Neural Networks. CoRR , abs/1905.11946. Rep\u00e9r\u00e9 \u00e0 http://arxiv.org/abs/1905.11946. Tanaka, H. & Kunin, D. (2021). Noether\u2019s Learning Dynamics : Role of Symmetry Breaking in Neural Networks. Advances in Neural Information Processing Systems , 34. Tang, Y., Han, K., Guo, J., Xu, C., Li, Y., Xu, C. & Wang, Y. (2021). An image patch is a wave : Phase-aware vision mlp. arXiv preprint arXiv :2111.12294 . Tang, Y., Han, K., Guo, J., Xu, C., Li, Y., Xu, C. & Wang, Y. (2022). An image patch is a wave : Phase-aware vision mlp. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 10935\u201310944. Tian, C., Xu, Y., Zuo, W., Lin, C.-W. & Zhang, D. (2021). Asymmetric CNN for image superresolution. IEEE Transactions on Systems, Man, and Cybernetics : Systems . Toews, M. & Wells, W. (2009, 06). SIFT-Rank : Ordinal description for invariant feature correspondence. 0, 172-177. doi : 10.1109/CVPRW.2009.5206849. Tolias, G., Sicre, R. & J\u00e9gou, H. (2016). Particular object retrieval with integral max-pooling of CNN activations. Trockman, A. & Kolter, J. Z. (2021). Orthogonalizing convolutional layers with the cayley transform. arXiv preprint arXiv :2104.07167 . Tseng, H.-Y., Lee, H.-Y., Huang, J.-B. & Yang, M.-H. (2020). Cross-domain few-shot classification via learned feature-wise transformation. arXiv preprint arXiv :2001.08735 ."
        },
        "Visual Localization with Environment Outline Prior": {
            "authors": [],
            "url": "https://v-pnk.github.io/data/Panek2022POSTER.pdf",
            "ref_texts": "[25] Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detectorfree local feature matching with transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021)",
            "ref_ids": [
                "25"
            ],
            "1": "The coarse estimate is then refined in the second step by performing local image features matching [17,22,25] between the query image and the triangulated points followed by camera pose estimation.",
            "2": "In: CVPR (2016)\n[25] Sun, J."
        },
        "AutoRecon: Automated 3D Object Discovery and Reconstruction Supplemental Material": {
            "authors": [],
            "url": "https://zju3dv.github.io/autorecon/files/autorecon_supp.pdf",
            "ref_texts": "[12] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In CVPR , 2021. 1",
            "ref_ids": [
                "12"
            ],
            "1": "Preprocessing The SfM point clouds built with LoFTR [12] matches are dense but noisy and usually contain too many points for NCut to process."
        },
        "EagleAI: Estimation of Attitude Geo-localizing Landmarks on Earth": {
            "authors": [],
            "url": "https://webthesis.biblio.polito.it/secure/26817/1/tesi.pdf",
            "ref_texts": "[29]Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8922\u20138931, 2021.",
            "ref_ids": [
                "29"
            ],
            "1": "LoFTR LoFTR (Detector-Free Local Feature Matching with Transformers) [29] is a novel area-based method for image matching.",
            "2": "About keypoint matching techniques, instead, two algorithms that might provide remarkable results if properly trained are, as already mentioned, SuperGlue [26] and LoFTR [29] networks."
        },
        "\u4f1a\u8b70\u5831\u544a: IEEE/CVF International Conference on Computer Vision and Pattern Recognition (CVPR 2021)": {
            "authors": [],
            "url": "https://www.jstage.jst.go.jp/article/jjsai/36/6/36_798/_pdf",
            "ref_texts": "[Sun 21b] Sun, J., Shen, Z., Wang, Y., Bao, H. and Zhou, X.: LoFTR: Detector-free local feature matching with transformers, Proc. IEEE/CVF Computer Vision and Pattern Recognition\u02a2 CVPR\u02a3\u02a22021\u02a3",
            "ref_ids": [
                "Sun 21b"
            ],
            "1": "Transformer for Visual Task(\u4e2d\u5d8b) Transformer [Vaswani 17]\u0972\u0f67\u0dfc\u0efa\u0370\n\u0398\u0395\u036f\u034d\u0394\u027d\u03af\u03ef\u03d0\u03e1\u0294\u03bb\u03cf\u03b4\u03e3\u03ef\u0dfc\u0efa\u0370\u0378\u027c Detection Transformer \u02a2DETR\u02a3\u0355 Object Detection \u0370\u038b\u0f17\u0f3b\n\u035e\u0395\u0368 [Carion 20] \u035c\u0371\u039b\u0356\u036c\u0354\u035a\u0374\n\u035e\u0387\u035f\u0387\u0373\u03bb\u03b5\u03ab\u0374\u0c26\u0f3b\u035e\u0395\u0394\u0391\u034f\u0374\u0373\u036c\u0368\u027d\u035c\u0377\u0f76\n\u0395\u0374\u09d0\u0394\u0391\u034f\u0374 CVPR 2021 \u0370\u0378\u027cTransformer \u039b\u035e\n\u0680\u075a[Chen 21a, Kim \n21, Sun 21b, Wang 21c, Wang 21f, Zheng 21]\u075f\n\u0b2e\u0c24\u0374\u0378\u027cSemantic Segmentation [Zheng \n21]\u038dObject Tracking [Wang 21c] \u027cVideo Instance Segmentation [Wang 21f] \u027cFeature Matching [Sun \n21b]\u027c Human-Object Interaction Detection [Kim 21]\n\u038b\u0568\u0afe\u0354\u0392 CNN\u0370\u0c9b\u0bc3\u0ba8\u095a\u039b\n\u034d\u027cTransformer-Encoder\u0362\u0394\u03c9\u03bf\u03c4\n\u0b04\u0370\u034b\u0394\u0374\u038b\u0354\u0354\u0398\u0392\u0363\u027c\u0948\u0f52\u0916\u0e4f\u039b\u0b47\u0356\u0358\u09cd\u0573\u0394\u0a51\u0cf3\u0355\u0c98\u0392\u0395\u0394\u035c\u0371\u0354\u0392Transformer\u035e\u0355\u034f\u0354\u0355\u0351\u0394\u027d\n\u035e\u0392\u0374\u027c 2020\u0d52\u027c Vision Transformer [Dosovitskiy \n \n21]\u0370\u0378\u03d7\u03bf\u03c4\u03c4\u03d0\u03bf\u03ab\u0377\u04b0\u036d\u0374\n\u0589\u035e\u0395\u036f\u034d\u0394\u027d\u0366\u0377\u0368\u038a\u027c\u0fe6\u0e08\u0b0e\u0e43\u03b1\u03a0\u03c4\u0370\u034b\u0394 arXiv\u035d\u0371\u0377\u03a6\u0294\n \n\u0f89\u0374\u0ee8\u039b\u0d11\u0394\u0d9e\u0f41\u0355\u034b\u0394\u027d\n7.",
            "2": "IEEE/CVF Computer Vision and Pattern Recognition \u02a2CVPR\u02a3\u02a22021\u02a3\n[Sun 21b] Sun, J."
        }
    }
}