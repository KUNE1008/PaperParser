{
    "title": "PVNet: Pixel-wise Voting Network for 6DoF Object Pose Estimation",
    "id": 96,
    "valid_pdf_number": "243/548",
    "matched_pdf_number": "170/243",
    "matched_rate": 0.6995884773662552,
    "citations": {
        "Gdr-net: Geometry-guided direct regression network for monocular 6d object pose estimation": {
            "authors": [
                "Gu Wang",
                "Fabian Manhardt",
                "Federico Tombari",
                "Xiangyang Ji"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_GDR-Net_Geometry-Guided_Direct_Regression_Network_for_Monocular_6D_Object_Pose_CVPR_2021_paper.pdf",
            "ref_texts": ""
        },
        "Pvn3d: A deep point-wise 3d keypoints voting network for 6dof pose estimation": {
            "authors": [
                "Yisheng He",
                "Wei Sun",
                "Haibin Huang",
                "Jianran Liu",
                "Haoqiang Fan",
                "Jian Sun"
            ],
            "url": "https://openaccess.thecvf.com/content_CVPR_2020/papers/He_PVN3D_A_Deep_Point-Wise_3D_Keypoints_Voting_Network_for_6DoF_CVPR_2020_paper.pdf",
            "ref_texts": "[37] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019. 1,2,4,5,6,7",
            "ref_ids": [
                "37"
            ],
            "1": "However, these methods usually had poor generalization due to the nonlinearity of the rotation space explained by [37].",
            "2": "Instead, recent works utilized DNNs to detect 2D keypoints of an object, and computed 6D pose parameters with Perspectiven-Point (PnP) algorithms [37,36,41,47].",
            "3": "To better deal with truncated and occluded scenes, [37] proposes a pixel-wise voting network to vote for the 2D keypoints location.",
            "4": "PVNet [37] uses per-pixel voting for 2D Keypoints to combine the advantages of Dense methods and keypoint-based methods.",
            "5": "Therefore, we follow [37] and use the farthest point sampling (FPS) algorithm to select keypoints on the mesh.",
            "6": "Also, we follow [37] and add synthesis images into our training set.",
            "7": "4%\n11636\n RGB RGBD PoseCNN DeepIM\n[26,52]PVNet [37]CDPN\n[27]Implicit ICP[45]SSD-6D ICP[22]PointFusion[50]DF(perpixel)[50]DF(iterative)[50]PVN3D ape 77.",
            "8": "DF(RT)[50] DF(3D KP)[50] Ours(RT) Ours(2D KPC) Ours(2D KP) PVNet[37] Ours(Corr) Ours(3D KP) ADD-S 92.",
            "9": "Note that other existing 2D keypoints detection approaches, such as heatmap [33,24,34] and vector voting [37] models may also suffer from overlapped keypoints.",
            "10": "1\n[37] S."
        },
        "Ffb6d: A full flow bidirectional fusion network for 6d pose estimation": {
            "authors": [
                "Yisheng He",
                "Haibin Huang",
                "Haoqiang Fan",
                "Qifeng Chen",
                "Jian Sun"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2021/papers/He_FFB6D_A_Full_Flow_Bidirectional_Fusion_Network_for_6D_Pose_CVPR_2021_paper.pdf",
            "ref_texts": "[46] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019. 1, 2,5,7",
            "ref_ids": [
                "46"
            ],
            "1": "Recently, the dramatic growth of deep learning techniques motivates several works to tackle this problem using convolution neural networks (CNNs) on RGB images [68,46,70,34].",
            "2": "Instead, 2Dkeypoint-based [53,52,42,29,43,46,72,38] detect 2D keypoints of objects to build the 2D-3D correspondence for pose estimation.",
            "3": "Keypoint selection Previous works [46,17] select keypoints from the target object surface using the Farthest Point Sampling (FPS) algorithm.",
            "4": "In this way, the selected keypoints spread on the object surface and stabilize the following pose estimation procedure [46,17].",
            "5": "We split the training and testing set following previous works [68,46] and generate synthesis images for training following [46,17].",
            "6": "1d) as in [20,46].",
            "7": "Qualitative results are reported in 3008\n RGB RGB-D PoseCNN DeepIM\n[68,33]PVNet[46] CDPN[34] DPOD[70] PointFusion[69]DenseFusion[65]G2LNet[7]PVN3D[17] Our FFB6D MEAN 88.",
            "8": "[26]Pix2Pose [45]PVNet [46] ADD-0."
        },
        "Deep snake for real-time instance segmentation": {
            "authors": [
                "Sida Peng",
                "Wen Jiang",
                "Huaijin Pi",
                "Xiuli Li",
                "Hujun Bao",
                "Xiaowei Zhou"
            ],
            "url": "https://openaccess.thecvf.com/content_CVPR_2020/papers/Peng_Deep_Snake_for_Real-Time_Instance_Segmentation_CVPR_2020_paper.pdf",
            "ref_texts": "[33] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 3",
            "ref_ids": [
                "33"
            ],
            "1": "An alternative method is to use standard CNNs to regress a pixel-wise vector field from the input image to guide the evolution of the initial contour [37,33,40]."
        },
        "Pointdsc: Robust point cloud registration using deep spatial consistency": {
            "authors": [
                "Xuyang Bai",
                "Zixin Luo",
                "Lei Zhou",
                "Hongkai Chen",
                "Lei Li",
                "Zeyu Hu",
                "Hongbo Fu",
                "Lan Tai"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2021/papers/Bai_PointDSC_Robust_Point_Cloud_Registration_Using_Deep_Spatial_Consistency_CVPR_2021_paper.pdf",
            "ref_texts": ""
        },
        "Dpod: 6d pose object detector and refiner": {
            "authors": [
                "Sergey Zakharov",
                "Ivan Shugurov",
                "Slobodan Ilic"
            ],
            "url": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Zakharov_DPOD_6D_Pose_Object_Detector_and_Refiner_ICCV_2019_paper.pdf",
            "ref_texts": "[25] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "25"
            ],
            "1": "Recent deep learning-based approaches, such as SSD6D [15], YOLO6D [33], AAE [31], PoseCNN [34] and PVNet [25], are the current top performers for this task in RGB images.",
            "2": "The majority is trained on real data [33, 34, 25, 14] while only SSD6D [15] and AAE [31] are trained on syn1941\n thetic renderings.",
            "3": "Here we review the following ones: SSD6D [15], YOLO6D [33], BB8 [26], iPose [14], AAE [31], PoseCNN [34] and PVNet [25].",
            "4": "Among the methods that are specifically designed to be robust to occlusions we would like to highlight iPose [14], 1942\n PoseCNN [34], and PVNet [25].",
            "5": "PVNet [25] takes a different approach and designs a network which for every pixel in the image regresses an offset to some predefined keypoints.",
            "6": ", BB8 [26], YOLO6D [33], PVNet [25], use the training split of the real dataset.",
            "7": "Train data Synthetic + Refinement Real + Refinement Object SSD6D [15] AAE [31] Ours SSD6D [22] Ours YOLO6D [33] PoseCNN [34] PVNet [25] Ours DeepIM [18] Ours Ape 2.",
            "8": "Analogously to other related papers [33, 15, 25, 34], we measure the accuracy of pose estimation using theADD score [12].",
            "9": "If trained on real data, our method is the second best after [25].",
            "10": "MethodYOLO6D\n[33]PoseCNN\n[34]SSD6D\n+ Ref [22]HMap [24]PVNet [25]Ours Ours +Ref Mean 6.",
            "11": "We demonstrated that for both, real and synthetic training data, our detector outperforms other related works, such as [33, 34], by a large margin and performs similarly to [25]."
        },
        "DexYCB: A benchmark for capturing hand grasping of objects": {
            "authors": [
                "Wei Chao",
                "Wei Yang",
                "Yu Xiang",
                "Pavlo Molchanov",
                "Ankur Handa",
                "Jonathan Tremblay",
                "Yashraj S. Narang",
                "Karl Van",
                "Umar Iqbal",
                "Stan Birchfield",
                "Jan Kautz",
                "Dieter Fox"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Chao_DexYCB_A_Benchmark_for_Capturing_Hand_Grasping_of_Objects_CVPR_2021_paper.pdf",
            "ref_texts": "[27] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. In CVPR , 2019. 1",
            "ref_ids": [
                "27"
            ],
            "1": "State-of-the-art approaches for both 3D object pose [37, 20,34,27,40,26,19] and 3D hand pose estimation [49, 23,18,2,9,14,31] rely on deep learning and thus require large datasets with labeled hand or object poses for training."
        },
        "Onepose: One-shot object pose estimation without cad models": {
            "authors": [
                "Jiaming Sun",
                "Zihao Wang",
                "Siyu Zhang",
                "Xingyi He",
                "Hongcheng Zhao",
                "Guofeng Zhang",
                "Xiaowei Zhou"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Sun_OnePose_One-Shot_Object_Pose_Estimation_Without_CAD_Models_CVPR_2022_paper.pdf",
            "ref_texts": "[27] Sida Peng, Xiaowei Zhou, Yuan Liu, Haotong Lin, Qixing Huang, and Hujun Bao. PVNet: pixel-wise voting network for 6dof object pose estimation. T-PAMI , 2020. 2, 6",
            "ref_ids": [
                "27"
            ],
            "1": "Compared with previous instance-level method PVNet [27] and category-level method Objectron [4], OnePose achieves better precision without training for any object instances or categories in the validation set, while taking only 58 msto process one frame on GPU.",
            "2": "In contrast, the latter type of methods first find correspondences between image pixels and 3D object coordinates either by regression [22, 24, 25] or by voting [26, 27], and then compute the pose with Perspective-n-Points (PnP).",
            "3": "2) Instance-level method PVNet [26, 27]."
        },
        "Hybridpose: 6d object pose estimation under hybrid representations": {
            "authors": [
                "Chen Song",
                "Jiaru Song",
                "Qixing Huang"
            ],
            "url": "http://openaccess.thecvf.com/content_CVPR_2020/papers/Song_HybridPose_6D_Object_Pose_Estimation_Under_Hybrid_Representations_CVPR_2020_paper.pdf",
            "ref_texts": "[34] Sida Peng, Yuan Liu, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Pvnet: Pixel-wise voting network for 6dof pose estimation. CoRR , abs/1812.11788, 2018. 1,2,3,4,6,7,8",
            "ref_ids": [
                "34"
            ],
            "1": "While early works typically formulate pose estimation as end-to-end pose classification [39] or pose regression [16,42], recent pose estimation methods usually leverage keypoints as an intermediate representation [38,34], and align predicted 2D keypoints with ground-truth 3D keypoints.",
            "2": "To express the geometric information in an RGB image, a prevalent intermediate representation is keypoints, which achieves stateof-the-art performance [34,32,36].",
            "3": "Alternative keypoint representations include vector-fields [34] and patches [14].",
            "4": "To mitigate pose error, several works assign different weights to different predicted elements in the 2D-3D alignment stage [34,32].",
            "5": "The keypoint network fK\n\u03b8employs an off-the-shelf prediction network [34].",
            "6": "In our experiments, HybridPose incorporates an off-the-shelf architecture called PVNet [34], which is the state-of-the-art keypoint-based pose estimator that employs a voting scheme to predict both visible and invisible keypoints.",
            "7": "Our keypoint annotation strategy follows that of [34], i.",
            "8": "A voting-based keypoint localization scheme [34] is applied to extract the coordinates of 2D keypoints from this 2|K|-channel tensor and the segmentation mask M.",
            "9": "[38], BB8 [36], Pix2Pose [30], PVNet [34], CDPN [20], and DPOD [44].",
            "10": "HybridPose outperforms PVNet [34], the backbone model we use to predict keypoints.",
            "11": "[14], PVNet [34], and DPOD [44].",
            "12": "In terms of ADD(-S), our approach improves PVNet [34] from 40."
        },
        "Epos: Estimating 6d pose of objects with symmetries": {
            "authors": [
                "Tomas Hodan",
                "Daniel Barath",
                "Jiri Matas"
            ],
            "url": "https://openaccess.thecvf.com/content_CVPR_2020/papers/Hodan_EPOS_Estimating_6D_Pose_of_Objects_With_Symmetries_CVPR_2020_paper.pdf",
            "ref_texts": "[50] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. CVPR , 2019. 1, 2, 3",
            "ref_ids": [
                "50"
            ],
            "1": "Recent methods, which are mostly based on convolutional neural networks, produce dense correspondences [4, 48, 69] or predict 2D image locations of pre-selected 3D keypoints [52, 61, 50].",
            "2": "A popular approach is to establish 2D-3D correspondences by predicting the 2D projections of a fixed set of 3D keypoints, which are pre-selected for each object model, and solve for the object pose using P nP-RANSAC [52, 49, 47, 61, 65, 15, 29, 50].",
            "3": "On the other hand, regression-based methods [61, 69, 50] need to compromise among the possible corresponding locations and tend to return the average, which is often not a valid solution."
        },
        "Honnotate: A method for 3d annotation of hand and object poses": {
            "authors": [
                "Shreyas Hampali",
                "Mahdi Rad",
                "Markus Oberweger",
                "Vincent Lepetit"
            ],
            "url": "https://openaccess.thecvf.com/content_CVPR_2020/papers/Hampali_HOnnotate_A_Method_for_3D_Annotation_of_Hand_and_Object_CVPR_2020_paper.pdf",
            "ref_texts": ""
        },
        "Fs-net: Fast shape-based network for category-level 6d object pose estimation with decoupled rotation mechanism": {
            "authors": [
                "Wei Chen",
                "Xi Jia",
                "Hyung Jin",
                "Jinming Duan",
                "Linlin Shen",
                "Ales Leonardis"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2021/papers/Chen_FS-Net_Fast_Shape-Based_Network_for_Category-Level_6D_Object_Pose_Estimation_CVPR_2021_paper.pdf",
            "ref_texts": ""
        },
        "Epro-pnp: Generalized end-to-end probabilistic perspective-n-points for monocular object pose estimation": {
            "authors": [
                "Hansheng Chen",
                "Pichao Wang",
                "Fan Wang",
                "Wei Tian",
                "Lu Xiong",
                "Hao Li"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Chen_EPro-PnP_Generalized_End-to-End_Probabilistic_Perspective-N-Points_for_Monocular_Object_Pose_Estimation_CVPR_2022_paper.pdf",
            "ref_texts": "[31] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 1, 2, 4, 7",
            "ref_ids": [
                "31"
            ],
            "1": "BB8 [32] and RTM3D [23] locate the corners of the 3D bounding box as keypoints, while PVNet [31] defines the keypoints by farthest point sampling and Deep MANTA [9] by handcrafted templates.",
            "2": "Existing work [11,31] on learning uncertainty-aware correspondences only considers the former, hence lacking the discriminative ability.",
            "3": "Comparison to the State of the Art As shown in Table 2, despite modified from the lower baseline, EPro-PnP easily reaches comparable performance to the top pose refiner RePOSE [20], which adds extra overhead to the PnP-based initial estimator PVNet [31]."
        },
        "Geometry-based distance decomposition for monocular 3d object detection": {
            "authors": [
                "Xuepeng Shi",
                "Qi Ye",
                "Xiaozhi Chen",
                "Chuangrong Chen",
                "Zhixiang Chen",
                "Kyun Kim"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Shi_Geometry-Based_Distance_Decomposition_for_Monocular_3D_Object_Detection_ICCV_2021_paper.pdf",
            "ref_texts": "[35] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019.",
            "ref_ids": [
                "35"
            ],
            "1": "In 6D object pose estimation, PVNet [35] and SegDriven [17] regress the 2D keypoints of objects.",
            "2": "These works [35, 17, 23, 44] recover the pose or distance by several factors, such as 2D keypoints, 2D bounding boxes, and object physical size, which achieves interpretable and robust pose or distance estimation.",
            "3": "In 6D object pose estimation, PVNet [35] and SegDriven [17] regress 2D keypoints of objects, then optimize the estimation of the 6D pose by solving a Perspective-n-Point (PnP) problem."
        },
        "Semi-supervised 3d hand-object poses estimation with interactions in time": {
            "authors": [
                "Shaowei Liu",
                "Hanwen Jiang",
                "Jiarui Xu",
                "Sifei Liu",
                "Xiaolong Wang"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Semi-Supervised_3D_Hand-Object_Poses_Estimation_With_Interactions_in_Time_CVPR_2021_paper.pdf",
            "ref_texts": "[43] Sida Peng, Yuan Liu, Qi-Xing Huang, Hujun Bao, and Xiaowei Zhou. Pvnet: Pixel-wise voting network for 6dof pose estimation. CVPR , pages 4556\u20134565, 2019.",
            "ref_ids": [
                "43"
            ],
            "1": "There are also two main paradigms to perform object 6-Dof pose estimation, with one directly regressing the pose as network outputs [28,67] and another regressing the projected 3D object control points location in the image and recovering the pose with 2D-to-3D correspondence [45,60,43,24]."
        },
        "So-pose: Exploiting self-occlusion for direct 6d pose estimation": {
            "authors": [
                "Yan Di",
                "Fabian Manhardt",
                "Gu Wang",
                "Xiangyang Ji",
                "Nassir Navab",
                "Federico Tombari"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Di_SO-Pose_Exploiting_Self-Occlusion_for_Direct_6D_Pose_Estimation_ICCV_2021_paper.pdf",
            "ref_texts": "[28] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof poseestimation. In CVPR , 2019. 2,6,7",
            "ref_ids": [
                "28"
            ],
            "1": "[28] demonstrate that keypoints away from the object surface induce largererrors and, therefore, instead sample several keypoints onthe object model based on farthest point sampling.",
            "2": "HybridPose [36] follows and develops [28] by introducing hybrid representations.",
            "3": "0 PVNet [28] M 73.",
            "4": "Stage HybridPose GDR-Net Ours DPOD DeepIM\n[45] [28] [12] [36] [43] (SO-Pose) [47] [19] P.",
            "5": "621 PVNet [28] M 0."
        },
        "Single-stage 6d object pose estimation": {
            "authors": [
                "Yinlin Hu",
                "Pascal Fua",
                "Wei Wang",
                "Mathieu Salzmann"
            ],
            "url": "http://openaccess.thecvf.com/content_CVPR_2020/papers/Hu_Single-Stage_6D_Object_Pose_Estimation_CVPR_2020_paper.pdf",
            "ref_texts": ""
        },
        "Sgpa: Structure-guided prior adaptation for category-level 6d object pose estimation": {
            "authors": [
                "Kai Chen",
                "Qi Dou"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_SGPA_Structure-Guided_Prior_Adaptation_for_Category-Level_6D_Object_Pose_Estimation_ICCV_2021_paper.pdf",
            "ref_texts": "[20] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof poseestimation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4561\u20134570, 2019. 1,2",
            "ref_ids": [
                "20"
            ],
            "1": "Dif-ferent from conventional instance-level [12,20,30,35] object pose estimation, which gives instance CAD models andpredicts poses for the instances that have been seen during training, category-level task requires capturing the general properties while accounting for the large variation of differPrior Point CloudCamera Instance I Camera Instance IIw/o Prior adaptationw/ Prior adaptation Figure 1.",
            "2": "Methods [20,25,2,13,17,16] mainly focus on learning a robust embedding that is con-ditioned on the object pose.",
            "3": "The second group of methods [20,12,25,13] assume the object 3D CAD model is available."
        },
        "Zebrapose: Coarse to fine surface encoding for 6dof object pose estimation": {
            "authors": [
                "Yongzhi Su",
                "Mahdi Saleh",
                "Torben Fetzer",
                "Jason Rambach",
                "Nassir Navab",
                "Benjamin Busam",
                "Didier Stricker",
                "Federico Tombari"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Su_ZebraPose_Coarse_To_Fine_Surface_Encoding_for_6DoF_Object_Pose_CVPR_2022_paper.pdf",
            "ref_texts": "[49] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "49"
            ],
            "1": "In contrast to previous works where there is no guaranteed putative correspondence [48,49,67], our encoding promotes direct pixel-to-surface matching just by means of a look-up table.",
            "2": "BB8 [52] firstly defines the 3D object bounding box corners as the keypoints and PVNet [49] reaches high recall rate in LM [27] dataset by predicting the keypoints with a dense pixel-wise voting for sampled keypoints on the object.",
            "3": "Since the LM-O dataset includes only a limited number of training images, [34, 49] additionally render a large number of synthetic images for training."
        },
        "Learning canonical shape space for category-level 6d object pose and size estimation": {
            "authors": [
                "Dengsheng Chen",
                "Jun Li",
                "Zheng Wang",
                "Kai Xu"
            ],
            "url": "http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Learning_Canonical_Shape_Space_for_Category-Level_6D_Object_Pose_and_CVPR_2020_paper.pdf",
            "ref_texts": "[17] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6dof pose estimation. In Proc. CVPR , pages 4561\u20134570, 2019. 1,2",
            "ref_ids": [
                "17"
            ],
            "1": "Most existing works have so far been addressing instance-level 6D pose estimation where each target object has a corresponding CAD model with exact shape and size [17].",
            "2": "This circumvents the difficulty in estimating dense correspondence between two representations as in other methods [17,29].",
            "3": "PVNet [17] is a unique approach of feature point detection using CNNs: A vector field is estimated for the input RGB image based on which the feature points are voted."
        },
        "H2o: Two hands manipulating objects for first person interaction recognition": {
            "authors": [
                "Taein Kwon",
                "Bugra Tekin",
                "Jan Stuhmer",
                "Federica Bogo",
                "Marc Pollefeys"
            ],
            "url": "http://openaccess.thecvf.com/content/ICCV2021/papers/Kwon_H2O_Two_Hands_Manipulating_Objects_for_First_Person_Interaction_Recognition_ICCV_2021_paper.pdf",
            "ref_texts": "[59] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 3",
            "ref_ids": [
                "59"
            ],
            "1": "While a significant amount of research has focused on predicting the pose of hands [27, 54, 56, 58, 70, 93, 94, 98] or objects [5, 48, 59, 78, 83, 90] in isolation, joint understanding of handobject interactions has received far less attention."
        },
        "Surfemb: Dense and continuous correspondence distributions for object pose estimation with learnt surface embeddings": {
            "authors": [
                "Rasmus Laurvig",
                "Anders Glent"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Haugaard_SurfEmb_Dense_and_Continuous_Correspondence_Distributions_for_Object_Pose_Estimation_CVPR_2022_paper.pdf",
            "ref_texts": "[26] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "26"
            ],
            "1": "Many establish 2D-3D correspondences [12, 21, 25, 26, 28, 31] followed by PnP-RANSAC [9], and mainly differ in how they establish correspondences.",
            "2": "Some methods establish correspondences for a fixed set of object keypoints [26, 28] while others establish dense (pixel-wise) query model key model contrastive loss kq k+Figure 1.",
            "3": "Other learning based approaches are based on establishing 2D-3D correspondences [5, 12, 21, 25, 26, 28, 31] followed by a variant of PnP-RANSAC.",
            "4": "PVNet [26] regresses vector fields toward the 2D projections of a set of fixed 3D key points and handles symmetries like BB8.",
            "5": "725 PVNet [26] RGB \u2713 0."
        },
        "Variable compliance control for robotic peg-in-hole assembly: A deep-reinforcement-learning approach": {
            "authors": [
                "Cristian Camilo",
                "Damien Petit",
                "Ixchel Georgina",
                "Kensuke Harada"
            ],
            "url": "https://www.mdpi.com/2076-3417/10/19/6923/pdf",
            "ref_texts": "29. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 16\u201320 June 2019; pp. 4561\u20134570.",
            "ref_ids": [
                "29"
            ],
            "1": "We considered the second assumption fair given the advances in vision-recognition techniques, wherein the 6D poses of objects can be estimated from single RGB images [28,29] or RGB images with depth maps (RGB-D) [30,31]."
        },
        "Fs6d: Few-shot 6d pose estimation of novel objects": {
            "authors": [
                "Yisheng He",
                "Yao Wang",
                "Haoqiang Fan",
                "Jian Sun",
                "Qifeng Chen"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/He_FS6D_Few-Shot_6D_Pose_Estimation_of_Novel_Objects_CVPR_2022_paper.pdf",
            "ref_texts": "[38] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019. 2, 6[39] Gabriel Peyr \u00b4e, Marco Cuturi, et al. Computational optimal transport: With applications to data science. Foundations and Trends\u00ae in Machine Learning , 11(5-6):355\u2013607, 2019.",
            "ref_ids": [
                "38",
                "39"
            ],
            "1": "Learning-based approaches includes direct pose regression [53,58], dense correspondence exploration [29] and recent keypoint-based approaches [15, 16, 38], which improve the performance by large margins.",
            "2": "The Sinkhorn Algorithm [39] is applied for differentiable optimization as well.",
            "3": "1d) as in [19, 38].",
            "4": "2, 6[39] Gabriel Peyr \u00b4e, Marco Cuturi, et al."
        },
        "Rnnpose: Recurrent 6-dof object pose refinement with robust correspondence field estimation and pose optimization": {
            "authors": [
                "Yan Xu",
                "Yee Lin",
                "Guofeng Zhang",
                "Xiaogang Wang",
                "Hongsheng Li"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Xu_RNNPose_Recurrent_6-DoF_Object_Pose_Refinement_With_Robust_Correspondence_Field_CVPR_2022_paper.pdf",
            "ref_texts": "[34] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "34"
            ],
            "1": "These methods may estimate the object\u2019s bounding box corners [35,45], predict dense 2D-3D correspondence maps [33] or vote the keypoints by all object pixels [34].",
            "2": "At the beginning of the first rendering cycle, a reference image Iref is rendered with the object\u2019s CAD model according to its initial pose Pinit(estimated by any direct methods [34,52]).",
            "3": "Here, the initial poses for pose refinement are originally from PVNet [34] but added with significant disturbances for robustness testing.",
            "4": "We follow similar conventions in data processing and synthetic data generation as the previous works [20,34].",
            "5": "For the initial poses, we mainly rely on PoseCNN [52] and PVNet [34], two typical direct estimation methods, following [23] and [20].",
            "6": "Robustness comparison with RePOSE by degrading the initial poses (from PVNet [34]) with Gaussian noise on LINEMOD dataset.",
            "7": "The comparison of estimation accuracy with competitive direct methods (PoseCNN [52], PVNet [34] and HybridPose [38]) and refinement methods (DPOD [58], DeepIM [23] and RePOSE\n[20]) on LINEMOD dataset in terms of the ADD(-S) metric.",
            "8": "Object PoseCNN [52] PVNet [34] HybridPose [38] GDR-Net [51] DPOD [58] RePOSE [20] Ours Ape 9.",
            "9": "For the LINEMOD dataset, we compare with the recent pose refinement methods RePOSE [20], DPOD [58] and DeepIM [23] as well as some direct estimation baselines [34, 38, 52].",
            "10": "4 the PVNet [34], although the pose accuracy of PVNet is much better as exhibited in Table 3."
        },
        "Dualposenet: Category-level 6d object pose and size estimation using dual pose network with refined learning of pose consistency": {
            "authors": [
                "Jiehong Lin",
                "Zewei Wei",
                "Zhihao Li",
                "Songcen Xu",
                "Kui Jia",
                "Yuanqing Li"
            ],
            "url": "http://openaccess.thecvf.com/content/ICCV2021/papers/Lin_DualPoseNet_Category-Level_6D_Object_Pose_and_Size_Estimation_Using_Dual_ICCV_2021_paper.pdf",
            "ref_texts": "[20] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 1, 2",
            "ref_ids": [
                "20"
            ],
            "1": ", the above 7DoF setting) and instance-level 6D object pose estimation [12, 8, 14, 16, 32, 26, 20, 17, 29, 18].",
            "2": "More recent solutions build on the power of deep networks and can directly estimate object poses from RGB images alone [16, 32, 26, 20] or RGB-D ones [17, 29]."
        },
        "Gpv-pose: Category-level object pose estimation via geometry-guided point-wise voting": {
            "authors": [
                "Yan Di",
                "Ruida Zhang",
                "Zhiqiang Lou",
                "Fabian Manhardt",
                "Xiangyang Ji",
                "Nassir Navab",
                "Federico Tombari"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Di_GPV-Pose_Category-Level_Object_Pose_Estimation_via_Geometry-Guided_Point-Wise_Voting_CVPR_2022_paper.pdf"
        },
        "Satellite pose estimation with deep landmark regression and nonlinear pose refinement": {
            "authors": [
                "Bo Chen",
                "Jiewei Cao",
                "Alvaro Parra",
                "Jun Chin"
            ],
            "url": "http://openaccess.thecvf.com/content_ICCVW_2019/papers/R6D/Chen_Satellite_Pose_Estimation_with_Deep_Landmark_Regression_and_Nonlinear_Pose_ICCVW_2019_paper.pdf",
            "ref_texts": "[26] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 2,3",
            "ref_ids": [
                "26"
            ],
            "1": "Inspired by works that combine the strength of deep neural networks and geometric optimisation [26,25, 35], our approach contains three main components: 1.",
            "2": "While the keypoint matching problem can be solved using machine learning, deep CNN-based feature learning methods typically fix the 2D-3D keypoint associations and learn to predict the image locations of each corresponding 3D keypoint such as [26,25,35].",
            "3": "2,3\n[26] S."
        },
        "Coupled iterative refinement for 6d multi-object pose estimation": {
            "authors": [
                "Lahav Lipson",
                "Zachary Teed",
                "Ankit Goyal",
                "Jia Deng"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Lipson_Coupled_Iterative_Refinement_for_6D_Multi-Object_Pose_Estimation_CVPR_2022_paper.pdf",
            "ref_texts": "[28] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "28"
            ]
        },
        "Wide-depth-range 6d object pose estimation in space": {
            "authors": [
                "Yinlin Hu",
                "Sebastien Speierer",
                "Wenzel Jakob",
                "Pascal Fua",
                "Mathieu Salzmann"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Hu_Wide-Depth-Range_6D_Object_Pose_Estimation_in_Space_CVPR_2021_paper.pdf",
            "ref_texts": ""
        },
        "Onepose++: Keypoint-free one-shot object pose estimation without CAD models": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/e43f900f571de6c96a70d5724a0fb565-Paper-Conference.pdf",
            "ref_texts": "[39] Sida Peng, Xiaowei Zhou, Yuan Liu, Haotong Lin, Qixing Huang, and Hujun Bao. PVNet: pixel-wise voting network for 6dof object pose estimation. T-PAMI , 2020. 1, 2, 3, 8, 9",
            "ref_ids": [
                "39"
            ],
            "1": "However, most existing methods [39,29,38,55,2,4,37] either rely on high-fidelity object CAD models or require training a separate network for each object category.",
            "2": "The experiments show that our method outperforms all existing one-shot pose estimation methods [48,33] by a large margin and even achieves comparable results with instance-level methods [39,29] which are trained for each object instance with a CAD model.",
            "3": "Instance-level methods estimate object poses either by directly regressing poses from images [58,20,29] or construct 2D-3D correspondences and then solve poses with PnP [39,59].",
            "4": "Our method is compared with PVNet[39] on objects with CAD models in the OnePose-LowTexture dataset using the ADD(S)-0.",
            "5": "2) Instance-level baselines [39,29] that require CAD-models and need to be trained separately for each object.",
            "6": "For the comparison with PVNet [39], we follow its original training setting, which first samples 8keypoints on the object surface and then trains a network using 5000 synthetic images for each object.",
            "7": "On the OnePose-LowTexture dataset, the proposed method is compared with PVNet [39] on the subset objects with scanned models.",
            "8": "4 Results on LINEMOD We compare the proposed method with OnePose [48] and Gen6D [33] which are under the One-shot setting, and Instance-level methods PVNet [39] and CDPN [29] onADD(S)-0.",
            "9": "Our method has lower or comparable performance with instance-level methods [39,29], which are trained to fit each object instance, and thus perform well naturally, at the expense of the tedious training for each object."
        },
        "Ifor: Iterative flow minimization for robotic object rearrangement": {
            "authors": [
                "Ankit Goyal",
                "Arsalan Mousavian",
                "Chris Paxton",
                "Wei Chao",
                "Brian Okorn",
                "Jia Deng",
                "Dieter Fox"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Goyal_IFOR_Iterative_Flow_Minimization_for_Robotic_Object_Rearrangement_CVPR_2022_paper.pdf",
            "ref_texts": "[50] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. In CVPR , 2019. 2",
            "ref_ids": [
                "50"
            ],
            "1": "This makes explicit object pose estimation [33, 35, 50, 61, 66] a necessary part of the pipeline, and the full system susceptible to pose estimation error from real vision systems.",
            "2": ", detecting and segmenting objects [4, 6, 22, 38, 70] and estimating their 6D poses [33, 35, 50, 61, 66]."
        },
        "A vector-based representation to enhance head pose estimation": {
            "authors": [
                "Zongcheng Chu",
                "Dongfang Liu",
                "Yingjie Chen",
                "Zhiwen Cao"
            ],
            "url": "http://openaccess.thecvf.com/content/WACV2021/papers/Chu_A_Vector-Based_Representation_to_Enhance_Head_Pose_Estimation_WACV_2021_paper.pdf",
            "ref_texts": "[20] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "20"
            ],
            "1": "The approaches can be divided into two categories: [20, 33, 27] first estimate the object mask to determine its location in the image, then build the correspondence between the image pixels and the available 3D models."
        },
        "Repose: Fast 6d object pose refinement via deep texture rendering": {
            "authors": [
                "Shun Iwase",
                "Xingyu Liu",
                "Rawal Khirodkar",
                "Rio Yokota",
                "Kris M. Kitani"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Iwase_RePOSE_Fast_6D_Object_Pose_Refinement_via_Deep_Texture_Rendering_ICCV_2021_paper.pdf",
            "ref_texts": ""
        },
        "Osop: A multi-stage one shot object pose estimation framework": {
            "authors": [
                "Ivan Shugurov",
                "Fu Li",
                "Benjamin Busam",
                "Slobodan Ilic"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Shugurov_OSOP_A_Multi-Stage_One_Shot_Object_Pose_Estimation_Framework_CVPR_2022_paper.pdf",
            "ref_texts": "[37] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 1, 2",
            "ref_ids": [
                "37"
            ],
            "1": "According to the BOP challenge [14], which combines publicly available 6 DoF pose estimation datasets and offers standardized evaluation and comparison procedures, the field is dominated by deep learning methods [2, 12, 16, 19, 21, 22, 22, 24\u201327, 36, 37, 47, 49\u201352, 59].",
            "2": "In particular, IPose [16], YOLO6D [53], PVNet [37], HybridPose [50] and [19] predict a sparse set of the pre-defined keypoints."
        },
        "Templates for 3d object pose estimation revisited: Generalization to new objects and robustness to occlusions": {
            "authors": [
                "Van Nguyen",
                "Yinlin Hu",
                "Yang Xiao",
                "Mathieu Salzmann",
                "Vincent Lepetit"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Nguyen_Templates_for_3D_Object_Pose_Estimation_Revisited_Generalization_to_New_CVPR_2022_paper.pdf",
            "ref_texts": ""
        },
        "Autolabeling 3d objects with differentiable rendering of sdf shape priors": {
            "authors": [
                "Sergey Zakharov",
                "Wadim Kehl",
                "Arjun Bhargava",
                "Adrien Gaidon"
            ],
            "url": "https://openaccess.thecvf.com/content_CVPR_2020/papers/Zakharov_Autolabeling_3D_Objects_With_Differentiable_Rendering_of_SDF_Shape_Priors_CVPR_2020_paper.pdf",
            "ref_texts": "[33] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 2",
            "ref_ids": [
                "33"
            ],
            "1": "The authors in [43,31,24,17,33] apply such representations for monocular pose estimation of known CAD models."
        },
        "End-to-end learnable geometric vision by backpropagating pnp optimization": {
            "authors": [
                "Bo Chen",
                "Alvaro Parra",
                "Jiewei Cao",
                "Nan Li",
                "Jun Chin"
            ],
            "url": "https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_End-to-End_Learnable_Geometric_Vision_by_Backpropagating_PnP_Optimization_CVPR_2020_paper.pdf",
            "ref_texts": "[36] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 2,7",
            "ref_ids": [
                "36"
            ],
            "1": "Other pose estimation approaches that combine deep learning with geometric optimization (PnP solver) [35,37, 47,36,10] adopt a two-stage strategy: first learn to predict the 2D landmarks or fiducial points from the input image, then perform pose estimation by solving PnP on the 2D-3D correspondences.",
            "2": "For each object we\u2022obtain a 3D model representation consisting of 15 landmarks by using the Farthest Point Sampling (FPS) [36] over the original object mesh, \u2022randomly reserve 400images as the test set and set the remaining (about 800, depending on the object) as the training set, and \u2022train a model to predict the 6DOF object pose from the input image.",
            "3": "We provide the result of the current state-of-the-art PVNet [36] as a reference."
        },
        "Gapartnet: Cross-category domain-generalizable object perception and manipulation via generalizable and actionable parts": {
            "authors": [
                "Haoran Geng",
                "Helin Xu",
                "Chengyang Zhao",
                "Chao Xu",
                "Li Yi",
                "Siyuan Huang",
                "He Wang"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Geng_GAPartNet_Cross-Category_Domain-Generalizable_Object_Perception_and_Manipulation_via_Generalizable_and_CVPR_2023_paper.pdf",
            "ref_texts": "[41] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "41"
            ],
            "1": "Instance-level object pose estimation works [18, 22, 30,41,47,52,62] assume known CAD models and thus have their limitations."
        },
        "G2l-net: Global to local network for real-time 6d pose estimation with embedding vector features": {
            "authors": [
                "Wei Chen",
                "Xi Jia",
                "Hyung Jin",
                "Jinming Duan",
                "Ales Leonardis"
            ],
            "url": "https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_G2L-Net_Global_to_Local_Network_for_Real-Time_6D_Pose_Estimation_CVPR_2020_paper.pdf",
            "ref_texts": "[29] Sida Peng, Yuan Liu, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Pvnet: Pixel-wise voting network for 6dof pose estimation. arXiv preprint arXiv:1812.11788 , 2018. 1,2,4, 5,6,7",
            "ref_ids": [
                "29"
            ],
            "1": "Introduction Real-time performance is important in many computer vision tasks, such as, object detection [35,23], semantic segmentation [36,10], object tracking [5,11], and pose estimation [29,38,16].",
            "2": "While there exist some real-time deep learning methods [29,34,38,45]\n(>20fps), they use only RGB information from an image.",
            "3": "These methods use handcrafted features that are not robust to background clutter and image variations [44,37,29].",
            "4": "Learning-based methods [33,29,34,28,15,38] alleviate this problem by training their model to predict 2D keypoints and compute the object pose by the PnP algorithm [9,20].",
            "5": "Another way is, as proposed in [29], to use the farthest point sampling (FPS) algorithm to sample the keypoints in each object model.",
            "6": "Different from other state-of-the-art methods [29,42,4], we adopt a multilayer perceptron (MLP) that takes pointwise embedding vector features as input and outputs the rotation of object as shown in Figure 5.",
            "7": "In experiments, we have found that our proposed method can make faster and more accurate predictions than the methods [29,42,4].",
            "8": "(3) When evaluating on YCB-Video dataset, same as [42, 29,21], we use the ADD-S AUC metric proposed in [42],Table 1.",
            "9": "Method PVNet [29]PoseCNN + DeepIM [42,21]DPOD [45] Frustum-P [30]Hinterstoisser [13]DenseFusion [40] Ours Input RGB RGB RGB RGB+Depth Depth RGB+Depth RGB+Depth Refinement \u00d7 /check /check(\u00d7) \u00d7 /check /check(\u00d7) \u00d7 Ape 43."
        },
        "Latentfusion: End-to-end differentiable reconstruction and rendering for unseen object pose estimation": {
            "authors": [
                "Keunhong Park",
                "Arsalan Mousavian",
                "Yu Xiang",
                "Dieter Fox"
            ],
            "url": "http://openaccess.thecvf.com/content_CVPR_2020/papers/Park_LatentFusion_End-to-End_Differentiable_Reconstruction_and_Rendering_for_Unseen_Object_Pose_CVPR_2020_paper.pdf",
            "ref_texts": "[33] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4561\u20134570, 2019. 2",
            "ref_ids": [
                "33"
            ],
            "1": "The second category formulates the pose estimation by predicting a set of 2D image features, such as the projection of 3D box corners [42,45,14,33] and direction of the center of the object [49], then recovering the pose of the object using the predictions."
        },
        "Symmetry and uncertainty-aware object slam for 6dof object pose estimation": {
            "authors": [
                "Nathaniel Merrill",
                "Yuliang Guo",
                "Xingxing Zuo",
                "Xinyu Huang",
                "Stefan Leutenegger",
                "Xi Peng",
                "Liu Ren",
                "Guoquan Huang"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Merrill_Symmetry_and_Uncertainty-Aware_Object_SLAM_for_6DoF_Object_Pose_Estimation_CVPR_2022_paper.pdf",
            "ref_texts": ""
        },
        "Stereobj-1m: Large-scale stereo image dataset for 6d object pose estimation": {
            "authors": [
                "Xingyu Liu",
                "Shun Iwase",
                "Kris M. Kitani"
            ],
            "url": "http://openaccess.thecvf.com/content/ICCV2021/papers/Liu_StereOBJ-1M_Large-Scale_Stereo_Image_Dataset_for_6D_Object_Pose_Estimation_ICCV_2021_paper.pdf",
            "ref_texts": "[27] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 1, 2, 6, 7, 8",
            "ref_ids": [
                "27"
            ],
            "1": "To increase data size for training large-scale neural networks, previous works have explored leveraging synthetically rendered [33, 27, 10] or augmented images [35] with 3D mesh models.",
            "2": "We implement two state-of-the-art methods [27, 17] as the baseline comparisons for 6D pose estimation using stereo on the StereOBJ-1M dataset.",
            "3": "Specifically, we implement PVNet [27] and KeyPose [17], two classic keypoint-based 6D pose estimation frameworks that have achieved state-of-the-art performance on various datasets.",
            "4": "PVNet [27] is a single-RGB keypoint-based method.",
            "5": "25 Table 4: The results of PVNet [27] on single-object pose estimation in terms of ADD(-S) AUC andADD(-S) accuracy on StereOBJ-1M dataset.",
            "6": "The monocular method PVNet [27] is adapted to its stereo variant where keypoints in both stereo images are predicted individually."
        },
        "Hot-net: Non-autoregressive transformer for 3d hand-object pose estimation": {
            "authors": [],
            "url": "https://cse.buffalo.edu/~jsyuan/papers/2020/lin_mm20.pdf",
            "ref_texts": "[37] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. 2019. PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition .",
            "ref_ids": [
                "37"
            ],
            "1": "In the last decade, we have witnessed a rapid advance towards both 3D hand pose estimation [4,5,11,13,14,24,30,33,41,42,47, 47,51,56,57,60] and object pose estimation [25,26,37,38,46,52, 53,55] in isolation."
        },
        "6D pose estimation of objects: Recent technologies and challenges": {
            "authors": [],
            "url": "https://www.mdpi.com/2076-3417/11/1/228/pdf",
            "ref_texts": "32. Peng, S.; Liu, Y.; Huang, Q.; Bao, H.; Zhou, X. PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 18\u201323 June 2018.",
            "ref_ids": [
                "32"
            ],
            "1": "To solve this problem, Hu [32] et al."
        },
        "Keypoint-graph-driven learning framework for object pose estimation": {
            "authors": [
                "Shaobo Zhang",
                "Wanqing Zhao",
                "Ziyu Guan",
                "Xianlin Peng",
                "Jinye Peng"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Keypoint-Graph-Driven_Learning_Framework_for_Object_Pose_Estimation_CVPR_2021_paper.pdf",
            "ref_texts": "[26] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 1,2,5,6,7,8",
            "ref_ids": [
                "26"
            ],
            "1": "Recently, deep learning approaches [13,39,4,29,34,10,26,24,25,18,3] have shown impressive results of pose estimation in RGB\n*Corresponding authorimages.",
            "2": "Some keypoint-based approaches [27,25,34,10,26,31,42] build the correspondence using sparse 2D keypoints on objects as an intermediate representation for pose estimation.",
            "3": "Synthetic data generation Given 3D models of the objects, first, we define the keypoints on the surface of them as proposed in PVnet [26] whereKkeypoints are selected using the farthest point sampling (FPS) algorithm.",
            "4": "We use blender [26] to render these 3D models from different camera viewpoints to sufficiently cover the objects and project the keypoints to images under the viewpoints.",
            "5": "To evaluate the accuracy of the estimated pose, we use two standard metrics for LINEMOD used in other related paper [36,41,26] which are ADD and ADD-S (for symmetric objects).",
            "6": "labels w/o manual pose labels w/ manual pose labels Training data Syn Syn+Real Real Method AAE [33]1MHP [20]1DPOD [41]Self6D [36]1Ours YOLO6D [34] DPOD PVNet [26] CDPN [18] Ape 4.",
            "7": "We compare our method with state-of-the-art 6D pose estimation methods (AAE [33], MHP [20], DPOD [41] Self6D\n[36]) that use the synthetic images generated by 3D CAD models and the methods (YOLD6D [34], DPOD [41], PVNet [26], CDPN[18]) using real images with manual 3D annotations for training.",
            "8": "labels w/o manual pose labels w/ manual pose labels Training data Syn Syn+Real Real Method DPOD [41] CDPN [18] Self6D [36] Ours YOLO6D [34] HMap [23] PVNet [26] Mean 6.",
            "9": "We use the model trained on the synthetic images for testing on the Occlusion dataset and compare our method with the three methods (DPOD [41], CDPN [18] and Self6D [36]) that do not require manual pose labels for training and three methods (YOLO6D [34], HMap [23] and PVNet [26]) using manual pose labels for training.",
            "10": "1,2\n[26] S."
        },
        "UDA-COPE: unsupervised domain adaptation for category-level object pose estimation": {
            "authors": [
                "Taeyeop Lee",
                "Uk Lee",
                "Inkyu Shin",
                "Jaesung Choe",
                "Ukcheol Shin",
                "In So",
                "Jin Yoon"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Lee_UDA-COPE_Unsupervised_Domain_Adaptation_for_Category-Level_Object_Pose_Estimation_CVPR_2022_paper.pdf",
            "ref_texts": "[27] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4561\u20134570, 2019. 1",
            "ref_ids": [
                "27"
            ],
            "1": "Previous 6D object pose estimation methods follow the instance-level pose estimation schemes [12, 13, 25, 27, 31, 34,38] that rely on given 3D CAD model information (e."
        },
        "Dsc-posenet: Learning 6dof object pose estimation via dual-scale consistency": {
            "authors": [
                "Zongxin Yang",
                "Xin Yu",
                "Yi Yang"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2021/papers/Yang_DSC-PoseNet_Learning_6DoF_Object_Pose_Estimation_via_Dual-Scale_Consistency_CVPR_2021_paper.pdf",
            "ref_texts": "[29] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , pages 4561\u20134570, 2019. 2, 4, 7",
            "ref_ids": [
                "29"
            ],
            "1": "When an object undergoes occlusions or drastic illumination changes, those methods might fail to estimate object poses accurately [29].",
            "2": "Fully-supervised deep model based methods: Deep learning based methods have demonstrated promising pose estimation performance [29, 43, 47, 46, 19, 22].",
            "3": "Instead of treating pose estimation as a classification task, recent approaches directly regress 3D boundingboxes [30, 38], local features [43, 29] or coordinate maps [47, 42, 18, 51] of objects, and then predict object poses via PnP.",
            "4": "CPDN [49], DPOD [47] and Pix2Pose [18] output the 2D UV coordinates or 3D coordinates of 3D object models from images, while PoseCNN [43] and PVNet [29] employ Hough voting to localize object keypoints from estimated vector fields.",
            "5": "Self-supervised DSC-PoseNet Inspired by recent keypoint based estimation methods [29, 34], we use the intermediate object representation, i.",
            "6": "PVNet [29] predicts a vector field for each keypoint and employs voting to determine keypoint locations.",
            "7": ", AAE [37], MHP [25], DPOD [47] and PVNet [29], as well as RGBD based methods, i."
        },
        "Uni6d: A unified cnn framework without projection breakdown for 6d pose estimation": {
            "authors": [
                "Xiaoke Jiang",
                "Donghai Li",
                "Hao Chen",
                "Ye Zheng",
                "Rui Zhao",
                "Liwei Wu"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Jiang_Uni6D_A_Unified_CNN_Framework_Without_Projection_Breakdown_for_6D_CVPR_2022_paper.pdf",
            "ref_texts": "[30] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "30"
            ],
            "1": "We follow previous work [30,53] to split the training and testing sets, and we also obtain synthesis images for the training set as the same with [6, 53].",
            "2": "For LineMOD dataset, we follow [16,30] to report the accuracy of distance less than 10% of the objects\u2019 diameter (ADD-0."
        },
        "Single-view robot pose and joint angle estimation via render & compare": {
            "authors": [
                "Yann Labbe",
                "Justin Carpentier",
                "Mathieu Aubry",
                "Josef Sivic"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Labbe_Single-View_Robot_Pose_and_Joint_Angle_Estimation_via_Render__CVPR_2021_paper.pdf",
            "ref_texts": "[44] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 2",
            "ref_ids": [
                "44"
            ],
            "1": "For rigid objects, however, methods based on 2D keypoints [34,3,7,6,45,52,23,50,44,43,18] have been recently outperformed by render & compare methods that forgo explicit detection of 2D keypoints but instead use the entire shape of the object by comparing the rendered view of the 3D model to the input image and iteratively refining the object\u2019s 6D pose [59,31,25].",
            "2": "A set of sparse [45,52,23,50,44,43,18] or dense [56,41,49,59] features is detected on the object in the image using a CNN and theresulting 2D-to-3D correspondences are used to recover the camera pose using PnP [29]."
        },
        "Object pose estimation with statistical guarantees: Conformal keypoint detection and geometric uncertainty propagation": {
            "authors": [
                "Heng Yang",
                "Marco Pavone"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Object_Pose_Estimation_With_Statistical_Guarantees_Conformal_Keypoint_Detection_and_CVPR_2023_paper.pdf",
            "ref_texts": "[72] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dofpose estimation. IEEE Trans. Pattern Anal. Machine Intell. , 2022. 1,2,8",
            "ref_ids": [
                "72"
            ],
            "1": "One of the most popular paradigms for object pose estimation is a two-stage pipeline [20,71,72,79,81,85,89,101], where the first stage detects (semantic) keypoints of the objects on the image, and the second stage computes the object pose by solving an optimization known as Perspectiven-Points (PnP) that minimizes reprojection errors of the detected keypoints.",
            "2": ", PVNet [72]) and show that the average pose achieves better or similar accuracy.",
            "3": "Sparse methods define a handful of keypoints and predict locations of the keypoints via direct regression [74,89], probabilistic heatmap [67,71], or voting [72].",
            "4": "Baselines (results adapted from [72]) Conformalized heatmap Tekin PoseCNN Oberweger PVNet gt-ball gt-ellipse frcnn-ball frcnn-ellipse objects [89][95][67][72] \u270f=0."
        },
        "Vs-net: Voting with segmentation for visual localization": {
            "authors": [
                "Zhaoyang Huang",
                "Han Zhou",
                "Yijin Li",
                "Bangbang Yang",
                "Yan Xu",
                "Xiaowei Zhou",
                "Hujun Bao",
                "Guofeng Zhang",
                "Hongsheng Li"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Huang_VS-Net_Voting_With_Segmentation_for_Visual_Localization_CVPR_2021_paper.pdf",
            "ref_texts": "[38] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "38"
            ],
            "1": "Keypoint is widely utilized as an intermediate representation in object pose estimation [38,20,34,37,50].",
            "2": "Recently, PVNet [38] significantly improves robustness and accuracy of object pose estimation by detecting keypoints with pixel-wise votes, inspired by which, we propose to detect scene-specific landmarks with pixelwise votes.",
            "3": "The initial estimation of the 2D location \u02c6ljof the landmark jis computed from RANSAC with a vote intersection model [38], which generates multiple landmark location hypotheses by computing intersections of two randomly sampled directional votes and choosing the hypothesis having the most inlier votes."
        },
        "Self-supervised geometric perception": {
            "authors": [
                "Heng Yang",
                "Wei Dong",
                "Luca Carlone",
                "Vladlen Koltun"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Self-Supervised_Geometric_Perception_CVPR_2021_paper.pdf",
            "ref_texts": "[58] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , pages 4561\u20134570, 2019. 1,6",
            "ref_ids": [
                "58"
            ],
            "1": "Learned feature descriptors have been shown to consistently and significantly outperform their hand-crafted counterparts across applications such as relative camera pose estimation [69,61], 3D point cloud registration [21,32], and object detection and pose estimation [58,86,64,72].",
            "2": "For example, ground-truth relative camera poses are needed for training image keypoint descriptors [69,54,27], pairwise rigid transformations are required for training point cloud descriptors [21,32,74,85,70], and object poses are used to train image keypoint predictors [58,86].",
            "3": "For example, we also present the formulation for object detection and pose estimation [64,58,86,15], and discuss the application of SGP in the Supplementary Material."
        },
        "Category-level 6d object pose estimation in the wild: A semi-supervised learning approach and a new dataset": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/afe99e55be23b3523818da1fefa33494-Paper-Conference.pdf",
            "ref_texts": "[34] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019. 1",
            "ref_ids": [
                "34"
            ],
            "1": "One is performing instance-level 6D pose estimation, where a model is trained to estimate the pose of one exact instance with an existing 3D model [13,34,22,50,32,5,14]."
        },
        "Crt-6d: Fast 6d object pose estimation with cascaded refinement transformers": {
            "authors": [
                "Pedro Castro",
                "Kyun Kim"
            ],
            "url": "https://openaccess.thecvf.com/content/WACV2023/papers/Castro_CRT-6D_Fast_6D_Object_Pose_Estimation_With_Cascaded_Refinement_Transformers_WACV_2023_paper.pdf",
            "ref_texts": "[34] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019.",
            "ref_ids": [
                "34"
            ],
            "1": "ject pose estimators [41, 6, 34, 39, 35, 47, 9, 24, 40, 32, 3].",
            "2": ") These 5746\n features are transformed into an intermediate representation [34, 32, 47, 35, 15] which are then used to extract pose (using PnP [25] or other variations [40, 15]) or pose is extracted directly [7, 47, 9, 24].",
            "3": "NOCS, keypoint heatmaps [48, 35, 34, 51].",
            "4": "This shortfall was noticed by PVNet [34], which suggests the use of the surface region to find suitable keypoints.",
            "5": ", K} chosen for OSKFs are generated using the farthest point sampling algorithm [34], where Kis a hyperparameter of the number of used keypoints.",
            "6": "1 8 1 8 1 1 8 8 Method PVNet [34] GDR [47] GDR [47] SO-Pose [9] ZebraPose [40] RePose [20] DeepIM [28] CRT-6D Ape 15.",
            "7": "Repose [20] proposed a faster refinement method at 18ms with 5 iterations however they require a good initialization (they use PVNet [34] which itself takes over 25ms) and it only support a single object per model."
        },
        "Generative category-level shape and pose estimation with semantic primitives": {
            "authors": [
                "Anonymous Submission"
            ],
            "url": "https://proceedings.mlr.press/v205/li23d/li23d.pdf",
            "ref_texts": "[9]S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "9"
            ],
            "1": "Early works for this task are focused on instance-level pose estimation [7,8,9,10,11], which aligns the observed object with the given CAD model.",
            "2": "The second are correspondence-based methods [9,10,11,23].",
            "3": "For example, PVNet [9] predicts the 3D keypoints on the RGB image by a voting scheme.",
            "4": "[9]S."
        },
        "Sar-net: Shape alignment and recovery network for category-level 6d object pose and size estimation": {
            "authors": [
                "Haitao Lin",
                "Zichang Liu",
                "Chilam Cheang",
                "Yanwei Fu",
                "Guodong Guo",
                "Xiangyang Xue"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Lin_SAR-Net_Shape_Alignment_and_Recovery_Network_for_Category-Level_6D_Object_CVPR_2022_paper.pdf",
            "ref_texts": "[39] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019. 1, 3, 4, 5, 6, 7",
            "ref_ids": [
                "39"
            ],
            "1": "However, most 6D pose estimation works [8,9,15,19,28,37, 39,55,57] assume exact 3D CAD object models at instancelevel, which unfortunately greatly limits their practical applicability in real-world applications.",
            "2": "Another line of works [29,36,38,39,42,50,65] first regress object coordinates or keypoints in 2D images and then recover poses by Perspective-n-Point algorithm [25], e.",
            "3": ", PVNet [39].",
            "4": "In contrast to these keypoint voting methods [15,16,39], our approach focuses on a more practical setting without relying on exact object 3D models.",
            "5": "We further sample the category-level template shape into a sparse 3D point cloudKc\u2208R3\u00d7Nkby using Farthest Point Sampling (FPS) algorithm [39], where Nkis the number of points.",
            "6": "Inspired by previous 2D [39, 63] and 3D [15, 16, 40] keypoint voting methods, we treat the object center as a specific keypoint.",
            "7": "Compared with RGB(-D) methods [15, 39] or depth-only method [12, 13], our SAR-Net achieves comparable results in terms of ADD(-S) metric as in Tab.",
            "8": "Training data Methods ape can cat driller eggbox glue RGB(S+R) PVNet [39] 43."
        },
        "Ove6d: Object viewpoint encoding for depth-based 6d object pose estimation": {
            "authors": [
                "Dingding Cai",
                "Janne Heikkila",
                "Esa Rahtu"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Cai_OVE6D_Object_Viewpoint_Encoding_for_Depth-Based_6D_Object_Pose_Estimation_CVPR_2022_paper.pdf",
            "ref_texts": "[36] Sida Peng, Xiaowei Zhou, Yuan Liu, Haotong Lin, Qixing Huang, and Hujun Bao. Pvnet: pixel-wise voting network for 6dof object pose estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence , 2020. 1, 2, 6, 7, 8",
            "ref_ids": [
                "36"
            ],
            "1": "In recent works, the object pose estimation problem is commonly approached by either establishing local correspondences between the object 3D model and the observed data [16, 17, 36], or via direct regression [6, 39].",
            "2": "Related Work Pose estimation from RGB data Most RGB-based object 6D pose estimation methods [1,20,33,35,36,38,44,57] attempt to establish sparse or dense 2D-3D correspondences between the 2D coordinates in the RGB image and the 3D coordinates on the object 3D model surface.",
            "3": "5 PVNet [36] RGB 40.",
            "4": ")PVNet [36] RGB 42.",
            "5": "2 PVNet [36] RGBD \u2713 79.",
            "6": "0% recall on LMO with PVNet [36])."
        },
        "Monocinis: Camera independent monocular 3d object detection using instance segmentation": {
            "authors": [
                "Jonas Heylen",
                "Mark De",
                "Bruno Dawagne",
                "Marc Proesmans",
                "Luc Van",
                "Wim Abbeloos",
                "Hazem Abdelkawy",
                "Daniel Olmeda"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021W/3DODI/papers/Heylen_MonoCInIS_Camera_Independent_Monocular_3D_Object_Detection_Using_Instance_Segmentation_ICCVW_2021_paper.pdf",
            "ref_texts": "[65] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp. 4561\u20134570, 2019.[66] Y . Hu, J. Hugonot, P. Fua, and M. Salzmann, \u201cSegmentationdriven 6d object pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp. 3385\u20133394, 2019.",
            "ref_ids": [
                "65",
                "66"
            ],
            "1": "Several existing methods can be used to map the 2D predicted RPs to a 3D object pose [60, 65, 66, 61, 62].",
            "2": "[65] S.",
            "3": "[66] Y ."
        },
        "Stablepose: Learning 6d object poses from geometrically stable patches": {
            "authors": [
                "Yifei Shi",
                "Junwen Huang",
                "Xin Xu",
                "Yifan Zhang",
                "Kai Xu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Shi_StablePose_Learning_6D_Object_Poses_From_Geometrically_Stable_Patches_CVPR_2021_paper.pdf",
            "ref_texts": "[32] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019. 2",
            "ref_ids": [
                "32"
            ],
            "1": ", [36,43,32,51,39] and a survey [10])."
        },
        "Focal length and object pose estimation via render and compare": {
            "authors": [
                "Georgy Ponimatkin",
                "Yann Labbe",
                "Bryan Russell",
                "Mathieu Aubry",
                "Josef Sivic"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Ponimatkin_Focal_Length_and_Object_Pose_Estimation_via_Render_and_Compare_CVPR_2022_paper.pdf",
            "ref_texts": "[34] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. In CVPR , pages 4561\u20134570, 2019. 2",
            "ref_ids": [
                "34"
            ],
            "1": "Previous approaches for this task primarily rely on establishing local 2D-3D correspondences between an image 3825\n and a 3D model using either hand-crafted [2,3,7,8,17,27] or CNN features [12,19,20,31,32,34,35,38,41,42,47,48], followed by robust camera pose estimation using PnP [23].",
            "2": "Both of these strategies rely on shallow hand-designed image features and have been revisited with learnable deep convolutional neural networks (CNNs)\n[19,20,31,32,34,35,38,41,42,47,48]."
        },
        "Self-supervised category-level 6D object pose estimation with deep implicit shape representation": {
            "authors": [
                "Wanli Peng",
                "Jianhang Yan",
                "Hongtao Wen",
                "Yi Sun"
            ],
            "url": "https://ojs.aaai.org/index.php/AAAI/article/view/20104/19863",
            "ref_texts": "2016. 3d-r2n2: A unified approach for single and multiview 3d object reconstruction. In European conference on computer vision, 628\u2013644. Springer. Ester, M.; Kriegel, H.-P.; Sander, J.; and Xu, X. 1996. Density-based spatial clustering of applications with noise. InInt. Conf. Knowledge Discovery and Data Mining, volume 240, 6. Fan, H.; Su, H.; and Guibas, L. J. 2017. A point set generation network for 3d object reconstruction from a single image. In Proceedings of the IEEE conference on computer vision and pattern recognition, 605\u2013613. Groueix, T.; Fisher, M.; Kim, V . G.; Russell, B.; and Aubry, M. 2018. AtlasNet: A Papier-M \u02c6ach\u00b4e Approach to Learning 3D Surface Generation. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). He, K.; Gkioxari, G.; Doll \u00b4ar, P.; and Girshick, R. 2017. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, 2961\u20132969. He, Y .; Sun, W.; Huang, H.; Liu, J.; Fan, H.; and Sun, J. 2020. Pvn3d: A deep point-wise 3d keypoints voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 11632\u201311641. Kingma, D. P.; and Ba, J. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. Lin, J.; Wei, Z.; Li, Z.; Xu, S.; Jia, K.; and Li, Y . 2021. DualPoseNet: Category-level 6D Object Pose and Size Estimation using Dual Pose Network with Refined Learning of Pose Consistency. arXiv preprint arXiv:2103.06526. Liu, S.; Zhang, Y .; Peng, S.; Shi, B.; Pollefeys, M.; and Cui, Z. 2020. Dist: Rendering deep implicit signed distance function with differentiable sphere tracing. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019\u20132028. Lorensen, W. E.; and Cline, H. E. 1987. Marching cubes: A high resolution 3D surface construction algorithm. ACM siggraph computer graphics, 21(4): 163\u2013169. Manhardt, F.; Wang, G.; Busam, B.; Nickel, M.; Meier, S.; Minciullo, L.; Ji, X.; and Navab, N. 2020. CPS++: Improving Class-level 6D Pose and Shape Estimation From Monocular Images With Self-Supervised Learning. arXiv preprint arXiv:2003.05848. Mescheder, L.; Oechsle, M.; Niemeyer, M.; Nowozin, S.; and Geiger, A. 2019. Occupancy networks: Learning 3d reconstruction in function space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 4460\u20134470. Park, J. J.; Florence, P.; Straub, J.; Newcombe, R.; and Lovegrove, S. 2019. Deepsdf: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 165\u2013174. Park, K.; Mousavian, A.; Xiang, Y .; and Fox, D. 2020. Latentfusion: End-to-end differentiable reconstruction and rendering for unseen object pose estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 10710\u201310719. Peng, S.; Liu, Y .; Huang, Q.; Zhou, X.; and Bao, H. 2019. Pvnet: Pixel-wise voting network for 6dof pose estimation. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 4561\u20134570. Pitteri, G.; Ramamonjisoa, M.; Ilic, S.; and Lepetit, V . 2019. On object symmetries and 6d pose estimation from images. In2019 International Conference on 3D Vision (3DV), 614\u2013",
            "ref_ids": [
                "2016"
            ]
        },
        "Sparse steerable convolutions: An efficient learning of se (3)-equivariant features for estimation and tracking of object poses in 3d space": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/8c1b6fa97c4288a4514365198566c6fa-Paper.pdf",
            "ref_texts": "[17] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "17"
            ],
            "1": "These works can be broadly categorized into three types: i) template matching [12] by constructing templates to search for the best matched poses; ii) 2D-3D correspondence methods [1,14,16,19,17], which establish 2D-3D correspondence via 2D keypoint detection [19,17] or dense 3D coordinate predictions [1,14,16], followed by a PnP algorithm to obtain the target pose; iii) direct pose regression [26,13,23] via deep networks."
        },
        "Pr-gcn: A deep graph convolutional network with point refinement for 6d pose estimation": {
            "authors": [
                "Guangyuan Zhou",
                "Huiqun Wang",
                "Jiaxin Chen",
                "Di Huang"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_PR-GCN_A_Deep_Graph_Convolutional_Network_With_Point_Refinement_for_ICCV_2021_paper.pdf",
            "ref_texts": "[29] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "29"
            ],
            "1": "PVNet [29] proposes a deep offset prediction model to alleviate negative impacts of occlusions.",
            "2": "Similar to [9, 29, 38], we select the candidate with the highest confidence score as the estimated pose, formulated as: (\u02c6Ro,\u02c6to) = argmaxn (\u02c6R(k) o,\u02c6t(k) o)|k=1,\u00b7\u00b7\u00b7,Kos(k) o.",
            "3": "RGB based methods RGB-D based methods Object PoseCNN* PVNet CDPN DPOD DPVL PF* SSD6D\u2020DF* PVN3D G2L* Ours* Ours [40, 18] [29] [19] [44] [42] [41] [14] [38] [9] [4] ape 77.",
            "4": "Object PoseCNN [40] DeepHeat [26] SS [12] Pix2pose [28] PVNet [29] HybridPose [34] PVN3D [9] Ours Ape 9.",
            "5": "We first compare PR-GCN to the state-of-the-art methods on Linemod, including the RGB based models: PoseCNN (+DeepIM) [40, 18], PVNet [29], CDPN [19], DPOD [44] and DPVL [42] and the RGB-D based ones: Point Fusion [41], SSD6D (+ICP) [14], Dense Fusion [38], PVN3D [9] and G2L[4].",
            "6": "1 ness of PR-GCN to inter-object occlusions, we display detailed results on Occlusion Linemod, in comparison with PoseCNN [40], DeepHeat [26], SS [12], Pix2Pose [28], PVNet [29], HybridPose [34] and PVN3D [9]."
        },
        "Complementary bi-directional feature compression for indoor 360deg semantic segmentation with self-distillation": {
            "authors": [
                "Zishuo Zheng",
                "Chunyu Lin",
                "Lang Nie",
                "Kang Liao",
                "Zhijie Shen",
                "Yao Zhao"
            ],
            "url": "https://openaccess.thecvf.com/content/WACV2023/papers/Zheng_Complementary_Bi-Directional_Feature_Compression_for_Indoor_360deg_Semantic_Segmentation_With_WACV_2023_paper.pdf",
            "ref_texts": "[26] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "26"
            ],
            "1": "mantic segmentation aims to assign each pixel in the image a category label and is critical for various applications such as pose estimation [26], autonomous vehicles [31], augmented reality [2]."
        },
        "Hpnet: Deep primitive segmentation using hybrid representations": {
            "authors": [
                "Siming Yan",
                "Zhenpei Yang",
                "Chongyang Ma",
                "Haibin Huang",
                "Etienne Vouga",
                "Qixing Huang"
            ],
            "url": "http://openaccess.thecvf.com/content/ICCV2021/papers/Yan_HPNet_Deep_Primitive_Segmentation_Using_Hybrid_Representations_ICCV_2021_paper.pdf",
            "ref_texts": "[18] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019 , pages 4561\u20134570. Computer Vision Foundation / IEEE, 2019. 3",
            "ref_ids": [
                "18"
            ],
            "1": "The main idea, which has proven to be successful for keypoint-based 6D object pose estimation [18], is to use a large-scale training set and a small-scale validation set."
        },
        "TTA-COPE: Test-Time Adaptation for Category-Level Object Pose Estimation": {
            "authors": [
                "Taeyeop Lee",
                "Jonathan Tremblay",
                "Valts Blukis",
                "Bowen Wen",
                "Uk Lee",
                "Inkyu Shin",
                "Stan Birchfield",
                "In So",
                "Jin Yoon"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_TTA-COPE_Test-Time_Adaptation_for_Category-Level_Object_Pose_Estimation_CVPR_2023_paper.pdf",
            "ref_texts": "[28] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4561\u20134570, 2019. 1",
            "ref_ids": [
                "28"
            ],
            "1": "Advanced methods that focus on diverse variations of object 6D pose estimation have been introduced, such as known 3D objects (instancelevel) [28, 38], category-level [18, 36, 43], few-shot [52], and zero-shot pose estimation [13, 47]."
        },
        "3d object detection and pose estimation of unseen objects in color images with local surface embeddings": {
            "authors": [
                "Giorgia Pitteri",
                "Aurelie Bugeau",
                "Slobodan Ilic",
                "Vincent Lepetit"
            ],
            "url": "http://openaccess.thecvf.com/content/ACCV2020/papers/Pitteri_3D_Object_Detection_and_Pose_Estimation_of_Unseen_Objects_in_ACCV_2020_paper.pdf",
            "ref_texts": ""
        },
        "A survey on deep learning based methods and datasets for monocular 3D object detection": {
            "authors": [],
            "url": "https://www.mdpi.com/2079-9292/10/4/517/pdf",
            "ref_texts": "72. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. PVNet: Pixel-wise Voting Network for 6DOF Pose Estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 16\u201320 June 2019; pp. 4561\u20134570.",
            "ref_ids": [
                "72"
            ],
            "1": "For example, a pixel-wise voting network (PVNet) [72] predicts pixel-level indicators corresponding to the key points so that they can handle truncation or occlusion of object parts.",
            "2": "PVNet [72] also uses a denser key point prediction method, as shown in Figure 11.",
            "3": "Overview of the keypoint localization in PVNet [72].",
            "4": "The most recent trend in monocular 3D object detection is learning deep neural networks to directly regress the 6D pose from a single image [25\u201327,68,75] or to estimate the 2D positions of 3D key points and solve the PnP algorithm [28\u201330,72,76,78,79]."
        },
        "Learning to detect scene landmarks for camera localization": {
            "authors": [
                "Tien Do",
                "Ondrej Miksik",
                "Joseph De",
                "Hyun Soo",
                "Sudipta N. Sinha"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Do_Learning_To_Detect_Scene_Landmarks_for_Camera_Localization_CVPR_2022_paper.pdf",
            "ref_texts": "[52] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, 2019. 3[53] No \u00b4e Pion, Martin Humenberger, Gabriela Csurka, Yohann Cabon, and Torsten Sattler. Benchmarking image retrieval for visual localization. In 3DV, 2020. 2",
            "ref_ids": [
                "52",
                "53"
            ],
            "1": "Further, image retrieval-based methods use scalable techniques [25, 27,45,79] to estimate the query camera pose by interpolating poses of the retrieved database images [11, 53,79,80].",
            "2": "These were initially proposed to find the 6DoF pose of small objects using random forests [34], random ferns [49], and nowadays, CNNs [48, 51,52,55].",
            "3": "3[53] No \u00b4e Pion, Martin Humenberger, Gabriela Csurka, Yohann Cabon, and Torsten Sattler."
        },
        "Uni6dv2: Noise elimination for 6d pose estimation": {
            "authors": [],
            "url": "https://proceedings.mlr.press/v206/sun23b/sun23b.pdf",
            "ref_texts": "2125. Mallick, T., Das, P. P., and Majumdar, A. K. (2014). Characterizations of noise in kinect depth images: A review. IEEE Sensors journal , 14(6):1731\u20131740. Marchand, E., Uchiyama, H., and Spindler, F. (2015). Pose estimation for augmented reality: a hands-on survey. IEEE transactions on visualization and computer graphics, 22(12):2633\u20132651. Mo, N., Gan, W., Yokoya, N., and Chen, S. (2022). Es6d: A computation efficient and symmetry-aware 6d pose regression framework. arXiv preprint arXiv:2204.01080 . Mingshan Sun, Ye Zheng, Tianpeng Bao, Jianqiu Chen, et al. Newell, A., Yang, K., and Deng, J. (2016). Stacked hourglass networks for human pose estimation. In European conference on computer vision , pages 483\u2013499. Oberweger, M., Rad, M., and Lepetit, V . (2018). Making deep heatmaps robust to partial occlusions for 3d object pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV) , pages 119\u2013134. Peng, S., Liu, Y ., Huang, Q., Zhou, X., and Bao, H. (2019a). Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "2125"
            ]
        },
        "Monocular relative pose estimation pipeline for uncooperative resident space objects": {
            "authors": [],
            "url": "https://re.public.polimi.it/bitstream/11311/1216796/3/PIAZM_OA_01-22.pdf",
            "ref_texts": "[24]Peng, S., Liu, Y., Huang, Q., Zhou, X., and Bao, H., \u201cPVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation,\u201d 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , IEEE, 2019, pp. 4556\u20134565. https: //doi.org/10.1109/CVPR.2019.00469.",
            "ref_ids": [
                "24"
            ],
            "1": "[24]Peng, S."
        },
        "Es6d: A computation efficient and symmetry-aware 6d pose regression framework": {
            "authors": [
                "Ningkai Mo",
                "Wanshui Gan",
                "Naoto Yokoya",
                "Shifeng Chen"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Mo_ES6D_A_Computation_Efficient_and_Symmetry-Aware_6D_Pose_Regression_Framework_CVPR_2022_paper.pdf",
            "ref_texts": "[26] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019. 1",
            "ref_ids": [
                "26"
            ],
            "1": "In recent years, methods based on the deep neural network (DNN) have gradually emerged [17, 22, 25, 26, 40]."
        },
        "Pfrl: Pose-free reinforcement learning for 6d pose estimation": {
            "authors": [
                "Jianzhun Shao",
                "Yuhang Jiang",
                "Gu Wang",
                "Zhigang Li",
                "Xiangyang Ji"
            ],
            "url": "http://openaccess.thecvf.com/content_CVPR_2020/papers/Shao_PFRL_Pose-Free_Reinforcement_Learning_for_6D_Pose_Estimation_CVPR_2020_paper.pdf",
            "ref_texts": "[25] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "25"
            ],
            "1": "Some people [24, 29, 25] followed the conventional way to build the 2D3D correspondences and subsequently solve the pose via Figure 1."
        },
        "Learning deep network for detecting 3d object keypoints and 6d poses": {
            "authors": [
                "Wanqing Zhao",
                "Shaobo Zhang",
                "Ziyu Guan",
                "Wei Zhao",
                "Jinye Peng",
                "Jianping Fan"
            ],
            "url": "https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhao_Learning_Deep_Network_for_Detecting_3D_Object_Keypoints_and_6D_CVPR_2020_paper.pdf",
            "ref_texts": "[25] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019.",
            "ref_ids": [
                "25"
            ],
            "1": "Other methods [33, 25] are 6D object pose detection pipelines containing a CNN architecture for object detection.",
            "2": "[25] S."
        },
        "Monocular localization with vector HD map (MLVHM): A low-cost method for commercial IVs": {
            "authors": [],
            "url": "https://www.mdpi.com/1424-8220/20/7/1870/pdf",
            "ref_texts": "31. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 16\u201320 June 2019.",
            "ref_ids": [
                "31"
            ],
            "1": "In recent years, with the development of deep learning and the improvement of on-board computing ability, the information contained in an image can be interpreted down to pixel-level resolution [30], thereby enabling more complex image feature recognition such as semantic information extraction [31,32]."
        },
        "PoET: Pose Estimation Transformer for Single-View, Multi-Object 6D Pose Estimation": {
            "authors": [
                "Anonymous Submission"
            ],
            "url": "https://proceedings.mlr.press/v205/jantos23a/jantos23a.pdf",
            "ref_texts": "[20] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "20"
            ],
            "1": "In recent years, advancements in deep learning for computer vision tasks have been applied to singleview, image-based 6D pose estimation, either to replace components of classical approaches [17, 18, 19, 20], or as end-to-end learned methods, where the 6D pose is directly estimated from the input using convolutional neural networks (CNNs).",
            "2": "[20] S."
        },
        "Detarnet: Decoupling translation and rotation by siamese network for point cloud registration": {
            "authors": [
                "Zhi Chen",
                "Fan Yang",
                "Wenbing Tao"
            ],
            "url": "https://ojs.aaai.org/index.php/AAAI/article/view/19917/19676",
            "ref_texts": ""
        },
        "PointPoseNet: Point pose network for robust 6D object pose estimation": {
            "authors": [
                "Wei Chen",
                "Jinming Duan",
                "Hector Basevi",
                "Hyung Jin",
                "Ales Leonardis"
            ],
            "url": "http://openaccess.thecvf.com/content_WACV_2020/papers/Chen_PonitPoseNet_Point_Pose_Network_for_Robust_6D_Object_Pose_Estimation_WACV_2020_paper.pdf",
            "ref_texts": "[22] S. Peng, Y . Liu, Q. Huang, H. Bao, and X. Zhou. Pvnet: Pixel-wise voting network for 6dof pose estimation. arXiv preprint arXiv:1812.11788 , 2018.",
            "ref_ids": [
                "22"
            ],
            "1": "Instead of regressing 6D pose directly, some other methods [25, 27, 20, 33, 22, 21] make use of 2D keypoints as an intermediate representation for pose estimation.",
            "2": "Inspired by [23, 22], we estimate 6D object pose via multi-stage.",
            "3": "Our method shares features of PVNet [22]: both methods use unit vector regression to estimate pose, however, our method takes point cloud, and instead of using 2D keypoints we use 3D keypoints, then we use our proposed scoring mechanism to access the final pose which is different to the optimization based method in [22].",
            "4": "Learning-based methods [25, 22, 27, 20] address this problem by training their model to predict the 2D keypoints and estimate the pose by PnP algorithm [3, 19].",
            "5": "Different to [22, 38] which predict dense vectors pointing to 2D keypoints, our network predicts dense vectors pointing to 3D keypoints.",
            "6": "However, different from 2D cases in [22, 38] where two nonparallel lines always have an intersection, two nonparallel lines in 3D can have no intersection.",
            "7": "Another way is proposed in [22] which uses the farthest point sampling (FPS) algorithm to sample the keypoints.",
            "8": "We refer to the mechanism which uses the mean value of all pose hypotheses without scoring mechanism as \u201cMEA\u201d and the mechanism using similar optimization method as [22] to compute the final pose from pose hypotheses as \u201cOPT\u201d.",
            "9": "OPT means using similar optimization method as [22] to compute final pose from pose hypotheses.",
            "10": "From Table 2, we can see that our method outperforms its 2D counterpart PVNet [22], the baseline and other state-of-the-art methods, which shows that our method can better utilize 3D information from depth image.",
            "11": "For symmetric objects Egg Box andGlue we use ADD-S metric Method PVNet [22]PoseCNN + DeepIM [38, 13]Frustum-P [23] Hinterstoisser [5] DenseFusion [35] Ours Ape 43.",
            "12": "[22] S."
        },
        "6dof pose estimation of transparent object from a single rgb-d image": {
            "authors": [],
            "url": "https://www.mdpi.com/1424-8220/20/23/6790/pdf",
            "ref_texts": "13. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation. In Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 16\u201318 June 2019; pp. 4561\u20134570. [CrossRef]",
            "ref_ids": [
                "13"
            ],
            "1": "The recovered surface normal, the plane, and the UV map are essential components of the extended point-cloud representation, which contains rich geometric information for 6DoF pose estimation: (1) the recovered surface normal contains important hint to estimate the object\u2019s relative pose (3DoF rotation); (2) the plane where the object placed is closely related to the object\u2019s 3D position; and (3) the UV map encodes the 2D coordinates of points on image, which is crucial for 6DoF object pose estimation [1,13].",
            "2": "The roles of these three components are as follows: (1) PUV(x) indicates the 2D location of a pixel on image plane, and it is an important hint for 6DoF pose estimation [1,13]; (2) Pnorm(x)indicates the surface normal of object, and it is important for relative pose (3DoF rotation) estimation; and (3) Pplane (x)provides the hint where the object is placed, and it helps conduct more accurate 3DoF translation estimation."
        },
        "Interacting Hand-Object Pose Estimation via Dense Mutual Attention": {
            "authors": [
                "Rong Wang",
                "Wei Mao",
                "Hongdong Li"
            ],
            "url": "https://openaccess.thecvf.com/content/WACV2023/papers/Wang_Interacting_Hand-Object_Pose_Estimation_via_Dense_Mutual_Attention_WACV_2023_paper.pdf",
            "ref_texts": "[31] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "31"
            ],
            "1": "Hand-Object Pose Estimation Most previous works tackle 3D hand pose estimation [17, 25, 40, 50, 47] and object pose estimation [27, 31, 44, 49] separately."
        },
        "E2EK: End-to-end regression network based on keypoint for 6D pose estimation": {
            "authors": [],
            "url": "https://uwe-repository.worktribe.com/index.php/preview/9655564/E2EK_RAL_finalversion_plain.pdf",
            "ref_texts": ""
        },
        "Knowledge Distillation for 6D Pose Estimation by Aligning Distributions of Local Predictions": {
            "authors": [
                "Shuxuan Guo",
                "Yinlin Hu",
                "Jose M. Alvarez",
                "Mathieu Salzmann"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Guo_Knowledge_Distillation_for_6D_Pose_Estimation_by_Aligning_Distributions_of_CVPR_2023_paper.pdf",
            "ref_texts": ""
        },
        "Multi-Object Manipulation via Object-Centric Neural Scattering Functions": {
            "authors": [
                "Stephen Tian",
                "Yancheng Cai",
                "Xing Yu",
                "Sergey Zakharov",
                "Katherine Liu",
                "Adrien Gaidon",
                "Yunzhu Li",
                "Jiajun Wu"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Tian_Multi-Object_Manipulation_via_Object-Centric_Neural_Scattering_Functions_CVPR_2023_paper.pdf",
            "ref_texts": "[41] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. In CVPR , pages 4561\u20134570, 2019. 2",
            "ref_ids": [
                "41"
            ],
            "1": "Many prior works investigate the problem of estimating rigid object poses from RGB images, including deep-learning approaches based on correspondences [24, 35, 40, 41, 47, 58] as well as direct regression [9, 13, 29, 53, 59]."
        },
        "A pose proposal and refinement network for better 6d object pose estimation": {
            "authors": [
                "Ameni Trabelsi",
                "Mohamed Chaabane",
                "Nathaniel Blanchard",
                "Ross Beveridge"
            ],
            "url": "https://openaccess.thecvf.com/content/WACV2021/papers/Trabelsi_A_Pose_Proposal_and_Refinement_Network_for_Better_6D_Object_WACV_2021_paper.pdf",
            "ref_texts": "[21] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6DoF pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "21"
            ],
            "1": "Most existing RGB-based methods [13, 20, 21, 23] take advantage of deep learning techniques used for object detection [5, 10, 16, 25] or image segmentation [17] and leverage them for 6D pose estimation.",
            "2": "First, the 2D-projection error, analogously to [21], measures the average distance between the 2D projections in the image space 2387\n Table 1.",
            "3": "We use a threshold of 2 cm for the ADD(-S) metric Methods HMap[20] PVNet[21] DeepIM\u2020[15] OURS\u2020\n2D-Proj 39.",
            "4": "We report percentages of correctly estimated poses averaged over all object classes Method Tekin[28] PVNet[21] SSD6D\u2020[13] DeepIM\u2020[15] OURS\u2020 ADD(-S) 55.",
            "5": "We report percentages of correctly estimated poses averaged over all object classes Method HMap[20] PVNet[21] BB8\u2020[23] DeepIM\u2020[15] OURS\u2020 ADD(-S) 30."
        },
        "Ikea-manual: Seeing shape assembly step by step": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/b645d1a085bcb39bece5c03703b62464-Paper-Datasets_and_Benchmarks.pdf",
            "ref_texts": "[37] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DOF pose estimation. In CVPR , 2019. 6",
            "ref_ids": [
                "37"
            ],
            "1": "Combined with the annotation of assembly parts, this annotation can be leveraged in pose estimation [36,37] and single-view 3D reconstruction [38, 39] tasks."
        },
        "Shape-Constraint Recurrent Flow for 6D Object Pose Estimation": {
            "authors": [
                "Yang Hai",
                "Rui Song",
                "Jiaojiao Li",
                "Yinlin Hu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Hai_Shape-Constraint_Recurrent_Flow_for_6D_Object_Pose_Estimation_CVPR_2023_paper.pdf",
            "ref_texts": ""
        },
        "Reconstruct locally, localize globally: A model free method for object pose estimation": {
            "authors": [
                "Ming Cai",
                "Ian Reid"
            ],
            "url": "https://openaccess.thecvf.com/content_CVPR_2020/papers/Cai_Reconstruct_Locally_Localize_Globally_A_Model_Free_Method_for_Object_CVPR_2020_paper.pdf",
            "ref_texts": "[40] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 1,3,8",
            "ref_ids": [
                "40"
            ],
            "1": "This predefined structural information contributes variously to the classical geometry methods[10,57,31,25,39] and recent machine learning based methods[41,22,48,54,27,47,20,40,52,55,7].",
            "2": "As for the CNN-based approaches, this model acts such as the supervision for network learning[2,38,4,22], a source for synthetic image generation[40,27,22,9] and/or an agent for post-process refinement[27,41,22]etc.",
            "3": "PVNet[40] proposes a method that automatically discovers a set of keypoints on the 3D object surface based on the physical structure, to ensure that their 2D projection are all within the silhouette.",
            "4": "[40,42,35] use the textured object model and random poses to generate a large amount of synthetic images to augment (or replace) the limited training images, preventing the network from overfitting.",
            "5": "LINEMOD contains 13 objects sequences 3159\n w/ CAD model w/o CAD model methodBB8\n[41]BB8 w/ rSSD-6D w/ r [22]Tekin [48]DeepIM w/ r [27]DenseFusion [52]Pix2Pose [38]PVNet w/ r [40]SSD-6D\n[22]LieNet [11]Ours ape 27.",
            "6": "Our method outperforms more than half of the learning-based methods and achieves comparable result with the state-of-the-art method, which use a large amount of synthetic training images from new viewpoints [40] and/or 3D model for refinement [52,27].",
            "7": "ADD-10 results are shown in Table 3following the test scheme of [40].",
            "8": "The additional landmark branch provides consistence for objects across mul-Tekin [48]PoseCNN\n[54]Oberweger [35]PVNet [40]Pix2Pose [38]Ours ape 2."
        },
        "Sequential voting with relational box fields for active object detection": {
            "authors": [
                "Qichen Fu",
                "Xingyu Liu",
                "Kris Kitani"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Fu_Sequential_Voting_With_Relational_Box_Fields_for_Active_Object_Detection_CVPR_2022_paper.pdf",
            "ref_texts": "[26] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "26"
            ],
            "1": "Our method overcomes inconsistencies across the RBF by using a technique similar to [9,26,32,36], where our voting function finds the consensus from pixel-wise predictions by selecting the bounding box with a majority vote.",
            "2": "[26, 36] use pixel-wise predictions with Hough voting to localize keypoint for pose estimation."
        },
        "Rigidity-Aware Detection for 6D Object Pose Estimation": {
            "authors": [
                "Yang Hai",
                "Rui Song",
                "Jiaojiao Li",
                "Mathieu Salzmann",
                "Yinlin Hu"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Hai_Rigidity-Aware_Detection_for_6D_Object_Pose_Estimation_CVPR_2023_paper.pdf",
            "ref_texts": ""
        },
        "DGECN: A depth-guided edge convolutional network for end-to-end 6D pose estimation": {
            "authors": [
                "Tuo Cao",
                "Fei Luo",
                "Yanping Fu",
                "Wenxiao Zhang",
                "Shengjie Zheng",
                "Chunxia Xiao"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Cao_DGECN_A_Depth-Guided_Edge_Convolutional_Network_for_End-to-End_6D_Pose_CVPR_2022_paper.pdf",
            "ref_texts": "[28] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "28"
            ],
            "1": "It is widely used in the three-dimensional registration of AR [1, 28, 45], robotic vision [27, 31] and 3D reconstruction [9, 10].",
            "2": "Current object pose estimation methods can be divided into two types: 1) the object poses are estimated using a single RGB image [17, 27, 28, 31, 45] or 2) an RGB image accompanying a depth image [14,39,41].",
            "3": "PVNet [28] and Seg-Driven [17] conducted segmentation coupled with voting for each correspondence to make the estimation more robust.",
            "4": "Afterwards, like GDR-Net [42] and PVNet [28], we locate each object in the image with the method of FCN [24].",
            "5": "To deal with multiple objects segmentation, previous works [17, 28, 41, 45] use existing detection or semantic segmentation algorithms.",
            "6": "The 3D keypoints are selected from the 3D object model as in [14, 28].",
            "7": "We follow [28] and adopt the farthest point sampling (FPS) algorithm to select keypoints on object surface.",
            "8": "4 PVNet [28]DG-PnP(Ours) 23.",
            "9": "Our DGECN is comparable to [7, 21, 42] and outperforms [16, 28].",
            "10": "9 PVNet [28] % 47."
        },
        "SMOC-Net: Leveraging Camera Pose for Self-Supervised Monocular Object Pose Estimation": {
            "authors": [
                "Tao Tan",
                "Qiulei Dong"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Tan_SMOC-Net_Leveraging_Camera_Pose_for_Self-Supervised_Monocular_Object_Pose_Estimation_CVPR_2023_paper.pdf",
            "ref_texts": "[22] Sida Peng, Xiaowei Zhou, Yuan Liu, Haotong Lin, Qixing Huang, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "22"
            ],
            "1": "Some existing methods for self-supervised monocular object pose estimation [16, 31] use only synthetic images with object poses (which are generated via Blender [22] or some other rendering tools [24, 29]) for training.",
            "2": "However, due to the domain gap between real and synthetic data, the performances of these self-supervised methods are significantly lower compared to the fully-supervised methods [22, 35].",
            "3": "Keypoint-based methods [22, 28, 32] established correspondences by detecting keypoints in 2D images.",
            "4": "[22] proposed a pixel level voting network (PVNet) by using the direction vector field to predict keypoints, which achieved good performance under severe truncation and occlusion.",
            "5": "6 PVNet [22] 43.",
            "6": "Here, we evaluate the proposed SMOC-Net on the LineMOD dataset in comparison to some state-of-the-art methods, including three fullysupervised methods (DPOD [41], PVNet [22], CDPN [17]), three self-supervised methods that are trained with only synthetic data (AAE [31], MHP [20], DPOD [41]), one selfsupervised method that is trained with both synthetic data and un-annotated real images (DSC-PoseNet [39]), and two self-supervised methods that are trained with synthetic data + un-annotated real images + depth images (Self6D [34], Self6D++ [33])."
        },
        "TANet: towards fully automatic tooth arrangement": {
            "authors": [],
            "url": "https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123600477.pdf",
            "ref_texts": "25. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4561{4570 (2019)",
            "ref_ids": [
                "25"
            ],
            "1": "It aims to infer the three-dimensional pose, which has six degrees of freedom, of an object present in an RGB image, [3, 7, 45, 5, 33, 34, 18, 25], RGB-D image [39, 40, 35], or point cloud data [26, 44, 29, 30]."
        },
        "Cullnet: Calibrated and pose aware confidence scores for object pose estimation": {
            "authors": [
                "Kartik Gupta",
                "Lars Petersson",
                "Richard Hartley"
            ],
            "url": "http://openaccess.thecvf.com/content_ICCVW_2019/papers/R6D/Gupta_CullNet_Calibrated_and_Pose_Aware_Confidence_Scores_for_Object_Pose_ICCVW_2019_paper.pdf",
            "ref_texts": "[17] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019. 3",
            "ref_ids": [
                "17"
            ],
            "1": "Recently proposed PV-Net [17] tries to address the problem of partial occlusion in RGB based object pose estimation by regressing for dense pixel-wise unit vectors pointing to the keypoints, which are combined together using RANSAC like voting scheme."
        },
        "Occlusion-aware region-based 3D pose tracking of objects with temporally consistent polar-based local partitioning": {
            "authors": [],
            "url": "https://zx007zls.github.io/zls_ch/TIP2020_preprint.pdf",
            "ref_texts": "[46] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 4561\u2013",
            "ref_ids": [
                "46"
            ],
            "1": "Apart from template matching, a lot of deep learning-based 3D object detection methods have been proposed recently [41]\u2013[46].",
            "2": "These methods train deep neural networks either to directly predict the pose parameters [41]\u2013\n[43], or to first predict the keypoint locations and then estimate the 6-DOF pose via the PnP algorithm [44]\u2013[46].",
            "3": "When GPU is available, deep learning-based methods (such as [44], [46]) could be incorporated for better performance.",
            "4": "[46] S."
        },
        "Occlusion-robust object pose estimation with holistic representation": {
            "authors": [
                "Bo Chen",
                "Jun Chin",
                "Marius Klimavicius"
            ],
            "url": "http://openaccess.thecvf.com/content/WACV2022/papers/Chen_Occlusion-Robust_Object_Pose_Estimation_With_Holistic_Representation_WACV_2022_paper.pdf",
            "ref_texts": "[44] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 1, 2, 5, 7, 8",
            "ref_ids": [
                "44"
            ],
            "1": "Rather than directly regressing the pose, two-stage approaches [23, 25, 35, 41, 42, 44, 46, 64, 51, 43, 57] first predict landmarks on the object to establish 2D-3D correspondences, then use a Perspective-n-Point (PnP) like algorithm to solve for the pose.",
            "2": "Current works to improve robustness often take the pixel-wise or patch-wise approach [44, 41, 23, 25, 35, 42], i.",
            "3": "PVNet [44] predicts the object mask and, for each pixel within the mask, unit vectors that points to the landmarks.",
            "4": "For LINEMOD, we follow the convention of previousworks [46, 57, 44, 64] by using 15% of the images of each object as training set and the remaining 85% as testing set.",
            "5": "For the YCB-Video dataset we also report the AUC metric proposed in [62] and adopted in [41, 44].",
            "6": "Implementation details For each object model we apply the farthest point sampling (FPS) algorithm [44] on the 3D point cloud and select 11 landmarks.",
            "7": "We also define a measure of incoherence ci=\u2225(xi\u2212x\u2217 i)\u2212m\u22252 (4)\n2934\n ADD(-S)Without refinement With refinement PVNet Pix2Pose DPOD CDPN GDR Ours SSD-6D DPOD+ HybridPose DeepIM\n[44] [42] [64] [35] [59] [27] [64] [51] [33] ape 43.",
            "8": "ADD(-S)Without refinement With refinement HM PVNet Hu Pix2Pose DPOD Hu2 GDR Ours DPOD+ HybridPose [41] [44] [23] [42] [64] [22] [59] [64] [51] ape 15.",
            "9": "Data efficiency The LINEMOD dataset has about 1200 images for each object, which results in approximately 180 images (15%)\n2935\n ADD(-S) AUC of ADD(-S) Without refinement Without refinement With refinement HM Hu Hu2 GDR Ours HM PVNet GDR Ours DeepIM CosyPose [41] [23] [22] [59] [41] [44] [59] [34] [30] master chef can 31.",
            "10": "For example, PVNet [44] renders 20000 images for each object and the same strategy is adopted in [52]."
        },
        "Stability-driven contact reconstruction from monocular color images": {
            "authors": [
                "Zimeng Zhao",
                "Binghui Zuo",
                "Wei Xie",
                "Yangang Wang"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Zhao_Stability-Driven_Contact_Reconstruction_From_Monocular_Color_Images_CVPR_2022_paper.pdf",
            "ref_texts": "[43] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , pages 4561\u20134570, 2019. 2",
            "ref_ids": [
                "43"
            ],
            "1": "With the rapid increase of 3D hand datasets [16,33,61,64,68] and object datasets [23, 33, 61], data-driven methods [2, 15, 25, 26, 30, 37, 43, 54, 60, 63, 66, 67] become popular in the community."
        },
        "CAD2Render: A Modular Toolkit for GPU-accelerated Photorealistic Synthetic Data Generation for the Manufacturing Industry": {
            "authors": [
                "Steven Moonen",
                "Bram Vanherle",
                "Taoufik Bourgana",
                "Abdellatif Bey",
                "Nick Michiels"
            ],
            "url": "https://openaccess.thecvf.com/content/WACV2023W/PIES-CV/papers/Moonen_CAD2Render_A_Modular_Toolkit_for_GPU-Accelerated_Photorealistic_Synthetic_Data_Generation_WACVW_2023_paper.pdf",
            "ref_texts": "[17] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019.",
            "ref_ids": [
                "17"
            ],
            "1": "The identification of the objects and subsequent position and pose estimation was done with two state of the art networks: YoloV4 [2] for object detection and PVNET [17] for pose estimation."
        },
        "Super-BPD: Super boundary-to-pixel direction for fast image segmentation": {
            "authors": [
                "Jianqiang Wan",
                "Yang Liu",
                "Donglai Wei",
                "Xiang Bai",
                "Yongchao Xu"
            ],
            "url": "https://openaccess.thecvf.com/content_CVPR_2020/papers/Wan_Super-BPD_Super_Boundary-to-Pixel_Direction_for_Fast_Image_Segmentation_CVPR_2020_paper.pdf",
            "ref_texts": "[32] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6dof pose estimation. In Proc. of CVPR , pages 4561\u20134570, 2019. 3",
            "ref_ids": [
                "32"
            ],
            "1": "PifPaf [24] and PVNet [32] leverage direction cue for 2D human pose estimation and 6 DoF pose estimation, respectively."
        },
        "Single-shot scene reconstruction": {
            "authors": [
                "Anonymous Submission"
            ],
            "url": "https://openreview.net/pdf?id=CGn3XKSf7vf",
            "ref_texts": "[7] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019.",
            "ref_ids": [
                "7"
            ],
            "1": "Alternatively, 3D detection pipelines detect separate objects and recover their masks and 3D bounding boxes [3, 6, 7, 4], or incorporate relationships between objects by using a graph or physical simulation [8, 9, 10].",
            "2": "The current state-of-the-art methods in object pose estimation almost exclusively belong to the latter group with such representatives as PVNet [7], CDPN [28], EPOS [27], Pix2Pose [4], GDR-Net [29] and DPOD [3, 6].",
            "3": "[7] S."
        },
        "TexPose: Neural Texture Learning for Self-Supervised 6D Object Pose Estimation": {
            "authors": [
                "Hanzhi Chen",
                "Fabian Manhardt",
                "Nassir Navab",
                "Benjamin Busam"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_TexPose_Neural_Texture_Learning_for_Self-Supervised_6D_Object_Pose_Estimation_CVPR_2023_paper.pdf",
            "ref_texts": "[39] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019 , pages 4561\u20134570. Computer Vision Foundation / IEEE, 2019. 1,2",
            "ref_ids": [
                "39"
            ],
            "1": "Noteworthy, accuracy and runtime have both recently made a huge leap forward thanks to deep learning [19,23,31,39,40].",
            "2": "Unfortunately, most of these methods heavily rely on a massive amount of labelled data for supervision to learn precise models withstrong generalisation capabilities [16,39,53,59].",
            "3": "Correspondence-based methods establish 2D-3D correspondences [37,39,40,42,59], prior to leveraging a variant of the RANSAC&PnP paradigm to solvefor pose."
        },
        "Edge enhanced implicit orientation learning with geometric prior for 6D pose estimation": {
            "authors": [],
            "url": "https://haopan.github.io/papers/6dpose.pdf",
            "ref_texts": "[27] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 4561\u20134570.",
            "ref_ids": [
                "27"
            ],
            "1": "While [28] and [33] use bounding box corners as keypoints, a recent work [27] explores using designated surface keypoints for more robust 2D keypoint localization.",
            "2": "[27] S."
        },
        "Bcot: A markerless high-precision 3d object tracking benchmark": {
            "authors": [
                "Jiachen Li",
                "Bin Wang",
                "Shiqiang Zhu",
                "Xin Cao",
                "Fan Zhong",
                "Wenxuan Chen",
                "Te Li",
                "Jason Gu",
                "Xueying Qin"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Li_BCOT_A_Markerless_High-Precision_3D_Object_Tracking_Benchmark_CVPR_2022_paper.pdf",
            "ref_texts": "[32] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. In CVPR , pages 4561\u20134570. Computer Vision Foundation / IEEE, 2019. 1",
            "ref_ids": [
                "32"
            ],
            "1": "Despite the rapid development of single-frame 6DOF pose estimation methods [32, 41], for video analysis 3D tracking can be more accurate and more efficient, and thus is indispensable."
        },
        "HS-Pose: Hybrid Scope Feature Extraction for Category-level Object Pose Estimation": {
            "authors": [
                "Linfang Zheng",
                "Chen Wang",
                "Yinghan Sun",
                "Esha Dasgupta",
                "Hua Chen",
                "Ales Leonardis",
                "Wei Zhang",
                "Hyung Jin"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_HS-Pose_Hybrid_Scope_Feature_Extraction_for_Category-Level_Object_Pose_Estimation_CVPR_2023_paper.pdf",
            "ref_texts": ""
        },
        "3D object tracking with adaptively weighted local bundles": {
            "authors": [],
            "url": "https://jcst.ict.ac.cn/fileup/1000-9000/PDF/2021-3-7-1272.pdf",
            "ref_texts": "[6] Peng S, Liu Y, Huang Q, Zhou X, Bao H. PVNet: Pixel-wise voting network for 6DoF pose estimation. In Proc. the 2019 IEEE Conference on Computer Vision and Pattern Recognition , June 2019, pp.4561-4570. DOI: 10.1109/CVPR.2019.00469.",
            "ref_ids": [
                "6"
            ],
            "1": "3 this is difierent from the 3D object detection and 6DOF pose estimation from a single image, which has been greatly advanced using learning-based approaches[6,7].",
            "2": "[6] Peng S, Liu Y, Huang Q, Zhou X, Bao H."
        },
        "Data-driven object pose estimation in a practical bin-picking application": {
            "authors": [],
            "url": "https://www.mdpi.com/1424-8220/21/18/6093/pdf",
            "ref_texts": "16. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation. In Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 15\u201320 June 2019; pp. 4556\u20134565.",
            "ref_ids": [
                "16"
            ],
            "1": "Some recent methods use CNNs to regress 2D keypoints, using them as an intermediate representation for the Perspective-n-Point (PnP) algorithm to compute 6DoF pose parameters [13\u201315], and other methods improve on this approach by using pixel-wise predictions to provide a flexible representation for localizing occluded or truncated keypoints [8,16]."
        },
        "CorNet: generic 3D corners for 6D pose estimation of new objects without retraining": {
            "authors": [
                "Giorgia Pitteri",
                "Slobodan Ilic",
                "Vincent Lepetit"
            ],
            "url": "http://openaccess.thecvf.com/content_ICCVW_2019/papers/R6D/Pitteri_CorNet_Generic_3D_Corners_for_6D_Pose_Estimation_of_New_ICCVW_2019_paper.pdf",
            "ref_texts": "[24] S. Peng, Y . Liu, Q. Huang, H. Bao, and X. Zhou. Pvnet: Figure 9: Some qualitative results on Object #20 in Scene #13 of the T-LESS dataset. Figure 10: Some qualitative results on Object #20 in Scene #14 of the T-LESS dataset. Figure 11: Some qualitative results on Object #26 and Object #29 in Scene #15 of the T-LESS dataset. Pixel-Wise V oting Network for 6DoF Pose Estimation. CoRR , abs/1812.11788, 2018.",
            "ref_ids": [
                "24"
            ],
            "1": "It is therefore often desirable to rely on color images, and many methods to do so have been proposed recently [19, 25, 31, 18, 35, 24].",
            "2": "Also focusing on occlusion handling, PVNet [24] proposed a network that for each pixel regresses an offset to predefined keypoints.",
            "3": "Somewhat related to our approach, [18, 4, 37, 24] first predict the 3D coordinates of the image locations lying on the objects, in the object coordinate system, and predict the 3D object pose through hypotheses sampling with preemptive RANSAC.",
            "4": "[24] S."
        },
        "Strumononet: Structure-aware monocular 3d prediction": {
            "authors": [
                "Zhenpei Yang",
                "Li Erran",
                "Qixing Huang"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_StruMonoNet_Structure-Aware_Monocular_3D_Prediction_CVPR_2021_paper.pdf",
            "ref_texts": "[29] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019 , pages 4561\u20134570. Computer Vision Foundation / IEEE, 2019.",
            "ref_ids": [
                "29"
            ],
            "1": "Examples include learning a machine translator between two minor languages by composing machine translators via a mother language [19], solving 6D object pose prediction via intermediate keypoint detections [1,31,28,36,27,29,34], and predicting 3D human poses through 2D keypoint predictions [44]."
        },
        "Online object searching by a humanoid robot in an unknown environment": {
            "authors": [],
            "url": "https://raw.githubusercontent.com/aescande/website/master/papers/2021_RAL_Tsuru.pdf",
            "ref_texts": ""
        },
        "Collaborative Viewpoint Adjusting and Grasping via Deep Reinforcement Learning in Clutter Scenes": {
            "authors": [
                "Firstname Lastname",
                "Firstname Lastname",
                "Firstname Lastname"
            ],
            "url": "https://www.mdpi.com/2075-1702/10/12/1135/pdf",
            "ref_texts": "9. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 15\u201320 June 2019; pp. 4561\u20134570.",
            "ref_ids": [
                "9"
            ],
            "1": "According to [9], PVNet is used to vote on the projected 2D feature points, and then find their corresponding relationship to calculate the 6D pose of the object."
        },
        "WeLSA: Learning to Predict 6D Pose from Weakly Labeled Data Using Shape Alignment": {
            "authors": [],
            "url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680633.pdf",
            "ref_texts": "28. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: CVPR (2019)",
            "ref_ids": [
                "28"
            ],
            "1": "Alternatively, [33,28,16] estimate a predefined set of sparse keypoints instead of dense correspondences, which has proven to be more robust to occlusions."
        },
        "DeepFlux for skeleton detection in the wild": {
            "authors": [],
            "url": "http://www.cs.toronto.edu/~sven/Papers/IJCVDeepFlux21.pdf",
            "ref_texts": "1. Ahn, J., Cho, S., & Kwak, S. (2019). Weakly supervised learningof instance segmentation with inter-pixel relations. InProceedingsof IEEE international conference on computer vision and patternrecognition(pp. 2209\u20132218).2. Bai, M., & Urtasun, R. (2017). Deep watershed transform forinstance segmentation. InProceedings of IEEE internationalconference on computer vision and pattern recognition(pp. 2858\u20132866).3. Bai, X., Wang, X., Latecki, L. J., Liu, W., & Tu, Z. (2009). Activeskeleton for non-rigid object detection. InProceedings of IEEEinternational conference on computer vision(pp. 575\u2013582).4. Blum, H. (1973). Biological shape and visual science (part i).Jour-nal of Theoretical Biology,38(2), 205\u2013287.5. Borenstein, E., & Ullman, S. (2002). Class-specific, top-down seg-mentation. InProceedings of European conference on computervision(pp. 109\u2013122).6. Chen, L. C., Hermans, A., Papandreou, G., Schroff, F., Wang, P.,& Adam, H. (2018). Masklab: Instance segmentation by refiningobject detection with semantic and direction features. InProceed-ings of IEEE international conference on computer vision andpattern recognition(pp. 4013\u20134022).7. Chen, L. C., Papandreou, G., Kokkinos, I., Murphy, K., & Yuille,A. L. (2018). Deeplab: Semantic image segmentation with deepconvolutional nets, atrous convolution, and fully connected crfs.IEEE Transactions on Pattern Analysis and Machine Intelligence,40(4), 834\u2013848.8. Chen, X., Fang, H., Lin, T. Y., Vedantam, R., Gupta, S., Doll\u00e1r, P.,& Zitnick, C. L. (2015). Microsoft coco captions: Data collectionand evaluation server. CoRRabs/1504.00325.9. Ci, H., Wang, C., & Wang, Y. (2018). Video object segmentationby learning location-sensitive embeddings. InProceedings of Euro-pean conference on computer vision(pp. 501\u2013516).10. Deng, J., Dong, W., Socher, R., Li, L. J., Li, K., & Li, F. F. (2009).Imagenet: A large-scale hierarchical image database. InProceed-ings of IEEE international conference on computer vision andpattern recognition(pp. 248\u2013255).11. Dickinson, S. J. (2009).Object categorization: Computer andhuman vision perspectives.C a m b r i d g e :C a m b r i d g eU n i v e r s i t yPress.12. Dimitrov, P., Damon, J. N., & Siddiqi, K. (2013). Flux invariants forshape. InProceedings of IEEE international conference on com-puter vision and pattern recognition.13. Ding, J., Xue, N., Long, Y., Xia, G. S., & Lu, Q. (2019). LearningRoI transformer for oriented object detection in aerial images. InProceedings of IEEE international conference on computer visionand pattern recognition(pp. 2849\u20132858).14. Doll\u00e1r, P., & Zitnick, C. L. (2015). Fast edge detection using struc-tured forests.IEEE Transactions on Pattern Analysis and MachineIntelligence,37(8), 1558\u20131570.15. Dufresne-Camaro, C. O., Rezanejad, M., Tsogkas, S., Siddiqi, K.,&D i c k i n s o n ,S .(2 0 2 0 ) .A p p e a r a n c es h o c kg r a m m a rf o rf a s tm e d i a laxis extraction from real images. InProceedings of IEEE interna-tional conference on computer vision and pattern recognition.16. Everingham, M., Van Gool, L., Williams, C. K., Winn, J., & Zisser-man, A. (2010). The pascal visual object classes (voc) challenge.International Journal of Computer Vision,88(2), 303\u2013338.17. Felzenszwalb, P. F., & Huttenlocher, D. P. (2005). Pictorial struc-tures for object recognition.International Journal of ComputerVision,61(1), 55\u201379.18. Felzenszwalb, P. F., & Huttenlocher, D. P. (2012). Distance trans-forms of sampled functions.Theory of Computing,8(1), 415\u2013428.19. Girshick, R., Shotton, J., Kohli, P., Criminisi, A., & Fitzgibbon, A.(2011). Efficient regression of general-activity human poses fromdepth images. InProceedings of IEEE international conference oncomputer vision(pp. 415\u2013422).20. He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learningfor image recognition. InProceedings of IEEE international con-ference on computer vision and pattern recognition(pp. 770\u2013778).21. Jang, J. H., & Hong, K. S. (2001). A pseudo-distance map for thesegmentation-free skeletonization of gray-scale images. InPro-ceedings of IEEE international conference on computer vision(vol. 2, pp. 18\u201323).22. Jerripothula, K. R., Cai, J., Lu, J., & Yuan, J. (2017). Objectco-skeletonization with co-segmentation. InProceedings of IEEEinternational conference on computer vision and pattern recogni-tion(pp. 3881\u20133889).23. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick,R., Guadarrama, S., & Darrell, T. (2014). Caffe: Convolutionalarchitecture for fast feature embedding. InProceedings of ACMmultimedia(pp. 675\u2013678).24. Jiang, Y., Zhu, X., Wang, X., Yang, S., Li, W., Wang, H., Fu, P.,&L u o ,Z .(2 0 1 7 ) .R 2 C N N :R o t a t i o n a lr e g i o nC N Nf o ro r i e n t a t i o nrobust scene text detection. PreprintarXiv:1706.09579.123 International Journal of Computer Vision25. Ke, W., Chen, J., Jiao, J., Zhao, G., & Ye, Q. (2017) SRN: Side-output residual network for object symmetry detection in the wild.InProceedings of IEEE international conference on computervision and pattern recognition(pp. 302\u2013310).26. Kinga, D., & Adam, J. B.: A method for stochastic optimization.InProceedings of international conference on learning represen-tations(vol. 5).27. Kreiss, S., Bertoni, L., & Alahi, A. (2019) PifPaf: Composite fieldsfor human pose estimation. InProceedings of IEEE internationalconference on computer vision and pattern recognition(pp. 11977\u201311986).28. Levinshtein, A., Sminchisescu, C., & Dickinson, S. (2013). Multi-scale symmetric part detection and grouping.International Journalof Computer Vision,104(2), 117\u2013134.29. Lindeberg, T. (1998). Edge detection and ridge detection with auto-matic scale selection.International Journal of Computer Vision,30(2), 117\u2013156.30. Lindeberg, T. (2013). Scale selection properties of generalizedscale-space interest point detectors.Journal of Mathematical Imag-ing and Vision,46(2), 177\u2013210.31. Liu, C., Ke, W., Qin, F., & Ye, Q. (2018). Linear span network forobject skeleton detection. InProceedings of European conferenceon computer vision(pp. 136\u2013151).32. Liu, T. L., Geiger, D., & Yuille, A. L. (1998). Segmenting by seek-ing the symmetry axis. InProceedings of international conferenceon pattern recognition(vol. 2, pp. 994\u2013998).33. Liu, X., Lyu, P., Bai, X., & Cheng, M. M. (2017). Fusing image andsegmentation cues for skeleton extraction in the wild. InProceed-ings of ICCV workshop on detecting symmetry in the wild(vol. 6,p. 8).34. Liu, Y., Cheng, M. M., Hu, X., Wang, K., & Bai, X. (2017). Richerconvolutional features for edge detection. InProceedings of IEEEinternational conference on computer vision and pattern recogni-tion(pp. 5872\u20135881).35. Long, J., Shelhamer, E., & Darrell, T. (2015) Fully convolutionalnetworks for semantic segmentation. InProceedings of IEEE inter-national conference on computer vision and pattern recognition(pp. 3431\u20133440).36. Luo, W., Li, Y., Urtasun, R., & Zemel, R. (2016). Understanding theeffective receptive field in deep convolutional neural networks. InProceedings of advances in neural information processing systems(pp. 4898\u20134906).37. Ma, J., Shao, W., Ye, H., Wang, L., Wang, H., Zheng, Y., et al.(2018). Arbitrary-oriented scene text detection via rotation pro-posals.IEEE Transactions on Multimedia,20(11), 3111\u20133122.38. Maninis, K. K., Pont-Tuset, J., Arbel\u00e1ez, P., & Van Gool, L.(2018). Convolutional oriented boundaries: From image segmen-tation to high-level tasks.IEEE Transactions on Pattern Analysisand Machine Intelligence,40(4), 819\u2013833.39. Marr, D., & Nishihara, H. K. (1978). Representation and recog-nition of the spatial organization of three-dimensional shapes.Proceedings of the Royal Society of London B: Biological Sciences,200(1140), 269\u2013294.40. Martin, D., Fowlkes, C., Tal, D., & Malik, J. (2001). A database ofhuman segmented natural images and its application to evaluatingsegmentation algorithms and measuring ecological statistics. InProceedings of IEEE international conference on computer vision(vol. 2, pp. 416\u2013423).41. Martin, D. R., Fowlkes, C. C., & Malik, J. (2004). Learning todetect natural image boundaries using local brightness, color, andtexture cues.IEEE Transactions on Pattern Analysis and MachineIntelligence,26(5), 530\u2013549.42. M\u00e1ttyus, G., Luo, W., & Urtasun, R. (2017). Deeproadmapper:Extracting road topology from aerial images. InProceedings of theIEEE international conference on computer vision.43. Mattyus, G., Wang, S., Fidler, S., & Urtasun, R. (2015). Enhancingroad maps by parsing aerial images around the world. InProceed-ings of the IEEE international conference on computer vision(pp.1689\u20131697).44. Nedzved, A., Ablameyko, S., & Uchida, S. (2006). Gray-scalethinning by using a pseudo-distance map. InProceedings of IEEEinternational conference on pattern recognition.45. Peng, S., Liu, Y., Huang, Q., Zhou, X., & Bao, H. (2019). PVNet:Pixel-wise voting network for 6dof pose estimation. InProceedingsof IEEE international conference on computer vision and patternrecognition(pp. 4561\u20134570).46. Ren, Z., Yuan, J., Meng, J., & Zhang, Z. (2013). Robust part-basedhand gesture recognition using kinect sensor.IEEE Transactionson Multimedia,15(5), 1110\u20131120.47. Shen, W., Bai, X., Hu, R., Wang, H., & Latecki, L. J. (2011).Skeleton growing and pruning with bending potential ratio.Pat-tern Recognition,44(2), 196\u2013209.48. Shen, W., Bai, X., Hu, Z., & Zhang, Z. (2016). Multiple instancesubspace learning via partial random projection tree for localreflection symmetry in natural images.Pattern Recognition,52,306\u2013316.49. Shen, W., Zhao, K., Jiang, Y., Wang, Y., Bai, X., & Yuille, A.(2017). Deepskeleton: Learning multi-task scale-associated deepside outputs for object skeleton extraction in natural images.IEEETransactions on Image Processing,26(11), 5298\u20135311.50. Shen, W., Zhao, K., Jiang, Y., Wang, Y., Zhang, Z., & Bai, X.(2016). Object skeleton extraction in natural images by fusingscale-associated deep side outputs. InProceedings of IEEE inter-national conference on computer vision and pattern recognition(pp. 222\u2013230).51. Shotton, J., Fitzgibbon, A., Cook, M., Sharp, T., Finocchio, M.,Moore, R., Kipman, A., & Blake, A. (2011) Real-time humanpose recognition in parts from single depth images. InProceedingsof IEEE international conference on computer vision and patternrecognition(pp. 1297\u20131304).52. Siddiqi, K., Bouix, S., Tannenbaum, A., & Zucker, S. W. (2002).Hamilton-jacobi skeletons.International Journal of ComputerVision,48(3), 215\u2013231.53. Siddiqi, K., & Pizer, S. M. (2008).Medial Representations: Math-ematics., Algorithms and Applications Berlin: Springer.54. Siddiqi, K., Shokoufandeh, A., Dickinson, S. J., & Zucker, S. W.(1999). Shock graphs and shape matching.International Journalof Computer Vision,35(1), 13\u201332.55. Sie Ho Lee, T., Fidler, S., & Dickinson, S. (2013). Detecting curvedsymmetric parts using a deformable disc model. InProceedingsof IEEE international conference on computer vision(pp. 1753\u20131760).56. Simonyan, K., & Zisserman, A. (2015). Very deep convolutionalnetworks for large-scale image recognition. InProceedings of inter-national conference on learning representations.57. Sironi, A., Lepetit, V., & Fua, P. (2014). Multiscale centerline detec-tion by learning a scale-space distance transform. InProceedingsof IEEE international conference on computer vision and patternrecognition(pp. 2697\u20132704).58. Trinh, N. H., & Kimia, B. B. (2011). Skeleton search: Category-specific object recognition and segmentation using a skeletal shapemodel.International Journal of Computer Vision,2,2 1 5 \u2013 2 4 0 .59. Tsogkas, S., & Dickinson, S. (2017) AMAT: Medial axis transformfor natural images. InProceedings of IEEE international confer-ence on computer vision(pp. 2727\u20132736).60. Tsogkas, S., & Kokkinos, I. (2012). Learning-based symmetrydetection in natural images. InProceedings of European confer-ence on computer vision(pp. 41\u201354).61. Wang, Y., Xu, Y., Tsogkas, S., Bai, X., Dickinson, S., & Siddiqi, K.(2019). Deepflux for skeletons in the wild. InProceedings of IEEE123 International Journal of Computer Visioninternational conference on computer vision and pattern recogni-tion(pp. 5287\u20135296).62. Wei, S. E., Ramakrishna, V., Kanade, T., & Sheikh, Y. (2016).Convolutional pose machines. InProceedings of IEEE interna-tional conference on computer vision and pattern recognition(pp.4724\u20134732).63. Xia, G., Hu, J., Hu, F., Shi, B., Bai, X., Zhong, Y., et al. (2017).AID: A benchmark data set for performance evaluation of aerialscene classification.IEEE Transactions Geoscience and RemoteSensing,55(7), 3965\u20133981.64. Xia, G. S., Bai, X., Ding, J., Zhu, Z., Belongie, S., Luo, J., Datcu,M., Pelillo, M., & Zhang, L. (2018) DOTA: A large-scale datasetfor object detection in aerial images. InProceedings of IEEE inter-national conference on computer vision and pattern recognition(pp. 3974\u20133983).65. Xie, S., & Tu, Z. (2015). Holistically-nested edge detection. InProceedings of IEEE international conference on computer vision(pp. 1395\u20131403).66. Xu, W., Parmar, G., & Tu, Z. (2019). Geometry-aware end-to-endskeleton detection. InBritish Machine Vision Conference.67. Xu, Y., Wang, Y., Zhou, W., Wang, Y., Yang, Z., & Bai, X. (2019).Textfield: Learning a deep direction field for irregular scene textdetection.IEEE Transactions on Image Processing,28(11), 5566\u20135579.68. Yang, X., Sun, H., Fu, K., Yang, J., Sun, X., Yan, M., et al. (2018).Automatic ship detection in remote sensing images from googleearth of complex scenes based on multiscale rotation dense featurepyramid networks.Remote Sensing,10(1), 132.69. Yu, Z., & Bajaj, C. (2004). A segmentation-free approach for skele-tonization of gray-scale images via anisotropic vector diffusion. InProceedings of IEEE international conference on computer visionand pattern recognition(pp. 415\u2013420).70. Zhang, Q., & Couloigner, I. (2007). Accurate centerline detectionand line width estimation of thick lines using the radon transform.IEEE Transactions on Image Processing,16(2), 310\u2013316.71. Zhang, Z., Shen, W., Yao, C., & Bai, X. (2015). Symmetry-basedtext line detection in natural scenes. InProceedings of IEEE inter-national conference on computer vision and pattern recognition(pp. 2558\u20132567).72. Zhao, K., Shen, W., Gao, S., Li, D., & Cheng, M. M. (2018). Hi-fi:Hierarchical feature integration for skeleton detection. InProceed-ings of international joint conference on artificial intelligence(pp.1191\u20131197).73. Zhu, S. C., & Yuille, A. L. (1996). Forms: A flexible object recog-nition and modelling system.International Journal of ComputerVision,20(3), 187\u2013212.74. Zucker, S. W. (2012). Local field potentials and border owner-ship: A conjecture about computation in visual cortex.Journal ofPhysiology-Paris,106,2 9 7 \u2013 3 1 5 .Publisher\u2019s NoteSpringer Nature remains neutral with regard to juris-dictional claims in published maps and institutional affiliations.",
            "ref_ids": [
                "1"
            ],
            "1": "O u rw o r ki sa l s or e l a t e dt othe approaches in [1,2,6,9,27,38,45,67]w h i c hl e a r nd i r e c -tion cues for edge detection, instance segmentation, and poseestimation.",
            "2": "Finally, direction cues are also used to improveinstance segmentation in [1]a n dd i r e c t i o nfi e l d sp o i n t i n gtowards keypoints are used for pose estimation in [27,45]."
        },
        "Synpo-net\u2014accurate and fast cnn-based 6dof object pose estimation using synthetic training": {
            "authors": [
                "Yongzhi Su",
                "Jason Rambach",
                "Alain Pagani",
                "Didier Stricker"
            ],
            "url": "https://www.mdpi.com/1424-8220/21/1/300/pdf"
        },
        "Estimating 6D aircraft pose from keypoints and structures": {
            "authors": [
                "Runze Fan",
                "Bing X",
                "Zhenzhong Wei"
            ],
            "url": "https://www.mdpi.com/2072-4292/13/4/663/pdf"
        },
        "Sim2real object-centric keypoint detection and description": {
            "authors": [
                "Chengliang Zhong",
                "Chao Yang",
                "Fuchun Sun",
                "Jinshan Qi",
                "Xiaodong Mu",
                "Huaping Liu",
                "Wenbing Huang"
            ],
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/20482/20241",
            "ref_texts": "2019. Key. net: Keypoint detection by handcrafted and learned cnn filters. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 5836\u20135844. Bay, H.; Tuytelaars, T.; and Van Gool, L. 2006. SURF: Speeded Up Robust Features. In Leonardis, A.; Bischof, H.; and Pinz, A., eds., Computer Vision \u2013 ECCV 2006, 404\u2013417. Chai, C.-Y .; Hsu, K.-F.; and Tsao, S.-L. 2019. Multi-step pick-andplace tasks using object-centric dense correspondences. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 4004\u20134011. IEEE. Chan, J.; Addison Lee, J.; and Kemao, Q. 2017. BIND: Binary integrated net descriptors for texture-less object recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2068\u20132076. DeTone, D.; Malisiewicz, T.; and Rabinovich, A. 2018. SuperPoint: Self-Supervised Interest Point Detection and Description. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. Dusmanu, M.; Rocco, I.; Pajdla, T.; Pollefeys, M.; Sivic, J.; Torii, A.; and Sattler, T. 2019. D2-net: A trainable cnn for joint description and detection of local features. In Proceedings of the ieee/cvf conference on computer vision and pattern recognition, 8092\u20138101. Fischler, M. A.; and Bolles, R. C. 1981. Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography. Commun. ACM, 24(6): 381\u2013395. Florence, P.; Manuelli, L.; and Tedrake, R. 2018. Dense Object Nets: Learning Dense Visual Object Descriptors By and For Robotic Manipulation. Conference on Robot Learning. Godard, C.; Mac Aodha, O.; Firman, M.; and Brostow, G. J. 2019. Digging into Self-Supervised Monocular Depth Prediction. Han, X.; Leung, T.; Jia, Y .; Sukthankar, R.; and Berg, A. C. 2015. MatchNet: Unifying feature and metric learning for patch-based matching. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 3279\u20133286. He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep Residual Learning for Image Recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770\u2013778. Ioffe, S.; and Szegedy, C. 2015. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, 448\u2013456. PMLR.J. Lee, J. P. B. H., D. Kim. 2019. SFNet: Learning Object-aware Semantic Flow. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Kendall, A.; and Gal, Y . 2017. What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision? In Guyon, I.; Luxburg, U. V .; Bengio, S.; Wallach, H.; Fergus, R.; Vishwanathan, S.; and Garnett, R., eds., Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc. Kingma, D. P.; and Ba, J. 2017. Adam: A Method for Stochastic Optimization. arXiv:1412.6980. Kulkarni, T. D.; Gupta, A.; Ionescu, C.; Borgeaud, S.; Reynolds, M.; Zisserman, A.; and Mnih, V . 2019. Unsupervised Learning of Object Keypoints for Perception and Control. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc. Liu, Z.; Liu, W.; Qin, Y .; Xiang, F.; Gou, M.; Xin, S.; Roa, M. A.; Calli, B.; Su, H.; Sun, Y .; and Tan, P. 2021. OCRTOC: A CloudBased Competition and Benchmark for Robotic Grasping and Manipulation. arXiv:2104.11446. Lowe, D. G. 2004. Distinctive Image Features from Scale-Invariant Keypoints. International Journal of Computer Vision, 60: 91\u2013110. Mikolajczyk, K.; and Schmid, C. 2005. A performance evaluation of local descriptors. IEEE transactions on pattern analysis and machine intelligence, 27(10): 1615\u20131630. Paszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.; Chanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.; Desmaison, A.; Kopf, A.; Yang, E.; DeVito, Z.; Raison, M.; Tejani, A.; Chilamkurthy, S.; Steiner, B.; Fang, L.; Bai, J.; and Chintala, S. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In Advances in Neural Information Processing Systems 32, 8024\u20138035. Curran Associates, Inc. Peng, S.; Liu, Y .; Huang, Q.; Zhou, X.; and Bao, H. 2019. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 4561\u20134570. Piasco, N.; Sidib \u00b4e, D.; Demonceaux, C.; and Gouet-Brunet, V .",
            "ref_ids": [
                "2019"
            ]
        },
        "Dynamical pose estimation": {
            "authors": [
                "Heng Yang",
                "Chris Doran",
                "Jacques Slotine"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Yang_Dynamical_Pose_Estimation_ICCV_2021_paper.pdf",
            "ref_texts": "[39] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , pages 4561\u20134570, 2019. 2",
            "ref_ids": [
                "39"
            ],
            "1": "4 Problem (1), when specialized to the primitives 1-7, includes a broad class of fundamental perception problems concerning pose estimation from visual measurements, and finds extensive applications to object detection and localization [30,39], motion estimation and 3D reconstruction [58,56], and simultaneous localization and mapping [10,52,42]."
        },
        "Digital Twin Tracking Dataset (DTTD): A New RGB+ Depth 3D Dataset for Longer-Range Object Tracking Applications": {
            "authors": [
                "Weiyu Feng",
                "Seth Z. Zhao",
                "Chuanyu Pan",
                "Adam Chang",
                "Yichen Chen",
                "Zekun Wang",
                "Allen Y. Yang"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023W/VDU/papers/Feng_Digital_Twin_Tracking_Dataset_DTTD_A_New_RGBDepth_3D_Dataset_CVPRW_2023_paper.pdf",
            "ref_texts": "[23] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 3",
            "ref_ids": [
                "23"
            ],
            "1": "6 DoF Object Pose Estimation Most data-driven methods for object pose estimation take RGB [18, 23, 29, 30] or RGB-D images [10, 11, 15, 22, 27] as input."
        },
        "SD-pose: Semantic decomposition for cross-domain 6D object pose estimation": {
            "authors": [
                "Zhigang Li",
                "Yinlin Hu",
                "Mathieu Salzmann",
                "Xiangyang Ji"
            ],
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/16298/16105",
            "ref_texts": "2019 IEEE/CVF International Conference on Computer Vision (ICCV) . Massa, F.; Marlet, R.; and Aubry, M. 2016. Crafting a multi-task CNN for viewpoint estimation. arXiv preprint arXiv:1609.03894 . Park, K.; Patten, T.; and Vincze, M. 2019. Pix2Pose: PixelWise Coordinate Regression of Objects for 6D Pose Estimation. Pavlakos, G.; Zhou, X.; Chan, A.; Derpanis, K. G.; and Daniilidis, K. 2017. 6-dof object pose from semantic keypoints. In Robotics and Automation (ICRA), 2017 IEEE International Conference on, 2011\u20132018. IEEE. Peng, S.; Liu, Y .; Huang, Q.; Zhou, X.; and Bao, H. 2019. PVNet: Pixel-wise V oting Network for 6DoF Pose Estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 4561\u20134570. Rad, M.; and Lepetit, V . 2017. BB8: A Scalable, Accurate, Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects without Using Depth. In IEEE International Conference on Computer Vision (ICCV). Rad, M.; Oberweger, M.; Lepetit, V .; Lepetit, V .; and Lepetit, V . 2018. Domain Transfer for 3D Pose Estimation from Color Images without Manual Annotations. Asian Conference on Computer Vision (ACCV) ."
        },
        "Geometric change detection in digital twins": {
            "authors": [
                "Tiril Sundby",
                "Julia Maria",
                "Adil Rasheed",
                "Mandar Tabib",
                "Omer San"
            ],
            "url": "https://www.mdpi.com/2673-6470/1/2/9/pdf",
            "ref_texts": "21. Peng, S.; Liu, Y.; Huang, Q.; Bao, H.; Zhou, X. PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation. arXiv 2018 , arXiv:1812.11788.",
            "ref_ids": [
                "21"
            ]
        },
        "Canonical voting: Towards robust oriented bounding box detection in 3d scenes": {
            "authors": [
                "Yang You",
                "Zelin Ye",
                "Yujing Lou",
                "Chengkun Li",
                "Lu Li",
                "Lizhuang Ma",
                "Weiming Wang",
                "Cewu Lu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/You_Canonical_Voting_Towards_Robust_Oriented_Bounding_Box_Detection_in_3D_CVPR_2022_paper.pdf",
            "ref_texts": "[14] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "14"
            ],
            "1": "PVNet [14] regresses pixel-wise unit vectors pointing to the predefined keypoints and solves a Perspective-n-Point (PnP) problem for pose estimation in RGB images."
        },
        "Shape Enhanced Keypoints Learning with Geometric Prior for 6D Object Pose Tracking": {
            "authors": [
                "Mateusz Majcher",
                "Bogdan Kwolek"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022W/DLGC/papers/Majcher_Shape_Enhanced_Keypoints_Learning_With_Geometric_Prior_for_6D_Object_CVPRW_2022_paper.pdf",
            "ref_texts": "[20] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. In CVPR , pages 4556\u20134565, 2019. 2, 3",
            "ref_ids": [
                "20"
            ],
            "1": "To mitigate this effect, many top performing two-stage approaches are either based on pixel-vise voting [20] or on generating an ensemble of predictions from each image pixel or patch [17], and then aggregating them to improve final predictions.",
            "2": "Relevant Work Top-performing methods on existing benchmarks, rather than directly regressing the object pose, are based on twostage approaches [12, 16, 17, 20, 21, 24, 25, 31, 33], which first predict landmarks of the object (intermediate features) with established 2D-3D correspondences, and then utilize a PnP like algorithm to determine the pose.",
            "3": "To better cope with occluded objects, [20] proposed a neural network for pixel-wise voting for the 2D keypoints location.",
            "4": "While [21, 25] utilize bounding box corners as keypoints, more recent approaches [20] use designated surface keypoints.",
            "5": "In contrast to [20] which predicts dense vectors pointing to 2D keypoints, our algorithm votes for keypoint confidences on the basis of the (segmented) object shape.",
            "6": "2\n[20] S."
        },
        "Research on non-pooling YOLOv5 based algorithm for the recognition of randomly distributed multiple types of parts": {
            "authors": [
                "Zehua Yu",
                "Ling Zhang",
                "Xingyu Gao",
                "Yang Huang",
                "Xiaoke Liu"
            ],
            "url": "https://www.mdpi.com/1424-8220/22/23/9335/pdf",
            "ref_texts": "2. Peng, S.; Zhou, X.; Liu, Y.; Lin, H.; Huang, Q.; Bao, H. PVNet: Pixel-Wise Voting Network for 6DoF Object Pose Estimation. IEEE Trans. Pattern. Anal. Mach. Intell. 2022 ,44, 3212\u20133223. [CrossRef] [PubMed]",
            "ref_ids": [
                "2"
            ],
            "1": "[2] proposed a pixel-wise voting network (PVNet) that first classified objects in images using a CNN, and then computed the object pose information."
        },
        "Robust 6-DoF Pose Estimation under Hybrid Constraints": {
            "authors": [
                "Hong Ren",
                "Lin Lin",
                "Yanjie Wang",
                "Xin Dong"
            ],
            "url": "https://www.mdpi.com/1424-8220/22/22/8758/pdf",
            "ref_texts": "27. Peng, S.; Zhou, X.; Liu, Y.; Lin, H.; Huang, Q.; Bao, H. PVNet: Pixel-Wise Voting Network for 6DoF Object Pose Estimation. IEEE Trans. Pattern Anal. Mach. Intell. 2022 ,44, 3212\u20133223. [CrossRef]",
            "ref_ids": [
                "27"
            ],
            "1": "proposed the PVNet [27] algorithm, which predicts the direction of all pixels in the object to the keypoints, and uses a RANSAC algorithm-based voting method to determine the keypoints, thus improving the robustness of the pose estimation against occlusion greatly.",
            "2": "Therefore, the strategy of [27] was used to add synthetic images to the training set, resulting in a total of 20,000 training images per class."
        },
        "ASM-Net: Category-level pose and shape estimation using parametric deformation": {
            "authors": [],
            "url": "https://www.bmvc2021-virtualconference.com/assets/papers/1277.pdf",
            "ref_texts": ""
        },
        "6 dof pose estimation of textureless objects from multiple rgb frames": {
            "authors": [],
            "url": "https://campar.cs.tum.edu/pub/kaskman2020eccvw/kaskman2020eccvw.pdf",
            "ref_texts": "40. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4561{4570 (2019)",
            "ref_ids": [
                "40"
            ],
            "1": "In Pixel-wise Voting Network (PVNet) [40].",
            "2": "The network outputs per-seed point classiffcation labels as well as estimation of the keypoint direction vectors, which are further used for the RANSAC-based voting for the object keypoint locations, similar to PVNet [40]."
        },
        "A deep learning framework for accurate vehicle yaw angle estimation from a monocular camera based on part arrangement": {
            "authors": [
                "Wenjun Huang",
                "Wenbo Li",
                "Luqi Tang",
                "Xiaoming Zhu",
                "Bin Zou"
            ],
            "url": "https://www.mdpi.com/1424-8220/22/20/8027/pdf",
            "ref_texts": "9. Peng, S.D.; Liu, Y.; Huang, Q.X.; Zhou, X.W.; Bao, H.J.; Soc, I.C. PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 15\u201320 June 2019.",
            "ref_ids": [
                "9"
            ],
            "1": "Image algorithm researchers pursue information about the shape [3], distance [4,5], velocity [6,7], position, and orientation [8,9] of objects.",
            "2": "The first form involves the construction of a 2D\u20133D correspondence by matching images with 3D model renderings and then using the perspective-n-point method [18] to solve the object pose [9,29,30]."
        },
        "Instancepose: Fast 6dof pose estimation for multiple objects from a single rgb image": {
            "authors": [
                "Lee Aing",
                "Nung Lie",
                "Chiu Chiang",
                "Shiang Lin"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021W/CVinHRC/papers/Aing_InstancePose_Fast_6DoF_Pose_Estimation_for_Multiple_Objects_From_a_ICCVW_2021_paper.pdf",
            "ref_texts": "[19] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4556\u20134565, 2019. 1, 2, 6, 7",
            "ref_ids": [
                "19"
            ],
            "1": "Introduction Many studies [8, 14, 9, 10, 27, 15, 1, 18, 19, 25, 17, 4, 13, 24, 11] involve 6DoF object pose estimation using a single RGB image.",
            "2": "After adding and stacking several stages to fulfill a condition of multiple object pose estimation, some systems [8, 14, 19] become slow or the network structures become more complex and performance decreases significantly.",
            "3": "Two studies, [9, 19] predict the unit-vector fields to estimate the pose.",
            "4": "The maximum processing speed for this method with an 2625\n Metrics 2DPro ADD(S) MethodsPoseCNN S-Driven PVNet S-StageOursPoseCNN S-Driven Pix2Pose PVNet S-StageOurs[26] [10] [19] [9] [26] [10] [18] [19] [9] ape 34.",
            "5": "Datasets The datasets that are used for training come from the Normal LINEMOD dataset [7] and the rendered dataset [19].",
            "6": "However, for the object \u201d glue\u201d, 2626\n Metrics 5CMD 2CMD 5CMD 10CMD MethodsDeepIM PVNetOurs[13] [19] ape 51.",
            "7": "Metrics Time consumption (ms) Methods [10] [18] [19] [9] Ours Data loading 10.",
            "8": "1, 4, 6, 7\n[19] S."
        },
        "DRNet: A depth-based regression network for 6D object pose estimation": {
            "authors": [
                "Lei Jin",
                "Xiaojuan Wang",
                "Mingshu He",
                "Jingyue Wang"
            ],
            "url": "https://www.mdpi.com/1424-8220/21/5/1692/pdf",
            "ref_texts": "28. Peng, S.; Liu, Y.; Huang, Q.; Bao, H.; Zhou, X. PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation. In Proceedings of the CVPR, Long Beach, CA, USA, 15\u201321 June 2019; pp. 4561\u20134570. Sensors 2021 ,21, 1692 14 of 14",
            "ref_ids": [
                "28"
            ],
            "1": "[28] proposed a Pixel-wise Voting Network (PVNet) to identify keypoints with the aid of RANSAC-based voting.",
            "2": "We render 10,000 images for each object in the Linemod dataset as [28].",
            "3": "In addition, we also compare our method with PVNet [28], which uses key points to figure out poses.",
            "4": "4 for the average ADD(S) [28], which calculates the ADD AUC for asymmetric objects and the ADD-S AUC for symmetric objects.",
            "5": "Methods BB8 [53] PoseCNN [20] Pix2Pose [54] PVNet [28] CDPN [51] Our Mean 43."
        },
        "A survey on monocular 3D object detection algorithms based on deep learning": {
            "authors": [
                "Graham Douglas"
            ],
            "url": "https://iopscience.iop.org/article/10.1088/1742-6596/1518/1/012049/pdf",
            "ref_texts": "[19] S. Peng, Y. Liu, Q. Huang, H. Bao, and X. Zhou, \u201cPVNet: Pixel -wise Voting Network for 6DoF Pose Estimation,\u201d arXiv:1812.11788 [cs], Dec. 2018. ",
            "ref_ids": [
                "19",
                "cs"
            ],
            "1": "PVNet[19] uses the dense algorithm in the prediction stage of key points.",
            "2": "03446 [cs], Jan.",
            "3": "07570 [cs], Mar.",
            "4": "02781 [cs], Dec.",
            "5": "03677 [cs, stat], Sep.",
            "6": "00496 [cs], Dec.",
            "7": "10247 [cs], Nov.",
            "8": "08188 [cs], Nov.",
            "9": "00199 [cs], Nov.",
            "10": "10955 [cs], Mar.",
            "11": "12681 [cs], Apr.",
            "12": "06038 [cs], Aug.",
            "13": "08848 [cs], Nov.",
            "14": "[19] S.",
            "15": "11788 [cs], Dec.",
            "16": "00593 [cs], Dec.",
            "17": "08848 [cs], Nov.",
            "18": "02413 [cs], Jun.",
            "19": "07179 [cs], Dec.",
            "20": "01690 [cs], 2019.",
            "21": "11444 [cs], Mar.",
            "22": "12222 [cs], Nov."
        },
        "End-to-end learning improves static object geo-localization from video": {
            "authors": [
                "Mohamed Chaabane",
                "Lionel Gueguen",
                "Ameni Trabelsi",
                "Ross Beveridge",
                "Stephen O"
            ],
            "url": "http://openaccess.thecvf.com/content/WACV2021/papers/Chaabane_End-to-End_Learning_Improves_Static_Object_Geo-Localization_From_Video_WACV_2021_paper.pdf",
            "ref_texts": "[24] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6DoF pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "24"
            ],
            "1": "5D Pose Estimation Many state-of-the-art methods for object pose estimation [15, 22, 24, 30, 35] use 3D models of the objects."
        },
        "Vision-based Neural Scene Representations for Spacecraft": {
            "authors": [
                "Anne Mergy",
                "Gurvan Lecuyer",
                "Dawa Derksen",
                "Dario Izzo"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2021W/AI4Space/papers/Mergy_Vision-Based_Neural_Scene_Representations_for_Spacecraft_CVPRW_2021_paper.pdf",
            "ref_texts": "[37] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "37"
            ],
            "1": "In theory, this information could be inferred using Machine Learning methods [37,12] which usually rely on the detection of keypoints and thus requires extensive annotation for training."
        },
        "Ensemble of 6 DoF Pose estimation from state-of-the-art deep methods.": {
            "authors": [
                "Ibon Merino"
            ],
            "url": "https://addi.ehu.es/bitstream/handle/10810/61846/1-s2.0-S0925231223003934-main.pdf?sequence=1&isAllowed=y",
            "ref_texts": "[27] S. Peng, Y. Liu, Q. Huang, X. Zhou, H. Bao, Pvnet: Pixel-wise voting network for 6dof pose estimation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561\u20134570.",
            "ref_ids": [
                "27"
            ],
            "1": "[27] introduces a pixel-wise voting network (PVNet) to regress pixel-wise vectors pointing to the keypoints and uses those vectors to vote for the location of keypoints.",
            "2": "[27] S."
        },
        "Joint Hand and Object Pose Estimation from a Single RGB Image using High\u2010level 2D Constraints": {
            "authors": [],
            "url": "https://diglib.eg.org/xmlui/bitstream/handle/10.1111/cgf14685/v41i7pp383-394.pdf?sequence=1"
        },
        "SD-Pose: Structural Discrepancy Aware Category-Level 6D Object Pose Estimation": {
            "authors": [
                "Guowei Li",
                "Dongchen Zhu",
                "Guanghui Zhang",
                "Wenjun Shi",
                "Tianyu Zhang",
                "Xiaolin Zhang",
                "Jiamao Li"
            ],
            "url": "https://openaccess.thecvf.com/content/WACV2023/papers/Li_SD-Pose_Structural_Discrepancy_Aware_Category-Level_6D_Object_Pose_Estimation_WACV_2023_paper.pdf",
            "ref_texts": "[27] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "27"
            ],
            "1": "So far, instancelevel 6D pose estimation works [19, 29, 22, 27, 38, 17, 16] have made considerable progress.",
            "2": "For the correspondence between 2D and 3D [27, 29, 30], the pose is obtained by solving a PnP problem [21].",
            "3": "Indirect voting [27, 17] first selects key point positions through RANSAC [10] voting and then calculates the 6D pose of the object according to the correspondence between key points."
        },
        "From IR images to point clouds to pose: point cloud-based AR glasses pose estimation": {
            "authors": [
                "Ahmet Firintepe",
                "Carolin Vey",
                "Stylianos Asteriadis",
                "Alain Pagani",
                "Didier Stricker"
            ],
            "url": "https://www.mdpi.com/2313-433X/7/5/80/pdf",
            "ref_texts": "1. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation. In Proceedings of the The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 15\u201320 June 2019.",
            "ref_ids": [
                "1"
            ],
            "1": "Given that depth information usually necessitates the use of dedicated hardware, image-only approaches for pose estimation have received significant attention over the last years [1,3,4,6,8].",
            "2": "Because handcrafting features is time-consuming and prone to errors, Deep Learning-based approaches have gained popularity and outperform traditional approaches [1,3,4,8].",
            "3": "Recent feature-based Deep Learning methods use Deep Neural Networks to estimate the objects\u2019 keypoints and combine them with PnP , partly relying on traditional methods [1,2,11].",
            "4": "[1] provide a state-of-the-art approach based on keypoint regression and further PnP execution.",
            "5": "Especially the definition of keypoints benefits pose estimation when dealing with occlusions and truncation [1]."
        },
        "Robust 2D/3D vehicle parsing in arbitrary camera views for CVIS": {
            "authors": [
                "Hui Miao",
                "Feixiang Lu",
                "Zongdai Liu",
                "Liangjun Zhang",
                "Dinesh Manocha",
                "Bin Zhou"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Miao_Robust_2D3D_Vehicle_Parsing_in_Arbitrary_Camera_Views_for_CVIS_ICCV_2021_paper.pdf",
            "ref_texts": "[36] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "36"
            ],
            "1": ", AM3D [30], DPOD [60], PV-Net [36], D4LCN [9])."
        },
        "Pose estimation of primitive-shaped objects from a depth image using superquadric representation": {
            "authors": [],
            "url": "https://www.mdpi.com/2076-3417/10/16/5442/pdf",
            "ref_texts": "3. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation. In Proceedings of the CVPR 2019, Long Beach, CA, USA, 15\u201321 June 2019.",
            "ref_ids": [
                "3"
            ]
        },
        "Deep quaternion pose proposals for 6D object pose tracking": {
            "authors": [
                "Mateusz Majcher",
                "Bogdan Kwolek"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021W/DSC/papers/Majcher_Deep_Quaternion_Pose_Proposals_for_6D_Object_Pose_Tracking_ICCVW_2021_paper.pdf",
            "ref_texts": ""
        },
        "A 3D Keypoints Voting Network for 6DoF Pose Estimation in Indoor Scene": {
            "authors": [
                "Huikai Liu",
                "Gaorui Liu",
                "Yue Zhang",
                "Linjian Lei",
                "Hui Xie",
                "Yan Li",
                "Shengli Sun"
            ],
            "url": "https://www.mdpi.com/2075-1702/9/10/230/pdf",
            "ref_texts": "7. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 15\u201320 June 2019; pp. 4561\u20134570.",
            "ref_ids": [
                "7"
            ],
            "1": "CNN is also used in pose estimation, PVNet [7] regress the 2d keypoint through the end-to-end network, and then use the PnP algorithm, estimate the 6d pose by calculating the 2d-3d correspondence relationship of the object.",
            "2": "PVNet [7] first votes the keypoints through RANSAC, then utilizes the 2D-3D correspondence to calculate the 6D pose.",
            "3": "We compare our method with the RGB based methods PoseCNN [9], PVNet [7] and RGDB based methods PointFusion [49], Densefusion [12], PVN3D [15]."
        },
        "DeepRM: Deep Recurrent Matching for 6D Pose Refinement": {
            "authors": [
                "Alexander Avery",
                "Andreas Savakis"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023W/RHOBIN/papers/Avery_DeepRM_Deep_Recurrent_Matching_for_6D_Pose_Refinement_CVPRW_2023_paper.pdf",
            "ref_texts": "[22] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNET: Pixel-wise voting network for 6dof pose estimation. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition , 2019June:4556\u20134565, dec 2019. 2, 5, 6",
            "ref_ids": [
                "22"
            ],
            "1": "To further address the problem of occlusion, PVNet [22] introduced a pixel-wise voting network using RANSAC, resulting in an estimator that is capable of detecting keypoints, even when they are occluded.",
            "2": "Evaluation Metrics To evaluate the performance against other state-of-theart methods, we follow [7, 17, 18, 22, 31, 32] and use the ADD metric [12].",
            "3": "31 PVNet [22] 1 73.",
            "4": "9 PVNet [22] \u22c6 1 40.",
            "5": "Initial predictions are obtained from PVNet [22], where DeepRM outperforms all existing methods except for ZebraPose [27] and CRT-6D [4]."
        },
        "Reflective texture-less object registration using multiple edge features for augmented reality assembly": {
            "authors": [],
            "url": "https://www.researchsquare.com/article/rs-1578869/latest.pdf",
            "ref_texts": "23. S. Peng, Y. Liu, Q. Huang, H. Bao, and X. Zhou, \"PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation,\" 2018.",
            "ref_ids": [
                "23"
            ],
            "1": "PVNet [23] predicted the direction from each pixel to each key point, so the spatial probability distribution of two-dimensional key points can be obtained just like RANSAC."
        },
        "Learning to Estimate Object Poses without Real Image Annotations.": {
            "authors": [
                "Haotong Lin",
                "Sida Peng",
                "Zhize Zhou",
                "Xiaowei Zhou"
            ],
            "url": "https://www.ijcai.org/proceedings/2022/0162.pdf",
            "ref_texts": "[Park et al., 2019 ]Kiru Park, Timothy Patten, and Markus Vincze. Pix2pose: Pixel-wise coordinate regression of objects for 6d pose estimation. In ICCV, pages 7668\u20137677, 2019.[Peng et al., 2019 ]Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR, pages 4561\u2013",
            "ref_ids": [
                "Park et al\\., 2019 ",
                "Peng et al\\., 2019 "
            ]
        },
        "Smart Task Assistance in Mixed Reality for Astronauts": {
            "authors": [
                "Qingwei Sun",
                "Wei Chen",
                "Jiangang Chao",
                "Wanhong Lin",
                "Zhenying Xu",
                "Ruizhi Cao"
            ],
            "url": "https://www.mdpi.com/1424-8220/23/9/4344/pdf",
            "ref_texts": "29. Peng, S.; Zhou, X.; Liu, Y.; Lin, H.; Huang, Q.; Bao, H. PVNet: Pixel-Wise Voting Network for 6DoF Object Pose Estimation. IEEE Trans. Pattern Anal. Mach. Intell. 2022 ,44, 3212\u20133223. [CrossRef] [PubMed]",
            "ref_ids": [
                "29"
            ]
        },
        "Pose estimation from RGB images of highly symmetric objects using a novel multi-pose loss and differential rendering": {
            "authors": [],
            "url": "https://vbn.aau.dk/ws/files/458094775/IROS_2021_Pose_Estimation_from_RGB_Images_of_Highly_Symmetric_Objects_using_a_Novel_Multi_Pose_Loss_and_Differential_Rendering.pdf",
            "ref_texts": "[9] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR , June 2019.",
            "ref_ids": [
                "9"
            ],
            "1": "Lately, many of these methods have started to be replaced or complemented by machine learning methods [1], [9], [10], [11], [12].",
            "2": "[9] S."
        },
        "Region pixel voting network (RPVNet) for 6D pose estimation from monocular image": {
            "authors": [
                "Feng Xiong",
                "Chengju Liu",
                "Qijun Chen"
            ],
            "url": "https://www.mdpi.com/2076-3417/11/2/743/pdf",
            "ref_texts": "29. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation. In Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Los Alamitos, CA, USA, 16\u201320 June 2019; IEEE Computer Society: Los Alamitos, CA, USA, 2019; pp. 4556\u20134565.",
            "ref_ids": [
                "29"
            ],
            "1": "Thus, PVNet [29], which is the state-of-the-art, predicts vector pointing to a keypoint in the image for every pixel that belongs to the object, and produce the keypoints via a voting procedure, and then a PnP procedure is adopted for final pose estimation.",
            "2": "Instead of using the whole features for keypoints detection as in PVNet [29], the regions specified by the predicted bounding boxes are utilized to select local features by RoIAlign [9].",
            "3": "As suggested by PVNet [29], Nkis set to 8 in our model.",
            "4": "The proposed method adopts a strategy of direction map [29].",
            "5": "Truncation LINEMOD [29] dataset is created by randomly cropping each image of LINEMOD.",
            "6": "Second, the experimental results of many studies [10,12,17,29] show that 2D projection error is more \u201ctolerant\u201d than ADD metric, i.",
            "7": "Two-Stage End-to-End RPVNet Heatmap Tekin [16] BB8 [19] Cull [17] PVNet [29] Deep6D [10] SSD-6D [12] Ape 55.",
            "8": "A similar phenomenon was found in the study of PVNet [29], when comparing predicting keypoints on object, and predicting bounding-box corners.",
            "9": "Given an input of a 480\u0002640 RGB image, the average processing speed was 42 ms, or 23 fps, which is close to PVNet (25 fps) [29]."
        },
        "ParametricNet: 6DoF pose estimation network for parametric shapes in stacked scenarios": {
            "authors": [],
            "url": "https://yongjinliu.github.io/files/2021-ParametricNet-6DoF-Pose-Estimation-Network-for-Parametric-Shapes-in-Stacked-Scenarios.pdf",
            "ref_texts": "[15] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d in IEEE/CVF Conference on CVPR , 2019, pp. 4556\u20134565.",
            "ref_ids": [
                "15"
            ],
            "1": "However, their generalization abilities were not satisfied due to the non-linearity of the rotation space [15].",
            "2": "[15] S."
        },
        "Iterative Coarse-to-Fine 6D-Pose Estimation Using Back-propagation": {
            "authors": [],
            "url": "http://mprg.jp/data/MPRG/C_group/C20210929_araki.pdf",
            "ref_texts": ""
        },
        "Towards an egocentric framework for rigid and articulated object tracking in virtual reality": {
            "authors": [],
            "url": "https://wevr.adalsimeone.me/2020/WEVR2020_Taylor.pdf",
            "ref_texts": "[24] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conf. on Comp. Vision and Pattern Recognition , 2019.",
            "ref_ids": [
                "24"
            ],
            "1": "Works such as PoseCNN [39] and PVNet [24] demonstrate accurate 6DoF pose predictions from RGB images, even in complex, uncontrolled environments [1, 21, 38, 40].",
            "2": "[24] S."
        },
        "Shape-coded aruco: Fiducial marker for bridging 2d and 3d modalities": {
            "authors": [
                "Lilika Makabe",
                "Hiroaki Santo",
                "Fumio Okura",
                "Yasuyuki Matsushita"
            ],
            "url": "https://openaccess.thecvf.com/content/WACV2022/papers/Makabe_Shape-Coded_ArUco_Fiducial_Marker_for_Bridging_2D_and_3D_Modalities_WACV_2022_paper.pdf",
            "ref_texts": "[27] Sida Peng, Xiaowei Zhou, Yuan Liu, Haotong Lin, Qixing Huang, and Hujun Bao. PVNet: Pixel-wise V oting Network for 6DoF Object Pose Estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) , 2020. early access.",
            "ref_ids": [
                "27"
            ],
            "1": "Some recent methods use deep neural networks to infer the 6 degrees-of-freedom (DoF) object poses from a textured 3D shape and image observations [13, 39, 41, 27, 9]."
        },
        "FC-TrackNet: Fast Convergence Net for 6D Pose Tracking in Synthetic Domains": {
            "authors": [
                "Di Jia",
                "Qian Wang",
                "Jun Cao",
                "Peng Cai",
                "Zhiyang Jin"
            ],
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/27077/26849",
            "ref_texts": "7487184. Jonathan, T.; Thang, T.; Balakumar, S.; Yu, X.; Dieter, F.; and Stan, B. 2018. Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects. Paper presented at the 2nd Con-ference on Robot Learning. Zurich, CH, October 29-31. Kehl, W.; Manhardt, F.; Tombari, F.; Ilic, S.; and Navab, N. 2017. SSD-6D: Making RGB -Based 3D Detection and 6D Pose Estimation Great Again. In proceedings of the International Conference on Computer Vision.Venice: Institute of Electrical and Electronics Engineers. doi.org/10.1109/ICCV. 2019. 00777. Li, Y.; Wang, G.; Ji, X.; and Fox, D. 2020. DeepIM: Deep Iterative Matching for 6D Pose Estimation. Int J Comput Vis 128: 657 \u2013678. doi.org/10.1007/s11263019-01250 -9. Li, Z.; Wang, G.; and Ji, X. 2019. CDPN: Coordinates -Based Disentangled Pose Network for Real -Time RGB -Based 6 -DoF Object Pose Estimation. In proceedings of the International Conference on Computer Vision.Se oul: Institute of Electrical and Electronics Engineers. doi.org/10.1109/ICCV. 2019. 00777. Mitash, C.; Bowen, W.; Kostas, B.; and Abdeslam, B. 2020. Scene-level Pose Estimation for Multiple Instances of Densely Packed Objects. Paper presented at the Confer ence on Robot Learning. Boston, MA, October 30-November 1. Pavlakos, G.; Zhou, X.; Chan, A.; Derpanis, K. G.; and Daniilidis, K. 2017. 6-DoF object pose from semantic keypoints. In proceedings of the IEEE International Conference on Robotics and Auto-mation .Singapore: Institute of Electrical and Electronics Engineers. doi.org/10.1109/ICRA. 2017. 7989233. Peng, S.; Zhou, X.; Liu, Y.; Lin, H.; Huang, Q.; and Bao, H. 2020. PVNet: Pixel-Wise Voting Network for 6DoF Object Pose Estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence 44(6): 3212 \u20133223. doi.org/10.1109/TPAMI. 2020. 3047388. Sundermeyer, M.; Marton, Z.-C.; Durner, M.; Brucker, M.; and Triebel, R. 2018. Implicit 3D Orientation Learning for 6D Object Detection from RGB Images. Paper presented at the European Conference on Computer Vision. Munich, DE, September 8-14. Tobin, J.; Fong, R.; Ray, A.; Schneider, J.; Zaremba, W.; and Abbeel, P. 2017. Domain randomization for transferring deep neural networks from simulation to the real world. In proceedings of the Intelligent Robots and Systems.Venice: Institute of Electrical and Electronics Engineers. doi.org/10.1109/IROS. 2017. 8202133. Wang, C.; Xu, D.; Zhu, Y.; Mart\u00edn -Mart\u00edn, R.; Lu, C.; Fei-Fei, L.; and Savarese, S. 2019. DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion. In proceedings of the Computer Vision and Pattern Recognition.California: Computer Vision and Pattern Recog nition. doi.org/10.1109/CVPR. 2019. 00346. Wen, B.; Mitash, C.; Ren, B.; and Bekris, K. E. 2020. se(3) -TrackNet: Data -driven 6D Pose Tracking by Calibrating Image Residuals in Synthetic Domains. In proceedings of the Intelligent Robots and Systems.Nevada: Institute of Electrical and Electronics Engineers. doi.org/10.1109/IROS45743. 2020. 9341314. W\u00fcthrich, M.; Pastor, P.; Kalakrishnan, M.; Bohg, J.; and Schaal, S. 2013. Probabilistic object tracking using a range camera. In proceedings of the Intelligent Ro bots and Systems.Tokyo: Institute of Electrical and Electronics Engineers. doi.org/10.1109/IROS. 2013. ",
            "ref_ids": [
                "7487184"
            ]
        },
        "Bridging the reality gap for pose estimation networks using sensor-based domain randomization": {
            "authors": [
                "Frederik Hagelskjaer",
                "Anders Glent"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021W/3DODI/papers/Hagelskjaer_Bridging_the_Reality_Gap_for_Pose_Estimation_Networks_Using_Sensor-Based_ICCVW_2021_paper.pdf",
            "ref_texts": "[24] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019. 2, 7",
            "ref_ids": [
                "24"
            ],
            "1": "In PVNet [24], the network instead locates keypoints by first segmenting the object and then letting all remaining pixels vote for keypoint locations.",
            "2": "However, as we have trained only on synthetic data, our method is tested both using the 85% split and using all images in the dataset;\n940\n Training DataReal Synthetic Modality RGB RGB-D RGB-D\n[24] [32] [35] [7] [10] [35] [19] Ours Ape 43.",
            "3": "The competing methods are DPOD [35], SSD-6D [19] (obtained from [32]), PVNet [24], DenseFusion [32], PointV oteNet [7] and PVN3D [10].",
            "4": "Training DataReal Synthetic Modality RGB RGB-D RGB-D\n[34] [24] [34] [7] [10] Ours Ape 9."
        },
        "Cross-Attention-Based Reflection-Aware 6D Pose Estimation Network for Non-Lambertian Objects from RGB Images": {
            "authors": [
                "Chenrui Wu",
                "Long Chen",
                "Shiqing Wu"
            ],
            "url": "https://www.mdpi.com/2075-1702/10/12/1107/pdf",
            "ref_texts": "17. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 15\u201320 June 2019; pp. 4561\u20134570.",
            "ref_ids": [
                "17"
            ],
            "1": "PVNet [17] selected eight key points from an object\u2019s surface via the farthest-point-sampling algorithm.",
            "2": "Methods Used for Comparison and Metrics We compared our method with other state-of-the-art 6D pose estimation methods, including PVNet [17] and PSGMN [23].",
            "3": "PVNet [17] PSGMN [23] Ours Object ADDS VSD ADDS VSD ADDS VSD Obj_01 73."
        },
        "VideoPose: Estimating 6D object pose from videos": {
            "authors": [],
            "url": "https://apoorvabeedu.github.io/assets/pdf/VideoPose.pdf",
            "ref_texts": "[25] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4561\u2013",
            "ref_ids": [
                "25"
            ],
            "1": "The classical solution for such 6-DOF pose estimation problems utilises a feature point matching mechanism, followed by Perspective-n-Point (PnP) to correct the estimated pose [26, 29, 25, 12].",
            "2": "Thus, the YCB-Video dataset [36], TLESS [10], and OccludedLINEMOD dataset [16, 25] were introduced.",
            "3": "These datasets have enabled the emergence of novel network designs such as PoseCNN, DPOD [37] and PVNet [36, 25].",
            "4": "To address the problems of heavy occlusions and ambiguities,[25, 12, 23, 22] learn to detect keypoints and then perform PnP."
        },
        "6d object pose estimation in cluttered scenes from RGB images": {
            "authors": [],
            "url": "http://jcst.ict.ac.cn/fileup/1000-9000/PDF/2022-3-16-1311.pdf",
            "ref_texts": "[46] Peng S, Liu Y, Huang Q, Zhou X, Bao H. PVNet: Pixelwise voting network for 6DoF pose estimation. In Proc. the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition , June 2019, pp.4561-4570. DOI: 10.1109/CVPR.2019.00469.",
            "ref_ids": [
                "46"
            ],
            "1": "We compare our proposed approach with the following recent advanced methods: PoseCNN[11], Seg-Driven[14], Tekin[28], SilhoNet[34], Pix2Pose[35], BB8[44], Heatmaps[45], PVnet[46], CDPN[47], and ourprevious work (OCP)[17].",
            "2": "80 PVnet[46]15.",
            "3": "[46] Peng S, Liu Y, Huang Q, Zhou X, Bao H."
        },
        "Iterative pose refinement for object pose estimation based on RGBD data": {
            "authors": [],
            "url": "https://www.mdpi.com/1424-8220/20/15/4114/pdf",
            "ref_texts": "13. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. Pvnet: pixel-wise voting network for 6dof pose estimation. In Proceedings of the 2019 IEEE /CVF Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 15\u201321 June 2019.",
            "ref_ids": [
                "13"
            ],
            "1": "Unlike the approaches [23,30] that use regression to make pose estimate, keypoint-based methods [13,15,17] provide an alternative solution by using pnp solver.",
            "2": "To deal with such problem, plenty of existing methods [13] focus on how to extract reliable object key points for pnp solver to make accurate pose estimate.",
            "3": "PVNet [13] determines the key points with pixel-wise voting network to avoid estimation error under occlusion, and then utilizes uncertainty-driven pnp to estimate object pose.",
            "4": "To deal with such problem, plenty of existing methods [13] focu s on how to extract reliable object key points for pnp solver to make accurate pose estimate.",
            "5": "PVNet [13] determines the key points with pixel-wise voting network to avoid estimation error under occl usion, and then utilizes uncertainty-driven pnp to estimate object pose.",
            "6": "Table 1 shows the accuracy in ADD(-S) metric of the proposed method, in comparison with the state-of-the-art approaches, including BB8 [15], SSD-6D [16], PVNet [13], Tien [10], and DenseFusion [11].",
            "7": "[16] PVNet [13] Tien [10] DenseFusion [11] Proposed Method Ape 40."
        },
        "6D pose estimation of object based on fused region-level feature in cluttered scenes": {
            "authors": [
                "Xiangpeng Liu",
                "Huiping Duanmu",
                "Kang An",
                "Wancheng Wang",
                "Yaqing Song",
                "Qingying Gu",
                "Bo Yuan",
                "Danning Wang"
            ],
            "url": "http://www.lib.shnu.edu.cn/_upload/article/files/a7/4e/a6110c2a40a7b8abb3c1d00023d5/a70d3159-043b-40ac-8e16-4c1fe2f9bd59.pdf",
            "ref_texts": "[23]Peng S, Zhou X, Liu Y, Lin H and Bao H 2019 PVNet: pixel-wise voting network for 6DoF object pose estimation Proc. of IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) (16\u201320 June 2019) pp 4561\u201370",
            "ref_ids": [
                "23"
            ],
            "1": "For instance, pixel-wise voting network (PVNet) [23] regresses pixel-wise vectors pointing to the key points and uses these vectors to vote for the key points\u2019 locations.",
            "2": "On the LINEMOD data set, we used the average distance measurement ADD [42] and 2D reprojection [23] for the measurement.",
            "3": "RGB RGB-D Object categoryBB8 [24]\n(%)PVNet [23] (%)HybridPose [25]PoseCNN\n[14] (%)AAE\n[44] (%)SSD-6D\n[20] (%)DenseFusion [16] (%)Proposed (%) Ape 40.",
            "4": "on Computer Vision and Pattern Recognition (CVPR) (18\u201322 June 2018) pp 292\u2013301\n[23]Peng S, Zhou X, Liu Y, Lin H and Bao H 2019 PVNet: pixel-wise voting network for 6DoF object pose estimation Proc."
        },
        "Attention voting network with prior distance augmented loss for 6DoF pose estimation": {
            "authors": [],
            "url": "https://www.jstage.jst.go.jp/article/transinf/E104.D/7/E104.D_2020EDP7235/_pdf",
            "ref_texts": "[7]S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d Proc. IEEE Conference on Computer Vision and Pattern Recognition, pp.4561\u20134570, 2019.",
            "ref_ids": [
                "7"
            ],
            "1": "Most recent state-of-the-artmethods, such as [7]\u2013[10],[19], adopt a two-stage pipeline based on dense prediction.",
            "2": "Unit direction vector-field representation and Hough voting scheme were proposed for robust 2D keypoint localization and demonstrated its superiority in [6],[7],[17].",
            "3": "However, the Hough voting scheme is non-di fferentiable so that it cannot be integrated into the learning of vector-field represen-tation for joint training, and [6],[7],[17]just use smooth/lscript1 loss[24]to learn it.",
            "4": "A U-structure fully convolutional network is adopted by[6],[7],[14],[17]for 6DoF pose estimation.",
            "5": "xEmbedding the AFAM into the PVNet [7](a standard UNet), we build an Attention Voting Network to further im-prove the performance of our method.",
            "6": "[7],[22]demonstrate that selecting the surface point of the 3D model as the keypoint can acquire more accurate poses.",
            "7": "[7] achieve state-of-the-art performance via their unit directionvector-field representation and Hough voting scheme.",
            "8": "1, the Attention Voting Network predicts pixelwise semantic labels and vector-field representation, then the Hough voting layer [7]uses the intermediate representation to locate the 2D keypoint as described in 3.",
            "9": "tion points of predefined 3D keypoints associated with the 3D object models, where the keypoint localization is implemented through the Hough voting scheme [7]based on the semantic mask and vector-field representation.",
            "10": "Instead, we follow [7],[14] to define the 3D point set for each object by the Farthest Point Sampling (FPS) algorithm [50].",
            "11": "Vector-field representation consists of pixel-wise unit direction vectors for each 2Dkeypoint [6],[7],[17].",
            "12": "[7], Capellen et al.",
            "13": "To learn pixel-wise prediction, the low-resolution feature maps be repeatedly performed feature fusion, convolution, and bi-linear upsampling until the size is restored to H\u00d7W, where the feature fusion can be implemented via channel connec-tion as in [7],[25] or our attention module.",
            "14": "The number of images in each sequence isnot enough to train the deep network, thus we use the codeprovided by [7]to render 10,000 synthetic images for each sequence.",
            "15": "For multiple instances scenes, we generate voting centers through clustering and assign masks to the nearest voting center as [7],[14].",
            "16": "Wefollow previous works [7]to divide the training and testing set.",
            "17": "For YCB-Video dataset,we follow [6],[7]and compute the area under the accuracythreshold curve, i.",
            "18": "MethodsBBS Pix2pose DPOD CDPN PVNet Ours [15] [23] [8] [10] [7] PDAL AFAM PDAL +AFAM ape 27.",
            "19": "MethodsBBS CDPN Oberweger PVNet Ours [15] [10] [20] [7] PDAL AFAM PDAL +AFAM LINEOMD 83.",
            "20": "MethodsPoseCNN Pix2pose DPOD PVNet Ours [6] [23] [8] [7] PDAL AFAM PDAL +AFAM ape 9.",
            "21": "Comparing our methods with PVNet [7], both PDAL and AFAM have significant improvements on most objects, especiallyon ape, cat, duck, etc.",
            "22": "MethodsPoseCNN Oberweger PVNet Ours [6] [20] [7] PDAL AFAM PDAL +AFAM\n2D Projection 39.",
            "23": "Green 3D bounding boxes represent the ground truth poses, blue 3D bounding boxes correspond to the poses predicted by thebaseline (PVNet [7]), yellow and red 3D bounding boxes respectively represent the predicted poses of PDAL and AFAM.",
            "24": "CDPN [10] and the PVNet [7].",
            "25": "[7]S."
        },
        "Iterative 3D Deformable Registration from Single-view RGB Images using Differentiable Rendering.": {
            "authors": [],
            "url": "https://www.scitepress.org/Papers/2022/108171/108171.pdf",
            "ref_texts": "468. Peng, S., Liu, Y ., Huang, Q., Zhou, X., and Bao, H. (2019). PVNet: Pixel-wise voting network for 6DOF pose estimation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4561\u20134570. Periyasamy, A. S., Schwarz, M., and Behnke, S. (2019). Refining 6D object pose predictions using abstract render-and-compare. In IEEE-RAS 19th International Conference on Humanoid Robots (Humanoids) , pages 739\u2013746. Pharr, M., Jakob, W., and Humphreys, G. (2016). Physically based rendering: From theory to implementation. Morgan Kaufmann. Ravi, N., Reizenstein, J., Novotny, D., Gordon, T., Lo, W.Y ., Johnson, J., and Gkioxari, G. (2020). Accelerating 3D deep learning with PyTorch3D. In European Conference on Computer Vision (ECCV) . Rodriguez, D., Cogswell, C., Koo, S., and Behnke, S.",
            "ref_ids": [
                "468"
            ]
        },
        "A 3D real object recognition and localization on SLAM based augmented reality environment": {
            "authors": [
                "Jongin Choe",
                "Ang University",
                "Sanghyun Seo",
                "Ang University"
            ],
            "url": "https://american-cse.org/sites/csci2020proc/pdfs/CSCI2020-6SccvdzjqC7bKupZxFmCoA/762400a745/762400a745.pdf",
            "ref_texts": ""
        },
        "SaMfENet: Self-Attention Based Multi-Scale Feature Fusion Coding and Edge Information Constraint Network for 6D Pose Estimation": {
            "authors": [
                "Zhuoxiao Li",
                "Xiaobing Li",
                "Shihao Chen",
                "Jialong Du",
                "Yong Li"
            ],
            "url": "https://www.mdpi.com/2227-7390/10/19/3671/pdf",
            "ref_texts": "2. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Padua, Italy, 18\u201323 July 2019; pp. 4561\u20134570.",
            "ref_ids": [
                "2"
            ],
            "1": ", PVNet [2], BB8 [3], and PoseCNN [4], and RGB-D data input-based networks, e.",
            "2": "Moreover, PVNet [2] extracts the features of the object through a convolutional neural network at the beginning and adopts pixel-level unit vector representation.",
            "3": "Input RGB RGB-D MethodBB8\n[3]PVNet [2]PoseCNN [4]\n+ DeepIM [18]HRPose [37]SSD-6D [14]\n+ ICP [21]MSCNet [9]DenseFusion [5]EANet [10]Ours ape 40."
        },
        "An improved estimation algorithm of space targets pose based on multi-modal feature fusion": {
            "authors": [
                "Jiang Hua",
                "Tonglin Hao",
                "Liangcai Zeng",
                "Gui Yu"
            ],
            "url": "https://www.mdpi.com/2227-7390/9/17/2085/pdf",
            "ref_texts": "3. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation. In Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 15\u201320 June 2019; IEEE: Long Beach, CA, USA; pp. 4556\u20134565.",
            "ref_ids": [
                "3"
            ],
            "1": "With the development of deep learning technology in the field of two-dimensional images, the method of remote pose estimating from images directly based on CNN architecture has been proposed multiple times [3]."
        },
        "Multi-view object pose distribution tracking for pre-grasp planning on mobile robots": {
            "authors": [],
            "url": "https://findresearcher.sdu.dk/ws/files/206261625/Multi_view_pose_distribution_tracking_ICRA2022_camera_ready_2_.pdf",
            "ref_texts": "[28] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp.",
            "ref_ids": [
                "28"
            ],
            "1": "To improve robustness against occlusions,recent works have focused on the prediction of object key-points [26], [27], [28] and then solve the Perspective-n-Point problem for pose estimation.",
            "2": "[28] S."
        },
        "Deep learning on point clouds with applications in vehicle self-localization": {
            "authors": [],
            "url": "https://oparu.uni-ulm.de/xmlui/bitstream/handle/123456789/48402/diss_nico_engel.pdf?sequence=3",
            "ref_texts": "[PLH+19]Peng, Sida; Liu, Yuan; Huang, Qixing; Zhou, Xiaowei; and Bao, Hujun: PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation . In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR",
            "ref_ids": [
                "PLH\\+19"
            ],
            "1": "[PLH+19]Peng, Sida; Liu, Yuan; Huang, Qixing; Zhou, Xiaowei; and Bao, Hujun: PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation ."
        },
        "Object pose estimation for robotic grasping based on multi-view keypoint detection": {
            "authors": [
                "Zheyuan Hu",
                "Beihang University",
                "Renluan Hou",
                "Beihang University",
                "Jianwei Niu",
                "Beihang University",
                "Zhengzhou University",
                "Xiaolong Yu",
                "Beihang University",
                "Tao Ren",
                "Beihang University",
                "Qingfeng Li",
                "Beihang University"
            ],
            "url": "http://www.cloud-conf.net/ispa2021/proc/pdfs/ISPA-BDCloud-SocialCom-SustainCom2021-3mkuIWCJVSdKJpBYM7KEKW/264600b295/264600b295.pdf",
            "ref_texts": "[12] Sida Peng, Y uan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE/CVF Conf. on Comp. Vision and Pattern Recog., pages 4561\u20134570, 2019.",
            "ref_ids": [
                "12"
            ],
            "1": "6DoF pose estimation can be divided into correspondencebased methods [10], templat-based methods [11], and votingbased methods [12]."
        },
        "Development of Vision Guided Real-Time Trajectory Planning System for Autonomous Ground Refuelling Operations using Hybrid Dataset": {
            "authors": [],
            "url": "https://dspace.lib.cranfield.ac.uk/bitstream/handle/1826/19049/Development_of_vision_guided_real-time_trajectory_planning_system-2023.pdf?sequence=1"
        },
        "Generative model for spacecraft image synthesis using limited dataset": {
            "authors": [],
            "url": "https://taehajeffpark.com/files/papers/parkdamico_aas2020.pdf",
            "ref_texts": ""
        },
        "Kosnet: A unified keypoint, orientation and scale network for probabilistic 6d pose estimation": {
            "authors": [],
            "url": "http://groups.csail.mit.edu/robotics-center/public_papers/Hashimoto20.pdf",
            "ref_texts": ""
        },
        "Experimental Evaluation of Affordance Detection Applied to 6-DoF Pose Estimation for Intelligent Robotic Grasping of Household Objects": {
            "authors": [
                "Aidan Keaveny"
            ],
            "url": "https://uwspace.uwaterloo.ca/bitstream/handle/10012/17716/Keaveny_Aidan.pdf?sequence=3",
            "ref_texts": "[19] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao. PVNET: Pixel-wise Voting Network for 6DoF Pose Estimation. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition , pages 4556\u20134565, 2019. ISSN 10636919. doi: 10.1109/CVPR.2019.00469.",
            "ref_ids": [
                "19"
            ],
            "1": "3 Classical Pose Estimation Classical methods for 6-DoF pose estimation can be broadly classified as template-based methods [15, 16, 17] or feature-based methods [7, 11, 18, 19, 20].",
            "2": "PnP is still widely used in many deep learning frameworks for 6-DoF pose estimation today [11, 18, 19] but feature extractors swap classical algorithms, such as SIFT [21] or SURF [22], for more powerful and robust CNNs.",
            "3": "More recently, the trend has been to learn robust keypoints, descriptors, and matching through a large number of sample images [11, 18, 19].",
            "4": "An alternative to learning sparse keypoints is offered by dense methods in Pixel-wise Voting Network (PVNet) [19], which predict unit vectors pointing to keypoints for each pixel.",
            "5": "The latter has been more effective in estimating pose under heavy occlusion [19].",
            "6": "Thus, the P nP algorithm is still widely used in many deep learning frameworks for 6-DoF pose estimation today and such frameworks can be deployed in real-time [18, 19].",
            "7": "2 Heterogeneous RGB-D Architectures One of the main issues with architectures, such as DOPE [18] or PVNet [19], that utilize only 2D RGB images is that they can perform poorly in terms of accuracy, as errors that are small on the image plane can be large in 3D space.",
            "8": "[19] S."
        },
        "Keypoint Matching via Random Network Consensus": {
            "authors": [],
            "url": "https://openreview.net/pdf?id=WhbWzFg8cZ",
            "ref_texts": "11 Under review as a conference paper at ICLR 2023 David G Lowe. Distinctive image features from scale-invariant keypoints. International journal of computer vision , 60(2):91\u2013110, 2004. Zixin Luo, Tianwei Shen, Lei Zhou, Jiahui Zhang, Yao Yao, Shiwei Li, Tian Fang, and Long Quan. Contextdesc: Local descriptor augmentation with cross-modality context. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 2527\u20132536, 2019. Zixin Luo, Lei Zhou, Xuyang Bai, Hongkai Chen, Jiahui Zhang, Yao Yao, Shiwei Li, Tian Fang, and Long Quan. Aslfeat: Learning local features of accurate shape and localization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 6589\u20136598, 2020. Krystian Mikolajczyk and Cordelia Schmid. Scale & affine invariant interest point detectors. International journal of computer vision , 60(1):63\u201386, 2004. Anastasiia Mishchuk, Dmytro Mishkin, Filip Radenovic, and Jiri Matas. Working hard to know your neighbor\u2019s margins: Local descriptor learning loss. Advances in neural information processing systems , 30, 2017. Dmytro Mishkin, Filip Radenovic, and Jiri Matas. Repeatability is not enough: Learning affine regions via discriminability. In Proceedings of the European Conference on Computer Vision (ECCV) , pp. 284\u2013300, 2018. Raul Mur-Artal and Juan D Tard \u00b4os. Orb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras. IEEE transactions on robotics , 33(5):1255\u20131262, 2017. Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan D Tardos. Orb-slam: a versatile and accurate monocular slam system. IEEE transactions on robotics , 31(5):1147\u20131163, 2015. Hyeonwoo Noh, Andre Araujo, Jack Sim, Tobias Weyand, and Bohyung Han. Large-scale image retrieval with attentive deep local features. In Proceedings of the IEEE international conference on computer vision , pp. 3456\u20133465, 2017. Yuki Ono, Eduard Trulls, Pascal Fua, and Kwang Moo Yi. Lf-net: Learning local features from images. Advances in neural information processing systems , 31, 2018. F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V . Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research , 12:2825\u20132830, 2011. Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 4561\u20134570, 2019. Charles R Qi, Or Litany, Kaiming He, and Leonidas J Guibas. Deep hough voting for 3d object detection in point clouds. In proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 9277\u20139286, 2019. Jerome Revaud, Cesar De Souza, Martin Humenberger, and Philippe Weinzaepfel. R2d2: Reliable and repeatable detector and descriptor. Advances in neural information processing systems , 32, 2019. Edward Rosten and Tom Drummond. Machine learning for high-speed corner detection. In European conference on computer vision , pp. 430\u2013443. Springer, 2006. Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary Bradski. Orb: An efficient alternative to sift or surf. In 2011 International conference on computer vision , pp. 2564\u20132571. Ieee, 2011. Paul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, and Marcin Dymczyk. From coarse to fine: Robust hierarchical localization at large scale. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 12716\u201312725, 2019. Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue: Learning feature matching with graph neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 4938\u20134947, 2020."
        },
        "Joint Learning of Object Detection and Pose Estimation using Augmented Autoencoder": {
            "authors": [],
            "url": "https://www.mva-org.jp/Proceedings/2021/papers/P1-10.pdf",
            "ref_texts": "[13] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019.[14] Yinlin Hu, Pascal Fua, Wei Wang, and Mathieu Salzmann. Single-stage 6d object pose estimation. In CVPR , 2020.",
            "ref_ids": [
                "13",
                "14"
            ],
            "1": ", local feature detection and matching, appearance matching, and pose refinement [7, 8, 9, 10, 11, 12, 13, 14, 15]) for pose estimation.",
            "2": "[14] Yinlin Hu, Pascal Fua, Wei Wang, and Mathieu Salzmann."
        },
        "Towards real-time Scan-versus-BIM: methods applications and challenges": {
            "authors": [],
            "url": "https://ec-3.org/publications/conferences/EC32021/papers/EC32021_176.pdf",
            "ref_texts": ""
        },
        "Advances in biplanar X-ray imaging: calibration and 2D/3D registration": {
            "authors": [],
            "url": "https://repository.uantwerpen.be/docstore/d:irua:15079",
            "ref_texts": "[41]S. Peng, Y. Liu, Q. Huang, H. Bao, and X. Zhou, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d in 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , (Long Beach, CA, USA, USA), June 2019.",
            "ref_ids": [
                "41"
            ],
            "1": "PVNet [41] is another deep learning 67 CHAPTER 4.",
            "2": "The bounding landmarks [41] are selected from the object voxels, while the SIFT landmarks are obtained from 3D SIFT keypoints extracted for conventional image matching [92].",
            "3": "Finally, BoneNet, inspired by PVNet [41], is trained to detect 2D landmarks in fluoroscopy images automatically.",
            "4": "1) where the distances between the measured and reference landmarks are penalized by different weights \u03c9ikbased on their hypothesis covariances [41], which will be further discussed in Section 4.",
            "5": "[41] introduced a technique based on Euclidean distance between voxels and the object\u2019s center (C) to define 3D landmarks of an object given its 3D model.",
            "6": "Two types of landmarks are determined, namely bounding (similar to [41]) and SIFT (Scale-Invariant Feature Transform) landmarks [92].",
            "7": "[41] trained a deep neural network (PVNet) to automatically detect 2D landmarks in an optical image scene.",
            "8": "PVNet was designed for accurate inference of only nine landmarks in optical images [41], which resulted in an unstable and slow convergence when applying to X-ray images with a higher number of landmarks.",
            "9": "Landmark 2D coordinates are then converted to 2D vector fields as in [41].",
            "10": "The exact coordinates of each landmark are computed from its voting vector field using the voting scheme described in [41].",
            "11": "1) is penalized with the inverse of the covariance \u03c3as a higher \u03c3represents a less accurate estimation of the corresponding landmark [41].",
            "12": "The learning loss is composed of smooth L1and cross entropy loss \u2113(\u00b7)for vector field and segment training, respectively [41].",
            "13": "Like PVNet [41], the adam optimizer [38] minimizes the smooth L1 loss, which is equivalent to the Huber loss [101], and cross entropy loss (chapter 9, Murphy 2012 [102]) for the vector fields and object segment learning, respectively.",
            "14": "A multistep learning rate scheduler [41] that adjusts the base learning rate of 10\u22125by a multiplication rate of 0.",
            "15": "[41] was applied to compute the exact coordinates of each landmark based on its masked voting vector field.",
            "16": "One of the most relevant models is PVNet [41], which was introduced to detect 2D landmarks in optical images.",
            "17": "The method employed an automated procedure to robustly detect 3D landmarks compared to the CoM-based technique [41].",
            "18": "[41]S."
        },
        "Fiducial Points-supported Object Pose Tracking on RGB Images via Particle Filtering with Heuristic Optimization.": {
            "authors": [],
            "url": "https://www.scitepress.org/Papers/2021/102371/102371.pdf",
            "ref_texts": "(2017). SSD-6D: Making RGB-Based 3D Detection and 6D Pose Estimation Great Again. In IEEE Int. Conf. on Computer Vision , pages 1530\u20131538. Lepetit, V ., Moreno-Noguer, F., and Fua, P. (2009). EPnP: An accurate O(n) solution to the PnP problem. Int. J. Comput. Vision , 81(2):155\u2013166. Lepetit, V ., Pilet, J., and Fua, P. (2004). Point matching as a classification problem for fast and robust object pose estimation. In CVPR , pages 244\u2013250. Lin, T.-Y ., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., and Zitnick, C. L. (2014). Microsoft COCO: Common Objects in Context. In ECCV , pages 740\u2013755. Springer. Lopatin, V . and Kwolek, B. (2020). 6D pose estimation of texture-less objects on RGB images using CNNs. In The 19th Int. Conf. on Artificial Intelligence and Soft Computing , pages 180\u2013192. Springer. Majcher, M. and Kwolek, B. (2020). 3D Model-Based 6D Object Pose Tracking on RGB Images Using Particle Filtering and Heuristic Optimization. In 15th Int. The Int. Conf. on Computer Vision Theory and Applications (VISAPP) , pages 690\u2013697, vol. 5. SciTePress. Marchand, E., Uchiyama, H., and Spindler, F. (2016). Pose estimation for augmented reality: A hands-on survey. IEEE Trans. on Vis. and Comp. Graphics , 22(12):2633\u20132651. Peng, S., Liu, Y ., Huang, Q., Zhou, X., and Bao, H. (2019). PVNet: Pixel-Wise V oting Network for 6DoF Pose Estimation. In IEEE Conf. on Comp. Vision and Patt. Rec., pages 4556\u20134565. Prisacariu, V . A. and Reid, I. D. (2012). PWP3D: RealTime Segmentation and Tracking of 3D Objects. Int. J. Comput. Vision , 98(3):335\u2013354. Rad, M. and Lepetit, V . (2017). BB8: A scalable, accurate, robust to partial occlusion method for predicting the 3D poses of challenging objects without using depth. InIEEE Int. Conf. on Comp. Vision , pages 3848\u20133856. Ronneberger, O., Fischer, P., and Brox, T. (2015). UNet: Convolutional networks for biomedical image segmentation. In MICCAI , pages 234\u2013241. Springer."
        },
        "Simultaneous Object Detection and Pose Estimation under Domain Shift": {
            "authors": [
                "Stefan Thalhammer"
            ],
            "url": "https://scholar.archive.org/work/frqksitta5brpfxfjmkw6rgeqe/access/wayback/https://repositum.tuwien.at/bitstream/20.500.12708/120374/1/Thalhammer%20Stefan%20-%202022%20-%20Simultaneous%20Object%20Detection%20and%20Pose%20Estimation...pdf",
            "ref_texts": ""
        },
        "MULTI LEVEL REFINEMENT ENRICHED FEATURE PYRAMID NETWORK FOR SCALE AND CLASS IMBALANCE IN OBJECT DETECTION": {
            "authors": [],
            "url": "http://eprints.utm.my/id/eprint/101479/1/LubnaAzizPSC2022.pdf.pdf",
            "ref_texts": "185 Pang, Y., Wang, T., Anwer, R. M., Khan, F. S., and Shao, L. (2019a). Efficie nt featurized image pyramid network for single shot detector. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Pang, Y., Wang, T., Anwer, R. M., Khan, F. S., and Shao, L. (2019b). Efficient featurized image pyramid networ k for single shot detector. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Peng, S., Liu, Y., Huang, Q., Zhou, X., and Bao, H. (2019). Pvnet: Pixel -wise voting network for 6dof pose estimation. Proceedings of the IEEE/CVF C onference on Computer Vision and Pattern Recognition, Picron, C., and Tuytelaars, T. (2021). Trident Pyramid Networks: The importance of processing at the feature pyramid level for better object detection. arXiv preprint arXiv:2110.04004 . Pitts, W., and McCulloch, W. S. (1947). How we know universals the perception of auditory and visual forms. The Bulletin of mathematical biophysics , 9(3), "
        },
        "OLF: RGB-D Adaptive Late Fusion for Robust 6D Pose Estimation": {
            "authors": [],
            "url": "https://hal.science/hal-04085729/document",
            "ref_texts": "[25] Peng, S., Liu, Y., Huang, Q., Zhou, X., and Bao, H., \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in [CVPR ], (2019).",
            "ref_ids": [
                "25",
                "CVPR "
            ],
            "1": ", \u201cDensefusion: 6d object pose estimation by iterative dense fusion,\u201d in [CVPR ], (2019).",
            "2": ", \u201cPvn3d: A deep point-wise 3d keypoints voting network for 6dof pose estimation,\u201d in [CVPR ], (2020).",
            "3": ", \u201cLearning descriptors for object recognition and 3d pose estimation,\u201d in [CVPR ], (2015).",
            "4": ", \u201cPointfusion: Deep sensor fusion for 3d bounding box estimation,\u201d in [CVPR ], (2018).",
            "5": ", \u201cViewpoints and keypoints,\u201d in [CVPR ], (2015).",
            "6": ", \u201cData-driven 3d voxel patterns for object category recognition,\u201d in [CVPR ], (2015).",
            "7": ", \u201c3d bounding box estimation using deep learning and geometry,\u201d in [CVPR ], (2017).",
            "8": ", \u201cFrustum pointnets for 3d object detection from rgb-d data,\u201d in [CVPR ], (2018).",
            "9": ", \u201cPointnet: Deep learning on point sets for 3d classification and segmentation,\u201d in [CVPR ], (2017).",
            "10": ", \u201cVoxelnet: End-to-end learning for point cloud based 3d object detection,\u201d in [CVPR ], (2018).",
            "11": ", \u201cindependent object class detection using 3d feature maps,\u201d in [CVPR ], (2008).",
            "12": ", \u201cRecovering 6d object pose and predicting next-best-view in the crowd,\u201d in [CVPR ], (2016).",
            "13": "[25] Peng, S.",
            "14": ", \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in [CVPR ], (2019).",
            "15": ", \u201cMorefusion: Multi-object reasoning for 6d pose estimation from volumetric fusion,\u201d in [CVPR ], (2020).",
            "16": ", \u201cFfb6d: A full flow bidirectional fusion network for 6d pose estimation,\u201d in [CVPR ], (June 2021)."
        },
        "Pose Guided Feature Learning for 3D Object Tracking on RGB Videos.": {
            "authors": [],
            "url": "https://www.scitepress.org/Papers/2022/108868/108868.pdf",
            "ref_texts": "(2017). Nonlinear Bayesian filtering and learning: A neuronal dynamics for perception. Scientific Reports , 7(1).Majcher, M. and Kwolek, B. (2021). Deep quaternion pose proposals for 6D object pose tracking. In Proceedings of the IEEE/CVF Int. Conf. on Computer Vision (ICCV) Workshops , pages 243\u2013251. Manhardt, F., Wang, G., Busam, B., Nickel, M., Meier, S., Minciullo, L., Ji, X., and Navab, N. (2020). CPS++: Improving class-level 6D pose and shape estimation from monocular images with self-supervised learning. arXiv 2003.05848. Newell, A., Yang, K., and Deng, J. (2016). Stacked hourglass networks for human pose estimation. In ECCV , pages 483\u2013499. Springer. Pavlakos, G., Zhou, X., Chan, A., Derpanis, K. G., and Daniilidis, K. (2017). 6-DoF object pose from semantic keypoints. In IEEE Int. Conf. on Robotics and Automation (ICRA) , pages 2011\u20132018. Peng, S., Liu, Y ., Huang, Q., Zhou, X., and Bao, H. (2019). PVNet: Pixel-Wise V oting Network for 6DoF Pose Estimation. In IEEE Conf. on Comp. Vision and Patt. Rec., pages 4556\u20134565. Prisacariu, V . A. and Reid, I. D. (2012). PWP3D: RealTime Segmentation and Tracking of 3D Objects. Int. J. Comput. Vision , 98(3):335\u2013354. Rad, M. and Lepetit, V . (2017). BB8: A scalable, accurate, robust to partial occlusion method for predicting the 3D poses of challenging objects without using depth. InIEEE Int. Conf. on Comp. Vision , pages 3848\u20133856. Tekin, B., Sinha, S. N., and Fua, P. (2018). Real-time seamless single shot 6D object pose prediction. In IEEE/CVF Conf. on Comp. Vision and Pattern Rec."
        },
        "Pose Tracking vs. Pose Estimation of AR Glasses with Convolutional, Recurrent, and Non-local Neural Networks: A Comparison": {
            "authors": [],
            "url": "https://www-live.dfki.de/fileadmin/user_upload/import/11858_Firintepe2021_EUROXR.pdf",
            "ref_texts": "26. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: PVNet: Pixel-Wise Voting Network for 6DoF Pose Estimation. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (June 2019)",
            "ref_ids": [
                "26"
            ],
            "1": "The keypoint-based methods either use the keypoints from the object's surface [28, 34, 35] or directly predict the 2D projections from 3D models of the object using furthest point algorithm [26, 33, 6].",
            "2": "[26] also removed the ROI pooled orientation prediction branch and used 2D keypoints predicted using Hough-based voting to estimate the pose, creating a hybrid between the two major categories."
        },
        "Learned Perception Systems for Self-Driving Vehicles": {
            "authors": [],
            "url": "https://mountainscholar.org/bitstream/handle/10217/235337/Chaabane_colostate_0053A_17155.pdf?sequence=1",
            "ref_texts": ""
        },
        "Learning and Leveraging Kinematics for Robot Motion Planning under Uncertainty": {
            "authors": [],
            "url": "https://repositories.lib.utexas.edu/bitstream/handle/2152/87739/JAIN-DISSERTATION-2021.pdf?sequence=1",
            "ref_texts": "[107] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561{4570, 2019.",
            "ref_ids": [
                "107"
            ],
            "1": "4 Rigid Body Pose Estimation Articulation model estimation for objects can be viewed as a subset of the body of work on rigid body pose estimation [15, 16, 21, 73, 104, 107, 120, 126, 136, 146, 148, 150].",
            "2": "Some selected recent work on estimating point estimates for rigid body poses are [21, 73, 104, 107, 126, 146, 148, 150]."
        },
        "DenseTransformer: Direct 6D OPE using self-attention on dense representations": {
            "authors": [],
            "url": "https://fse.studenttheses.ub.rug.nl/28988/1/mAI_2022_DesaiN.pdf",
            "ref_texts": "[33] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 4561\u20134570, 2019.",
            "ref_ids": [
                "33"
            ],
            "1": "[33], [16] further reduces complexity of this method by using a pixel wise or regional voting schemes within estimated keypoints.",
            "2": "e, the RGB information is first used to extract features, which are then refined using depth information ([17], [42], [6], [33]).",
            "3": "[39] proposed an end-to-end framework called the PPR-Net for 3D data where the technique is similar to the point-wise voting used in PVNet [33].",
            "4": "1 Preparing Joint Representation The network attempts to create a jointly learnt representation ([63], [21]) as opposed to a mere concatenation of individually learnt representations ([64], [65], [33]).",
            "5": "[33] S."
        },
        "Recovering 6D pose of rigid object from point cloud at the level of instance and category": {
            "authors": [],
            "url": "https://etheses.bham.ac.uk/id/eprint/12424/7/Chen2022PhD.pdf",
            "ref_texts": "[60] Sida Peng, Yuan Liu, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Pvnet: Pixel-wise voting network for 6dof pose estimation. arXiv preprint arXiv:1812.11788 , 2018.",
            "ref_ids": [
                "60"
            ]
        },
        "Perception Systems for Robust Autonomous Navigation in Natural Environments": {
            "authors": [],
            "url": "https://mountainscholar.org/bitstream/handle/10217/235264/Trabelsi_colostate_0053A_17016.pdf?sequence=1",
            "ref_texts": ""
        },
        "Alignment of rendered images with photographs for testing appearance models": {
            "authors": [],
            "url": "http://www2.imm.dtu.dk/~jerf/papers/alignment_lowres.pdf",
            "ref_texts": "9. S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPVNet: Pixel-wise voting network for 6DoF pose estimation,\u201d in Proceedings of CVPR",
            "ref_ids": [
                "9"
            ],
            "1": "For pose estimation, a large dataset is usually employed to train a statistical model [8,9].",
            "2": "[9] present an improved method inspired by Tekin and others that indeed seems not to require a posteriori pose refinement."
        },
        "Object 6DoF Pose Estimation for Power Grid Manipulating Robots": {
            "authors": [],
            "url": "http://mvr.whu.edu.cn/pubs/6dof-ICIG-v3-final.pdf",
            "ref_texts": "15. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561{4570 (2019)",
            "ref_ids": [
                "15"
            ],
            "1": "In such conditions, the prediction ability of these method will be signiffcantly degraded [8, 15]."
        },
        "An intelligent robotic vision system with environment perception": {
            "authors": [],
            "url": "https://etheses.whiterose.ac.uk/31259/1/Yixiang_s_PhD_thesis_final.pdf",
            "ref_texts": "[23] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the 131 IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "23"
            ],
            "1": "However, this paradigm suffers from severe shortcomings in terms of low detection accuracy for texture-less objects and expensive post-processing steps [23].",
            "2": "[101] \u00d7 PVNet[23]\u221a Point CloudCorrespondence-based3DMatch[81]\u221a PPFnet[86]\u221a\n3DFeat-Net[82]\u221a Rpm-net[102]\u221a Template-basedFrustum PointNets[103]\u221a PointFusion[104]\u221a DenseFusion[15]\u221a CloudPose[105]\u221a Voting-basedTombariandDiStefano[18] \u00d7 Woodford et al.",
            "3": "[23], Pixel-wise Voting Network (PVNet), which detect vectors between pixel and keypoints rather than directly regressing keypoints.",
            "4": "[9] proposed PVN3D, that is, an extension of PV-Net [23] in the 3D domain shown in the Figure 2.",
            "5": "As we reviewed in Chapter 2, correspondence-based 6D pose estimation [94, 14, 23, 96] commonly use a PnP algorithm or its variant to compute the 6D pose of an object."
        },
        "Object Recognition and Pose Estimation from RGB-D Data Using Active Sensing": {
            "authors": [],
            "url": "http://web-ext.u-aizu.ac.jp/conference/ieeeuoas/files/CFP_GS_Info_2022/Posters/24.pdf"
        },
        "Graph Neural Network based on Geometric and Appearance Attention for 6D Pose Estimation": {
            "authors": [],
            "url": "https://dl.acm.org/doi/pdf/10.1145/3488933.3488959",
            "ref_texts": "[18] Peng, S., Liu, Y., Huang, Q., Zhou, X., & Bao, H. (2019). Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 4561-4570).",
            "ref_ids": [
                "18"
            ],
            "1": "Instead, indirect methods [6,18,20] establish 2D-3D correspondences between 2D keypoints and 3D keypoints, then solve poses by PnP.",
            "2": "Specifically, we use MLP to estimate the vector from the point pto the center point cfrom the feature: v=c\u2212p |c\u2212p|2(5) where vis a unit vector from point to the center, and use RANSAC for voting [18, 23] to get the coordinate of center.",
            "3": "We fellow previous works [18] to only use this dataset for evaluation by a model trained on LineMOD dataset.",
            "4": "125 Graph Neural Network based on Geometric and Appearance Attention for 6D Pose Estimation AIPR 2021, September 24\u201326, 2021, Xiamen, China Table 2: ADD(-S) performance on LineMOD dataset RGB RGB-D PoseCNN DeepIM [10, 23]PVNet [18] CDPN [11] Dense-Fusion [22]Tian et al.",
            "5": "Table 3: ADD(-S) performance on Occlusion LineMOD dataset PVNet [18] Pix2Pose [17] DPOD [26] DPVL [25] Ours + Colored ICP ape 15.",
            "6": "[18] Peng, S."
        },
        "Optimization of visual SLAM by semantic analysis of the environment": {
            "authors": [],
            "url": "https://theses.hal.science/tel-03967982/document",
            "ref_texts": "2018. Peng, J., Shi, X., Wu, J., & Xiong, Z., (2019), An object-oriented semantic SLAM system towards dynamic environments for mobile manipulation, 2019 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM) , 199\u2013204. Peng, S., Liu, Y., Huang, Q., Zhou, X., & Bao, H., (2019), PVNet: Pixel-wise Voting Network for 6DoF pose estimation, IEEE Conf. on Computer Vision and Pattern Recognition , 4561\u20134570. Pham, Q.-H., Hua, B.-S., Nguyen, T., & Yeung, S.-K., (2019), Real-time progressive 3d semantic segmentation for indoor scenes, 2019 IEEE Winter Conference on Applications of Computer Vision (WACV) , 1089\u20131098. Pham, Q.-H., Nguyen, T., Hua, B.-S., Roig, G., & Yeung, S.-K., (2019), Jsis3d: joint semantic-instance segmentation of 3d point clouds with multi-task pointwise networks and multi-value conditional random fields, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 8827\u20138836. Pizer, S. M., Amburn, E. P., Austin, J. D., Cromartie, R., Geselowitz, A., Greer, T., ter HaarRomeny,B.,Zimmerman,J.B.,&Zuiderveld,K.,(1987),Adaptivehistogram equalization and its variations, Computer vision, graphics, and image processing , 39(3), 355\u2013368. Pizer, S. M., Johnston, R. E., Ericksen, J. P., Yankaskas, B. C., & Muller, K. E., (1990), Contrast-limited adaptive histogram equalization: speed and effectiveness, [1990]",
            "ref_ids": [
                "2018"
            ],
            "1": "[Sucar and Hayet, 2018] formulates the problem of scale estimation as a bayesian estimation using object bounding boxes and a priori sizes.",
            "2": "To decrease inference time we retrained a light 2D detection algorithm (namely tiny YOLOv3 [Redmon and Farhadi, 2018] with a darknet backbone) on the driller.",
            "3": ", 2018] have been using LiDAR scans instead of images as an input for SLAM: [Behley and Stachniss, 2018] is a full SLAM system based only on LiDAR data, which represents the map using a set of surfels.",
            "4": ", 2019] improves on [Behley and Stachniss, 2018] by integrating a CNN to segment LiDAR scans [Milioto et al."
        },
        "\u4f4d\u59ff\u89c6\u89c9\u6d4b\u91cf\u65b9\u6cd5\u53ca\u5e94\u7528\u7efc\u8ff0": {
            "authors": [],
            "url": "https://www.researching.cn/ArticlePdf/m00002/2023/60/3/0312010.pdf",
            "ref_texts": ""
        },
        "6D Pose Estimation of Weakly Textured Object Driven by Decoupling Analysis and Algorithm Fusion Strategy": {
            "authors": [],
            "url": "https://www.researchsquare.com/article/rs-3105669/latest.pdf",
            "ref_texts": ""
        },
        "6D UAV pose estimation for ship landing guidance": {
            "authors": [],
            "url": "https://welcome.isr.tecnico.ulisboa.pt/wp-content/uploads/2021/12/2021141083.pdf",
            "ref_texts": "[14] G. Billings and M. Johnson-Roberson. SilhoNet: An RGB Method for 6D Object Pose Estimation. IEEE Robotics and Automation Letters, 2019.[15] S. Peng, Y . Liu, Q. Huang, X. Zhou and H. Bao. PVNet: Pixelwise V oting Network for 6DoF Pose Estimation. IEEE conference on computer vision and pattern recognition, 2019.",
            "ref_ids": [
                "14",
                "15"
            ],
            "1": "Whereas some methods directly regress the pose hypotheses [12], others form a pipeline of different sub-networks to regress pose parameters [13], [14].",
            "2": "Alternatively, some Convolution Neural Networks (CNNs) architectures produce features, as keypoints, subsequently used to compute the pose estimation [15].",
            "3": "[14] G.",
            "4": "[15] S."
        },
        "Model-free Bin-Picking: Food Processing and Parcel Processing Use Cases": {
            "authors": [],
            "url": "https://i-rim.it/wp-content/uploads/2020/12/I-RIM_2020_paper_153.pdf",
            "ref_texts": "[5] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR , 2019.",
            "ref_ids": [
                "5"
            ],
            "1": "This problem, often referred to as random bin picking , rely on robust 3D pose estimation algorithms that exploit either 2D or 3D vision technologies [1], [6], with an increasingly trend towards datadriven approcehs based on deep models [3], [5].",
            "2": "[5] S."
        },
        "MixedFusion: 6D Object Pose Estimation from Decoupled RGB-Depth Features": {
            "authors": [],
            "url": "https://ailb-web.ing.unimore.it/icpr/media/posters/10918.pdf"
        },
        "An Exploration of the Virtual Digital Twin Capture for Spatial Tasks and its Applications": {
            "authors": [],
            "url": "https://hammer.purdue.edu/ndownloader/files/34929147",
            "ref_texts": "[4]S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
            "ref_ids": [
                "4"
            ],
            "1": "[4]S."
        },
        "Direct pose estimation from RGB images using 3D objects": {
            "authors": [],
            "url": "https://dergipark.org.tr/en/download/article-file/2402755",
            "ref_texts": ""
        },
        "\u57fa\u4e8e\u865a\u62df\u76f8\u673a\u7684\u4f4d\u59ff\u4f30\u8ba1\u7814\u7a76\u8fdb\u5c55": {
            "authors": [],
            "url": "https://www.researching.cn/ArticlePdf/m00002/2022/59/14/1415003.pdf"
        },
        "Deep Learning for Object Detection: Training Data Generation using Parametric CAD Modelling and Gazebo Simulation": {
            "authors": [
                "Akber Khan"
            ],
            "url": "https://trepo.tuni.fi/bitstream/handle/10024/135807/KhanAkberAli.pdf?sequence=4",
            "ref_texts": "[29] S. Peng, Y. Liu, Q. Huang, X. Zhou and H. Bao, \"PVNET: Pixel -wise voting network for 6dof pose estimation,\" in Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019 06-01, vol. 2019, pp. 4556 \u20134565 , 2019. ",
            "ref_ids": [
                "29"
            ],
            "1": "PVNet [29] is an example of an indirect voting -based technique and outperforms some of the earlier method s.",
            "2": "[29] S."
        },
        "A Survey on Deep Learning Based Methods and Datasets for Monocular 3D Object Detection. Electronics 2021, 10, 517": {
            "authors": [],
            "url": "https://pdfs.semanticscholar.org/076b/052fe9aa43e1f8619cc9e8aab29966a32f6d.pdf",
            "ref_texts": "72. Peng, S.; Liu, Y.; Huang, Q.; Zhou, X.; Bao, H. PVNet: Pixel-wise Voting Network for 6DOF Pose Estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 16\u201320 June 2019; pp. 4561\u20134570.",
            "ref_ids": [
                "72"
            ],
            "1": "For example, a pixel-wise voting network (PVNet) [72] predicts pixel-level indicators corresponding to the key points so that they can handle truncation or occlusion of object parts.",
            "2": "PVNet [72] also uses a denser key point prediction method, as shown in Figure 11.",
            "3": "Overview of the keypoint localization in PVNet [72].",
            "4": "The most recent trend in monocular 3D object detection is learning deep neural networks to directly regress the 6D pose from a single image [25\u201327,68,75] or to estimate the 2D positions of 3D key points and solve the PnP algorithm [28\u201330,72,76,78,79]."
        },
        "Impact of Segmentation and Color Spaces in 6D Pose Estimation": {
            "authors": [],
            "url": "http://www.di.ubi.pt/~lfbaa/pubs/icarsc2021.pdf",
            "ref_texts": "[4] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2019, pp.",
            "ref_ids": [
                "4"
            ],
            "1": "These methods try to solve the 6D pose estimation problem [3], [4], [5].",
            "2": "As in previous methods [2], [3], [4], [5] that tackle the 6D pose estimation using the LineMOD dataset, we use the Average Distance of Model Points (ADD) [6] as an evaluation metric for non-symmetric objects, and for the egg-box and glue (symmetric objects) we use the Average Closest Point Distance (ADD-S) [5].",
            "3": "As in previous works in 6D pose estimation [1], [2], [3], [4], [5] we use the same evaluation metrics for the LineMOD dataset.",
            "4": "[4] S."
        },
        "Graph-Theoretic Outlier Rejection: From Instance to Category-Level Perception": {
            "authors": [],
            "url": "https://dspace.mit.edu/bitstream/handle/1721.1/139117/shi-jnshi-sm-AeroAstro-2021-thesis.pdf?sequence=1&isAllowed=y",
            "ref_texts": "[83] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "83"
            ],
            "1": "Such approaches first recover the position of semantic keypoints [82] in the images with neural networks, and then recover the 3D pose of the object by solving a geometric optimization problem [78, 82, 83, 96, 52]."
        },
        "Efficient learning methods for robot grasping oriented pose estimation": {
            "authors": [],
            "url": "https://tsukuba.repo.nii.ac.jp/record/2002128/files/DA010090_abstract.pdf",
            "ref_texts": ""
        },
        "Real-time embedded reconstruction of dynamic objects for a 3D maritime situational awareness picture": {
            "authors": [],
            "url": "https://elib.dlr.de/193059/1/MARESEC_2022_11_final.pdf",
            "ref_texts": "[20] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , June 2019.",
            "ref_ids": [
                "20"
            ],
            "1": "The tasks of object detection, instance segmentation and pose estimation can be trained using a single back-end, as shown by [20].",
            "2": "[20] S."
        },
        "Network and system for pose and size estimation": {
            "authors": [],
            "url": "https://patentimages.storage.googleapis.com/00/a6/37/ab887606a738b2/US20220292698A1.pdf"
        },
        "Fall and activity detection framework on a robotic platform for older person care": {
            "authors": [],
            "url": "https://pearl.plymouth.ac.uk/bitstream/handle/10026.1/16638/2020belmonte_klein10512143phd.pdf?sequence=3&isAllowed=n",
            "ref_texts": "[123] Sida Peng, Yuan Liu, Qixing Huang, Hujun Bao, and Xiaowei Zhou. PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation. arXiv:1812.11788 [cs] , December 2018.",
            "ref_ids": [
                "123",
                "cs"
            ],
            "1": "Additional useful information may be available by considering object 6D information [71, 185, 123] and even concurrently with CNN skeleton pose data [169].",
            "2": "07750 [cs] , May 2017.",
            "3": "3531 [cs] , May 2014.",
            "4": "2524 [cs] , November 2013.",
            "5": "03677\n[cs, stat] , September 2016.",
            "6": "03167\n[cs], February 2015.",
            "7": "08928 [cs] , August 2019.",
            "8": "01783\n[cs], February 2016.",
            "9": "11788 [cs] , December 2018.",
            "10": "4506 [cs] , May 2014.",
            "11": "10507 [cs] , September 2017.",
            "12": "10494 [cs] , September 2017.",
            "13": "0402\n[cs], December 2012.",
            "14": "00567 [cs] , December 2015.",
            "15": "05910 [cs] , June 2019.",
            "16": "00859 [cs] , August 2016.",
            "17": "00199 [cs] , November 2017.",
            "18": "04161 [cs] , November 2017."
        },
        "Supplementary Material for EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation": {
            "authors": [],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/supplemental/Chen_EPro-PnP_Generalized_End-to-End_CVPR_2022_supplemental.pdf",
            "ref_texts": "[13] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 5",
            "ref_ids": [
                "13"
            ],
            "1": "9 ms overhead to the base pose estimator PVNet [13] at the same batch size, measured on RTX 2080 Super GPU, which is slower than ours."
        },
        "A Comprehensive Review on 3D Object Detection and 6D Pose Estimation With Deep Learning": {
            "authors": [
                "Sabera Hoque"
            ],
            "url": "https://figshare.utas.edu.au/ndownloader/files/40754126",
            "ref_texts": "[191] S. Peng, Y. Liu, Q. Huang, H. Bao, and X. Zhou, ``PVNet: Pixel-wise voting network for 6DoF pose estimation,'' Tech. Rep., 2018.",
            "ref_ids": [
                "191"
            ],
            "1": "Also, [191] recently removed the ROI pooled orientation layer and introduced PVNet (Pixelwise Voting Network) to deny pixel-based vectors and use them for key-point positions.",
            "2": "[191] S."
        },
        "An Open-source Recipe for Building Simulated Robot Manipulation Benchmarks": {
            "authors": [],
            "url": "https://jiayuan-gu.github.io/pdf/icra23-compare.pdf",
            "ref_texts": "[25] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019, pp. 4561\u20134570.",
            "ref_ids": [
                "25"
            ],
            "1": "In the real-world setup, we first estimate the 6-DoF object poses in the camera space by PVNet [25] and then transform the object poses to the robot base using the relative pose between the camera and the base of the robot arm obtained by hand-eye-calibration.",
            "2": "[25] S."
        },
        "\u57fa\u4e8e\u4e09\u7ef4\u68c0\u6d4b\u7f51\u7edc\u7684\u673a\u5668\u4eba\u6293\u53d6\u65b9\u6cd5": {
            "authors": [],
            "url": "http://femt.cnjournals.com/yqyb/article/pdf/20210817",
            "ref_texts": ""
        },
        "Review on 6D Object Pose Estimation With the Focus on Indoor Scene Understanding. 2022\u037e 2 (4): 41": {
            "authors": [],
            "url": "https://www.oajaiml.com/uploads/archivepdf/24821141.pdf",
            "ref_texts": ""
        },
        "Robotic assembly, using RGBD-based object pose estimation & grasp detection.": {
            "authors": [
                "Jaana Kunnari"
            ],
            "url": "https://trepo.tuni.fi/bitstream/handle/10024/123320/AhmadSaad.pdf?sequence=2",
            "ref_texts": ""
        },
        "Planung und Simulation taktiler, intelligenter und kollaborativer Roboterf\u00e4higkeiten in der Montage": {
            "authors": [
                "Metzner Maximilian"
            ],
            "url": "https://opus4.kobv.de/opus4-fau/files/21521/Metzner_Diss_MB_414.pdf",
            "ref_texts": ""
        },
        "Augmented Reality Pilot Assistance System for Helicopter Shipboard Operations": {
            "authors": [],
            "url": "https://mediatum.ub.tum.de/doc/1655458/document.pdf",
            "ref_texts": "[114] Peng, S., Liu, Y., Huang, Q., Bao, H., Zhou, X. (2018). PVNet: Pixel -wise Voting Network for 6DoF Pose Estimation. Retrieved from https://arxiv.org/pdf/1812.11788 ",
            "ref_ids": [
                "114"
            ],
            "1": "Therefore, well known approaches for real -time 6D pose estimation are analyzed within this work: Pose CNN [157] , PV Net [114] , Dense Fusion \n[148] and Single Shot 6D Pose [136] .",
            "2": "051406 \n[114] Peng, S."
        },
        "Learning Innovations for State Estimation": {
            "authors": [],
            "url": "http://www.gerard-kennedy.com/files/iros-2021.pdf",
            "ref_texts": "[31] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019.",
            "ref_ids": [
                "31"
            ],
            "1": "A recent popular approach is to first regress to an intermediate representation such as keypoints, from which pose can be obtained via 2D-3D correspondences and a PnP algorithm [36], [32], [31], [40], [41], [28], [17].",
            "2": "Of these approaches, some, including [36], [32] regress to a set of bounding box corners, while others regress to vector fields [31], [34], [40], or dense correspondences.",
            "3": "State Estimation for Object Pose and Depth Refinement For object pose estimation we choose PVNet [31] to be the baseline network that provides bX(0).",
            "4": "The high dimensional regressor \u2018state\u2019 is the collection of unit vector fields Xk ij=\u0011k ij k\u0011k ijk22R2\u0002K\u0002 M\u0002N: (9) The function hmaps this vector field representation of keypoints to object pose via a RANSAC and uncertaintydriven PnP framework (EPnP), as discussed in [31].",
            "5": "Forobject pose estimation the baseline network is PVNet [31].",
            "6": "Forobject pose estimation we use a pretrained PVNet [31] to provide the initial estimate for each object.",
            "7": "We compute 8 keypoints via farthest point sampling, from which object pose is obtained via uncertainty-driven PnP [31]."
        },
        "Detecting and tracking outdoor gym geometry for AR display of exercise suggestions": {
            "authors": [],
            "url": "https://upcommons.upc.edu/bitstream/handle/2117/360080/M_Thesis_Aina_Maki.pdf?sequence=2"
        },
        "\u57fa\u4e8e\u5149\u573a EPI \u56fe\u50cf\u6808\u7684 6D \u4f4d\u59ff\u4f30\u8ba1\u65b9\u6cd5": {
            "authors": [],
            "url": "http://jemi.cnjournals.com/jemi/article/pdf/20230414"
        },
        "Vote from the Center: 6 DoF Pose Estimation in RGB-D Images by Radial Keypoint Voting (Supplementary Material)": {
            "authors": [],
            "url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136700331-supp.pdf",
            "ref_texts": "[S.17] Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4561\u20134570",
            "ref_ids": [
                "S\\.17"
            ]
        },
        "Monocular Markerless 6D Pose Estimation of ANYmal": {
            "authors": [],
            "url": "https://tenhearts.github.io/assets/pdf/plr_report.pdf",
            "ref_texts": "[14] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "14"
            ],
            "1": "Most recent state-of-the-art works with RGB images focus on first detecting 2D targets of the object in the given image and subsequently solving a Perspective-n-Point(PnP) problem with predicting 2D-3D correspondences for 6D poses[11, 12, 13, 14, 15, 16].",
            "2": "[14] S."
        },
        "Direct pose estimation from RGB images using 3D objects 3 Boyutlu nesneleri kullanarak imgelerden poz kestirimi": {
            "authors": [],
            "url": "https://jag.journalagent.com/z4/download_fulltext.asp?pdir=pajes&plng=tur&un=PAJES-08566",
            "ref_texts": ""
        },
        "R-SAC: Reinforcement Sample Consensus": {
            "authors": [],
            "url": "https://openreview.net/pdf?id=Urn3WzRwhXO",
            "ref_texts": "[13] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019.",
            "ref_ids": [
                "13"
            ],
            "1": "[13] S."
        },
        "6D Pose Estimation for Texture-less Objects": {
            "authors": [],
            "url": "https://www.sublimeforest.com/static/pdf/6dpose.pdf",
            "ref_texts": "[17] S. Peng, Y . Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019. 1, 3, 4",
            "ref_ids": [
                "17"
            ],
            "1": "[17] In the second stage, the refinement technique predicts a relative SE(3) transformation that matches a synthetic mesh rendered view of the object against the observed image.",
            "2": "[17]\n3.",
            "3": "PVNet: Inspired by [17], \u201dassuming there are C classes of objects and K keypoints for each class, PVNet takes as input the H*W*3 image, processes it with a fully convolutional architecture, and outputs the H*W*(K*2*C)tensor representing unit vectors and H*W*(C+1)tensor representing class probabilities\u201d.",
            "4": "5\n[17] S."
        },
        "Supplementary Material-FFB6D: A Full Flow Bidirectional Fusion Network for 6D Pose Estimation": {
            "authors": [],
            "url": "https://openaccess.thecvf.com/content/CVPR2021/supplemental/He_FFB6D_A_Full_CVPR_2021_supplemental.pdf",
            "ref_texts": "[15] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019. 3",
            "ref_ids": [
                "15"
            ],
            "1": "RGB RGB-D PoseCNN DeepIM\n[18, 10]PVNet[15] CDPN[11] DPOD[20] PointFusion[19]DenseFusion[17]G2LNet[1]PVN3D[5] Our FFB6D ape 77.",
            "2": "[9]Pix2Pose [14]PVNet [15]DPOD\n[20]Hu et al."
        },
        "Self-supervised Geometric Perception": {
            "authors": [],
            "url": "https://openaccess.thecvf.com/content/CVPR2021/supplemental/Yang_Self-Supervised_Geometric_Perception_CVPR_2021_supplemental.pdf",
            "ref_texts": "[12] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , pages 4561\u20134570, 2019. 3",
            "ref_ids": [
                "12"
            ],
            "1": "Recent works such as YOLO6D [14], PVNet [12], and DPOD [16] can all serve as the student network, despite using different methodologies.",
            "2": ", by rendering synthetic projections of the 3D models under different simulated poses, which is common in [16, 12, 14, 2].",
            "3": "2There are many different ways to establish 2D-3D correspondences, see PVNet [12], YOLO6D [14] and references therein."
        },
        "RNNPose: Recurrent 6-DoF Object Pose Refinement with Robust Correspondence Field Estimation and Pose Optimization": {
            "authors": [],
            "url": "https://decayale.github.io/publication/rnnpose/paper.pdf",
            "ref_texts": "[34] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixel-wise voting network for 6DoF pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4561\u2013",
            "ref_ids": [
                "34"
            ],
            "1": "These methods may estimate the object\u2019s bounding box corners [35,45], predict dense 2D-3D correspondence maps [33] or vote the keypoints by all object pixels [34].",
            "2": "At the beginning of the first rendering cycle, a reference image Iref is rendered with the object\u2019s CAD model according to its initial pose Pinit(estimated by any direct methods [34,53]).",
            "3": "Here, the initial poses for pose refinement are originally from PVNet [34] but added with significant disturbances for robustness testing.",
            "4": "We follow similar conventions in data processing and synthetic data generation as the previous works [20,34].",
            "5": "For the initial poses, we mainly rely on PoseCNN [52] and PVNet [34], two typical direct estimation methods, following [23] and [20].",
            "6": "1 cm following [34].",
            "7": "Robustness comparison with RePOSE by degrading the initial poses (from PVNet [34]) with Gaussian noise on LINEMOD dataset.",
            "8": "The comparison of estimation accuracy with competitive direct methods (PoseCNN [52], PVNet [34] and HybridPose [38]) and refinement methods (DPOD [59], DeepIM [23] and RePOSE\n[20]) on LINEMOD dataset in terms of the ADD(-S) metric.",
            "9": "Object PoseCNN [52] PVNet [34] HybridPose [38] GDR-Net [51] DPOD [59] RePOSE [20] Ours Ape 9.",
            "10": "For the LINEMOD dataset, we compare with the recent pose refinement methods RePOSE [20], DPOD [59] and DeepIM [23] as well as some direct estimation baselines [34, 38, 52].",
            "11": "4 the PVNet [34], although the pose accuracy of PVNet is much better as exhibited in Table 3."
        },
        "Supplementary Material for OnePose: One-Shot Object Pose Estimation without CAD Models": {
            "authors": [],
            "url": "http://www.cad.zju.edu.cn/home/gfzhang/papers/OnePose/onepose_supp.pdf",
            "ref_texts": "[8] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 3",
            "ref_ids": [
                "8"
            ],
            "1": "Implementation Details of the Evaluation of PVNet For the experiments of evaluating PVNet [8], we directly use the original implementation and training configurations provided by the authors at [1]."
        },
        "Supplementary Material: Gen6D: Generalizable Model-Free 6-DoF Object Pose Estimation from RGB Images": {
            "authors": [],
            "url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136920297-supp.pdf",
            "ref_texts": ""
        },
        "What Supervision Scales? Practical Learning Through Interaction": {
            "authors": [],
            "url": "https://digitalassets.lib.berkeley.edu/techreports/ucb/incoming/EECS-2020-128.pdf"
        },
        "VS-Net: Voting with Segmentation for Visual Localization-Supplementary Material": {
            "authors": [],
            "url": "http://openaccess.thecvf.com/content/CVPR2021/supplemental/Huang_VS-Net_Voting_With_CVPR_2021_supplemental.pdf",
            "ref_texts": "[6] Jeremie Papon, Alexey Abramov, Markus Schoeler, and Florentin W \u00a8org\u00a8otter. V oxel cloud connectivity segmentation supervoxels for point clouds. In Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on , Portland, Oregon, June 22-27 2013. 1[7] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4561\u20134570, 2019. 1",
            "ref_ids": [
                "6",
                "7"
            ],
            "1": "SuperV oxel [6] is a 3D oversegmentation algorithm that uniformly sets vast seeds in 3D space and raises a patch for each pre-defined seeds.",
            "2": "COLMAP [9] and the 3D surface we use reconstructed by Algorithm 1 Landmarks from V otes INPUT: pi: Pixel coordinates in current patch label di: Pixel votes at pi ^l(0) j: Initial landmark of landmark jfrom RANSAC-based approach [7]\n\u0012min: Threshold of minimum neighbor pixels \u0012dist: Threshold of neighbor pixel distance \u000fstep: Epsilon of landmark coordinate change OUTPUT: ^lbest j: Best voting landmark bvalid: Validity of the landmark ^lbest j ^l(0) j bvalid true t 1 whilet<Max Iteration do S=fkpi\u0000^l(t\u00001) jk2<\u0012distg ifjSj<\u0012minthen bvalid false break end if for all pi2Sdo Compute normal of voting map ni \u00140\u00001\n1 0\u0015 di end for ^l(t) j \u0000P Snin> i\u0001\u00001\u0000P Snin> ipi\u0001 ifjj^l(t) j\u0000^l(t\u00001) jjj2<\u000fstepthen break end if ^lbest j ^l(t) j t t+ 1 end while 7Scenes Cambridge Landmarks Ches.",
            "3": "2\n[6] Jeremie Papon, Alexey Abramov, Markus Schoeler, and Florentin W \u00a8org\u00a8otter."
        },
        "Neural Object Learning for 6D Pose Estimation Using a Few Cluttered Images": {
            "authors": [],
            "url": "https://openreview.net/pdf?id=Z6zwbn0i3Z1",
            "ref_texts": "28. Peng, S., Liu, Y., Huang, Q., Zhou, X., Bao, H.: Pvnet: Pixel-wise voting network for 6dof pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4556{4565 (June 2019)",
            "ref_ids": [
                "28"
            ],
            "1": "Recently, state-of-the-art performance has been accomplished by using both synthetic and real images [23, 24, 28].",
            "2": "To overcome this limitation, both real images and synthetic images are used for training [23, 24, 28, 45], which currently achieves state-of-the-art performance."
        },
        "Computer Vision-assisted Battery-free RFID Systems for Object Recognition, Localization and Orientation": {
            "authors": [
                "Zhongqin Wang"
            ],
            "url": "https://opus.lib.uts.edu.au/bitstream/10453/147331/2/02whole.pdf",
            "ref_texts": "[73] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE CVPR, pages 4561\u20134570. IEEE, 2019.",
            "ref_ids": [
                "73"
            ],
            "1": "Most computer vision-based solutions [52, 85, 73] require large-scale labeled image data."
        },
        "Deep object 6-DoF pose estimation using instance segmentation": {
            "authors": [],
            "url": "https://alife-robotics.co.jp/members2020/icarob/data/html/data/OS/OS24/OS24-1.pdf",
            "ref_texts": ""
        },
        "Evaluating Computer Vision Methods for Detection and Pose Estimation of Textureless Objects": {
            "authors": [
                "Name Last"
            ],
            "url": "https://uis.brage.unit.no/uis-xmlui/bitstream/handle/11250/2620242/Skutvik_Harald_Thirud.pdf?sequence=1",
            "ref_texts": "[10]Sida Peng, Yuan Liu, Qixing Huang, Hujun Bao, and Xiaowei Zhou. Pvnet: Pixelwise voting network for 6dof pose estimation. CoRR, abs/1812.11788, 2018. URL http://arxiv.org/abs/1812.11788 .",
            "ref_ids": [
                "10"
            ],
            "1": "2 Pose estimation techniques DeepIM [8], Keypoint detector localization [9] and PVNet [10] are examples of the current cutting edge pose estimation methods."
        },
        "Model-based 3D Tracking for Augmented Orthopedic Surgery": {
            "authors": [],
            "url": "https://hal.science/hal-03022939/document",
            "ref_texts": "[9] S Peng, Y Liu, Q Huang, X Zhou, H Bao. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . (2018) . Pvnet: Pixel -wise voting network for 6dof pose estimation . ",
            "ref_ids": [
                "9"
            ],
            "1": "Machine learning based pose inference [9] could also be investigated to initialize registration.",
            "2": "[9] S Peng, Y Liu, Q Huang, X Zhou, H Bao."
        },
        "Deep Snake for Real-Time Instance Segmentation": {
            "authors": [],
            "url": "https://ask.qcloudimg.com/draft/6837186/79j1ad13cu.pdf",
            "ref_texts": "[30] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In CVPR , 2019. 3",
            "ref_ids": [
                "30"
            ],
            "1": "An alternative method is to use standard CNNs to regress a pixel-wise vector field from the input image to guide the evolution of the initial contour [34, 30, 37]."
        },
        "Odhadov\u00e1n\u00ed rotace a translace netexturovan\u00e9ho objektu z jedn\u00e9 kamery": {
            "authors": [],
            "url": "https://dspace.cvut.cz/bitstream/handle/10467/96699/F3-BP-2021-Lukes-Michal-Michal%20Lukes%20thesis%20-%20final2.pdf?sequence=-1"
        },
        "Supplement to \u201cPose Proposal Critic: Robust Pose Refinement by Learning Reprojection Errors\u201d": {
            "authors": [],
            "url": "https://research.chalmers.se/publication/519547/file/519547_AdditionalFile_dad0f2b5.pdf",
            "ref_texts": "[9]Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. PVNet: Pixelwise voting network for 6DoF pose estimation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 2019. BRYNTE, KAHL: SUPPLEMENT TO \u201cPOSE PROPOSAL CRITIC\u201d 9",
            "ref_ids": [
                "9"
            ],
            "1": "Despite the sub-optimal pose proposals from PVNet [9], the poses are accurately recovered.",
            "2": "[8]PVNet [9]PoseCNN [11]\n+ DeepIM [6]PVNet [9]\n+ PPC (Ours) ape 17.",
            "3": "[8]PVNet [9]PoseCNN [11]\n+ DeepIM [6]PVNet [9]\n+ PPC (Ours) ape 69.",
            "4": "BRYNTE, KAHL: SUPPLEMENT TO \u201cPOSE PROPOSAL CRITIC\u201d 5 PVNet [9]PoseCNN [11]\n+ DeepIM [6]PVNet [9]\n+ PPC (Ours) ape 37.",
            "5": "1 Negative Depth Correction of Pose Proposals We observed that the pose proposals from PVNet [9] sometimes have negative depth, and in this case we switched sign for the object center position (in the camera frame), and rotated the object 180 degrees around the principal axis of the camera, in order to yield a feasible estimate with similar projection (the projection is identical for points on the plane which goes through the object center and is parallel to the principal plane of the camera).",
            "6": "This correction is done both when reporting the results of [9], and when reporting the results of our refinement."
        },
        "System and method for image inpainting": {
            "authors": [],
            "url": "https://patentimages.storage.googleapis.com/cc/b1/59/882ff0d691b4cd/US20220292651A1.pdf",
            "ref_texts": ""
        },
        "Systems and methods for pose detection and measurement": {
            "authors": [],
            "url": "https://patentimages.storage.googleapis.com/f6/43/5d/8fc8d44f24b8b1/US11295475.pdf"
        },
        "A Multi-view Pixel-wise Voting Network for 6DoF Pose Estimation": {
            "authors": [],
            "url": "https://thesis.unipd.it/bitstream/20.500.12608/31496/1/tesi_3d_pose_estimation_pdfA.pdf"
        },
        "\u7269\u54c1\u63b4\u307f\u4e0a\u3052\u4f5c\u696d\u3067\u6c42\u3081\u3089\u308c\u308b\u8996\u899a\u6a5f\u80fd": {
            "authors": [],
            "url": "https://www.jstage.jst.go.jp/article/jrsj/38/6/38_38_538/_pdf"
        },
        "Learning to Estimate 3D Object Pose from Synthetic Data": {
            "authors": [
                "Sergey Zakharov"
            ],
            "url": "https://mediatum.ub.tum.de/doc/1550255/document.pdf",
            "ref_texts": "[93]S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao. Pvnet: Pixel-wise voting network for 6dof pose estimation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2019.",
            "ref_ids": [
                "93"
            ],
            "1": ", BB8 [38], YOLO6D [40], PVNet [93], use the training split of the real dataset.",
            "2": "Analogously to other related papers [40, 50, 93, 101], we measure the accuracy of pose estimation using the ADD score [58].",
            "3": "Train data Synthetic +R e fi n e m e n t Real +R e fi n e m e n t Object SSD6D [50]AAE [92]Ours SSD6D [56]Ours YOLO6D [40]PoseCNN [101] PVNet [93]Ours DeepIM [9]Ours Ape 2.",
            "4": "Method YOLO6D [40] PoseCNN [101] SSD6D+Ref [56] HMap [102] PVNet [93] Ours Ours+Ref Mean 6.",
            "5": "If trained on real data, our method is the second best after [93].",
            "6": "Method Frames per second Refinement AAE [92] 4 200 ms/object SSD6D [50] 10 24 ms/object PVNet [93] 25 Ours 33 5 ms/object YOLO6D [40] 50 68\n4.",
            "7": "We demonstrated that for both, real and synthetic training data, our detector outperforms other related works, such as [40, 101], by a large margin and performs similarly to [93].",
            "8": "[92]\n[93]S."
        },
        "\ub85c\ubd07 \ud314\uc744 \ud65c\uc6a9\ud55c \uc815\ub9ac\uc791\uc5c5\uc744 \uc704\ud55c \ubb3c\uccb4 \uc790\uc138\ucd94\uc815 \ubc0f \uc774\ubbf8\uc9c0 \ub9e4\uce6d": {
            "authors": [
                "Media Contents"
            ],
            "url": "https://jkros.org/xml/31147/31147.pdf",
            "ref_texts": "[3] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixelwise voting network for 6dof pose estimation,\u201d 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , Long Beach, CA, USA, 2019, DOI: 10.1109/CVPR.2019.00469.",
            "ref_ids": [
                "3"
            ],
            "1": "\uae30\uc874\uc758 \uc790\uc138\ucd94\uc815 \uc54c\uace0\ub9ac\uc998 \uc911\uc5d0\uc11c \uc2ec\uce35\ud559\uc2b5\uc5d0 \uae30\ubc18\ud55c \uc5f0\uad6c\n\ub4e4[2,3]\uc774 \uc18d\ub3c4\uc640 \uc815\ud655\ub3c4\uba74\uc5d0\uc11c \uc88b\uc740 \uc131\ub2a5\uc744 \ubcf4\uc774\uace0 \uc788\ub2e4 .",
            "2": "[3] S."
        },
        "Visual slam in dynamic environments": {
            "authors": [],
            "url": "https://zaguan.unizar.es/record/100672/files/TESIS-2021-083.pdf",
            "ref_texts": "104 Mur-Artal, R. & Tard\u0013 os, J. D. (2017), `ORB-SLAM2: An open-source slam system for monocular, stereo, and RGB-D cameras', IEEE T-RO . Newcombe, R. A., Lovegrove, S. J. & Davison, A. J. (2011), DTAM: Dense tracking and mapping in real-time, in `ICCV', IEEE. Oquab, M., Bottou, L., Laptev, I. & Sivic, J. (2014), Learning and transferring midlevel image representations using convolutional neural networks, in `Proceedings of the IEEE conference on computer vision and pattern recognition', pp. 1717{1724. Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T. & Efros, A. A. (2016), Context encoders: Feature learning by inpainting, in `Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition', pp. 2536{2544. Paz, L. M., Pini\u0013 es, P., Tard\u0013 os, J. D. & Neira, J. (2008), `Large-scale 6-dof slam with stereo-in-hand', IEEE transactions onrobotics 24(5), 946{957. Peng, S., Liu, Y., Huang, Q., Zhou, X. & Bao, H. (2019), Pvnet: Pixel-wise voting network for 6dof pose estimation, in `Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition', pp. 4561{4570. Peris, M., Martull, S., Maki, A., Ohkawa, Y. & Fukui, K. (2012), Towards a simulation driven stereo vision system, in `Pattern Recognition (ICPR), 2012 21st International Conference on', IEEE, pp. 1038{1042. Pinheiro, P. O., Lin, T.-Y., Collobert, R. & Doll\u0013 ar, P. (2016), Learning to reffne object segments, in `European conference on computer vision', Springer, pp. 75{91. Porav, H., Maddern, W. & Newman, P. (2018), `Adversarial training for adverse conditions: Robust metric localisation using appearance transfer', IEEE International Conference onRobotics andAutomation . Ren, S., He, K., Girshick, R. & Sun, J. (2015), Faster r-cnn: Towards real-time object detection with region proposal networks, in `Advances in neural information processing systems', pp. 91{99. Ren, X. & Malik, J. (2003), Learning a classiffcation model for segmentation, in `nternational Conference on Computer Vision', IEEE, p. 10. Riazuelo, L., Montano, L. & Montiel, J. M. M. (2017), `Semantic visual SLAM in populated environments', ECMR . Rogers, J. G., Trevor, A. J., Nieto-Granda, C. & Christensen, H. I. (2010), SLAM with expectation maximization for moveable object tracking, in `2010 IEEE/RSJ International Conference on Intelligent Robots and Systems', IEEE, pp. 2077{2082. Romera, E., Alvarez, J. M., Bergasa, L. M. & Arroyo, R. (2017), `ERFNet', https: //github.com/Eromera/erfnet. Romera, E., Alvarez, J. M., Bergasa, L. M. & Arroyo, R. (2018), `ERFNet: Eflcient Residual Factorized ConvNet for Real-Time Semantic Segmentation', IEEE Transactions onIntelligent Transportation Systems 19(1), 263{272."
        }
    }
}