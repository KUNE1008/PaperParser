{
    "title": "Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans",
    "id": 8,
    "valid_pdf_number": "273/304",
    "matched_pdf_number": "217/273",
    "matched_rate": 0.7948717948717948,
    "citations": {
        "Unisurf: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction": {
            "authors": [
                "Michael Oechsle",
                "Songyou Peng",
                "Andreas Geiger"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Oechsle_UNISURF_Unifying_Neural_Implicit_Surfaces_and_Radiance_Fields_for_Multi-View_ICCV_2021_paper.pdf",
            "ref_texts": "[44] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , 2020. 2, 3",
            "ref_ids": [
                "44"
            ],
            "1": "NeRF [34] and follow-ups [6,28,35,36,44,45,49,53,63] use volume rendering by learning alpha-compositing of a radiance field along rays.",
            "2": "Several follow-up works (Neural Body [44] D-NeRF [45] and NeRD [6]) extract meshes using the volume density from NeRF, but none of them considers optimizing surfaces directly."
        },
        "Ibrnet: Learning multi-view image-based rendering": {
            "authors": [
                "Qianqian Wang",
                "Zhicheng Wang",
                "Kyle Genova",
                "Pratul P. Srinivasan",
                "Howard Zhou",
                "Jonathan T. Barron",
                "Ricardo Martin",
                "Noah Snavely",
                "Thomas Funkhouser"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2021/papers/Wang_IBRNet_Learning_Multi-View_Image-Based_Rendering_CVPR_2021_paper.pdf",
            "ref_texts": "[45] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. CVPR , 2021.",
            "ref_ids": [
                "45"
            ],
            "1": "While NeRF opens up many new research opportunities [30,36,43,45,55,58], it must be optimized for each new scene, taking hours or days to converge."
        },
        "Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps": {
            "authors": [
                "Christian Reiser",
                "Songyou Peng",
                "Yiyi Liao",
                "Andreas Geiger"
            ],
            "url": "http://openaccess.thecvf.com/content/ICCV2021/papers/Reiser_KiloNeRF_Speeding_Up_Neural_Radiance_Fields_With_Thousands_of_Tiny_ICCV_2021_paper.pdf",
            "ref_texts": "[39] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , 2021. 2",
            "ref_ids": [
                "39"
            ],
            "1": "Finally, [10, 12, 21, 22, 34, 36, 39, 41, 62, 67, 70] extend NeRF to videos."
        },
        "Humannerf: Free-viewpoint rendering of moving people from monocular video": {
            "authors": [
                "Yi Weng",
                "Brian Curless",
                "Pratul P. Srinivasan",
                "Jonathan T. Barron",
                "Ira Kemelmacher"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Weng_HumanNeRF_Free-Viewpoint_Rendering_of_Moving_People_From_Monocular_Video_CVPR_2022_paper.pdf",
            "ref_texts": "[48] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body:Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. CVPR , 2021.",
            "ref_ids": [
                "48"
            ],
            "1": "Previous neural rendering methods [2, 32, 35, 36, 48, 64, 73] typically assume multi-view input, careful lab capture, or do not perform well on humans due to non-rigid body motion.",
            "2": "Human-specific methods typically assume a SMPL template [33] as a prior, which helps constrain the motion space but also introduces artifacts in clothing and complex motions that are not captured by the SMPL model [47, 48].",
            "3": "[48] explored the use of learned structured latent codes embedded for point clouds (from MVS [53]) or reposed mesh vertices (from SMPL [33]) and learn an accompanying UNetor NeRF-based neural renderer.",
            "4": "We then define \u03c4as a function of the optimization iteration: \u03c4(t) =Lmax(0 , t\u2212Ts) Te\u2212Ts, (15)\n16213\n Subject 377 Subject 386 Subject 387 PSNR \u2191SSIM\u2191LPIPS* \u2193 PSNR \u2191SSIM\u2191LPIPS* \u2193 PSNR \u2191SSIM\u2191LPIPS* \u2193 Neural Body [48] 29.",
            "5": "58 Subject 392 Subject 393 Subject 394 PSNR \u2191SSIM\u2191LPIPS* \u2193 PSNR \u2191SSIM\u2191LPIPS* \u2193 PSNR \u2191SSIM\u2191LPIPS* \u2193 Neural Body [48] 30.",
            "6": "Evaluation dataset We evaluate our method on the ZJU-MoCap dataset [48], self-captured data (rugby, hoodie ), and YouTube videos downloaded from Internet (story3,way2sexy4,invisible5).",
            "7": "We compare our method with Neural Body [48] (typically used with multiple cameras) and HyperNeRF [46]\n(single moving camera around the subject), state-of-the-art methods for modeling humans and general scenes for novel view synthesis.",
            "8": "PSNR \u2191SSIM\u2191LPIPS* \u2193 Neural Body [48] 29."
        },
        "Neural actor: Neural free-view synthesis of human actors with pose control": {
            "authors": [
                "Lingjie Liu",
                "Marc Habermann",
                "Viktor Rudnev",
                "Kripasindhu Sarkar",
                "Jiatao Gu",
                "Christian Theobalt"
            ],
            "url": "https://dl.acm.org/doi/pdf/10.1145/3478513.3480528",
            "ref_texts": "2014), 1737\u20131746. https://doi.org/10.1007/s00138-013-0559-0 Kun Li, Jingyu Yang, Leijie Liu, Ronan Boulic, Yu-Kun Lai, Yebin Liu, Yubin Li, and Eray Molla. 2017. SPA: Sparse Photorealistic Animation Using a Single RGB-D Camera. IEEE Trans. Cir. and Sys. for Video Technol. 27, 4 (April 2017), 771\u2013783. https://doi.org/10.1109/TCSVT.2016.2556419 Ruilong Li, Shan Yang, David A. Ross, and Angjoo Kanazawa. 2021. Learn to Dance with AIST++: Music Conditioned 3D Dance Generation. arXiv:2101.08779 [cs.CV] Yining Li, Chen Huang, and Chen Change Loy. 2019. Dense Intrinsic Appearance Flow for Human Pose Transfer. In IEEE Conference on Computer Vision and Pattern Recognition . Z. Li, Simon Niklaus, Noah Snavely, and Oliver Wang. 2020. Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes. ArXiv abs/2011.13084 (2020). Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. 2020a. Neural Sparse Voxel Fields. NeurIPS (2020). Lingjie Liu, Weipeng Xu, Marc Habermann, Michael Zollh\u00f6fer, Florian Bernard, Hyeongwoo Kim, Wenping Wang, and Christian Theobalt. 2020b. Neural Human Video Rendering by Learning Dynamic Textures and Rendering-to-Video Translation. IEEE Transactions on Visualization and Computer Graphics PP (05 2020), 1\u20131. https://doi.org/10.1109/TVCG.2020.2996594 Lingjie Liu, Weipeng Xu, Michael Zollhoefer, Hyeongwoo Kim, Florian Bernard, Marc Habermann, Wenping Wang, and Christian Theobalt. 2019b. Neural Rendering and Reenactment of Human Actor Videos. ACM Transactions on Graphics (TOG) (2019). Wen Liu, Zhixin Piao, Jie Min, Wenhan Luo, Lin Ma, and Shenghua Gao. 2019a. Liquid warping gan: A unified framework for human motion imitation, appearance transfer and novel view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 5904\u20135913. Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, and Yaser Sheikh. 2019. Neural volumes: Learning dynamic renderable volumes from images. ACM Transactions on Graphics (TOG) 38, 4 (2019), 65.Stephen Lombardi, Tomas Simon, Gabriel Schwartz, Michael Zollhoefer, Yaser Sheikh, and Jason Saragih. 2021. Mixture of Volumetric Primitives for Efficient Neural Rendering. arXiv:2103.01954 [cs.GR] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. 2015. SMPL: A Skinned Multi-Person Linear Model. ACM Trans. Graphics (Proc. SIGGRAPH Asia) 34, 6 (Oct. 2015), 248:1\u2013248:16. Liqian Ma, Xu Jia, Qianru Sun, Bernt Schiele, Tinne Tuytelaars, and Luc Van Gool. 2017. Pose guided person image generation. In Advances in Neural Information Processing Systems . 405\u2013415. Liqian Ma, Qianru Sun, Stamatios Georgoulis, Luc van Gool, Bernt Schiele, and Mario Fritz. 2018. Disentangled Person Image Generation. Computer Vision and Pattern Recognition (CVPR) (2018). Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Gerard Pons-Moll, and Michael J. Black. 2019. AMASS: Archive of Motion Capture as Surface Shapes. In International Conference on Computer Vision . 5442\u20135451. Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. 2020. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. arXiv preprint arXiv:2003.08934 (2020). Franziska Mueller, Florian Bernard, Oleksandr Sotnychenko, Dushyant Mehta, Srinath Sridhar, Dan Casas, and Christian Theobalt. 2018. GANerated Hands for Real-Time 3D Hand Tracking from Monocular RGB. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (2018), 49\u201359. Natalia Neverova, Riza Alp G\u00fcler, and Iasonas Kokkinos. 2018. Dense Pose Transfer. European Conference on Computer Vision (ECCV) (2018). Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Steven M. Seitz, and Ricardo Martin-Brualla. 2020. Deformable Neural Radiance Fields. arXiv preprint arXiv:2011.12948 (2020). Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021a. Animatable Neural Radiance Fields for Human Body Modeling. ICCV (2021). Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021b. Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. In CVPR . Sergey Prokudin, Michael J. Black, and Javier Romero. 2021. SMPLpix: Neural Avatars from 3D Human Models. In Winter Conference on Applications of Computer Vision (WACV) . 1810\u20131819. Albert Pumarola, Antonio Agudo, Alberto Sanfeliu, and Francesc Moreno-Noguer. 2018. Unsupervised Person Image Synthesis in Arbitrary Poses. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. 2020a. D-NeRF: Neural Radiance Fields for Dynamic Scenes. arXiv:2011.13961 [cs.CV] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. 2020b. D-NeRF: Neural Radiance Fields for Dynamic Scenes. arXiv preprint arXiv:2011.13961"
        },
        "Non-rigid neural radiance fields: Reconstruction and novel view synthesis of a dynamic scene from monocular video": {
            "authors": [
                "Edgar Tretschk",
                "Ayush Tewari",
                "Vladislav Golyanik",
                "Michael Zollhofer",
                "Christoph Lassner",
                "Christian Theobalt"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Tretschk_Non-Rigid_Neural_Radiance_Fields_Reconstruction_and_Novel_View_Synthesis_of_ICCV_2021_paper.pdf",
            "ref_texts": "[58] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. arXiv e-prints , 2020.",
            "ref_ids": [
                "58"
            ],
            "1": "Others focus on moving human bodies [85, 58, 75] or more general objects [61, 37, 86, 57, 11, 36]."
        },
        "Advances in neural rendering": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2111.05849"
        },
        "Gram: Generative radiance manifolds for 3d-aware image generation": {
            "authors": [
                "Yu Deng",
                "Jiaolong Yang",
                "Jianfeng Xiang",
                "Xin Tong"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Deng_GRAM_Generative_Radiance_Manifolds_for_3D-Aware_Image_Generation_CVPR_2022_paper.pdf",
            "ref_texts": "[52] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021. 2",
            "ref_ids": [
                "52"
            ],
            "1": "Most of the NeRF-based methods [32,35,46,48,52] focus on scene-specific learning tasks where a network is trained to fit a set of posed images of a certain scene."
        },
        "Animatable neural radiance fields for modeling dynamic human bodies": {
            "authors": [
                "Sida Peng",
                "Junting Dong",
                "Qianqian Wang",
                "Shangzhan Zhang",
                "Qing Shuai",
                "Xiaowei Zhou",
                "Hujun Bao"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Peng_Animatable_Neural_Radiance_Fields_for_Modeling_Dynamic_Human_Bodies_ICCV_2021_paper.pdf",
            "ref_texts": "[49] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 2, 5, 6, 7",
            "ref_ids": [
                "49"
            ],
            "1": "We evaluate our approach on the H36M [19] and ZJUMoCap [49] datasets that capture dynamic humans in complex motions with synchronized cameras.",
            "2": "However, they have difficulty in recovering reasonable 3D human shapes when the camera views are too sparse, as shown in [49].",
            "3": "[49] combines NeRF with the SMPL model, allowing it to handle dynamic humans and synthesize photorealistic novel views from very sparse camera views.",
            "4": "ZJU-MoCap [49] records multi-view videos with 21 cameras and collects human poses using the marker-less motion capture system.",
            "5": "We follow the experimental protocol in [49].",
            "6": "We compare with state-of-the-art image synthesis methods [60, 62, 49] that also utilize SMPL priors.",
            "7": "3) Neural body [49] represents the human body with an implicit field conditioned on the latent codes anchored on the vertices of SMPL and renders the images using volume rendering.",
            "8": "Moreover, the proposed method achieves comparable results with the most recent state-of-the-art approach [49] as shown in Table 2, despite not being specifically designed for the novel view synthesis task.",
            "9": "PSNR SSIM NT\n[60]NHR\n[62]NB\n[49]Ours NT\n[60]NHR\n[62]NB\n[49]Ours novel view 22.",
            "10": "Table 2 shows that our model also outperforms [49] when generating images under novel human poses on ZJU-MoCap dataset.",
            "11": "For complex human poses, [60, 62, 49] give blurry and distorted rendering results.",
            "12": "For complex human poses, [60, 62, 49] tend to generate distorted rendering results."
        },
        "Nerf: Neural radiance field in 3d vision, a comprehensive review": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2210.00379",
            "ref_texts": "[59] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2021, pp. 9054\u20139063.",
            "ref_ids": [
                "59"
            ],
            "1": "The latter focuses on topological changes, and includes scenes such as a human subject opening and closing their eyes and mouth, peeling a banana, 3D printing a chicken toy, and a broom deforming The ZJU-MOCap LightStage dataset [59] is a multi-view (20+ cameras) motion capture dataset consisting of 9 dynamic human sequences consisting of exercise-like motions.",
            "2": "[89], CoNeRF [168] SemanticsSemantic-NeRF [91], NeSF [90], Fig-NeRF [88], Panoptic Neural Fields [50] Fundamental OperationsHDR/Tone Mapping RawNeRF [67], HDR-NeRF [169] Denoising/Deblurring/ Super-ResolutionRawNeRF [67], DeblurNeRF [170], NaN [171], NeRF-SR [172] Generative ModelsGANGIRAFFE [76], GRAF [77], \u0019-GAN [78] , GNeRF [79] [132], Stylenerf [80], EG3D [81] Diffusion DreamFusion [82], Magic3D [83], RealFusion [84]\n3D ReconstructionSDF NeuS [173], Neural RGB-D [174], Geo-NeuS [175], HF-NeuS [176] Occupancy UNISURF [177] Human NeRFFace Nerfies [57], HyperNeRF [58], RigNeRF [178], EG3D [81] HeadNeRF [179] BodyNeural Body [59], HumanNeRF [180], Zheng et al.",
            "3": "[59] S."
        },
        "Panoptic neural fields: A semantic object-aware neural scene representation": {
            "authors": [
                "Abhijit Kundu",
                "Kyle Genova",
                "Xiaoqi Yin",
                "Alireza Fathi",
                "Caroline Pantofaru",
                "Leonidas J. Guibas",
                "Andrea Tagliasacchi",
                "Frank Dellaert",
                "Thomas Funkhouser"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Kundu_Panoptic_Neural_Fields_A_Semantic_Object-Aware_Neural_Scene_Representation_CVPR_2022_paper.pdf",
            "ref_texts": ""
        },
        "Nerf-editing: geometry editing of neural radiance fields": {
            "authors": [
                "Jie Yuan",
                "Tian Sun",
                "Kun Lai",
                "Yuewen Ma",
                "Rongfei Jia",
                "Lin Gao"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Yuan_NeRF-Editing_Geometry_Editing_of_Neural_Radiance_Fields_CVPR_2022_paper.pdf",
            "ref_texts": "[47] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 2",
            "ref_ids": [
                "47"
            ]
        },
        "Ad-nerf: Audio driven neural radiance fields for talking head synthesis": {
            "authors": [
                "Yudong Guo",
                "Keyu Chen",
                "Sen Liang",
                "Jin Liu",
                "Hujun Bao",
                "Juyong Zhang"
            ],
            "url": "http://openaccess.thecvf.com/content/ICCV2021/papers/Guo_AD-NeRF_Audio_Driven_Neural_Radiance_Fields_for_Talking_Head_Synthesis_ICCV_2021_paper.pdf",
            "ref_texts": "[33] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2021. 2",
            "ref_ids": [
                "33"
            ],
            "1": "[33] integrate observations across video frames to enable novel view synthesis for human body from a sparse multi-view video."
        },
        "Generative neural articulated radiance fields": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/7dbafa7d2051218f364c9a38ef1150de-Paper-Conference.pdf",
            "ref_texts": "[9]Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2021.",
            "ref_ids": [
                "9"
            ]
        },
        "Neural articulated radiance field": {
            "authors": [
                "Atsuhiro Noguchi",
                "Xiao Sun",
                "Stephen Lin",
                "Tatsuya Harada"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Noguchi_Neural_Articulated_Radiance_Field_ICCV_2021_paper.pdf",
            "ref_texts": ""
        },
        "H-nerf: Neural radiance fields for rendering and temporal reconstruction of humans in motion": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/7d62a275027741d98073d42b8f735c68-Paper.pdf",
            "ref_texts": "[36] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE Conf. Comput. Vis. Pattern Recog. , 2021.",
            "ref_ids": [
                "36"
            ],
            "1": "Most related, Neural Body [36] attaches learnable features to the vertices of a SMPL body model [24].",
            "2": "Model Dataset PSNR\" SSIM\" LPIPS# Ch\u000210\u00003# NC\" IoU\" NeuralBody [36]RenderPeople 27.",
            "3": "We compare H-NeRF against NeuralBody [36]\n7 Figure 3: Frame Number Ablation."
        },
        "Headnerf: A real-time nerf-based parametric head model": {
            "authors": [
                "Yang Hong",
                "Bo Peng",
                "Haiyao Xiao",
                "Ligang Liu",
                "Juyong Zhang"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Hong_HeadNeRF_A_Real-Time_NeRF-Based_Parametric_Head_Model_CVPR_2022_paper.pdf",
            "ref_texts": "[42] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021. 2",
            "ref_ids": [
                "42"
            ],
            "1": "Benefiting from these advantages, NeRF has been widely used in many fields, such as 3D modeling [53, 58], human face/body digitization [12, 17, 42, 43, 48, 55], generating 4D free-view video [31,39], etc."
        },
        "Banmo: Building animatable 3d neural models from many casual videos": {
            "authors": [
                "Gengshan Yang",
                "Minh Vo",
                "Natalia Neverova",
                "Deva Ramanan",
                "Andrea Vedaldi",
                "Hanbyul Joo"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_BANMo_Building_Animatable_3D_Neural_Models_From_Many_Casual_Videos_CVPR_2022_paper.pdf",
            "ref_texts": "[37] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 2",
            "ref_ids": [
                "37"
            ],
            "1": "Similar to our goal, some recent works [24, 32, 36, 37, 44] produce posecontrollable NeRFs, but they rely on a human body model, or synchronized multi-view video inputs."
        },
        "Snarf: Differentiable forward skinning for animating non-rigid neural implicit shapes": {
            "authors": [
                "Xu Chen",
                "Yufeng Zheng",
                "Michael J. Black",
                "Otmar Hilliges",
                "Andreas Geiger"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_SNARF_Differentiable_Forward_Skinning_for_Animating_Non-Rigid_Neural_Implicit_Shapes_ICCV_2021_paper.pdf",
            "ref_texts": "[42] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , 2021. 3",
            "ref_ids": [
                "42"
            ],
            "1": "Recent [2, 3, 4, 6, 59] and concurrent works [9, 14, 22, 25, 26, 32, 40, 42, 44, 49, 50, 52, 53, 55] on learning 3D human models typically require a template mesh model with fixed topology, e."
        },
        "Neural fields in visual computing and beyond": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2111.11426"
        },
        "Neural 3d video synthesis from multi-view video": {
            "authors": [
                "Tianye Li",
                "Mira Slavcheva",
                "Michael Zollhofer",
                "Simon Green",
                "Christoph Lassner",
                "Changil Kim",
                "Tanner Schmidt",
                "Steven Lovegrove",
                "Michael Goesele",
                "Richard Newcombe",
                "Zhaoyang Lv"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Li_Neural_3D_Video_Synthesis_From_Multi-View_Video_CVPR_2022_paper.pdf",
            "ref_texts": "[44] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 3",
            "ref_ids": [
                "44"
            ],
            "1": "Several radiance field approaches have been proposed for modeling digital humans [16, 31, 40, 44, 46], but they can not directly be applied to general non-rigid scenes."
        },
        "Avatarclip: Zero-shot text-driven generation and animation of 3d avatars": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2205.08535",
            "ref_texts": "2021. Styleclip: Text-driven manipulation of stylegan imagery. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 2085\u20132094. Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Michael J Black. 2019. Expressive body capture: 3d hands, face, and body from a single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 10975\u201310985. Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021a. Animatable neural radiance fields for human body modeling. arXiv e-prints (2021), arXiv\u20132105. Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021b. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 9054\u20139063. Mathis Petrovich, Michael J Black, and G\u00fcl Varol. 2021. Action-Conditioned 3D Human Motion Synthesis with Transformer VAE. arXiv preprint arXiv:2104.05670 (2021). Leonid Pishchulin, Stefanie Wuhrer, Thomas Helten, Christian Theobalt, and Bernt Schiele. 2017. Building Statistical Shape Spaces for 3D Human Modeling. Pattern Recognition (2017). Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al .2021. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020 (2021). Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image generation. arXiv preprint arXiv:2102.12092 (2021). Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. 2016. Generative adversarial text to image synthesis. In International Conference on Machine Learning . PMLR, 1060\u20131069. Charles Rose, Michael F. Cohen, and Bobby Bodenheimer. 1998. Verbs and Adverbs: Multidimensional Motion Interpolation. IEEE Comput. Graph. Appl. 18, 5 (sep 1998), 32\u201340. https://doi.org/10.1109/38.708559 Shunsuke Saito, Jinlong Yang, Qianli Ma, and Michael J Black. 2021. SCANimate: Weakly supervised learning of skinned clothed avatar networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 2886\u20132897. Aditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang, Chin-Yi Cheng, and Marco Fumero. 2021. Clip-forge: Towards zero-shot text-to-shape generation. arXiv preprint arXiv:2110.02624 (2021). Kripasindhu Sarkar, Vladislav Golyanik, Lingjie Liu, and Christian Theobalt. 2021a. Style and pose control for image synthesis of humans from a single monocular view. arXiv preprint arXiv:2102.11263 (2021). Kripasindhu Sarkar, Lingjie Liu, Vladislav Golyanik, and Christian Theobalt. 2021b. HumanGAN: A Generative Model of Humans Images. arXiv preprint arXiv:2103.06902",
            "ref_ids": [
                "2021"
            ]
        },
        "Neural human performer: Learning generalizable radiance fields for human performance rendering": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper/2021/file/cf866614b6b18cda13fe699a3a65661b-Paper.pdf",
            "ref_texts": "[33] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021.",
            "ref_ids": [
                "33"
            ],
            "1": "Recently, neural radiance fields (NeRF) [26,11,17,30,33,35,36,47,50,52,53] have shown photo-realistic novel view synthesis results in per-scene optimization settings.",
            "2": "Alternatively, several methods [23,33] have proposed to learn person-specific global appearance features from multi-view observations.",
            "3": "We study the efficacy of Neural Human Performer on two multi-view human performance capture datasets, ZJU-MoCap [33] and AIST [16].",
            "4": "Furthermore, we compare ours with identity-specific methods [33,44,49] that also utilize a 3D human body model prior.",
            "5": "To regularize the training, Neural Body [33] combines NeRF with a deformable human body model (e.",
            "6": "Despite the promising results, these general deformable NeRF [17,53] and human-specific NeRF [11,9, 33,32] methods must be optimized for each new video separately, and generalize poorly on unseen scenarios.",
            "7": "3 Multi-view aggregation of skeletal and query features Given a query 3D point x\u2208R3, we retrieve the corresponding (time-augmented) skeletal feature s\u2032 c,tx\u2208Rdat the queried location via trilinear interpolation in the SMPL space with SparseConvNet [19], following [39, 34, 51, 33].",
            "8": "We experiment on the ZJU-MoCap [33] and AIST datasets [45,16].",
            "9": "We perform comparisons with the state-of-the-art Neural Body (NB) [33] that combines the body model prior SMPL and NeRF in a per-subject optimization setting.",
            "10": "We experiment with ZJU-MoCap dataset [33] which provides performance captures of 10 human subjects captured from 23 synchronized cameras, human body model parameters as well as the foreground mask corresponding to each frame.",
            "11": "Results of NT(Neural textures) [44], NHR (Neural human rendering) [49], NB (Neural body) [33] and ours.",
            "12": "Our method significantly outperforms all the baselines and the state-of-the-art Neural Body [33] by +3.",
            "13": "Even when we train a single model of our method for all the source sobjects (\u2018ours\u2019), ours achieves comparable results with the state-of-the-art per-subject based methods [33].",
            "14": "1c shows that in this case, our model (\u2018ours per-subject\u2019) achieves a significant improvement over the best-performing baseline [33] by +3 PSNR and +1.",
            "15": "The visualizations show that our 3D reconstructions align well with the input images, and is more accurate than even the per-subject method [33] (e.",
            "16": "NB (Neural Body) [33], PV A (Pixel volumetric avatar) [36], Pixel-NeRF [52] and ours.",
            "17": "datasets [33,16] have significantly different statistics both in terms of color distribution (background, lighting) and distance of the camera to the subject, making the cross-dataset generalization task extremely challenging.",
            "18": "9 Acknowledgments and Disclosure of Funding We thank Sida Peng of Zhejiang University, Hangzhou, China, for many very helpful discussions on a variety of implementation details of the Neural Body [33]."
        },
        "Hexplane: A fast representation for dynamic scenes": {
            "authors": [
                "Ang Cao",
                "Justin Johnson"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_HexPlane_A_Fast_Representation_for_Dynamic_Scenes_CVPR_2023_paper.pdf",
            "ref_texts": "[52] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans.",
            "ref_ids": [
                "52"
            ],
            "1": "Representing dynamic scenes by neural radiance fields is an essential extension of NeRF, enabling numerous real-world applications [27, 47, 52, 65, 78, 84, 91]."
        },
        "Neural head avatars from monocular rgb videos": {
            "authors": [
                "William Grassal",
                "Malte Prinzler",
                "Titus Leistner",
                "Carsten Rother",
                "Matthias Niessner",
                "Justus Thies"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Grassal_Neural_Head_Avatars_From_Monocular_RGB_Videos_CVPR_2022_paper.pdf",
            "ref_texts": "[57] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and 18662",
            "ref_ids": [
                "57"
            ],
            "1": "Motivated by their recent success in 3D scene reconstruction [67], neural radiance fields (NeRF) in combination with volumetric rendering [49] have been used to replace the discrete feature voxel grids [5,29,39,44,52\u201354,56,57,60,65,80]."
        },
        "Neural rays for occlusion-aware image-based rendering": {
            "authors": [
                "Yuan Liu",
                "Sida Peng",
                "Lingjie Liu",
                "Qianqian Wang",
                "Peng Wang",
                "Christian Theobalt",
                "Xiaowei Zhou",
                "Wenping Wang"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Neural_Rays_for_Occlusion-Aware_Image-Based_Rendering_CVPR_2022_paper.pdf",
            "ref_texts": "[36] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural Body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 2",
            "ref_ids": [
                "36"
            ],
            "1": "To further improve the rendering resolution, some methods [20, 23, 27, 31, 34, 36, 49] resort to pure neural fields encoded by neural networks to represent 3D scenes."
        },
        "Neuman: Neural human radiance field from a single video": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2203.12575.pdf?trk=public_post_comment-text",
            "ref_texts": "33. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9054\u20139063 (2021) 1, 3, 10, 20 NeuMan 17",
            "ref_ids": [
                "33"
            ],
            "1": "Recent efforts also focus on animation of these radiance field models [19,33,32,12,40] of human, with the aid of large controlled datasets, further extending the application domain of radiance-field-based modeling to enable augmented reality experiences.",
            "2": "Existing methods [33,19] require multi-cameras setup, consistent lighting and exposure, clean backgrounds, and accurate human geometry to train the NeRF models.",
            "3": "2 Related Work As our work is mainly based on neural radiance fields, we first review works on NeRF with a focus on works that aim to control and condition the radiance fields\u2014a necessity for rendering a human in the scene in the context of creating visual and immersive experiences [33,19].",
            "4": "While these methods have shown interesting and exciting results, they often require separate training of editable instances [9] or careful curation of training data [33].",
            "5": "Particularly related to our task of interest, various efforts have been made towards NeRF models conditioned by explicit human models, such as SMPL [22] or 3D skeleton [19,33,32,12,40].",
            "6": "Neural Body [33] associates a latent code to each SMPL vertex, and use sparse convolution to diffuse the latent code into the volume in observation space.",
            "7": "Generally, motion capture data [33,11] is captured with a static multi-cameras system in a Fig.",
            "8": "We apply NeuralBody [33] to our dataset in a monocular setting.",
            "9": "NeuralBody [33] overfits to the training observations, and produce poor rendering on the back of the subject, while ours generalize better and can faithfully render the back.",
            "10": "12: Novel View Reconstructions on public ZJU Mocap dataset [33]\n\u2013Ours and HumanNeRF [47] use only one camera view, NeuralBody [33] uses multiple camera views.",
            "11": "We also compare our method with HumanNeRF [47] and NeuralBody [33] on a ZJU Mocap dataset, as shown qualitative comparisons in Figure 12."
        },
        "Selfrecon: Self reconstruction your digital avatar from monocular video": {
            "authors": [
                "Boyi Jiang",
                "Yang Hong",
                "Hujun Bao",
                "Juyong Zhang"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Jiang_SelfRecon_Self_Reconstruction_Your_Digital_Avatar_From_Monocular_Video_CVPR_2022_paper.pdf",
            "ref_texts": "[39] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 2,6,7",
            "ref_ids": [
                "39"
            ],
            "1": "NeuralBody [39] reconstructs per frame\u2019s NeRF [34] field conditioned at body structured latent codes and utilizes the NeRF\nfield to synthesize new images.",
            "2": "Qualitative Evaluation We also qualitatively compare SelfRecon with multiframe prediction algorithm PaMIR [55], optimization method VideoAvatar [2] and NeRF [34] based neural rendering method NeuralBody [39] on several sequences of PeopleSnapshot dataset.",
            "3": "Comparison results with methods that use video or multi-frame images, including PaMIR [55], NeuralBody [39] and VideoAvatar [2]."
        },
        "Photorealistic monocular 3d reconstruction of humans wearing clothing": {
            "authors": [
                "Thiemo Alldieck",
                "Mihai Zanfir",
                "Cristian Sminchisescu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Alldieck_Photorealistic_Monocular_3D_Reconstruction_of_Humans_Wearing_Clothing_CVPR_2022_paper.pdf",
            "ref_texts": "[32] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE Conf. Comput. Vis. Pattern Recog. , 2021. 2",
            "ref_ids": [
                "32"
            ],
            "1": "Neural radiance fields [28] are a related class of representations specialized for image synthesis that have also been used to model humans [24, 32, 42]."
        },
        "Fourier plenoctrees for dynamic radiance field rendering in real-time": {
            "authors": [
                "Liao Wang",
                "Jiakai Zhang",
                "Xinhang Liu",
                "Fuqiang Zhao",
                "Yanshun Zhang",
                "Yingliang Zhang",
                "Minye Wu",
                "Jingyi Yu",
                "Lan Xu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Fourier_PlenOctrees_for_Dynamic_Radiance_Field_Rendering_in_Real-Time_CVPR_2022_paper.pdf",
            "ref_texts": "[41] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 2, 6",
            "ref_ids": [
                "41"
            ],
            "1": "skeleton [40] or parametric models [27, 41]) to explicitly calculate stable motion flows from model animations.",
            "2": "To demonstrate the overall performance of our approach, we compare to the existing free-viewpoint video methods based on neural rendering, including the voxel-based method Neural Volumes [28], and implicit methods iButter[58], ST-NeRF [71] and Neural Body [41] based on neural radiance field."
        },
        "Monocular dynamic view synthesis: A reality check": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/dab5a29f6614ec47ea0ca85c140226fd-Paper-Conference.pdf",
            "ref_texts": "[30] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021.",
            "ref_ids": [
                "30"
            ],
            "1": "Dynamic NeRFs reconstruct moving scenes from multi-view inputs or given pre-defined deformation template [28,29,30,31, 32,33,34,35].",
            "2": "The lack of simultaneous multi-view in the monocular video makes this problem more challenging compared to the multi-view setting, such as reconstructing moving people from multiple cameras [30, 34, 37]."
        },
        "Neumesh: Learning disentangled neural mesh-based implicit field for geometry and texture editing": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2207.11911",
            "ref_texts": "36. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9054\u20139063 (2021) 4",
            "ref_ids": [
                "36"
            ],
            "1": "NeRF [27] takes advantages of volume rendering to boost rendering quality, which inspires a lot of works, including surface reconstruction [32,55,65], human modeling [21,36], pose estimation [67], scene understanding [63] and relighting [48,71,2,72], etc."
        },
        "Semantic-aware implicit neural audio-driven video portrait generation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2201.07786",
            "ref_texts": "[45] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021. 2, 4, 5",
            "ref_ids": [
                "45"
            ],
            "1": "Naturally, naive NeRF is confined to static scenes, which triggers a branch of studies to extend NeRF for dynamic scenes [4, 18, 27, 31, 38\u201341, 43, 45, 48, 49, 58, 62].",
            "2": "We take inspiration from [44, 45, 72] to anchor a set of latent codes to the vertices of 3DMM model and diffuse to 3D space with SparseConvNet [19] to extract latent code volume.",
            "3": "[45].",
            "4": "The structured 3D feature extractor is borrowed from [45] that processes feature volume with 3D sparse convolutions and outputs latent code with 2\u0002,4\u0002, 8\u0002,16\u0002downsampled sizes."
        },
        "Geonerf: Generalizing nerf with geometry priors": {
            "authors": [
                "Mohammad Mahdi",
                "Yann Lepoittevin",
                "Francois Fleuret"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Johari_GeoNeRF_Generalizing_NeRF_With_Geometry_Priors_CVPR_2022_paper.pdf",
            "ref_texts": "[41] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021. 2",
            "ref_ids": [
                "41"
            ],
            "1": "Building upon NeRF many improvements were made [3, 13, 30, 34, 40, 41, 44, 48, 49], but the network needs to be optimized for hours or days for each new scene."
        },
        "Structured local radiance fields for human avatar modeling": {
            "authors": [
                "Zerong Zheng",
                "Han Huang",
                "Tao Yu",
                "Hongwen Zhang",
                "Yandong Guo",
                "Yebin Liu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_Structured_Local_Radiance_Fields_for_Human_Avatar_Modeling_CVPR_2022_paper.pdf",
            "ref_texts": "[55] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 3,5,6,7",
            "ref_ids": [
                "55"
            ],
            "1": "Human motions are usually much more challenging to learn using neural networks, and several works [30, 49,55] incorporated prior from a statistical body template to tackle this difficulty.",
            "2": "Recently,neural scene representations and rendering techniques are adopted for higher-fidelity results [35, 54,55].",
            "3": "For evaluation and comparison with baseline methods, we mainly use the following dataset: (1) Two dress sequences from [22], which are captured using 100 cameras but we manually select 20 views among them for computational efficiency; (2) One sweater sequences from [24] captured with 10 cameras; (3) Two sequences from ZJU-MoCap [55] captured with 23 cameras; and (4) three multi-view sequences collected by ourselves with 24 cameras1.",
            "4": "Comparison We mainly compare our method with Animatable NeRF [54] and Neural Body [55].",
            "5": "To conduct a fair comparison with Neural Body [55], we use their dataset and follow the same protocal in their paper.",
            "6": "In this comparison, we train our network using only 300 image frames from four views, as done in [55].",
            "7": "Quantitative comparison with Neural Body [55] and Animatable NeRF [54] on ZJU-MoCap dataset.",
            "8": "PSNR (\u2191) SSIM (\u2191) ID Pose Type [55] [54] Ours [55] [54] Ours 387Seen 25.",
            "9": "Comparison against Neural Body [55] in terms of both novel view synthesis and pose generation.",
            "10": "2shows that our model achieves higher accuracy than [55] in both metrics."
        },
        "Infonerf: Ray entropy minimization for few-shot neural volume rendering": {
            "authors": [
                "Mijeong Kim",
                "Seonguk Seo",
                "Bohyung Han"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Kim_InfoNeRF_Ray_Entropy_Minimization_for_Few-Shot_Neural_Volume_Rendering_CVPR_2022_paper.pdf",
            "ref_texts": "[23] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 2, 3, 5, 6, 7",
            "ref_ids": [
                "23"
            ],
            "1": "For example, PixelNeRF [37] takes advantage of the features extracted from seen images to compensate for missing information in unseen views while [14, 23] focus on a particular object class, e.",
            "2": "To overcome the limitation, several approaches exploit multi-view feature semantics by introducing an image encoder for NeRF to estimate color and opacity [14, 23, 37] or achieve semantic consistency between seen images and rendered novel views [11].",
            "3": "Datasets We describe the details of three benchmarks employed to evaluate our algorithm, which include the Realistic Synthetic 360\u25e6[20], ZJU-MoCap [23], and DTU [12] datasets.",
            "4": "Following [23], we sample 4 uniformly distributed viewpoints to construct a training set and use the remaining images for testing.",
            "5": "2 ZJU-MoCap For the ZJU-MoCap dataset, InfoNeRF is evaluated in comparison with NeRF [20], Neural V olume (NV) [17], and Neural Body (NB) [23], where all algorithms are trained with 4 images.",
            "6": "NB [23] has the geometric prior by exploiting the pretrained human body model (SMPL).",
            "7": "Method Prior PSNR \u2191 SSIM\u2191 LPIPS \u2193 NB [23] \u2713 24.",
            "8": "Therefore, the na \u00a8\u0131ve evaluation of the algorithms without scene prior is not de12917\n\n(a) Ground-truth (b) NeRF [20]\n (c) NV [17]\n (d) InfoNeRF (ours)\n (e) NB [23] Figure 3."
        },
        "Tava: Template-free animatable volumetric actors": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2206.08929",
            "ref_texts": "35.Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9054\u20139063 (2021)",
            "ref_ids": [
                "35"
            ],
            "1": "In our experiments, we demonstrate that the proposed approach outperforms state-of-the art approaches for animating and rendering human actors on the ZJU motion capture dataset [35].",
            "2": "TAVA: Template-free Animatable Volumetric Actors 3 Methods Template-freeNo Per-frame Latent Code3D Canonical SpaceDeformation NARF [29] \u2714 \u2714 \u2718 Inverse A-NeRF [42] \u2714 \u2718 \u2718 Inverse Animatable-NeRF [34] \u2718 \u2718 \u2714 Inverse HumanNeRF [45] \u2718 \u2714 \u2714 Inverse NeuralBody [35] \u2718 \u2718 \u2714\u2020Forward Ours (TAVA) \u2714 \u2714 \u2714 Forward Table 1.",
            "3": "NeuralBody [35] anchors latent codes on the vertices of a deformable mesh controlled by LBS.",
            "4": "Some [22,34,35,45] are built on top of the SMPL [25] body template, which prohibits them to be applied to creatures beyond humans.",
            "5": "Moreover, most of the aforementioned methods either introduce latent codes to better memorize the seen poses [42,34,35], or represent the deformation in the inverse direction from view space to the canonical space [29,42,34,45].",
            "6": "1 Datasets We conduct experiments on 1) four human subjects (313, 315, 377, 386) in the ZJU-Mocap dataset [35], a public multi-view video dataset for human motion, and 2) two synthetic animal subjects (Hare, Wolf) introduced in this paper, rendered from multiple views using Blender.",
            "7": "Prior works [34,35] create the train andvalsets on the ZJU-Mocap dataset by simply splitting each video with 500 \u223c2200 frames into two splits, where the training set has 60 \u223c300 frames and the validation set has 300 \u223c1000 frames.",
            "8": "For the view splits, we follow the protocol from [34,35] for ZJU-Mocap, where 4 views are used for training and 17 views for testing.",
            "9": "961 NeuralBody [35] 33.",
            "10": "We compare our work with two types of previous methods: 1) Templatefree methods, including NARF [29] and A-NeRF [42], as well as 2) SMPL-based methods, including Animatable-NeRF [34] and NeuralBody [35].",
            "11": "Thus the previous way [34,35] of splitting the dataset into two chunks with consecutive frames will cover similar poses in both sets, which is not suitable for evaluating the pose generalization ability.",
            "12": "2, for the template-based baselines Animatable-NeRF [34] and NeuralBody [35], we use their official implementations.",
            "13": "The ZJU-Mocap dataset has become an increasingly popular dataset to study human performance capture, reconstruction, and neural rendeirng [34,35,45].",
            "14": "952 NeuralBody [35] 33.",
            "15": "949 NeuralBody [35] 31.",
            "16": "969 NeuralBody [35] 33.",
            "17": "974 NeuralBody [35] 36."
        },
        "The power of points for modeling humans in clothing": {
            "authors": [
                "Qianli Ma",
                "Jinlong Yang",
                "Siyu Tang",
                "Michael J. Black"
            ],
            "url": "http://openaccess.thecvf.com/content/ICCV2021/papers/Ma_The_Power_of_Points_for_Modeling_Humans_in_Clothing_ICCV_2021_paper.pdf",
            "ref_texts": "[47] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , pages 9054\u20139063, June 2021. 3",
            "ref_ids": [
                "47"
            ],
            "1": "Neural implicit surfaces [11, 38, 43], on the other hand, do not require any pre-defined template, are flexible with surface topology, and have recently become a promising 10975\n choice for reconstructing [7, 24, 25, 47, 53, 54, 64, 65] and modeling [10, 12, 14, 39, 41, 55] shapes of 3D humans."
        },
        "Humannerf: Efficiently generated human radiance field from sparse inputs": {
            "authors": [
                "Fuqiang Zhao",
                "Wei Yang",
                "Jiakai Zhang",
                "Pei Lin",
                "Yingliang Zhang",
                "Jingyi Yu",
                "Lan Xu"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Zhao_HumanNeRF_Efficiently_Generated_Human_Radiance_Field_From_Sparse_Inputs_CVPR_2022_paper.pdf",
            "ref_texts": "[33] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 1, 6",
            "ref_ids": [
                "33"
            ],
            "1": "Remarkably, NeRF [29] and its dynamic extensions [24,33,35,50,53,64] enable photo-realistic novel view synthesis for dynamic scenes without heavy reliance on the reconstruction accuracy.",
            "2": "Comparison We first compare our HumanNeRF method with perscene optimization approaches including Neural Body [33], Neural V olumes [27] and ST-NeRF [64] both qualitatively and quantitatively.",
            "3": "Results from our approach exhibit much better textures and the geometries are complete and accurate both for the \u201cTaichi\u201d from public ZJUMoCap [33] and the \u201cBatman\u201d data collected by ourselves."
        },
        "Reconstructing personalized semantic facial nerf models from monocular video": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2210.06108",
            "ref_texts": "2022. Neural 3D Video Synthesis From Multi-View Video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 5521\u20135531. Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. 2020. Neural Sparse Voxel Fields. NeurIPS (2020). Xian Liu, Yinghao Xu, Qianyi Wu, Hang Zhou, Wayne Wu, and Bolei Zhou. 2022. Semantic-Aware Implicit Neural Audio-Driven Video Portrait Generation. arXiv preprint arXiv:2201.07786 (2022). Stephen Lombardi, Tomas Simon, Gabriel Schwartz, Michael Zollhoefer, Yaser Sheikh, and Jason Saragih. 2021. Mixture of volumetric primitives for efficient neural rendering. ACM Transactions on Graphics (TOG) 40, 4 (2021), 1\u201313. Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. 2020. Nerf: Representing scenes as neural radiance fields for view synthesis. In European conference on computer vision . Springer, 405\u2013421. Thomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. 2022. Instant Neural Graphics Primitives with a Multiresolution Hash Encoding. ACM Trans. Graph. 41, 4 (July 2022), 102:1\u2013102:15. Michael Niemeyer, Jonathan T Barron, Ben Mildenhall, Mehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. 2022. Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 5480\u20135490. Michael Niemeyer and Andreas Geiger. 2021. Giraffe: Representing scenes as compositional generative neural feature fields. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . 11453\u201311464. Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Steven M. Seitz, and Ricardo Martin-Brualla. 2021a. Nerfies: Deformable Neural Radiance Fields. ICCV (2021). Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, and Steven M. Seitz. 2021b. HyperNeRF: A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields. ACM Trans. Graph. 40, 6, Article 238 (dec 2021). Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al .2019. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems 32 (2019). Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021a. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 14314\u201314323. Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021b. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 9054\u20139063. A. Pumarola, A. Agudo, A.M. Martinez, A. Sanfeliu, and F. Moreno-Noguer. 2019. GANimation: One-Shot Anatomically Consistent Facial Animation. (2019). Anurag Ranjan, Timo Bolkart, Soubhik Sanyal, and Michael J Black. 2018. Generating 3D faces using convolutional mesh autoencoders. In Proceedings of the European Conference on Computer Vision (ECCV) . 704\u2013720. Sara Fridovich-Keil and Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. 2022. Plenoxels: Radiance Fields without Neural Networks. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. 2020. Graf: Generative radiance fields for 3d-aware image synthesis. Advances in Neural Information Processing Systems 33 (2020), 20154\u201320166. Aliaksandr Siarohin, St\u00e9phane Lathuili\u00e8re, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe.",
            "ref_ids": [
                "2022"
            ]
        },
        "Real-time deep dynamic characters": {
            "authors": [
                "Marc Habermann",
                "Lingjie Liu",
                "Weipeng Xu",
                "Michael Zollhoefer",
                "Gerard Pons",
                "Christian Theobalt"
            ],
            "url": "https://dl.acm.org/doi/pdf/10.1145/3450626.3459749",
            "ref_texts": "(2020). Amit Raj, Michael Zollhoefer, Tomas Simon, Jason Saragih, Shunsuke Saito, James Hays, and Stephen Lombardi. 2020. PVA: Pixel-aligned Volumetric Avatars. In arXiv:2101.02697 . Igor Santesteban, Miguel A. Otaduy, and Dan Casas. 2019. Learning-Based Animation of Clothing for Virtual Try-On. Comput. Graph. Forum 38 (2019), 355\u2013366. Kripasindhu Sarkar, Dushyant Mehta, Weipeng Xu, Vladislav Golyanik, and Christian Theobalt. 2020. Neural Re-Rendering of Humans from a Single Image. In European Conference on Computer Vision (ECCV) . Aliaksandra Shysheya, Egor Zakharov, Kara-Ali Aliev, Renat Bashirov, Egor Burkov, Karim Iskakov, Aleksei Ivakhnenko, Yury Malkov, Igor Pasechnik, Dmitry Ulyanov, Alexander Vakhitov, and Victor Lempitsky. 2019. Textured Neural Avatars. arXiv:1905.08776 [cs.CV] Chenyang Si, Wei Wang, Liang Wang, and Tieniu Tan. 2018. Multistage Adversarial Losses for Pose-Based Human Image Synthesis. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . Aliaksandr Siarohin, Enver Sangineto, Stephane Lathuiliere, and Nicu Sebe. 2018. Deformable GANs for Pose-based Human Image Generation. In CVPR 2018 . Yinghao Xu Qianqian Wang Qing Shuai Hujun Bao Xiaowei Zhou Sida Peng, Yuanqing Zhang. 2020. Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. arXiv preprint arXiv:2012.15838 (2020). Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Nie\u00dfner, Gordon Wetzstein, and Michael Zollh\u00f6fer. 2019a. DeepVoxels: Learning Persistent 3D Feature Embeddings. InComputer Vision and Pattern Recognition (CVPR) . Vincent Sitzmann, Michael Zollh\u00f6fer, and Gordon Wetzstein. 2019b. Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations. In Advances in Neural Information Processing Systems . Olga Sorkine and Marc Alexa. 2007. As-rigid-as-possible Surface Modeling. In Proceedings of the Fifth Eurographics Symposium on Geometry Processing (Barcelona, Spain)"
        },
        "Anifacegan: Animatable 3d-aware face image generation for video avatars": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/eae78bf2712f222f101bd7d12f875a57-Paper-Conference.pdf",
            "ref_texts": "[49] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u2013",
            "ref_ids": [
                "49"
            ],
            "1": "The original NeRF and most of its successors [43,33,45,49,48,40,32,73] focus on learning scene-specific representation using a set of posed images or a video sequence of a static or dynamic scene."
        },
        "Metaavatar: Learning animatable clothed human models from few depth images": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper/2021/file/1680829293f2a8541efa2647a0290f88-Paper.pdf",
            "ref_texts": "[57] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proc. of CVPR , 2021. 3",
            "ref_ids": [
                "57"
            ],
            "1": "Neural Implicit Representations: Neural implicit representations [11,45,46,52,55] have been used to tackle both image-based [27,35,36,57,59,63,64,87] and point cloud-based [7,12] clothed human reconstruction."
        },
        "Lisa: Learning implicit shape and appearance of hands": {
            "authors": [
                "Enric Corona",
                "Tomas Hodan",
                "Minh Vo",
                "Francesc Moreno",
                "Chris Sweeney",
                "Richard Newcombe",
                "Lingni Ma"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Corona_LISA_Learning_Implicit_Shape_and_Appearance_of_Hands_CVPR_2022_paper.pdf",
            "ref_texts": "[51] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural Body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 9054\u20139063, 2021. 2",
            "ref_ids": [
                "51"
            ],
            "1": "For modeling dynamic human bodies, Neural Body [51] attaches learnable vertex features to SMPL, and diffuses the features with sparse convolution for volumetric rendering."
        },
        "Deforming radiance fields with cages": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2207.12298",
            "ref_texts": "38. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9054\u20139063 (2021)",
            "ref_ids": [
                "38"
            ],
            "1": "Harada For some specific object categories, such as the human body or articulated objects, recent studies [24,32,33,37,38,41,46] enable the generation of the unseen scene by controlling the body shape or bone pose.",
            "2": "For the specific task of human body modeling, various works proposed to combine NeRF with a parametric human model to enable human body reposing [37, 38], shape control [24] or even clothing changes [46]."
        },
        "DisCoScene: Spatially Disentangled Generative Radiance Fields for Controllable 3D-aware Scene Synthesis": {
            "authors": [
                "Yinghao Xu",
                "Menglei Chai",
                "Zifan Shi",
                "Sida Peng",
                "Ivan Skorokhodov",
                "Aliaksandr Siarohin",
                "Ceyuan Yang",
                "Yujun Shen",
                "Ying Lee",
                "Bolei Zhou",
                "Sergey Tulyakov"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Xu_DisCoScene_Spatially_Disentangled_Generative_Radiance_Fields_for_Controllable_3D-Aware_Scene_CVPR_2023_paper.pdf",
            "ref_texts": "[38] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE Conf. Comput. Vis. Pattern Recog. , pages 9054\u20139063, 2021. 1",
            "ref_ids": [
                "38"
            ],
            "1": "Recent approaches like GRAF [40] and Pi-GAN [5] introduce 3D inductive bias by taking neural radiance fields [1, 28, 29, 36, 38] as the underlying representation, gaining the capability of geometry modeling and explicit camera control."
        },
        "Mofanerf: Morphable facial neural radiance field": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2112.02308",
            "ref_texts": "43. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: CVPR (2021)",
            "ref_ids": [
                "43"
            ],
            "1": "SMPL)[39,7,34,43] or skeleton[42] as prior to build NeRF for human body."
        },
        "A-sdf: Learning disentangled signed distance functions for articulated shape representation": {
            "authors": [
                "Jiteng Mu",
                "Weichao Qiu",
                "Adam Kortylewski",
                "Alan Yuille",
                "Nuno Vasconcelos",
                "Xiaolong Wang"
            ],
            "url": "http://openaccess.thecvf.com/content/ICCV2021/papers/Mu_A-SDF_Learning_Disentangled_Signed_Distance_Functions_for_Articulated_Shape_Representation_ICCV_2021_paper.pdf",
            "ref_texts": "[32]D. Katz and O. Brock. Manipulating articulated objects withinteractive perception. In2008 IEEE International Confer-ence on Robotics and Automation, pages 272\u2013277, 2008.3[33]Hyunjik Kim and Andriy Mnih. Disentangling by factoris-ing. InICML, 2018.3[34]Muhammed Kocabas, Nikos Athanasiou, and Michael J.Black. VIBE: video inference for human body pose andshape estimation. InCVPR, pages 5252\u20135262, 2020.2[35]Amit P. S. Kohli, Vincent Sitzmann, and Gordon Wetzstein.Inferring semantic information with 3d neural scene repre-sentations.CoRR, abs/2003.12673, 2020.2[36]Nilesh Kulkarni, Abhinav Gupta, David F Fouhey, and Shub-ham Tulsiani. Articulation-aware canonical surface map-ping. InCVPR, pages 452\u2013461, 2020.2[37]Tejas D Kulkarni, William F Whitney, Pushmeet Kohli, andJosh Tenenbaum. Deep convolutional inverse graphics net-work. InNeurIPS, pages 2539\u20132547, 2015.3[38]Tianye Li, Timo Bolkart, Michael J. Black, Hao Li, andJavier Romero. Learning a model of facial shape and ex-pression from 4d scans.ACM Trans. Graph., 36(6):194:1\u2013194:17, 2017.2[39]Xueting Li, Sifei Liu, Shalini De Mello, Kihwan Kim, Xi-aolong Wang, Ming-Hsuan Yang, and Jan Kautz. Onlineadaptation for consistent mesh reconstruction in the wild. InNeurIPS, 2020.3[40]Xiaolong Li, He Wang, Li Yi, Leonidas J. Guibas, A. LynnAbbott, and Shuran Song. Category-level articulated objectpose estimation. InCVPR, pages 3703\u20133712, 2020.1,5[41]Yiyi Liao, Simon Donn\u00b4e, and Andreas Geiger. Deep march-ing cubes: Learning explicit surface representations. InCVPR, pages 2916\u20132925, 2018.2[42]Andrew Liu, Shiry Ginosar, Tinghui Zhou, Alexei A. Efros,and Noah Snavely. Learning to factorize and relight a city.InECCV, 2020.3[43]Matthew Loper, Naureen Mahmood, Javier Romero, GerardPons-Moll, and Michael J. Black. SMPL: a skinned multi-person linear model.ACM Trans. Graph., 34(6):248:1\u2013248:16, 2015.2[44]Roberto Martin Martin, Sebastian H\u00a8ofer, and Oliver Brock.An integrated approach to visual perception of articulatedobjects. InICRA, pages 5091\u20135097, 2016.2[45]Roberto Mart\u00b4\u0131n-Mart\u00b4\u0131n, Clemens Eppner, and Oliver Brock.The RBO dataset of articulated objects and interactions.Int.J. Robotics Res., 38(9), 2019.2,8[46]Lars M. Mescheder, Michael Oechsle, Michael Niemeyer,Sebastian Nowozin, and Andreas Geiger. Occupancy net-works: Learning 3d reconstruction in function space. InCVPR, pages 4460\u20134470, 2019.2,3[47]Frank Michel, Alexander Krull, Eric Brachmann,Michael Ying Yang, Stefan Gumhold, and Carsten Rother.Pose estimation of kinematic chain instances via objectcoordinate regression. InBMVC, pages 181.1\u2013181.11.BMV A Press, 2015.2[48]Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:Representing scenes as neural radiance fields for view syn-thesis. InECCV, pages 405\u2013421, 2020.2[49]Mayank Mittal, David Hoeller, Farbod Farshidian, MarcoHutter, and Animesh Garg. Articulated object interactionin unknown scenes with whole-body mobile manipulation.arXiv preprint arXiv:2103.10534, 2021.3[50]Kaichun Mo, Leonidas Guibas, Mustafa Mukadam, Abhi-nav Gupta, and Shubham Tulsiani. Where2act: From pix-els to actions for articulated 3d objects.arXiv preprintarXiv:2101.02692, 2021.3[51]Ravi Teja Mullapudi, Steven Chen, Keyi Zhang, Deva Ra-manan, and Kayvon Fatahalian. Online model distillationfor efficient video inference.ICCV, Oct 2019.3[52]Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, ChristianRichardt, and Yong-Liang Yang. Hologan: Unsupervisedlearning of 3d representations from natural images. InICCV,pages 7587\u20137596, 2019.3[53]Michael Niemeyer, Lars M. Mescheder, Michael Oechsle,and Andreas Geiger. Occupancy flow: 4d reconstructionby learning particle dynamics. InICCV, pages 5378\u20135388,2019.2[54]Michael Niemeyer, Lars M. Mescheder, Michael Oechsle,and Andreas Geiger. Differentiable volumetric rendering:Learning implicit 3d representations without 3d supervision.InCVPR, pages 3501\u20133512, 2020.2[55]Atsuhiro Noguchi, Xiao Sun, Stephen Lin, and TatsuyaHarada. Neural articulated radiance field.arXiv preprintarXiv: 2104.03110, 2021.2[56]Mohamed Omran, Christoph Lassner, Gerard Pons-Moll, Pe-ter V . Gehler, and Bernt Schiele. Neural body fitting: Uni-fying deep learning and model based human pose and shapeestimation. In3DV, pages 484\u2013494, 2018.2[57]Pablo R. Palafox, Aljaz Bozic, Justus Thies, MatthiasNie\u00dfner, and Angela Dai. Npms: Neural parametric modelsfor 3d deformable shapes.arXiv preprint arXiv:2104.00702,2021.2[58]Jeong Joon Park, Peter Florence, Julian Straub, Richard A.Newcombe, and Steven Lovegrove. Deepsdf: Learning con-tinuous signed distance functions for shape representation.InCVPR, pages 165\u2013174, 2019.1,2,3,4,5,6,7,8[59]Taesung Park, Jun-Yan Zhu, Oliver Wang, Jingwan Lu, EliShechtman, Alexei Efros, and Richard Zhang. Swapping au-toencoder for deep image manipulation.NeurIPS, 33, 2020.3[60]Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang,Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body:Implicit neural representations with structured latent codesfor novel view synthesis of dynamic humans.arXiv preprintarXiv: 2012.15838, 2021.2[61]Stanislav Pidhorskyi, Donald A Adjeroh, and GianfrancoDoretto. Adversarial latent autoencoders. InCVPR, pages14104\u201314113, 2020.3[62]Anurag Ranjan, Timo Bolkart, Soubhik Sanyal, andMichael J. Black. Generating 3d faces using convolutionalmesh autoencoders. InECCV, pages 725\u2013741, 2018.2[63]Gernot Riegler, Ali Osman Ulusoy, and Andreas Geiger.Octnet: Learning deep 3d representations at high resolutions.InCVPR, pages 6620\u20136629, 2017.210",
            "ref_ids": [
                "32",
                "33",
                "34",
                "35",
                "36",
                "37",
                "38",
                "39",
                "40",
                "41",
                "42",
                "43",
                "44",
                "45",
                "46",
                "47",
                "48",
                "49",
                "50",
                "51",
                "52",
                "53",
                "54",
                "55",
                "56",
                "57",
                "58",
                "59",
                "60",
                "61",
                "62",
                "63"
            ],
            "1": "A-SDF: Learning Disentangled Signed Distance Functionsfor Articulated Shape RepresentationJiteng Mu1, Weichao Qiu2, Adam Kortylewski2, Alan Yuille2,Nuno Vasconcelos1, Xiaolong Wang11UC San Diego,2Johns Hopkins University InputsShape Space Joint Angle LaptopEyeglassesStapler(87\u00b0)(45\u00b0)(78\u00b0)(9\u00b0)(45\u00b0, 15\u00b0)(5\u00b0, 45\u00b0)(45\u00b0, 45\u00b0)(5\u00b0, 5\u00b0)(15\u00b0)(-9\u00b0)(-39\u00b0)(-60\u00b0)MLP45\u00b080\u00b063\u00b090\u00b0MLP Generated shapes at unseen articulations Figure 1: We represent articulated objects with separate codes for encoding shape and articulation.",
            "2": "Tounderstand articulated objects, recent works propose to traindeep networks for estimating per-part poses and the jointangle parameters of an object instance in a known cate-gory [40,82].",
            "3": "We build our articulated object model based on the deepimplicit Signed Distance Functions [58].",
            "4": "We observe that models with a single shape code in-put, such as DeepSDF [58], cannot encode the articulationvariation reliably.",
            "5": "More interestingly, our modelcan generalize to real-world depth images from the RBOdataset [45] and we quantitatively demonstrate superior per-formance over the baselines.",
            "6": "A large body ofwork [80,19,9,11,41,63,17,78,77,89,13,1,15] hasfocused on investigating efficient and accurate 3D objectrepresentations.",
            "7": "Recent advances suggest that representing3D objects as continuous and differentiable implicit func-tions [18,58,46,10,35,26,84,72,54,64,48,71,76] canmodel various topologies in a memory-efficient way.",
            "8": "Most of these work islimited to modeling static objects and scenes [18,26,84,72,54,64,48,71,76].",
            "9": "One line of work leverages para-metric mesh models [43,38,92,5] to estimate shape andarticulation for faces [74,62,66], hands[16], humans bod-ies [60,4,86,34,28,56,85,79], and animals [91,29,36,90] by directly inferring shape and articulation parameters.",
            "10": "To address the challenges, anotherline of work [57,53,75,12,8,55] employs neural networksto learn shapes from data.",
            "11": "[53]learned an implicit vector field and deformed shapes in aspatial-temporal space.",
            "12": "One recent popular paradigmof research [20,31,44,14,47,82] focuses on estimating6D poses of articulated objects.",
            "13": "However, 6D pose information may not be suffi-2\n13002\n cient for tasks that require detailed shape information, suchas robotic manipulation [32,83,50,49].",
            "14": "Previous work [37,22,7,24,33,30,23,52,67,6,88,87,68,61,42,2,59] has shown that dis-entangled representations are essential to learn meaningfullatent space for 2D image synthesis.",
            "15": "Learning on test in-stance has been recently applied for adapting a trainedmodel to out-of-distribution in multiple applications, in-cluding image recognition [25,51,73], super-resolution andsynthesis [70,69,3], and mesh reconstruction and genera-tion [39,27].",
            "16": "[39] exploits training intest-time with self-supervision for consistent mesh recon-struction in a single video.",
            "17": "Our model is based on DeepSDF [58].",
            "18": "Furthermore, compared to feed forward designs [46,64],the optimization-based shape modeling is naturally com-patible with Test-Time Adaptation to address the out-of-distribution data as described in Section3.",
            "19": ", eyeglasses)with both joints articulated to45\u0000is m= (45\u0000,45\u0000).",
            "20": "Similarly, the articulation code (joint angles) is3\n13003\n Inferred Shape CodesShapeEmbedding Inferred Shape CodesArticulated Signed Distance FunctionSampled PointSDF ValuesPart Labels (Optional)Shape Embedding NetArticulation NetInputOutput 90\u00b0 0\u00b045\u00b0 MLPArticulationEmbeddingMLP Shape CodeTraining InferenceGeneration Unseen Joint Angles20\u00b0Generated Shape Part Labels(Optional)Inferred Joint Angles75\u00b0Test Instance Fc layer Part Labels(Optional) Articulation CodeFigure 2: Overview of the proposed method.",
            "21": "(4)Following [58], we include a zero-mean multivariate-Gaussian prior per shape latent code\u0000to facilitate learn-ing a continuous shape manifold.",
            "22": "Following DeepSDF [58], 30,000 pointsare sampled for each shape for evaluation and Chamfer-L1distances shown in the paper are multiplied by 1,000.",
            "23": "Forjoint angle estimation, we follow [40] to evaluate the aver-age joint angle error in degrees for each joint.",
            "24": "Laptop Stapler Wash Door Oven Eyeglasses FridgeDeepSDF [58]0.",
            "25": "35 3.",
            "26": "61 5.",
            "27": "33 1.",
            "28": "39 2.",
            "29": "60 0.",
            "30": "55 2.",
            "31": "Laptop Stapler Wash Door Oven Eyeglasses FridgeDeepSDF [58]2.",
            "32": "33 1.",
            "33": "51 4.",
            "34": "c) Ours (w/o TTA)b) DeepSDF a) Inputs (39\u00b0)(51\u00b0)(39\u00b0)(51\u00b0)(0\u00b0)(90\u00b0)(-33\u00b0)(-21\u00b0)(-33\u00b0)(-21\u00b0)(-72\u00b0)(18\u00b0)Figure 4: Comparison for interpolation.",
            "35": "396.",
            "36": "However, the baselinemethod learns a non-structured shape space and the interpo-lation result could be a random point in a high dimensional6\n13006\n Laptop Stapler Washing Door Oven Eyeglasses FridgeDeepSDF [58](Interpolation)2.",
            "37": "33 1.",
            "38": "39 (1.",
            "39": "39) 3.",
            "40": "48 (2.",
            "41": "58) 0.",
            "42": "47)Ours0.",
            "43": "59) 3.",
            "44": "53)3.",
            "45": "44)0.",
            "46": "53 (0.",
            "47": "58 (6.",
            "48": "42 (2.",
            "49": "45) 3.",
            "50": "38 (1.",
            "51": "48) 2.",
            "52": "48 (3.",
            "53": "34) 1.",
            "54": "33 (1.",
            "55": "We quanti-LaptopDoorWashingRecon GenRecon GenRecon Gen2-viewDeepSDF [58]2.",
            "56": "40 4.",
            "57": "34 16.",
            "58": "38Ours(w/o TTA)0.",
            "59": "160.",
            "60": "62 0.",
            "61": "837.",
            "62": "61 1.",
            "63": "61 0.",
            "64": "391-viewDeepSDF [58]3.",
            "65": "755.",
            "66": "2414.",
            "67": "250.",
            "68": "58 11.",
            "69": "550.",
            "70": "386.",
            "71": "52Table 4: Chamfer-L1 distance comparison on partial point clouds.",
            "72": "As 3D meshes are not provided in this scenario, wesample two points for each depth observation followingDeepSDF [58].",
            "73": "DeepSDF Input DepthGeneration at unseen articulation Reconstruction-31\u00b0-45\u00b05\u00b0-31\u00b0-1\u00b0-1\u00b0-41\u00b0-12\u00b0-12\u00b0-41\u00b05\u00b0-45\u00b0-1\u00b0-1\u00b0-31\u00b0-31\u00b0ReconstructionOursCorresponding images (not input) Figure 8: Test on real-world depth images.",
            "74": "Reconstruction GenerationDeepSDF [58]4.",
            "75": "53 5.",
            "76": "RBO dataset [45] is a collection of 358 RGB-D video se-quences of humans manipulating articulated objects, withthe ground-truth poses of the rigid parts annotated by a mo-tion capture system.",
            "77": "We also go beyond syntheticdata and demonstrate the proposed method can reliably gen-erate 3D shapes from real-world depth images in the rbodataset [45].",
            "78": "This work was supported, in part, by grants fromDARPA LwLL, iARPA (DIV A) D17PC00342, NSF IIS-1924937, NSF1730158 CI-New: Cognitive Hardware and Software Ecosystem Com-munity Infrastructure (CHASE-CI), NSF ACI-1541349 CC*DNI PacificResearch Platform, and gifts from Qualcomm, TuSimple and Picsart.",
            "79": "InCVPR, pages 7488\u20137497, 2020.",
            "80": ", 38(4), 2019.",
            "81": "InICCV, pages 5419\u20135429, 2019.",
            "82": "InEuropean conference on computer vision,pages 561\u2013578.",
            "83": "InICCV, pages 5932\u20135941,2019.",
            "84": "03953, 2021.",
            "85": "InCVPR, pages 42\u201351, 2020.",
            "86": "InCVPR, pages 5939\u20135948,2019.",
            "87": "InCVPR, pages31\u201341, 2020.",
            "88": "InECCV, pages 612\u2013628, 2020.",
            "89": "InNeurIPS, pages 7433\u20137443, 2019.",
            "90": "InCVPR, pages 2463\u20132471, 2017.",
            "91": "InCVPR, pages10833\u201310842, 2019.",
            "92": "06126, 2019.",
            "93": "InCVPR, pages 4856\u20134865, 2020.",
            "94": "InICRA, pages 3305\u20133312,2015.",
            "95": "InCVPR, pages577\u2013584, 2011.",
            "96": "InCVPR, pages6000\u20136009, 2020.",
            "97": "03304, 2021.",
            "98": "InComputer Graphics Forum, volume 35, pages365\u2013374.",
            "99": "InCVPR, pages 4401\u20134410, 2019.",
            "100": "29\n13009\n\n[32]D.",
            "101": "3[33]Hyunjik Kim and Andriy Mnih.",
            "102": "3[34]Muhammed Kocabas, Nikos Athanasiou, and Michael J.",
            "103": "InCVPR, pages 5252\u20135262, 2020.",
            "104": "2[35]Amit P.",
            "105": "2[36]Nilesh Kulkarni, Abhinav Gupta, David F Fouhey, and Shub-ham Tulsiani.",
            "106": "InCVPR, pages 452\u2013461, 2020.",
            "107": "2[37]Tejas D Kulkarni, William F Whitney, Pushmeet Kohli, andJosh Tenenbaum.",
            "108": "InNeurIPS, pages 2539\u20132547, 2015.",
            "109": "3[38]Tianye Li, Timo Bolkart, Michael J.",
            "110": ", 36(6):194:1\u2013194:17, 2017.",
            "111": "2[39]Xueting Li, Sifei Liu, Shalini De Mello, Kihwan Kim, Xi-aolong Wang, Ming-Hsuan Yang, and Jan Kautz.",
            "112": "3[40]Xiaolong Li, He Wang, Li Yi, Leonidas J.",
            "113": "InCVPR, pages 3703\u20133712, 2020.",
            "114": "1,5[41]Yiyi Liao, Simon Donn\u00b4e, and Andreas Geiger.",
            "115": "2[42]Andrew Liu, Shiry Ginosar, Tinghui Zhou, Alexei A.",
            "116": "3[43]Matthew Loper, Naureen Mahmood, Javier Romero, GerardPons-Moll, and Michael J.",
            "117": ", 34(6):248:1\u2013248:16, 2015.",
            "118": "2[44]Roberto Martin Martin, Sebastian H\u00a8ofer, and Oliver Brock.",
            "119": "InICRA, pages 5091\u20135097, 2016.",
            "120": "2[45]Roberto Mart\u00b4\u0131n-Mart\u00b4\u0131n, Clemens Eppner, and Oliver Brock.",
            "121": ", 38(9), 2019.",
            "122": "2,8[46]Lars M.",
            "123": "InCVPR, pages 4460\u20134470, 2019.",
            "124": "2,3[47]Frank Michel, Alexander Krull, Eric Brachmann,Michael Ying Yang, Stefan Gumhold, and Carsten Rother.",
            "125": "2[48]Ben Mildenhall, Pratul P.",
            "126": "InECCV, pages 405\u2013421, 2020.",
            "127": "2[49]Mayank Mittal, David Hoeller, Farbod Farshidian, MarcoHutter, and Animesh Garg.",
            "128": "10534, 2021.",
            "129": "3[50]Kaichun Mo, Leonidas Guibas, Mustafa Mukadam, Abhi-nav Gupta, and Shubham Tulsiani.",
            "130": "3[51]Ravi Teja Mullapudi, Steven Chen, Keyi Zhang, Deva Ra-manan, and Kayvon Fatahalian.",
            "131": "3[52]Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, ChristianRichardt, and Yong-Liang Yang.",
            "132": "InICCV,pages 7587\u20137596, 2019.",
            "133": "3[53]Michael Niemeyer, Lars M.",
            "134": "InICCV, pages 5378\u20135388,2019.",
            "135": "2[54]Michael Niemeyer, Lars M.",
            "136": "InCVPR, pages 3501\u20133512, 2020.",
            "137": "2[55]Atsuhiro Noguchi, Xiao Sun, Stephen Lin, and TatsuyaHarada.",
            "138": "2[56]Mohamed Omran, Christoph Lassner, Gerard Pons-Moll, Pe-ter V .",
            "139": "In3DV, pages 484\u2013494, 2018.",
            "140": "2[57]Pablo R.",
            "141": "2[58]Jeong Joon Park, Peter Florence, Julian Straub, Richard A.",
            "142": "1,2,3,4,5,6,7,8[59]Taesung Park, Jun-Yan Zhu, Oliver Wang, Jingwan Lu, EliShechtman, Alexei Efros, and Richard Zhang.",
            "143": "NeurIPS, 33, 2020.",
            "144": "15838, 2021.",
            "145": "2[61]Stanislav Pidhorskyi, Donald A Adjeroh, and GianfrancoDoretto.",
            "146": "InCVPR, pages14104\u201314113, 2020.",
            "147": "3[62]Anurag Ranjan, Timo Bolkart, Soubhik Sanyal, andMichael J.",
            "148": "InECCV, pages 725\u2013741, 2018.",
            "149": "2[63]Gernot Riegler, Ali Osman Ulusoy, and Andreas Geiger.",
            "150": "InCVPR, pages 6620\u20136629, 2017.",
            "151": "02442, 2020.",
            "152": "InCVPR, pages 9243\u20139252, 2020.",
            "153": "09661, 2020.",
            "154": "InICML, pages 9229\u20139248, 2020.",
            "155": "InCVPR,pages 1493\u20131502, 2017.",
            "156": "04595, 2020.",
            "157": "InCVPR, pages 1466\u20131474, 2017.",
            "158": "InECCV,pages 20\u201338, 2018.",
            "159": "InCVPR, pages5830\u20135838, 2020.",
            "160": "InECCV, pages 55\u201371,2018.",
            "161": "03437, 2021.",
            "162": "InNeurIPS, pages 490\u2013500, 2019.",
            "163": "InECCV, volume 12357 ofLecture Notes in Com-puter Science, pages 34\u201351, 2020.",
            "164": "InICCV, pages 7738\u20137748, 2019.",
            "165": "InECCV, volume 12367 ofLecture Notes in Com-puter Science, pages 341\u2013357, 2020.",
            "166": "InICCV, pages 5359\u20135368, 2019.",
            "167": "InCVPR, pages 3955\u20133963, 2018.",
            "168": "InCVPR, pages 5524\u20135532, 2017."
        },
        "Relighting4d: Neural relightable human from videos": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2207.07104",
            "ref_texts": "34. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. arXiv:2012.15838 [cs] (Mar 2021), http:// arxiv.org/abs/2012.15838 2, 3, 4, 5, 9, 10, 11, 12",
            "ref_ids": [
                "34"
            ],
            "1": "It has been proved [5,6,19,24,28,30,32,34,35,41,42,54,58,63] that a scene can be represented as neural fields to enable novel view synthesis and relighting.",
            "2": "2 Related Work Neural scene representation [14,18,20,28,34,35,36,40,41,43,46,49,56] has witnessed significant progress in representing a 3D scene with deep neural networks.",
            "3": "To model dynamic humans, Neural Body [34] proposes to attach a set of latent codes to a deformable human body model (i.",
            "4": "Inspired by the local implicit representations [10, 34], we introduce a 4D neural field \u03c8conditioned on a parametric human model (SMPL [23] or SMPLX [33]) to represent a dynamic human performer, which maps the position xand time step tto the latent feature \u03c8t(x).",
            "5": "NeuralBody [34] employs a similar strategy on human representations.",
            "6": "2 Physically Based Rendering While differentiable volume rendering has been used in recent works [28,34,56], these methods focus on novel view synthesis with radiance fields.",
            "7": "Note that previous work [28,34] implicitly encodes R(\u00b7) in the radiance fields without modeling the reflectance.",
            "8": "Two variants of NeuralBody [34] (NB+A and NB+LE) fail to incorporate the lighting in a physical way, thus are unable to reasonably relight the human actor.",
            "9": "We validate our method on the People-Snapshot [1] dataset and ZJU-Mocap [34] dataset qualitatively.",
            "10": "And ZJU-Mocap [34] captures dynamic humans with complex motions using a multi-camera system.",
            "11": "Moreover, to demonstrate the importance of physically based rendering, we implement two variants on top of NeuralBody (NB) [34] which succeeds in novel view synthesis of dynamic humans but fails to incorporate lighting and reflectance.",
            "12": "Two variants of NeuralBody [34], NB+A and NB+LE, are good at reconstructing appearance but fail to incorporate novel illuminations in a perceptually salient way.",
            "13": "We demonstrate that our method is capable of relighting dynamic humans with complex motions from multi-views videos on the ZJU-Mocap dataset [34].",
            "14": "MethodRelighting Normal Map Diffuse Map PSNR \u2191SSIM\u2191LPIPS \u2193 Degree\u25e6\u2193 PSNR \u2191SSIM\u2191LPIPS \u2193 NB [34]+A 20.",
            "15": "2368 NB [34]+LE 22."
        },
        "Doublefield: Bridging the neural surface and radiance fields for high-fidelity human reconstruction and rendering": {
            "authors": [
                "Ruizhi Shao",
                "Hongwen Zhang",
                "He Zhang",
                "Mingjia Chen",
                "Pei Cao",
                "Tao Yu",
                "Yebin Liu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Shao_DoubleField_Bridging_the_Neural_Surface_and_Radiance_Fields_for_High-Fidelity_CVPR_2022_paper.pdf",
            "ref_texts": "[37] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 1, 2, 3, 6, 7, 8",
            "ref_ids": [
                "37"
            ],
            "1": "Introduction The surface fields [2, 31, 35] and the radiance fields [32, 63] have recently emerged as promising solutions for geometry modeling [12, 39, 40, 66] and texture rendering [37, 59, 65] of 3D human in an implicit and continuous manner, respectively.",
            "2": "Moreover, the radiance fields [21, 32, 36, 37, 44] entangle the learning of geometry and appearance in an implicit manner without effective mutual constraints, leading to inconsistent geometry reconstruction and relatively low training efficiency.",
            "3": "Even with high-resolution images as input, the limited representation power of features (feature map or feature volume) [37, 40] as well as the calibration and the geometry inference errors (especially for real captured data) will significantly deteriorate the detail reconstruction performance due to multi-view inconsistency for current implicit field based methods [37, 39, 64].",
            "4": "In comparison with existing approaches [37,39,64] built upon surface and radiance fields, DoubleField not only improves the reconstruction quality of both geometry and appearance but also has the capability to eliminate the prerequisite SMPL fitting in previous methods [37] and even handle loose clothing (e.",
            "5": "[37] propose to learn a neural radiance field with the guidance of a predefined template (i.",
            "6": "In addition, the highly flexible nature of the vanilla NeRF makes the training, and finetuning of its derivative solutions [37, 59] time-consuming.",
            "7": "Note that NeuralBody [37] can not handle additional objects which are far away from the human body like handbag.",
            "8": "including PIFu [39], PixelNeRF [59], NeuralBody [37], and PIFuHD [40].",
            "9": "605 NeuralBody [37] 1.",
            "10": "880 NeuralBody [37] 20.",
            "11": "For the comparison with NeuralBody [37], we regard NeuralBody as a frame-based method and train it on 6 viewpoint inputs for 15 hours.",
            "12": "5 compares the qualitative geometry reconstruction results of NeuralBody [37], PIFuHD [40], and our method.",
            "13": "Note that our method is finetuned with the multi-view images at one frame, while NeuralBody [37] is trained with the whole mutli-view video sequence as it fails in the geometry reconstruction when only one frame is given.",
            "14": "5, unlike NeuralBody [37], the surface reconstructed by our method is more consistent and contains more details.",
            "15": "We further evaluate the rendering quality on the ZJUmocap dataset [37] and our multi-view system.",
            "16": "Moreover, our method does not rely on human shape prior SMPL [28] compared with NeuralBody [37] and achieves photo-realistic rendering even under challenging scenarios like swinging skirt, topological changes and loose cloth, which demonstrates the strong generalization capacity of our method to real world data."
        },
        "Unsupervised learning of efficient geometry-aware neural articulated representations": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2204.08839",
            "ref_texts": "57. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural Body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: CVPR (2021)",
            "ref_ids": [
                "57"
            ],
            "1": "We demonstrate this approach by modeling the pose prior as a skeletal distribution [51,63], while noting that other models like meshes [57,56] may bring potential performance benefits.",
            "2": "They have achieved the state-of-the art in learning 3D shape [12,43,53], static [62,44,4] and dynamic scenes [58,37,54], articulated objects [57,14,7,65,11,56,51,63,2,69,39], and image synthesis [60,10].",
            "3": "Recently, articulated representations based on NeRF have been proposed [57,51,63,56,69,70,39].",
            "4": "The ZJU mocap dataset [57] consisting of three subjects (313, 315, 386) is used for training."
        },
        "Ulnef: Untangled layered neural fields for mix-and-match virtual try-on": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/4ee3ac2cd119023c79b0d21c4a464dc7-Paper-Conference.pdf",
            "ref_texts": "[51] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proc. of Computer Vision and Pattern Recognition (CVPR) , pages 9054\u20139063, 2021.",
            "ref_ids": [
                "51"
            ],
            "1": "NeuralBody [51] appends learnable features to the vertices of a surface body model, enabling free-viewpoint rendering of animatable humans."
        },
        "Mobrecon: Mobile-friendly hand mesh reconstruction from monocular image": {
            "authors": [
                "Xingyu Chen",
                "Yufeng Liu",
                "Yajiao Dong",
                "Xiong Zhang",
                "Chongyang Ma",
                "Yanmin Xiong",
                "Yuan Zhang",
                "Xiaoyan Guo"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Chen_MobRecon_Mobile-Friendly_Hand_Mesh_Reconstruction_From_Monocular_Image_CVPR_2022_paper.pdf",
            "ref_texts": "[57] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 2",
            "ref_ids": [
                "57"
            ],
            "1": "The implicit function [50] has merits of continuity and high resolution, which is recently used for digitizing articulated human [51, 34, 3, 59, 32, 57, 38, 39]."
        },
        "Multi-view consistent generative adversarial networks for 3d-aware image synthesis": {
            "authors": [
                "Xuanmeng Zhang",
                "Zhedong Zheng",
                "Daiheng Gao",
                "Bang Zhang",
                "Pan Pan",
                "Yi Yang"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Multi-View_Consistent_Generative_Adversarial_Networks_for_3D-Aware_Image_Synthesis_CVPR_2022_paper.pdf",
            "ref_texts": "[40] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 3",
            "ref_ids": [
                "40"
            ],
            "1": ", faster inference [17, 31, 42, 42, 43], pose estimation [23, 30, 33, 52, 57], generalization [5,7,45,49,58], video [16,27,28,40,54], and depth estimation [53]."
        },
        "PINA: Learning a personalized implicit neural avatar from a single RGB-D video sequence": {
            "authors": [
                "Zijian Dong",
                "Chen Guo",
                "Jie Song",
                "Xu Chen",
                "Andreas Geiger",
                "Otmar Hilliges"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Dong_PINA_Learning_a_Personalized_Implicit_Neural_Avatar_From_a_Single_CVPR_2022_paper.pdf",
            "ref_texts": "[48] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021. 2",
            "ref_ids": [
                "48"
            ],
            "1": "Implicit Human Models from 3D Scans Implicit neural representations [14, 37, 43] can handle topological changes better [10,44] and have been used to reconstruct clothed human shapes [12,26,27,30,48,49,51,52,59]."
        },
        "Learning implicit templates for point-based clothed human modeling": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2207.06955",
            "ref_texts": "50. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: CVPR (2021)",
            "ref_ids": [
                "50"
            ],
            "1": "Recently, based on neural radiance fields (NeRF) [43], attempts have been made to bypass the underlying geometry and synthesize rendered images of clothed humans directly [49,50,62,67]."
        },
        "Deepmulticap: Performance capture of multiple characters using sparse multiview cameras": {
            "authors": [
                "Yang Zheng",
                "Ruizhi Shao",
                "Yuxiang Zhang",
                "Tao Yu",
                "Zerong Zheng",
                "Qionghai Dai",
                "Yebin Liu"
            ],
            "url": "http://openaccess.thecvf.com/content/ICCV2021/papers/Zheng_DeepMultiCap_Performance_Capture_of_Multiple_Characters_Using_Sparse_Multiview_Cameras_ICCV_2021_paper.pdf",
            "ref_texts": "[44] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 6",
            "ref_ids": [
                "44"
            ],
            "1": "Performance on Real World Data We evaluate our method on ZJU-MoCap dataset [44], a multi-view real world dataset, with comparison to DeepVisualHull [25], a volumetric performance capture from sparse multi-view, Neural Body [44], a differentiable rendering method directly Figure 5: Performance on ZJU-Mocap dataset [44].",
            "2": "Our method outperforms state-of-the-art approaches including DeepVisualHull [25], PIFuHD [48] and Neural Body [44]."
        },
        "Avatargen: a 3d generative model for animatable human avatars": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2211.14589",
            "ref_texts": "[51] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 3",
            "ref_ids": [
                "51"
            ],
            "1": "Some methods augment NeRF with human body priors to enable 3D human reconstruction from sparse multi-view data [5, 51, 57, 64]."
        },
        "Dressing avatars: Deep photorealistic appearance for physically simulated clothing": {
            "authors": [
                "Donglai Xiang"
            ],
            "url": "https://dl.acm.org/doi/pdf/10.1145/3550454.3555456",
            "ref_texts": "(2007), 109\u2013118. Atsuhiro Noguchi, Xiao Sun, Stephen Lin, and Tatsuya Harada. 2021. Neural articulated radiance field. In Proceedings of the IEEE/CVF International Conference on Computer Vision . Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. 2017. Colored point cloud registration revisited. In Proceedings of the IEEE International Conference on Computer Vision . Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al .2019. Pytorch: An imperative style, high-performance deep learning library. Advances in Neural Information Processing Systems 32 (2019). Chaitanya Patel, Zhouyingcheng Liao, and Gerard Pons-Moll. 2020. Tailornet: Predicting clothing in 3d as a function of human pose, shape and garment style. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021a. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision . Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021b. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter Battaglia. 2021. Learning Mesh-Based Simulation with Graph Networks. In International Conferenceon Learning Representations . Gerard Pons-Moll, Sergi Pujades, Sonny Hu, and Michael J Black. 2017. ClothCap: Seamless 4D clothing capture and retargeting. ACM Transactions on Graphics (ToG)"
        },
        "Lightweight multi-person total motion capture using sparse multi-view cameras": {
            "authors": [
                "Yuxiang Zhang",
                "Zhe Li",
                "Liang An",
                "Mengcheng Li",
                "Tao Yu",
                "Yebin Liu"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Lightweight_Multi-Person_Total_Motion_Capture_Using_Sparse_Multi-View_Cameras_ICCV_2021_paper.pdf",
            "ref_texts": "[42] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , pages 9054\u20139063, 2021. 2",
            "ref_ids": [
                "42"
            ],
            "1": "Total Motion Capture Total motion capture methods, which aim at markerless multi-scale human behaviour capture (including body motion, facial expressions and hand gestures), have shown great potentials in human 4D reconstruction and highfidelity neural rendering [42, 49, 28, 63]."
        },
        "Nesf: Neural semantic fields for generalizable semantic segmentation of 3d scenes": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2111.13260",
            "ref_texts": ""
        },
        "Learning neural volumetric representations of dynamic humans in minutes": {
            "authors": [
                "Chen Geng",
                "Sida Peng",
                "Zhen Xu",
                "Hujun Bao",
                "Xiaowei Zhou"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Geng_Learning_Neural_Volumetric_Representations_of_Dynamic_Humans_in_Minutes_CVPR_2023_paper.pdf",
            "ref_texts": "[58] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021. 1, 2, 5, 6, 7",
            "ref_ids": [
                "58"
            ],
            "1": "Given a monocular video of a human performer, our model can be learned in \u223c5 minutes to produce photorealistic novel view rendering, which is 100 times faster than Neural Body [58].",
            "2": "Recently, some methods [58, 93] have shown that high-quality volumetric videos can be recovered from sparse multi-view videos by representing dynamic humans with neural scene representations.",
            "3": "Another line of works [28, 30, 34, 36, 40, 56, 58, 65, 92, 93, 95, 101 \u2013103, 105] exploits dynamic implicit neural representations and differentiable renderers to reconstruct 3D human models from 8760\n\n(b) Part-based voxelized human representation (a) Motion parameterization on 2D surface domainCanonical spaceDeformation space Lower resolutionHigher resolution 3D parameterization 4D space-time motionTime Time Query pointBody pose Point Density and colorFigure 2.",
            "4": "Our method is implemented purely with the PyTorch framework [53] to demonstrate the effectiveness of our representation It also enables us to fairly compare with baseline methods [34, 56, 58] implemented in PyTorch.",
            "5": "Datasets ZJU-MoCap [58] dataset is a widely-used benchmark for human modeling from videos.",
            "6": "Neural Body (NB) [58] anchors a set of latent codes to the SMPL mesh and regresses the radiance field from the posed latent codes.",
            "7": "Table 1 compares our method with NB [58], AN [56], PixelNeRF [100], NHP [34], HN [93] and AS [57] on novel view synthesis.",
            "8": "[57, 93] exhibit better results than [56, 58].",
            "9": "Although [56,58] have shown impressive rendering results given 4-view videos, they do not perform well on monocular inputs.",
            "10": "[58] implicitly aggregates the temporal information using structured latent 8764\n ZJU-MoCap MonoCapTraining TimePSNR \u2191SSIM\u2191LPIPS\u2217\u2193PSNR \u2191SSIM\u2191LPIPS\u2217\u2193 Ours \u02dc5 min 31.",
            "11": "47 NB [58] \u02dc10 h 29."
        },
        "Watch it move: Unsupervised discovery of 3D joints for re-posing of articulated objects": {
            "authors": [
                "Atsuhiro Noguchi",
                "Umar Iqbal",
                "Jonathan Tremblay",
                "Tatsuya Harada",
                "Orazio Gallo"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Noguchi_Watch_It_Move_Unsupervised_Discovery_of_3D_Joints_for_Re-Posing_CVPR_2022_paper.pdf",
            "ref_texts": "[41] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2021. 1, 2, 6, 7",
            "ref_ids": [
                "41"
            ],
            "1": "However, annotations are expensive and object-specific, which is why they are only available for limited classes of objects, such as people or faces [15, 41, 47].",
            "2": "They allow novel view and pose synthesis, but require ground truth poses [8,36,49], or dense 3D meshes [24, 30, 40, 41, 53] annotations for the training image.",
            "3": "Credits: human [41], dog [21].",
            "4": "We use the ZJU-MoCap dataset [41] for our experiments.",
            "5": "Since the ZJU-MoCap dataset [41] has ground truth SMPL annotations, we can use this mapping to re-pose our model to target frames not observed in training, as shown in Figure 7."
        },
        "Tensor4d: Efficient neural 4d decomposition for high-fidelity dynamic reconstruction and rendering": {
            "authors": [
                "Ruizhi Shao",
                "Zerong Zheng",
                "Hanzhang Tu",
                "Boning Liu",
                "Hongwen Zhang",
                "Yebin Liu"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Shao_Tensor4D_Efficient_Neural_4D_Decomposition_for_High-Fidelity_Dynamic_Reconstruction_and_CVPR_2023_paper.pdf",
            "ref_texts": "[39] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 2[40] Albert Pumarola, Enric Corona, and and Francesc MorenoNoguer Gerard Pons-Moll. D-nerf: Neural radiance fields for dynamic scenes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 10318\u2013",
            "ref_ids": [
                "39",
                "40"
            ],
            "1": ", D-NeRF) disentangle a dynamic scene into a canonical radiance field and a dynamic motion field [11,28,37,40,55].",
            "2": "D-Nerf [40] and NR-Nerf [55] follow a similar framework, but take only monocular videos as training data.",
            "3": "Using parametric body templates as the semantic prior, methods like Neural Body [39] and HumanNeRF [60] enable photo-realistic novel view synthesis of complex human performance.",
            "4": "To achieve better disentanglement of shape and motion, some methods like D-NeRF [40] propose deformable neural radiance field which adopts a canonical 3D representation with the 4D flow fields: f(x, y, z, t ) = (\u02c6x,\u02c6y,\u02c6z), g(\u02c6x,\u02c6y,\u02c6z, \u03b8, \u03d5 ) = (c, \u03c3),(2) where gis the radiance field in canonical configuration and fis a scene flow field representing the mapping between the scene at time instant tand the canonical space.",
            "5": "For monocular evaluation, we use the synthetic dataset provided by D-NeRF [40] and select 3 scenes (\u201clego\u201d, \u201cstandup\u201d and \u201cjumpingjacks\u201d) from this dataset, with the numbers of training frames ranging from 50 to 200.",
            "6": "We mainly compare our method against the following state-of-the-art baselines that are most related to our work: D-NeRF [40], NeRF-T, TiNeuV ox [12] and NeuS-T.",
            "7": "Comparison on monocular synthetic dataset against D-NeRF [40] and TiNeuV ox [12].",
            "8": "Comparison on sparse-view real-world dataset against D-Nerf [40], TiNeuV ox [12] and Neus-T [59].",
            "9": "MethodLego Standup Jumpingjacks MSE \u2193 PSNR \u2191 SSIM \u2191 LPIPS \u2193 MSE \u2193 PSNR \u2191 SSIM \u2191 LPIPS \u2193 MSE \u2193 PSNR \u2191 SSIM \u2191 LPIPS \u2193 D-NeRF [40] 7.",
            "10": "MethodSequence1-thz Earphone Sequence3-yxd MSE \u2193 PSNR \u2191 SSIM \u2191 LPIPS \u2193 MSE \u2193 PSNR \u2191 SSIM \u2191 LPIPS \u2193 MSE \u2193 PSNR \u2191 SSIM \u2191 LPIPS \u2193 D-NeRF [40] 3.",
            "11": "2[40] Albert Pumarola, Enric Corona, and and Francesc MorenoNoguer Gerard Pons-Moll."
        },
        "Nerfplayer: A streamable dynamic scene representation with decomposed neural radiance fields": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2210.15947",
            "ref_texts": "[62] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021. 2",
            "ref_ids": [
                "62"
            ],
            "1": "The scene representation in NeRF inspired a number of works focusing on 3D modeling, such as human face and body capture [24, 42,55, 61,62, 72], relighting [4,5, 71] and 3D content generation [9, 10, 22, 25, 31, 69, 78]."
        },
        "Recovering 3d human mesh from monocular images: A survey": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2203.01923",
            "ref_texts": "[35] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural Body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in CVPR , 2021, pp. 9054\u20139063.",
            "ref_ids": [
                "35"
            ],
            "1": "The recovery of human body meshes plays a key role in facilitating the downstream tasks such as clothed human reconstruction [28], [29], [30], [31], [32], [33], [34], rendering [35], [36], and avatar modeling [37], [38], [39], \u2022Yating Tian and Limin Wang are with the Department of Computer Science and Technology, Nanjing University, Nanjing 210023, China.",
            "2": "We also do not cover work on neural rendering [35], [46] that focuses on the appearance modeling instead of geometry.",
            "3": "3M 1 772 1 SMPL [293] ZJU-MoCap [35] 1 9 1 SMPL-X [294] Datasets with Pseudo3D LabelsLSP [295] 2,000 1\u2713 SMPL [21], [42], [44] LSP-Extended [296] 10,000 1 \u2713 SMPL [21], [44] MSCOCO [297] 38K \u22651\u2713 SMPL [21], [44] MPII [298] 24,920 3,913 >40k \u22651\u2713 SMPL [21], [42], [44] UP-3D [42] 8,515 1\u2713 SMPL [21], [44] PoseTrack [299] 66,374 550 550 >1\u2713 SMPL [21] SSP-3D [116] 311 62 62 1\u2713 SMPL [116] OCHuman [300] 4,731 8110 >1\u2713 SMPL [21] MTP [191] 3,731 148 1\u2713 SMPL-X [191] Ubody [243] >1,050K \u22651\u2713 SMPL-X [243] AGORA .",
            "4": "ZJU-MoCap [35] consists of 9 dynamic human sequences captured by 21 synchronized cameras in a multi-view setup.",
            "5": "[35] S."
        },
        "Instantavatar: Learning avatars from monocular video in 60 seconds": {
            "authors": [
                "Tianjian Jiang",
                "Xu Chen",
                "Jie Song",
                "Otmar Hilliges"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_InstantAvatar_Learning_Avatars_From_Monocular_Video_in_60_Seconds_CVPR_2023_paper.pdf",
            "ref_texts": "[49] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , 2021. 1, 2, 5, 7",
            "ref_ids": [
                "49"
            ],
            "1": "The emergence of powerful neural fields has enabled a number of methods for the reconstruction of animatable avatars from monocular videos of moving humans [1, 2, 6, 49, 62].",
            "2": "Recently, neural representations [37, 41, 45, 46] have emerged as a powerful tool to model 3D humans [3,6,8,10, 11, 13, 14, 22\u201326, 30, 31, 34, 38, 39, 39, 43, 44, 48, 49, 52, 53, 57, 59\u201363, 63, 64, 67, 69, 70].",
            "3": "Using neural representations, many works [6, 18, 26, 27, 30, 34, 43, 48, 49, 61, 62, 64, 69] can directly reconstruct high fidelity neural human avatars from a sparse set of views or a monocular video without prescanning personalized template.",
            "4": "16925\n male-3-casual male-4-casual female-3-casual female-4-casual PSNR\u2191SSIM\u2191LPIPS\u2193PSNR\u2191SSIM\u2191LPIPS\u2193PSNR\u2191SSIM\u2191LPIPS\u2193PSNR\u2191SSIM\u2191LPIPS\u2193 Neural Body [49] (\u223c14 hours) 24.",
            "5": "We report PSNR, SSIM and LPIPS [68] between real images and the images generated by our method and two SoTA methods, Neural Body [49] and Anim-NeRF [6].",
            "6": "Neural Body [49] This baseline learns a set of latent codes anchored to a deformable SMPL mesh.",
            "7": "When training all methods to convergence, our generated images are significantly better than Neural Body [49] and achieve on-par quality as SoTA method Anim-NeRF [6], as indicated by the image quality metrics in Tab.",
            "8": "We only require 1 minute to train on a single RTX 3090 while AnimNeRF [6] requires 13 hours on 2\u00d7RTX 3090 and Neural Body [49] requires 14 hours on 4\u00d7RTX 2080."
        },
        "High-Fidelity Clothed Avatar Reconstruction from a Single Image": {
            "authors": [
                "Tingting Liao",
                "Xiaomei Zhang",
                "Yuliang Xiu",
                "Hongwei Yi",
                "Xudong Liu",
                "Jun Qi",
                "Yong Zhang",
                "Xuan Wang",
                "Xiangyu Zhu",
                "Zhen Lei"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Liao_High-Fidelity_Clothed_Avatar_Reconstruction_From_a_Single_Image_CVPR_2023_paper.pdf",
            "ref_texts": "[33] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021. 3",
            "ref_ids": [
                "33"
            ],
            "1": "NeRFbased methods [33, 52, 57] optimize a goal using conditions on articulated cues."
        },
        "A-nerf: Articulated neural radiance fields for learning human shape, appearance, and pose": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper_files/paper/2021/file/65fc9fb4897a89789352e211ca2d398f-Paper.pdf",
            "ref_texts": "[46] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. InCVPR , 2021.",
            "ref_ids": [
                "46"
            ],
            "1": "Even more similar is the recent NeuralBody [46] representation, which combines a NeRF with a surface body model and underlying skeleton.",
            "2": "Following concurrent works [33,46], we also optimize an appearance code for each image to handle dynamic light effects.",
            "3": "While NeuralBody [46] can also learn photo-realistic human models from monocular images, their model anchors its representation on the SMPL 3D\n7 Source NeuralBody Ours Source NeuralBody Ours Source NeuralBody Ours Figure 5: Motion retargeting and animation.",
            "4": "[46] S."
        },
        "Learning 3d-aware image synthesis with unknown pose distribution": {
            "authors": [
                "Zifan Shi",
                "Yujun Shen",
                "Yinghao Xu",
                "Sida Peng",
                "Yiyi Liao",
                "Sheng Guo",
                "Qifeng Chen",
                "Yan Yeung"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Shi_Learning_3D-Aware_Image_Synthesis_With_Unknown_Pose_Distribution_CVPR_2023_paper.pdf",
            "ref_texts": "[25] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE Conf. Comput. Vis. Pattern Recog. , pages 9054\u20139063, 2021. 1",
            "ref_ids": [
                "25"
            ],
            "1": "Compared with 2D synthesis, 3D-aware image synthesis requires the understanding of the geometry underlying 2D images, which is commonly achieved by incorporating 3D representations, such as neural radiance fields (NeRF) [2, 16, 17, 24, 25, 50], into generative models like generative adversarial networks (GANs) [8]."
        },
        "Self-supervised neural articulated shape and appearance models": {
            "authors": [
                "Fangyin Wei",
                "Rohan Chabra",
                "Lingni Ma",
                "Christoph Lassner",
                "Michael Zollhofer",
                "Szymon Rusinkiewicz",
                "Chris Sweeney",
                "Richard Newcombe",
                "Mira Slavcheva"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Wei_Self-Supervised_Neural_Articulated_Shape_and_Appearance_Models_CVPR_2022_paper.pdf",
            "ref_texts": "[46] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2021. 3",
            "ref_ids": [
                "46"
            ],
            "1": "There are several works that build on top of neural radiance fields to capture scenes in motion [3,13,24, 43, 44, 46, 48, 63]."
        },
        "Dynibar: Neural dynamic image-based rendering": {
            "authors": [
                "Zhengqi Li",
                "Qianqian Wang",
                "Forrester Cole",
                "Richard Tucker",
                "Noah Snavely"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Li_DynIBaR_Neural_Dynamic_Image-Based_Rendering_CVPR_2023_paper.pdf"
        },
        "Being comes from not-being: Open-vocabulary text-to-motion generation with wordless training": {
            "authors": [
                "Junfan Lin",
                "Jianlong Chang",
                "Lingbo Liu",
                "Guanbin Li",
                "Liang Lin",
                "Qi Tian",
                "Wen Chen"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Being_Comes_From_Not-Being_Open-Vocabulary_Text-to-Motion_Generation_With_Wordless_Training_CVPR_2023_paper.pdf",
            "ref_texts": "[29] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021. 3",
            "ref_ids": [
                "29"
            ],
            "1": "This powerful representation ability of foundation model has led to 23223\n the emergence of zero-shot text-driven applications [9, 14, 26,29], including 3D meshes generation [16,17,23,28,39]."
        },
        "Imface: A nonlinear 3d morphable face model with implicit neural representations": {
            "authors": [
                "Mingwu Zheng",
                "Hongyu Yang",
                "Di Huang",
                "Liming Chen"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_ImFace_A_Nonlinear_3D_Morphable_Face_Model_With_Implicit_Neural_CVPR_2022_paper.pdf",
            "ref_texts": "[43] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 2",
            "ref_ids": [
                "43"
            ],
            "1": "In addition, current studies on INRs mostly focused on the watertight input, such as the ones in ShapeNet [60], and a number of methods on watertight human heads or bodies were proposed accordingly [3,14,19,43,45,47,48,59]."
        },
        "Animatable neural radiance fields from monocular rgb videos": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2106.13629",
            "ref_texts": "[30] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in CVPR , 2021. 2, 5, 6, 8",
            "ref_ids": [
                "30"
            ],
            "1": "These works [29], [30], [31], [32], [33], [34] combine scene representation network with parametric models [35], [13] to reconstruct dynamic humans.",
            "2": "Similar ideas with us have been used in recent works[30], [36], [37], but these methods usually require multi-view images or accurate registered SMPL.",
            "3": "Compared with NeuralBody[30] and SMPLpix[47], our approach can produce realistic images with well preserved identity and cloth details.",
            "4": "We also compare the proposed method with several state-of-the-art (SOTA) methods, including NeuralBody[30](NB) and SMPLpix[47].",
            "5": "Comparisons between NeuralBody[30] (first row) and Ours (second row) on novel pose synthesis task.",
            "6": "NeuralBody[30] is the most similar work to ours in the sense that it also combines NeRF with SMPL.",
            "7": "TABLE III QUANTITATIVE COMPARISON ABOUT NOVEL POSE SYNTHESIS WITH NEURAL BODY(NB)[30] ON THE I PER DATASET .",
            "8": "2\n[30] S."
        },
        "Humangen: Generating human radiance fields with explicit priors": {
            "authors": [
                "Suyi Jiang",
                "Haoran Jiang",
                "Ziyu Wang",
                "Haimin Luo",
                "Wenzheng Chen",
                "Lan Xu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_HumanGen_Generating_Human_Radiance_Fields_With_Explicit_Priors_CVPR_2023_paper.pdf",
            "ref_texts": "[57] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 3",
            "ref_ids": [
                "57"
            ],
            "1": "Embracing the developing of NeRF techniques [8, 9,40\u201342, 44,45,48,69,71,73,82,86], the human shape prior augmented NeRFs achieve modeling realistic human bodies [32, 37,50,57,89], learning animatable avatars [34, 56,74] and generalizing across different persons [32, 70,89] from temporal data."
        },
        "Datid-3d: Diversity-preserved domain adaptation using text-to-image diffusion for 3d generative model": {
            "authors": [
                "Gwanghyun Kim",
                "Se Young"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_DATID-3D_Diversity-Preserved_Domain_Adaptation_Using_Text-to-Image_Diffusion_for_3D_Generative_CVPR_2023_paper.pdf",
            "ref_texts": "[47] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021. 2",
            "ref_ids": [
                "47"
            ],
            "1": "These 3D generative models can be trained with single-view images and then can sample infinite 3D images in real-time, while 3D scene representation as neural implicit fields using NeRF [38] and its variants [3, 4, 8, 10, 14, 17, 20, 32 \u201334, 36, 45, 47, 50, 53, 54, 64, 66, 70 \u201373] require multi-view images and training for each scene.",
            "2": "Such 3D generative models can be trained using single-view images and then can sample infinite 3D images in real-time whereas 3D scene representation as neural implicit fields using Neural Radiance Field (NeRF) [38] and its variants [3, 4, 8, 10, 14, 17, 20, 32 \u201334, 36, 14204\n\n45, 47, 50, 53, 54, 64, 66, 70 \u201373] requires multi-view images and training time for each scene."
        },
        "High-fidelity human avatars from a single rgb camera": {
            "authors": [
                "Hao Zhao",
                "Jinsong Zhang",
                "Kun Lai",
                "Zerong Zheng",
                "Yingdi Xie",
                "Yebin Liu",
                "Kun Li"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Zhao_High-Fidelity_Human_Avatars_From_a_Single_RGB_Camera_CVPR_2022_paper.pdf",
            "ref_texts": "[33] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural Body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE Conf. Comput. Vis. Pattern Recog. , 2021. 3, 7, 8",
            "ref_ids": [
                "33"
            ],
            "1": "Neural Body [33] proposed a representation where the learned latent codes are anchored to a deformable mesh to provide the network with geometric guidance.",
            "2": "Method [33] [15] [12] Ours FID\u2193 81.",
            "3": "We compare our method with three state-of-the-art methods Neural Body [33], HF-NHMT [15] and StylePeople [12].",
            "4": "The trained models of [33] and [15] are generated by the official implementations, and the trained models of [12] on 20 videos of SelfieVideo are provided by the authors.",
            "5": "Novel view synthesis results of NeuralBody [33] (top row), HF-NHMT [15] (second row), StylePeople [12] (third row), and our method (bottom row)."
        },
        "Vid2avatar: 3d avatar reconstruction from videos in the wild via self-supervised scene decomposition": {
            "authors": [
                "Chen Guo",
                "Tianjian Jiang",
                "Xu Chen",
                "Jie Song",
                "Otmar Hilliges"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Guo_Vid2Avatar_3D_Avatar_Reconstruction_From_Videos_in_the_Wild_via_CVPR_2023_paper.pdf",
            "ref_texts": "[42] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body:Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021. 2",
            "ref_ids": [
                "42"
            ],
            "1": "Fitting neural implicit surfaces to videos has recently been demonstrated [23, 25, 42, 49, 50, 55, 71].",
            "2": "Recent works fit implicit neural fields to videos via neural rendering to obtain articulated human models [23\u201325, 42, 49, 50, 55, 71]."
        },
        "Reconstructing 3d human pose by watching humans in the mirror": {
            "authors": [
                "Qi Fang",
                "Qing Shuai",
                "Junting Dong",
                "Hujun Bao",
                "Xiaowei Zhou"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Fang_Reconstructing_3D_Human_Pose_by_Watching_Humans_in_the_Mirror_CVPR_2021_paper.pdf",
            "ref_texts": "[44] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 2",
            "ref_ids": [
                "44"
            ],
            "1": "Some other works use temporal constraints [24,55,44] or geometric self-consistency [8]."
        },
        "Local-to-global registration for bundle-adjusting neural radiance fields": {
            "authors": [
                "Yue Chen",
                "Xingyu Chen",
                "Xuan Wang",
                "Qi Zhang",
                "Yu Guo",
                "Ying Shan",
                "Fei Wang"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Local-to-Global_Registration_for_Bundle-Adjusting_Neural_Radiance_Fields_CVPR_2023_paper.pdf",
            "ref_texts": "[35] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021. 2",
            "ref_ids": [
                "35"
            ],
            "1": "They have seen widespread success in problems such as image synthesis [4,7,40], 3D shape [9,27,34], view-dependent appearance [6,18,29,33], and animation of humans [8,35,45]."
        },
        "Surface-aligned neural radiance fields for controllable 3d human synthesis": {
            "authors": [
                "Tianhan Xu",
                "Yasuhiro Fujita",
                "Eiichi Matsumoto"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Xu_Surface-Aligned_Neural_Radiance_Fields_for_Controllable_3D_Human_Synthesis_CVPR_2022_paper.pdf",
            "ref_texts": "[36] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 1, 2, 5, 6, 7",
            "ref_ids": [
                "36"
            ],
            "1": "Because manually designing high-quality 3D human models is usually labor-intensive, increasing studies [1\u20133,24,27,30,35,36] have proposed the reconstruction of 3D human models using only 2D observations.",
            "2": "Several approaches [24, 30, 35, 36] have been proposed to incorporate knowledge from a statistical 3D human model and its pose estimation with NeRF.",
            "3": "Other approaches transform the query points into local coordinate systems [30] or to a latent code representation [36], with the help of human pose estimation.",
            "4": "For modeling a dynamic human body, recent studies [24, 30, 35, 36] have proposed the use of prior knowledge of human pose and skinning weights of SMPL [26] to ease the learning of a deformation field.",
            "5": "Neural Body [36] uses the structured latent code anchored at the SMPL vertices to encode the pose information.",
            "6": "Specifically, NARF [30] transforms the position information of spatial points into each bone coordinate and uses all of them as input; Neural Body [36] uses a neural network to compute a latent code representing the position information relative to the body mesh and uses it as input.",
            "7": "Following the previous studies [28,35,36], we minimize the per-pixel mean squared error (MSE) between the rendered image and the ground truth image.",
            "8": "Datasets ZJU-MoCap [36] records human motion using 21 synchronized cameras and uses markerless motion capture to obtain human poses.",
            "9": "We follow [36] for data preprocessing and more training and testing details.",
            "10": "For the ZJU-Mocap dataset, we compare with Neural Body [36], which uses structured latent code to model the appearance of the human body.",
            "11": "For the ZJU-MoCap dataset, our approach outperforms [30] and shows a slightly better performance compared to [36], although these studies use per-frame optimization for better modeling of the training pose.",
            "12": "For both datasets, the performance of our approach almost consistently outperforms [36], [35], and [30].",
            "13": "Also, while we do not explicitly model the time-varying deformation components (such as using per-frame embedding in [35,36]), neural networks could implicitly model such components by inferring time from the skeleton pose information.",
            "14": "15888\n Training pose Unseen pose PSNR SSIM PSNR SSIM NARF [30] NB [36] Ours NARF [30] NB [36] Ours NARF [30] NB [36] Ours NARF [30] NB [36] Ours Twirl 29.",
            "15": "Results of the ZJU-MoCap dataset [36] in terms of PSNR and SSIM."
        },
        "Learned vertex descent: A new direction for 3d human model fitting": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2205.06254",
            "ref_texts": "61. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: CVPR. pp. 9054{9063 (2021)",
            "ref_ids": [
                "61"
            ],
            "1": "Recent works have already explored possible integrations between implicit and parametric representations for the tasks of 3D reconstruction [33,84], clothed human modeling [73,46,47], or human rendering [61]."
        },
        "Human performance modeling and rendering via neural animated mesh": {
            "authors": [
                "Fuqiang Zhao"
            ],
            "url": "https://dl.acm.org/doi/pdf/10.1145/3550454.3555451",
            "ref_texts": "3530127 Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao, Wenzheng Chen, Alex Evans, Thomas Mueller, and Sanja Fidler. 2021. Extracting Triangular 3D Models, Materials, and Lighting From Images. arXiv:2111.12503 (2021). Richard A. Newcombe, Dieter Fox, and Steven M. Seitz. 2015. DynamicFusion: Reconstruction and Tracking of Non-Rigid Scenes in Real-Time. In CVPR . Richard A. Newcombe, Shahram Izadi, Otmar Hilliges, David Molyneaux, David Kim, Andrew J. Davison, Pushmeet Kohli, Jamie Shotton, Steve Hodges, and Andrew Fitzgibbon. 2011. KinectFusion: Real-Time Dense Surface Mapping and Tracking. In Proc. of ISMAR . 127\u2013136. Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. 2019. Occupancy flow: 4d reconstruction by learning particle dynamics. In Proceedings of the IEEE/CVF international conference on computer vision . 5379\u20135389. Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. 2020. Differentiable volumetric rendering: Learning implicit 3d representations without 3d supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 3504\u20133515. Michael Oechsle, Songyou Peng, and Andreas Geiger. 2021. Unisurf: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 5589\u20135599. Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. 2019. Deepsdf: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 165\u2013174. Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Steven M Seitz, and Ricardo-Martin Brualla. 2020. Deformable Neural Radiance Fields. arXiv preprint arXiv:2011.12948 (2020). Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Steven M Seitz, and Ricardo Martin-Brualla. 2021. Nerfies: Deformable neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 5865\u20135874. Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021a. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 14314\u201314323. Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021b. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 9054\u20139063. PhotoScan 2019. AgiSoft PhotoScan Professional. http://www.agisoft.com/downloads/installer/. Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. 2021. D-nerf: Neural radiance fields for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 10318\u201310327. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention . Springer, 234\u2013241. Shunsuke Saito, Jinlong Yang, Qianli Ma, and Michael J Black. 2021. SCANimate: Weakly supervised learning of skinned clothed avatar networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 2886\u20132897. Sara Fridovich-Keil and Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. 2022. Plenoxels: Radiance Fields without Neural Networks. In CVPR . Johannes Lutz Sch\u00f6nberger and Jan-Michael Frahm. 2016. Structure-from-Motion Revisited. In Conference on Computer Vision and Pattern Recognition (CVPR) . Soumyadip Sengupta, Vivek Jayaram, Brian Curless, Steven M Seitz, and Ira Kemelmacher-Shlizerman. 2020. Background matting: The world is your green screen. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 2291\u20132300. Aliaksandra Shysheya, Egor Zakharov, Kara-Ali Aliev, Renat Bashirov, Egor Burkov, Karim Iskakov, Aleksei Ivakhnenko, Yury Malkov, Igor Pasechnik, Dmitry Ulyanov, et al.2019. Textured neural avatars. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 2387\u20132397. Miroslava Slavcheva, Maximilian Baust, Daniel Cremers, and Slobodan Ilic. 2017. Killingfusion: Non-rigid 3d reconstruction without correspondences. In Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition . 1386\u20131395. Miroslava Slavcheva, Maximilian Baust, and Slobodan Ilic. 2018. Sobolevfusion: 3d reconstruction of scenes undergoing free non-rigid motion. In Proceedings of the IEEE conference on computer vision and pattern recognition . 2646\u20132655. Robert W Sumner, Johannes Schmid, and Mark Pauly. 2007. Embedded deformation for shape manipulation. ACM Transactions on Graphics (TOG) 26, 3 (2007), 80. Guoxing Sun, Xin Chen, Yizhang Chen, Anqi Pang, Pei Lin, Yuheng Jiang, Lan Xu, Jingya Wang, and Jingyi Yu. 2021. Neural Free-Viewpoint Performance Rendering under Complex Human-object Interactions. In Proceedings of the 29th ACM International Conference on Multimedia . Xin Suo, Yuheng Jiang, Pei Lin, Yingliang Zhang, Minye Wu, Kaiwen Guo, and Lan Xu."
        },
        "TotalSelfScan: Learning Full-body Avatars from Self-Portrait Videos of Faces, Hands, and Bodies": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/589c5bd0aa4322e37813e8e41ddf8034-Paper-Conference.pdf",
            "ref_texts": "[41] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021.",
            "ref_ids": [
                "41"
            ],
            "1": "Recently, optimizing a network to represent a person-specific model shows impressive results [14,46,10,41,39].",
            "2": "Inspired by NeRF [33], Neural Body [41] optimizes the radiance field conditioned on the structured latent codes with only images as supervision.",
            "3": "Head Hands Total P2S\u2193CD\u2193P2S\u2193CD\u2193P2S\u2193CD\u2193 NeuralBody [41] 1.",
            "4": "We also compare with NeuralBody [41].",
            "5": "Head Hands Total PSNR \u2191SSIM \u2191LPIPS \u2193PSNR \u2191SSIM \u2191LPIPS \u2193PSNR \u2191SSIM \u2191LPIPS \u2193 NeuralBody [41] 18."
        },
        "Integratedpifu: Integrated pixel aligned implicit function for single-view human reconstruction": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2211.07955",
            "ref_texts": "23. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9054\u20139063 (2021)",
            "ref_ids": [
                "23"
            ],
            "1": "However, pixel-aligned implicit models, including PIFuHD, are observed to be prone to problems such as depth ambiguity and generation of meshes with broken limbs [10,23].",
            "2": "Human Parse Prediction As observed by [10,23], pixel-aligned implicit models, such as PIFu and PIFuHD, has a tendency to produce broken limbs in some of its reconstructed 3D meshes."
        },
        "Learning to stylize novel views": {
            "authors": [
                "Ping Huang",
                "Yu Tseng",
                "Saurabh Saini",
                "Maneesh Singh",
                "Hsuan Yang"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2021/papers/Huang_Learning_To_Stylize_Novel_Views_ICCV_2021_paper.pdf",
            "ref_texts": ""
        },
        "Renderdiffusion: Image diffusion for 3d reconstruction, inpainting and generation": {
            "authors": [
                "Titas Anciukevicius",
                "Zexiang Xu",
                "Matthew Fisher",
                "Paul Henderson",
                "Hakan Bilen",
                "Niloy J. Mitra",
                "Paul Guerrero"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Anciukevicius_RenderDiffusion_Image_Diffusion_for_3D_Reconstruction_Inpainting_and_Generation_CVPR_2023_paper.pdf",
            "ref_texts": "[47] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021. 2",
            "ref_ids": [
                "47"
            ],
            "1": "There has been exponential progress in the computer vision community on representing 3D scenes as neural fields [4,11,37,39,59,64], allowing for high-fidelity rendering in various reconstruction and image synthesis tasks [2, 33, 45, 47, 66]."
        },
        "Mps-nerf: Generalizable 3d human rendering from multiview images": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2203.16875",
            "ref_texts": "[5] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2021, pp. 9054\u20139063. 1, 2, 3, 5, 6, 7, 10",
            "ref_ids": [
                "5"
            ],
            "1": "While traditional methods [1], [2], [3] use dense multiview camera rigs or depth sensors to accomplish this task, recent neural rendering approaches [4], [5], [6], [7] have shown that free-view rendering and animation can be achieved using sparse color cameras, which could significantly reduce the device setup and capture cost.",
            "2": "In particular, promising results have been shown by methods [5], [6], [7] that are based on the neural radiance field (NeRF) [8] representation.",
            "3": "However, due to the high complexity of human motion and appearance, existing methods [5], [6], [7] are typically trained in a person-specific setup, i.",
            "4": "NeuralBody [5] adopts SMPL [15] and uses per-vertex latent code which is used to generate a continuous latent code volume.",
            "5": "To render the target image, we follow recent works [5], [6], [7] and base our rendering scheme on NeRF [8], which is a compact yet powerful representation for neural rendering.",
            "6": "Instead of using the sampling strategy in original NeRF [8] which samples points in the whole volume, we follow [5] to sample points within a 3D bounding box derived based on the SMPL model.",
            "7": "Instead of directly calculating PSNR and SSIM for the whole image, we follow AniNeRF [6] and NeuralBody [5] to project the 3D bounding box of a body onto image plane to obtain a 2D mask and only calculate PSNR and SSIM in the masked region.",
            "8": "TABLE 2: Comparison of our method with NB [5], AniNeRF [6] on the Human3.",
            "9": "NeuralBody [5] and AniNeRF [6] are person-specific models which only need camera parameters to render a novel view of these trained subjects.",
            "10": "Hence, for reference purpose, we compare our method with recent person-specific models NeuralBody (NB) [5] and Animat-able NeRF (AniNeRF) [6].",
            "11": "NeuralBody [5] and AniNeRF [6] are person-specific models which only need camera and pose parameters to render these trained subjects.",
            "12": "TABLE 3: Comparison of NB [5], AniNeRF [6], and our method on the THuman dataset.",
            "13": "9: Results of our method tested on two subjects from the ZJU-MoCap dataset [5].",
            "14": "4 More results on ZJU-MoCap Dataset We further tested our method on the ZJU-MoCap dataset [5], where we train on 6 subjects using 4 views as input.",
            "15": "For a reference, we also present the visual results of NeuralBody [5].",
            "16": "1\n[5] S."
        },
        "3D-aware semantic-guided generative model for human synthesis": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2112.01422",
            "ref_texts": "61. Peng, S., Zhang, Y ., Xu, Y ., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: CVPR (2021) 2",
            "ref_ids": [
                "61"
            ],
            "1": "An important class of implicit 3D representations are the Neural Radiance Fields (NeRFs), which can generate high-quality unseen views of complex scenes [50, 27, 12, 61, 60, 62, 7]."
        },
        "Style and pose control for image synthesis of humans from a single monocular view": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2102.11263",
            "ref_texts": "(2020). Amit Raj, Michael Zollhoefer, Tomas Simon, Jason Saragih, Shunsuke Saito, James Hays, and Stephen Lombardi. 2020. PVA: Pixel-aligned Volumetric Avatars. In arXiv:2101.02697 . Kripasindhu Sarkar, Dushyant Mehta, Weipeng Xu, Vladislav Golyanik, and Christian Theobalt. 2020. Neural Re-Rendering of Humans from a Single Image. In European Conference on Computer Vision (ECCV) . Alon Shoshan, Nadav Bhonker, Igor Kviatkovsky, and Gerard Medioni. 2021. GANControl: Explicitly Controllable GANs. arXiv:2101.02477 [cs.CV] Aliaksandr Siarohin, St\u00e9phane Lathuili\u00e8re, Enver Sangineto, and Nicu Sebe. 2019. Appearance and Pose-Conditioned Human Image Generation using Deformable GANs. Transactions on Pattern Analysis and Machine Intelligence (TPAMI) (2019). Aliaksandr Siarohin, Enver Sangineto, Stephane Lathuiliere, and Nicu Sebe. 2018. Deformable GANs for Pose-based Human Image Generation. In Computer Vision and Pattern Recognition (CVPR) . Yinghao Xu Qianqian Wang Qing Shuai Hujun Bao Xiaowei Zhou Sida Peng, Yuanqing Zhang. 2020. Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. arXiv preprint arXiv:2012.15838 (2020). Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014). Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Nie\u00dfner, Gordon Wetzstein, and Michael Zollh\u00f6fer. 2019a. DeepVoxels: Learning Persistent 3D Feature Embeddings. InComputer Vision and Pattern Recognition (CVPR) . Vincent Sitzmann, Michael Zollh\u00f6fer, and Gordon Wetzstein. 2019b. Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations. In Advances in Neural Information Processing Systems (NeurIPS) . Ayush Tewari, Mohamed Elgharib, Gaurav Bharaj, Florian Bernard, Hans-Peter Seidel, Patrick P\u00e9rez, Michael Z\u00f6llhofer, and Christian Theobalt. 2020a. StyleRig: Rigging StyleGAN for 3D Control over Portrait Images. In Computer Vision and Pattern Recognition (CVPR) . Ayush Tewari, Mohamed Elgharib, Mallikarjun BR, Florian Bernard, Hans-Peter Seidel, Patrick P\u00e9rez, Michael Z\u00f6llhofer, and Christian Theobalt. 2020b. PIE: Portrait Image 14 Embedding for Semantic Control. ACM Transactions on Graphics (Proceedings SIGGRAPH Asia) 39, 6 (2020). Justus Thies, Michael Zollh\u00f6fer, and Matthias Nie\u00dfner. 2019. Deferred neural rendering: image synthesis using neural textures. ACM Transactions on Graphics (TOG) 38"
        },
        "E-nerv: Expedite neural video representation with disentangled spatial-temporal context": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2207.08132",
            "ref_texts": "37. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9054\u20139063 (2021)",
            "ref_ids": [
                "37"
            ],
            "1": "Unlike regular grid-wise representations, the compact INRs are proved to be suitable for complex scenes [30] and arbitrary scale sampling [6], as well as in lots of 3D tasks [30,24,37,34] and image representations [43,6,27,59,57,40,64]."
        },
        "Neural capture of animatable 3d human from monocular video": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2208.08728",
            "ref_texts": "25. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: CVPR (2021) 2, 3, 5, 9, 12",
            "ref_ids": [
                "25"
            ],
            "1": "Inspired by NeRF, recent works [25,17,24,21] attempt to model 3D humans by conditioning the radiance ffeld on 3D poses / parametric meshes.",
            "2": "This is a non-trivial task as previous NeRF-based works for human modeling [25,24] sufier from degraded quality more or less when generalized to unseen human poses.",
            "3": "Common articulation choices are 3D pose skeletons [21,29] and parametric 3D mesh models [9,24,17,25].",
            "4": "{ZJU-MoCap [25]: This dataset contains multi-view video sequences of 9 objects with 21 cameras.",
            "5": "We also refer to the supplemental material for a comparsion to NeuralBody [25], the precursor method of AniNeRF."
        },
        "Modeling clothing as a separate layer for an animatable human avatar": {
            "authors": [
                "Donglai Xiang"
            ],
            "url": "https://dl.acm.org/doi/pdf/10.1145/3478513.3480545",
            "ref_texts": "(CVPR) . Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades, Gerard Pons-Moll, Siyu Tang, and Michael J. Black. 2020. Learning to Dress 3D People in Generative Clothing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . Wojciech Matusik, Chris Buehler, Ramesh Raskar, Steven J Gortler, and Leonard McMillan. 2000. Image-based visual hulls. In Proceedings of the 27th annual conference on Computer graphics and interactive techniques . 369\u2013374. Armin Mustafa, Hansung Kim, Jean-Yves Guillemaut, and Adrian Hilton. 2015. General Dynamic Scene Reconstruction From Multiple View Video. In Proceedings of the IEEE International Conference on Computer Vision (ICCV) . Rahul Narain, Armin Samii, and James F O\u2019brien. 2012. Adaptive anisotropic remeshing for cloth simulation. ACM Transactions on Graphics (TOG) 31, 6 (2012), 1\u201310. Ahmed A A Osman, Timo Bolkart, and Michael J. Black. 2020. STAR: A Sparse Trained Articulated Human Body Regressor. In Proceedings of the European Conference on Computer Vision (ECCV) . Springer, 598\u2013613. Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Steven M Seitz, and Ricardo-Martin Brualla. 2020. Deformable Neural Radiance Fields. arXiv preprint arXiv:2011.12948 (2020). Chaitanya Patel, Zhouyingcheng Liao, and Gerard Pons-Moll. 2020. TailorNet: Predicting Clothing in 3D as a Function of Human Pose, Shape and Garment Style. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021. Neural Body: Implicit Neural Representations With Structured Latent Codes for Novel View Synthesis of Dynamic Humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . Gerard Pons-Moll, Sergi Pujades, Sonny Hu, and Michael J Black. 2017. ClothCap: Seamless 4D clothing capture and retargeting. ACM Transactions on Graphics (TOG)"
        },
        "FreeNeRF: Improving Few-shot Neural Rendering with Free Frequency Regularization": {
            "authors": [
                "Jiawei Yang",
                "Marco Pavone",
                "Yue Wang"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Yang_FreeNeRF_Improving_Few-Shot_Neural_Rendering_With_Free_Frequency_Regularization_CVPR_2023_paper.pdf",
            "ref_texts": "[24] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021.",
            "ref_ids": [
                "24"
            ],
            "1": "The seminal work, Neural Radiance Fields (NeRF)\n[21], has been widely studied and advanced in a variety of applications [2,3,13,19,23,25,32], including novel view synthesis [18,21], 3D generation [10,25], deformation [23,26,28], video [7,14,15,24,35]."
        },
        "Danbo: Disentangled articulated neural body representations via graph neural networks": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2205.01666",
            "ref_texts": "41. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: CVPR (2021)",
            "ref_ids": [
                "41"
            ],
            "1": "1 Introduction Animating real-life objects in the digital world is a long-pursued goal in computer vision and graphics, and recent advances already enable 3D free-viewpoint video, animation, and human performance retargeting [17,41,53].",
            "2": "Not using an explicit surface poses a major difficulty as surface-based solutions exploit surface points to anchor neural features locally as vertex attributes [41], and leverage skinning weights to associate points on or close to the surface to nearby body parts [29,40].",
            "3": "For instance, one can anchor neural features spatially by associating each SMPL vertex with a learnable latent feature, and then either diffuse vertex features to the 3D space [26,41] or project the 3D query point to the SMPL surface for feature retrieval.",
            "4": "4 Experiments In the following, we evaluate the improvements upon the most recent surface-free neural body model A-NeRF [46], and compare against recent model-based solutions NeuralBody [41] and Anim-NeRF [40].",
            "5": "NeuralBody [41] Anim-NeRF [40] A-NeRF [46] DANBO (Ours) PSNR \u2191SSIM \u2191KID \u2193LPIPS \u2193PSNR \u2191SSIM \u2191KID \u2193LPIPS \u2193PSNR \u2191SSIM \u2191KID \u2193LPIPS \u2193PSNR \u2191SSIM \u2191KID \u2193LPIPS \u2193 S122.",
            "6": "NeuralBody [41] Anim-NeRF [40] A-NeRF [46] DANBO (Ours) PSNR \u2191SSIM \u2191KID \u2193LPIPS \u2193PSNR \u2191SSIM \u2191KID \u2193LPIPS \u2193PSNR \u2191SSIM \u2191KID \u2193LPIPS \u2193PSNR \u2191SSIM \u2191KID \u2193LPIPS \u2193 S1 22."
        },
        "Geometry-guided progressive nerf for generalizable and efficient neural human rendering": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2112.04312",
            "ref_texts": "28. Peng, S., Zhang, Y ., Xu, Y ., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: CVPR (2021) 2, 3, 4, 7, 8, 9, 10",
            "ref_ids": [
                "28"
            ],
            "1": "Our method can better handle self-occlusion (a) and high computational cost (b) issues than previous methods [12,28].",
            "2": "In (b), our progressive rendering pipeline leverages the geometric volume and the predicted density values to progressively reduce the number of sampling points and speed up the rendering, while previous methods [12,28] wastes large amount of computations at redundant empty regions.",
            "3": "It is worth noting that our multi-view enhanced geometry prior differs significantly from related methods that also utilize human body priors [28,12].",
            "4": "NB [28] learns a per-scene geometry embedding, which is hard to generalize to unseen human bodies; NHP [12] relies on temporal information to complement the base geometry model, which is less effective for regions occluded throughout the input video.",
            "5": "As shown in Figure 1 (b), different from previous methods [28,12], our pipeline decouples the density and color prediction process, leveraging the geometry volume as well as the predicted density values to reduce the number of sampling points for rendering progressively.",
            "6": "NB [28] combines NeRF with a parametric human body model SMPL [20] to regularize the training process.",
            "7": "4 Geometry-guided Progressive Rendering We render the human body in the target view through the volumetric rendering following previous NeRF-based methods [22,28,12].",
            "8": "Compared to the smallest pillar that contains the human body that is used by previous methods [28,12], the geometry volume is closer to the human body shape and contains much fewer redundant void sampling points.",
            "9": "ray, and then uniformly sample Npoints between its near and far bounds as [28,12].",
            "10": "1 Datasets and Metrics We train and evaluate our method on the ZJU-MoCap dataset [28] and THUman 1.",
            "11": "To evaluate the rendering performance, we choose two metrics: peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) following [22,28].",
            "12": "897 NB [28] ZJU-7 ZJU-7\u2713 \u2717 \u2717 28.",
            "13": "871 NB [28] ZJU-7 ZJU-7\u2713 \u2713 \u2717 23.",
            "14": "875 NB [28] ZJU-3 ZJU-3\u2713 \u2713 \u2717 22.",
            "15": "We also achieve competitive fitting performance on the training frames, even comparable to the per-scene optimization methods [37,39,28].",
            "16": "Method # r(M) (\u2193) # pd(M) (\u2193) # pc(M) (\u2193) Time (ms) (\u2193) Mem (GB) (\u2193) NB 2\u00d7[28] 0.",
            "17": "4%) Considering the limited GPU memory, our final GP-NeRF can process all the sampling points in one run, but GP-NeRF\u2020and NB [28] requires at least twice."
        },
        "Cla-nerf: Category-level articulated neural radiance field": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2202.00181",
            "ref_texts": "[17] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021.",
            "ref_ids": [
                "17"
            ],
            "1": "Leveraging the abundant prior knowledge of human bodies, efficient techniques [14], [15], [16], [17], [18], [19], [20], [21], [22], [23]1 have been developed to model the deformation of a wide variety of body shapes."
        },
        "3d-aware video generation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2206.14797",
            "ref_texts": "16 Published in Transactions on Machine Learning Research (06/2023) Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B. Goldman, Steven M. Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In Proc. of the IEEE International Conf. on Computer Vision (ICCV) , 2021a. Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Steven M Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In ICCV, pp. 5865\u20135874, 2021b. KeunhongPark, UtkarshSinha, PeterHedman, JonathanT.Barron, SofienBouaziz, DanB.Goldman, Ricardo Martin-Brualla, and Steven M. Seitz. Hypernerf: a higher-dimensional representation for topologically varying neural radiance fields. ACM Trans. on Graphics , 2021c. Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, and Steven M Seitz. Hypernerf: A higher-dimensional representation for topologically varying neural radiance fields. SIGGRAPH Asia , 2021d. Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, pp. 9054\u20139063, 2021. Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. Convolutional occupancy networks. In Proc. of the European Conf. on Computer Vision (ECCV) , 2020. Martin Piala and Ronald Clark. Terminerf: Ray termination prediction for efficient neural rendering. In Proc. of the International Conf. on 3D Vision (3DV) , 2021. Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , 2021a. Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes. In CVPR, pp. 10318\u201310327, 2021b. Ruslan Rakhimov, Denis Volkhonskiy, Alexey Artemov, Denis Zorin, and Evgeny Burnaev. Latent video transformer. arXiv.org , abs/2006.10704, 2020. Scott E. Reed, Kihyuk Sohn, Yuting Zhang, and Honglak Lee. Learning to disentangle factors of variation with manifold interaction. In Proc. of the International Conf. on Machine learning (ICML) , 2014. Yurui Ren, Ge Li, Yuanqi Chen, Thomas H Li, and Shan Liu. Pirenderer: Controllable portrait image generation via semantic neural rendering. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), 2021. Andreas R\u00f6ssler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Nie\u00dfner. Faceforensics++: Learning to detect manipulated facial images. In Proc. of the IEEE International Conf. on Computer Vision (ICCV) , 2019. Masaki Saito, Eiichi Matsumoto, and Shunta Saito. Temporal generative adversarial nets with singular value clipping. In Proc. of the IEEE International Conf. on Computer Vision (ICCV) , 2017. Masaki Saito, Shunta Saito, Masanori Koyama, and Sosuke Kobayashi. Train sparsely, generate densely: Memory-efficientunsupervisedtrainingofhigh-resolutiontemporalGAN. International Journal of Computer Vision (IJCV) , 2020. Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, and Hao Li. Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization. In Proc. of the IEEE International Conf. on Computer Vision (ICCV) , 2019. Mehdi S. M. Sajjadi, Henning Meyer, Etienne Pot, Urs Bergmann, Klaus Greff, Noha Radwan, Suhani Vora, Mario Lucic, Daniel Duckworth, Alexey Dosovitskiy, Jakob Uszkoreit, Thomas A. Funkhouser, and Andrea Tagliasacchi. Scene representation transformer: Geometry-free novel view synthesis through set-latent scene representations. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , 2022."
        },
        "Common pets in 3d: Dynamic new-view synthesis of real-life deformable categories": {
            "authors": [
                "Samarth Sinha",
                "Roman Shapovalov",
                "Jeremy Reizenstein",
                "Ignacio Rocco",
                "Natalia Neverova",
                "Andrea Vedaldi",
                "David Novotny"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Sinha_Common_Pets_in_3D_Dynamic_New-View_Synthesis_of_Real-Life_Deformable_CVPR_2023_paper.pdf",
            "ref_texts": "[36] Peng, S., Zhang, Y ., Xu, Y ., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. CoRR abs/2012.15838 (2020) 2",
            "ref_ids": [
                "36"
            ],
            "1": "4881\n adopt parametric models of motion such as linear blend skinning [13,23,36,44,51,52], but these are difficult to generalise beyond a few object categories such as humans.",
            "2": "3204\u20133215 (2021) 2\n[36] Peng, S."
        },
        "Neuralhdhair: Automatic high-fidelity hair modeling from a single image using implicit neural representations": {
            "authors": [
                "Keyu Wu",
                "Yifan Ye",
                "Lingchen Yang",
                "Hongbo Fu",
                "Kun Zhou",
                "Youyi Zheng"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Wu_NeuralHDHair_Automatic_High-Fidelity_Hair_Modeling_From_a_Single_Image_Using_CVPR_2022_paper.pdf",
            "ref_texts": ""
        },
        "Hand avatar: Free-pose hand animation and rendering from monocular video": {
            "authors": [
                "Xingyu Chen",
                "Baoyuan Wang",
                "Yeung Shum"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Hand_Avatar_Free-Pose_Hand_Animation_and_Rendering_From_Monocular_Video_CVPR_2023_paper.pdf",
            "ref_texts": "[40] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021.",
            "ref_ids": [
                "40"
            ],
            "1": "8683\n\n3D point query [9, 10, 20, 25, 37, 39, 40, 48, 51, 58, 62\u201364, 69, 72, 75] .",
            "2": "NeuralBody [40] attached latent codes to mesh vertices, which can diffuse into space with sparse convolution [15].",
            "3": "Compared to directly using vertex as the anchor [8,27,40], our barycentric anchors are more uniform to cover the hand surface."
        },
        "UNIF: United neural implicit functions for clothed human reconstruction and animation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2207.09835",
            "ref_texts": "[29] Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9054\u20139063 (2021)",
            "ref_ids": [
                "29"
            ],
            "1": "2 Human Body Reconstruction and Animation As the most popular mesh-based human body model, SMPL [17] and its variations [12, 30, 27] dominate the area of human body reconstruction for its expressiveness and flexibility, supporting innumerable downstream task [15, 26, 2, 9, 29, 28, 14].",
            "2": "Besides the minimal-clothed human body, later works also use neural implicit functions to model clothed humans [31, 34, 24, 29, 28].",
            "3": "14314\u201314323 (2021)\n[29] Peng, S."
        },
        "Explicitly controllable 3d-aware portrait generation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2209.05434",
            "ref_texts": "[98] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2021, pp. 9054\u20139063. 3",
            "ref_ids": [
                "98"
            ],
            "1": "Implicit representations [27], [28], [29], [92] in particular neural radiance field (NeRF) [33], [34], [36], [42], [93], [94] have been widely used in many areas such as 3D modeling [95], [96] and face/body digitization [66], [97], [98], [99], [100], [101], [102].",
            "2": "3\n[98] S."
        },
        "Neus2: Fast learning of neural implicit surfaces for multi-view reconstruction": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2212.05231",
            "ref_texts": "[45] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. CVPR , 1(1):9054\u20139063, 2021.",
            "ref_ids": [
                "45"
            ],
            "1": "Some works in human performance modeling [57, 32, 44, 8, 66, 39, 20, 45, 25, 63] can model large movements by introducing a deformable template as a prior."
        },
        "Gait recognition using 3-d human body shape inference": {
            "authors": [
                "Haidong Zhu",
                "Zhaoheng Zheng",
                "Ram Nevatia"
            ],
            "url": "https://openaccess.thecvf.com/content/WACV2023/papers/Zhu_Gait_Recognition_Using_3-D_Human_Body_Shape_Inference_WACV_2023_paper.pdf",
            "ref_texts": "[28] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021.",
            "ref_ids": [
                "28"
            ],
            "1": "With the introduction of NeRF [25], researchers also introduce Animatable NeRF [28] and Neural Body [28] for reconstructing the human body shape in the video sequence with SMPL priors."
        },
        "MonoHuman: Animatable Human Neural Field from Monocular Video": {
            "authors": [
                "Zhengming Yu",
                "Wei Cheng",
                "Xian Liu",
                "Wayne Wu",
                "Yee Lin"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_MonoHuman_Animatable_Human_Neural_Field_From_Monocular_Video_CVPR_2023_paper.pdf",
            "ref_texts": "[33] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. CVPR , 2021.",
            "ref_ids": [
                "33"
            ],
            "1": "However, previous methods [32, 33, 58] usually require carefully-collected multi-view videos with complicated systems and controlled studios, which limits the usage in general and personalized scenarios applications.",
            "2": "Previous rendering methods [33] can synthesize realistic novel view images of the human body, but hard to animate the avatar in unseen poses.",
            "3": "Dataset and Preprocessing We use ZJU-MoCap dataset [33] and in-the-wild video collected from the Internet to evaluate our method.",
            "4": "We compare our method with (1) NeuralBody [33], which uses structured latent code to represent the human body; (2) HumanNeRF [49], which achieves state-of-the-art image synthesis performance of digital human by learning a motion field mapping network from monocular video.",
            "5": "16948\n PSNR \u2191SSIM\u2191LPIPS* \u2193 Neural Body [33] 28.",
            "6": "NeuMan [16] onlyPSNR \u2191SSIM\u2191LPIPS* \u2193 Neural Body [33] 28.",
            "7": "PSNR \u2191SSIM\u2191LPIPS* \u2193 Neural Body [33] 28.",
            "8": "PSNR \u2191SSIM\u2191LPIPS* \u2193 Neural Body [33] 28."
        },
        "REC-MV: REconstructing 3D Dynamic Cloth from Monocular Videos": {
            "authors": [
                "Lingteng Qiu",
                "Guanying Chen",
                "Jiapeng Zhou",
                "Mutian Xu",
                "Junle Wang",
                "Xiaoguang Han"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Qiu_REC-MV_REconstructing_3D_Dynamic_Cloth_From_Monocular_Videos_CVPR_2023_paper.pdf",
            "ref_texts": "[41] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 2",
            "ref_ids": [
                "41"
            ],
            "1": "Inspired by the success of neural rendering methods [35,37,57] in scene reconstruction, many methods have been proposed to reconstruct 3D human from sparse-view [31,41,53,55,60] or monocular [19,47,49] videos."
        },
        "Physics informed neural fields for smoke reconstruction with sparse data": {
            "authors": [],
            "url": "https://dl.acm.org/doi/pdf/10.1145/3528223.3530169",
            "ref_texts": "2020. Convolutional occupancy networks. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part III 16 . Springer, 523\u2013540. Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021b. Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. In CVPR . Julien Philip, Micha\u00ebl Gharbi, Tinghui Zhou, Alexei A Efros, and George Drettakis.",
            "ref_ids": [
                "2020"
            ],
            "1": "[2020] use interpolated views as further constraints.",
            "2": "[2020] further partition and compress the neural embedding into concise components which enables compositional rendering.",
            "3": "[2020] derive implicit gradients to enable optimization and learn the surface radiance.",
            "4": "[2020] model dynamic scenes as 4D space-time irradiance fields.",
            "5": "[2020] condition ACM Trans.",
            "6": "[2020] train neural networks, \ud835\udc39\ud835\udc53\ud835\udc59\ud835\udc62\ud835\udc56\ud835\udc51 : (\ud835\udc65,\ud835\udc66,\ud835\udc67,\ud835\udc61)\u2192(\ud835\udc51,u,\ud835\udc5d), as the ansatz of the underlying solution functions.",
            "7": "[2020] for coordinate-based MLPs with positional encoding, e."
        },
        "Generative Deformable Radiance Fields for Disentangled Image Synthesis of Topology\u2010Varying Objects": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2209.04183"
        },
        "Generalizable neural performer: Learning robust radiance fields for human novel view synthesis": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2204.11798",
            "ref_texts": "[33] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. arXiv preprint arXiv:2012.15838 , 2020. 2, 3, 5, 7, 8, 9, 16, 17",
            "ref_ids": [
                "33"
            ],
            "1": "Recent works adopt neural networks to learn 3D geometry and appearance from data [21,33,36,54].",
            "2": "com/generalizableneural-performer/gnrspecific optimization [33], scanty pose generalization [36, 37], or unrealistic rendering [21, 36, 54].",
            "3": "Some current cutting-edge approaches rely on temporal coherence of the same subject [21,33] which requires geometry fitting or motion tracking across canonical models.",
            "4": "Methods Generalizable Render Latent Prior Occlusion Supervision NeRF [28] 7 3 2D pixelNerf [49] 3 3 2D 2D IBRNet [42] 3 3 2D 2D PIFu [36] 3 7 2D 3D PaMIR [54] 3 7 2D+3D SMPL (Depth) 3D NB [33] 7 3 2D+3D SMPL (Vertices) 2D GNR 3 3 2D+3D SMPL (SDF&Depth) 3 2D(3D\u0003) Table 1.",
            "5": "Recent datasets [15, 16, 33, 50] are inadequate to train or evaluate an effective model for synthesising human in realworld scenarios, which owns complex and diverse geometry and appearance.",
            "6": "To provide meaningful constraints to the implicit field, conventional ways are either expensive in computation and memory as they directly incorporate voxelized body volume/ local 3D patch features [33, 54, 56] , or lack geometry prior due to oversimplification of a relative direction to articulate skeleton representation [38].",
            "7": "ZJUMocap [33].",
            "8": ", ZJUMoCap [33] and CMU Plenoptic [16], HUMBI [50].",
            "9": "Human samples in these datasets either have limited cloth or accessory types [15, 16, 33, 48], small pose variance [15, 33] or without daily human-object interaction [50].",
            "10": "We evaluate two categories of baseline methods: (1)generalization methods, pixelNerf [49] and IBRNet [42]; (2)case-specific methods, NeuralBody (NB) [33], NeuralTexture (NT) [39], NHR [46] and NeuralV olumes (NV) [24].",
            "11": "The most related NB [33] extracts latent code from body model vertices, it tends to learn in-precise latent code in non-rigid region with large displacement over SMPLx model, seeHanfu andCosplay .",
            "12": "146 NB [33] 20.",
            "13": "We compare our methods with case-specific methods NV [24], NT [39], NHR [46] and NB [33] on this dataset.",
            "14": "Specifically, the first setting (denoted asyin Table S2) is case-specific training but testing on unseen poses , where generalizable methods such as IBRNet [42] and ours are trained following same protocol as case-specific methods such as [24, 33, 39, 46].",
            "15": "191 NB [33] 23.",
            "16": "Our method achieve comparable visual result on total unseen pose with optimized NB [33] model.",
            "17": "More Quantitative and Qualitative Results On V-Sense We compare our method to case-specific methods such as [24, 33, 46] on V-Sense dataset, and provide quantitative comparison in this subsection.",
            "18": "GNR preserves better geometry fidelity comparing to methods such as [33, 46] and high frequency texture than [24].",
            "19": "We compare our methods with case-specific methods NV [24], NHR [46], NB [33]."
        },
        "Deep generative models on 3d representations: A survey": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2210.15663",
            "ref_texts": "[207] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in CVPR , 2021. 11",
            "ref_ids": [
                "207"
            ],
            "1": "[205] CVPR 2022 Neural field Camera pose [50], [51], [193] Noguchi [206] arXiv 2022 Neural field Human pose [207], [208], [209], [210] IDE-3D [211] SIGGRAPH Asia 2022 Neural field Semantics [51], [198] EpiGRAF [212] arXiv 2022 Neural field Camera pose [51], [177] AvatarGen [213] arXiv 2022 Neural field Human pose [196], [214], [215] VolumeGAN [58] CVPR 2022 Hybrid 32.",
            "2": "11, 15\n[207] S."
        },
        "Capturing and animation of body and clothing from monocular video": {
            "authors": [],
            "url": "https://dl.acm.org/doi/pdf/10.1145/3550469.3555423",
            "ref_texts": "3504\u20133515. Mohamed Omran, Christoph Lassner, Gerard Pons-Moll, Peter Gehler, and Bernt Schiele. 2018. Neural body fitting: Unifying deep learning and model based human pose and shape estimation. In International Conference on Computer Vision (ICCV) . IEEE, 484\u2013494. Ahmed A. A. Osman, Timo Bolkart, and Michael J. Black. 2020. STAR: Sparse trained articulated human body regressor. In European Conference on Computer Vision (ECCV) . 598\u2013613. Chaitanya Patel, Zhouyingcheng Liao, and Gerard Pons-Moll. 2020. Tailornet: Predicting clothing in 3d as a function of human pose, shape and garment style. In Conference on Computer Vision and Pattern Recognition (CVPR) . 7365\u20137375. Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. 2019. Expressive Body Capture: 3D Hands, Face, and Body From a Single Image. In Conference on Computer Vision and Pattern Recognition (CVPR) . 10975\u201310985. Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021a. Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. In ICCV . Sida Peng, Shangzhan Zhang, Zhen Xu, Chen Geng, Boyi Jiang, Hujun Bao, and Xiaowei Zhou. 2022. Animatable Neural Implicit Surfaces for Creating Avatars from Videos. arXiv preprint arXiv:2203.08133 (2022). Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021b. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Conference on Computer Vision and Pattern Recognition (CVPR) . 9054\u20139063. Gerard Pons-Moll, Sergi Pujades, Sonny Hu, and Michael J Black. 2017. ClothCap: Seamless 4D clothing capture and retargeting. Transactions on Graphics (TOG) 36, 4 (2017), 1\u201315. Sergey Prokudin, Michael J. Black, and Javier Romero. 2021. SMPLpix: Neural Avatars from 3D Human Models. In Winter Conference on Applications of Computer Vision (WACV) . 1810\u20131819. Nikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia Gkioxari. 2020. Accelerating 3D Deep Learning with PyTorch3D. arXiv:2007.08501 (2020). Capturing and Animation of Body and Clothing from Monocular Video SA \u201922 Conference Papers, December 6\u20139, 2022, Daegu, Republic of Korea Yu Rong, Takaaki Shiratori, and Hanbyul Joo. 2021. FrankMocap: A Monocular 3D Whole-Body Pose Estimation System via Regression and Integration. In International Conference on Computer Vision Workshops (ICCV-W) . Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, and Hao Li. 2019. PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization. In International Conference on Computer Vision (ICCV) . Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul Joo. 2020. PIFuHD: MultiLevel Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization. InConference on Computer Vision and Pattern Recognition (CVPR) . Igor Santesteban, Miguel A Otaduy, and Dan Casas. 2019. Learning-based animation of clothing for virtual try-on. In Computer Graphics Forum , Vol. 38. Wiley Online Library, 355\u2013366. Shih-Yang Su, Frank Yu, Michael Zollh\u00f6fer, and Helge Rhodin. 2021. A-NeRF: Articulated neural radiance fields for learning human shape, appearance, and pose. Advances in Neural Information Processing Systems (NeurIPS) 34 (2021). Yating Tian, Hongwen Zhang, Yebin Liu, and Limin Wang. 2022. Recovering 3D Human Mesh from Monocular Images: A Survey. arXiv preprint arXiv:2203.01923 (2022). Garvita Tiwari, Bharat Lal Bhatnagar, Tony Tung, and Gerard Pons-Moll. 2020. SIZER: A dataset and model for parsing 3d clothing and learning size sensitive 3d clothing. InEuropean Conference on Computer Vision (ECCV) . Springer, 1\u201318. Raquel Vidaurre, Igor Santesteban, Elena Garces, and Dan Casas. 2020. Fully Convolutional Graph Neural Networks for Parametric Virtual Try-On. In Computer Graphics Forum , Vol. 39. Wiley Online Library, 145\u2013156. Yi Wang, Xin Tao, Xiaojuan Qi, Xiaoyong Shen, and Jiaya Jia. 2018. Image inpainting via generative multi-column convolutional neural networks. In Advances in Neural Information Processing Systems (NeurIPS) . 331\u2013340. Chung-Yi Weng, Brian Curless, Pratul P. Srinivasan, Jonathan T. Barron, and Ira Kemelmacher-Shlizerman. 2022. HumanNeRF: Free-Viewpoint Rendering of Moving People From Monocular Video. In Conference on Computer Vision and Pattern Recognition (CVPR) . 16210\u201316220. Donglai Xiang, Hanbyul Joo, and Yaser Sheikh. 2019. Monocular Total Capture: Posing Face, Body, and Hands in the Wild. In Conference on Computer Vision and Pattern Recognition (CVPR) . 10965\u201310974. Donglai Xiang, Fabian Prada, Timur Bagautdinov, Weipeng Xu, Yuan Dong, He Wen, Jessica Hodgins, and Chenglei Wu. 2021. Modeling clothing as a separate layer for an animatable human avatar. Transactions on Graphics (TOG) 40, 6 (2021), 1\u201315. Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, and Michael J. Black. 2022. ICON: Implicit Clothed humans Obtained from Normals. In Conference on Computer Vision andPattern Recognition (CVPR) . Hongyi Xu, Thiemo Alldieck, and Cristian Sminchisescu. 2021. H-NeRF: Neural radiance fields for rendering and temporal reconstruction of humans in motion. Advances in Neural Information Processing Systems (NeurIPS) 34 (2021). Hongyi Xu, Eduard Gabriel Bazavan, Andrei Zanfir, William T Freeman, Rahul Sukthankar, and Cristian Sminchisescu. 2020. GHUM & GHUML: Generative 3d human shape and articulated pose models. In Conference on Computer Vision and Pattern Recognition (CVPR) . 6184\u20136193. Lu Yang, Qing Song, Zhihui Wang, Mengjie Hu, Chun Liu, Xueshi Xin, Wenhe Jia, and Songcen Xu. 2020. Renovating Parsing R-CNN for Accurate Multiple Human Parsing. In European Conference on Computer Vision (ECCV) (Lecture Notes in Computer Science, Vol. 12357) . Springer, 421\u2013437. Ze Yang, Shenlong Wang, Sivabalan Manivasagam, Zeng Huang, Wei-Chiu Ma, Xinchen Yan, Ersin Yumer, and Raquel Urtasun. 2021. S3: Neural shape, skeleton, and skinning fields for 3D human modeling. In Conference on Computer Vision and Pattern Recognition (CVPR) . 13284\u201313293. Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. 2021. Volume rendering of neural implicit surfaces. Advances in Neural Information Processing Systems (NeurIPS) 34"
        },
        "Representing Volumetric Videos as Dynamic MLP Maps": {
            "authors": [
                "Sida Peng",
                "Yunzhi Yan",
                "Qing Shuai",
                "Hujun Bao",
                "Xiaowei Zhou"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Peng_Representing_Volumetric_Videos_As_Dynamic_MLP_Maps_CVPR_2023_paper.pdf",
            "ref_texts": "[46] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 2, 5, 7",
            "ref_ids": [
                "46"
            ],
            "1": "To model high-resolution scenes, [14, 16, 31, 42, 44, 46, 47, 71] extend NeRF to represent dynamic scenes.",
            "2": "Datasets To evaluate the performance of our approach, we conduct experiments of the ZJU-MoCap [46] and NHR [67] datasets.",
            "3": "We do not compare with Neural Body [46] and MVP [34], because their algorithms take tracked meshes as input, making the comparison unfair."
        },
        "Differentiable physics simulation of dynamics-augmented neural objects": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2210.09420",
            "ref_texts": "[19] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 9054\u20139063, 2021.[20] C. Gao, A. Saraf, J. Kopf, and J.-B. Huang, \u201cDynamic view synthesis from dynamic monocular video,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 5712\u20135721, 2021.",
            "ref_ids": [
                "19",
                "20"
            ],
            "1": "To allow modeling dynamics in addition to appearance, there have been various extensions to NeRF for deformable objects [11], [12], [13], [14], [15] and general motions [16], [17], [18], [19], [20].",
            "2": "[19] S.",
            "3": "[20] C."
        },
        "Flag3d: A 3d fitness activity dataset with language instruction": {
            "authors": [
                "Yansong Tang",
                "Jinpeng Liu",
                "Aoyang Liu",
                "Bin Yang",
                "Wenxun Dai",
                "Yongming Rao",
                "Jiwen Lu",
                "Jie Zhou",
                "Xiu Li"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_FLAG3D_A_3D_Fitness_Activity_Dataset_With_Language_Instruction_CVPR_2023_paper.pdf",
            "ref_texts": "[68] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , pages 9054\u20139063, 2021. 2, 3",
            "ref_ids": [
                "68"
            ],
            "1": "HMR ZJU-MoCap [68] 6 6 9 >1k \u00d7\u2713 \u2713 Lab HAR,HMR NTU RGB+D 120 [51] 106 120 114k \u00d7\u2713 Lab HAR,HAG HuMMan [11] 1000 500 400K 60M \u00d7\u2713 \u2713 Lab HAR,HMR HumanML3D [26] 14K \u2713 \u2713 \u2713 Lab HAG KIT Motion Language [71] 111 3911 \u2713 \u2713 Lab HAG HumanAct12 [28] 12 12 1191 90K \u00d7 \u00d7 \u2713 Lab HAG UESTC [35] 118 40 25K >5M \u00d7\u2713 Lab HAR,HAG Fit3D [22] 13 37 >3M \u00d7\u2713 \u2713 Lab HPE,RAC EC3D [115] 4 3 362 \u00d7\u2713 Lab HAR Yoga-82 [95] 82 29K \u00d7 \u00d7 \u00d7 Nat.",
            "2": "They are registered by marker-less multiview MoCap [57, 59, 66, 68, 108, 113], or marker/sensor based Mocap [33, 79, 96]."
        },
        "Real-time neural radiance talking portrait synthesis via audio-spatial decomposition": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2211.12368",
            "ref_texts": "[41] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , pages 9054\u20139063, 2021. 2",
            "ref_ids": [
                "41"
            ]
        },
        "Neural free-viewpoint performance rendering under complex human-object interactions": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2108.00362",
            "ref_texts": "[47] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021. Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. In CVPR .",
            "ref_ids": [
                "47"
            ],
            "1": "For photorealistic human performance rendering, various data representation have been explored, such as point-clouds [42,64], voxels [30], implicit representations [36,47,48,62] or hybrid neural texturing [57].",
            "2": "More recently, [26,44,47,48,61,65,74] extend neural radiance field (NeRF) [36] into the dynamic setting.",
            "3": "Recent approaches [47] and [57] adopt a sparse set of camera views to synthesize photo-realistic novel views of a performer."
        },
        "Learning cross-video neural representations for high-quality frame interpolation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2203.00137",
            "ref_texts": "[37] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of IEEE Conference Computer Vision and Pattern Recognition , 2021.",
            "ref_ids": [
                "37"
            ],
            "1": "The NF approach has also been extended to the video setting by learning the representation of temporally-varying scenes from a set of videos [17,8,50,37,16].",
            "2": "Examples include learning a moving human body from sparse multi-view videos [37], jointly learning deformable fields and scenes [18,8,36], and 3D video synthesis from multi-view videos [16].",
            "3": "[37] S."
        },
        "Ndf: Neural deformable fields for dynamic human modelling": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2207.09193",
            "ref_texts": "25. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: IEEE Conference on Computer Vision and Pattern Recognition. pp. 9054\u20139063 (2021)",
            "ref_ids": [
                "25"
            ],
            "1": "We evaluate our method on ZJU-MoCap [25] and DynaCap [7] datasets that capture dynamic humans in complex motions with synchronized cameras.",
            "2": "Neural Body [25] learns neural representations over the same set of latent codes anchored to the deformable human model SMPL [17], and naturally integrate observations across frames.",
            "3": "The feature space of NDF span the whole UV dimension, which records much more details compared with Neural Body [25], where shared canonical features are only located at SMPL vertices.",
            "4": "NDF does not need to be fine-tuned on novel pose images compared with Animatable NeRF [25] and can be applied to only sparse cameras compared with Neural Actor [14], where dense cameras are needed to precompute a realistic texture map.",
            "5": "1 Dataset and Metrics ZJU-MoCap [25] records multi-view videos with 21 synchronous cameras and collects the shape parameters of SMPL as well as the global translation and the SMPL\u2019s pose parameters with an off-the-shelf SMPL tracking system [32].",
            "6": "Following [25], we choose 9 sequences and 4 uniformly distributed cameras are used for training and the remaining cameras for testing.",
            "7": "The video clips for evaluating novel view synthesis and novel pose synthesis are also the same with [25].",
            "8": "Following typical protocols [20] and works most related to us [24] [25], we evaluate our method on image synthesis using two metrics: peak signal-tonoise ratio (PSNR) and structural similarity index (SSIM).",
            "9": "2 Performance on NVS and NPS We compare our method with state-of-the-art view synthesis methods [25,24] that also use SMPL models and can handle dynamic scenes.",
            "10": "Neural Body [25] represents the dynamic scene with an implicit field conditioned on a shared set of latent codes anchored on the vertices of SMPL and renders the images using volume rendering.",
            "11": "Table 1 shows the comparison of our method with Neural Body [25] and Animatable NeRF [24] on ZJU-MoCap dataset.",
            "12": "Figure 3 presents the qualitative comparison of our method with [25,24] on the ZJU-MoCap dataset.",
            "13": "Both [25] and [24] have difficulty in recovering fine details of the dynamic scene.",
            "14": "Neural Body [25] turns to over-smooth the result as shown in the third person and the fourth person of Figure 3.",
            "15": "For the second row, Neural Body [25] losses wrinkles on the back and Animatable NeRF [24] suffers from artifacts.",
            "16": "Table 2 shows the comparison of our method with Neural Body [25] and Animatable NeRF [24] on novel pose synthesis.",
            "17": "Neural Body [25] learns latent codes for training frames and does not model the dynamic change with respect to poses, thus it always suffers from artifacts when applied to novel pose synthesis.",
            "18": "Neural Body [25] fails to recover the face of the second person 10 Zhang and Chen Table 1.",
            "19": "PSNR SSIM NB [25] AN [24] OURS NB [25] AN [24] OURS\n313 30.",
            "20": "Results of novel pose synthesis on the ZJU-MoCap dataset in terms of PSNR and SSIM (higher is better) PSNR SSIM NB [25] AN [24] OURS NB [25] AN [24] OURS\n313 23.",
            "21": "4 Ablation Study We conduct ablation studies on one subject (313) of the ZJU-MoCap [25] dataset in terms of the novel view synthesis and novel pose synthesis performance.",
            "22": "PSNR SSIM NB [25] AN [24] OURS NB [25] AN [24] OURS novel view 23."
        },
        "Stochastic neural radiance fields: Quantifying uncertainty in implicit 3d representations": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2109.02123",
            "ref_texts": "[32] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural Body: 9 Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE Conf. Comput. Vis. Pattern Recog. , 2021. 2",
            "ref_ids": [
                "32"
            ],
            "1": "Other recent works [33, 8, 34, 19, 37, 32] have extended NeRF to cases with dynamic objects in the scene."
        },
        "LoRD: Local 4d implicit representation for high-fidelity dynamic human modeling": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2208.08622",
            "ref_texts": "53. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9054\u20139063 (2021)",
            "ref_ids": [
                "53"
            ],
            "1": "To tackle this problem, some recent works utilize local implicit representation for shape modeling [19,14,30,54] and neural rendering [53,39], but none of them has used it to build 4D representation that represents how 3D geometry deforms continuously over time.",
            "2": "As a popular line of works, NeRF-based [46] human modeling methods [53,55] typically do not satisfy both local and temporal modeling."
        },
        "Dreamavatar: Text-and-shape guided 3d human avatar generation via diffusion models": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2304.00916",
            "ref_texts": "[31] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE Conference on Computer Vision and Pattern Recognition , 2021. 3",
            "ref_ids": [
                "31"
            ],
            "1": "In recent years, various methods [28, 19, 31, 44, 7] have been proposed to utilize the neural rendering technique NeRF and train on 2D human videos for novel view synthesis."
        },
        "FvOR: Robust joint shape and pose optimization for few-view object reconstruction": {
            "authors": [
                "Zhenpei Yang",
                "Zhile Ren",
                "Miguel Angel",
                "Zaiwei Zhang",
                "Qi Shan",
                "Qixing Huang"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_FvOR_Robust_Joint_Shape_and_Pose_Optimization_for_Few-View_Object_CVPR_2022_paper.pdf",
            "ref_texts": "[43] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2021. 2",
            "ref_ids": [
                "43"
            ],
            "1": "Many of these approach assume ground truth camera poses as input [2,29,41,43,57,64,65,68]."
        },
        "Overview frequency principle/spectral bias in deep learning": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2201.07395",
            "ref_texts": "6.2 Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021. 6.2 Michelle Guo, Alireza Fathi, Jiajun Wu, and Thomas Funkhouser. Object-centric neural scene rendering. arXiv preprint arXiv:2012.08503 , 2020. 6.2 Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi Schmidt, Pratul P Srinivasan, Jonathan T Barron, and Ren Ng. Learned initializations for optimizing coordinate-based neural representations. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2846\u20132855, 2021. 6.2 Mengping Yang, Zhe Wang, Ziqiu Chi, and Yanbing Zhang. Fregan: Exploiting frequency components for training gans under limited data. arXiv preprint arXiv:2210.05461 , 2022. 6.2 Yanyan Li, Weilong Peng, Keke Tang, and Meie Fang. Spatio-frequency decoupled weak-supervision for face reconstruction. Computational Intelligence & Neuroscience , 2022. 6.2 Wei Hu, Lechao Xiao, Ben Adlam, and Jeffrey Pennington. The surprising simplicity of the early-time learning dynamics of neural networks. arXiv preprint arXiv:2006.14599 , 2020. 6.2 Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635 , 2018. 6.2 Haoran You, Chaojian Li, Pengfei Xu, Yonggan Fu, Yue Wang, Xiaohan Chen, Yingyan Lin, Zhangyang Wang, and Richard G Baraniuk. Drawing early-bird tickets: Towards more efficient training of deep networks. International Conference on Learning Representations , 2020. 6.2 Yonggan Fu, Han Guo, Meng Li, Xin Yang, Yining Ding, Vikas Chandra, and Yingyan Lin. CPT: Efficient Deep Neural Network Training via Cyclic Precision. arXiv:2101.09868 , January 2021."
        },
        "GM-NeRF: Learning Generalizable Model-based Neural Radiance Fields from Multi-view Images": {
            "authors": [
                "Jianchuan Chen",
                "Wentao Yi",
                "Liqian Ma",
                "Xu Jia",
                "Huchuan Lu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_GM-NeRF_Learning_Generalizable_Model-Based_Neural_Radiance_Fields_From_Multi-View_Images_CVPR_2023_paper.pdf",
            "ref_texts": "[31] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , pages 9054\u20139063, 2021. 1, 2, 3, 4, 5, 6, 7, 8",
            "ref_ids": [
                "31"
            ],
            "1": "To better generalize to unseen poses, NeuralBody [31] introduces a statistical body model SMPL [23] into neural radiance fields which can reconstruct vivid digital humans from a sparse multi-view video.",
            "2": "To alleviate this limitation, some works [6, 30, 31, 41, 48, 51] combine neural radiance fields [27] with SMPL [23] to represent the human body, which can be rendered to 2D images by differentiable rendering.",
            "3": "NeuralBody [31] optimizes a set of structured latent codes from scratch on vertices of the SMPL model for each specific identity.",
            "4": "Similar to NeuralBody [31], we use SparseConvNet [13] Dto diffuse the structured latent codes {zi}N i=1into the nearby space to form a 3D feature volume G.",
            "5": "0 [55] ZJUMocap [31] GeneBody [8] Model PSNR\u2191SSIM\u2191LPIPS\u2193PSNR\u2191SSIM\u2191LPIPS\u2193PSNR\u2191SSIM\u2191LPIPS\u2193PSNR\u2191SSIM\u2191LPIPS\u2193 IBRNet [47] 28.",
            "6": "Qualitative results of novel pose synthesis on ZJUMocap [31] datasets .",
            "7": "Novel View Synthesis Novel Pose Synthesis Model PSNR\u2191SSIM\u2191LPIPS\u2193PSNR\u2191SSIM\u2191LPIPS\u2193 NB [31] 28.",
            "8": "0 [55] and Multi-garment [4] and real-world datasets Genebody [8] and ZJUMocap [31] for the generalizable scene task.",
            "9": "Following the evaluation protocols used in NB [31], we select 4fixed view videos for training.",
            "10": "The SMPL parameters are obtained using EasyMocap [31].",
            "11": "Specially, we also use m= 3 views as input on ZJUMocap [31] dataset following the evaluation protocol used in KeypointNeRF.",
            "12": "We also compare with per-scene optimization methods NB [31], Ani-NeRF [30], A-NeRF [41], ARAH [48]."
        },
        "Layered-garment net: Generating multiple implicit garment layers from a single image": {
            "authors": [
                "Alakh Aggarwal",
                "Jikai Wang",
                "Steven Hogue",
                "Saifeng Ni",
                "Madhukar Budagavi",
                "Xiaohu Guo"
            ],
            "url": "https://openaccess.thecvf.com/content/ACCV2022/papers/Aggarwal_Layered-Garment_Net_Generating_Multiple_Implicit_Garment_Layers_from_a_Single_ACCV_2022_paper.pdf",
            "ref_texts": "38. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. (2021) 9054\u20139063",
            "ref_ids": [
                "38"
            ]
        },
        "NeuralDome: A Neural Modeling Pipeline on Multi-View Human-Object Interactions": {
            "authors": [
                "Juze Zhang",
                "Haimin Luo",
                "Hongdi Yang",
                "Xinru Xu",
                "Qianyang Wu",
                "Ye Shi",
                "Jingyi Yu",
                "Lan Xu",
                "Jingya Wang"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_NeuralDome_A_Neural_Modeling_Pipeline_on_Multi-View_Human-Object_Interactions_CVPR_2023_paper.pdf",
            "ref_texts": "[45] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 2, 6, 7",
            "ref_ids": [
                "45"
            ],
            "1": "Most notably, the variants of Neural Radiance Field (NeRF) [37] achieve compelling novel view synthesis, which can enable realtime rendering performance [38, 59, 67] even for dynamic scenes [45, 63, 77], and can be extended to the generative setting without per-scene training [23, 69, 80].",
            "2": "Existing works equip NeRF with pose-embeddings [23, 27, 40, 45, 80], learnable skinning weights [25, 44, 70] and even generalization across individuals [23, 66, 80].",
            "3": ", ST-NeRF [77], NeuralBody(NB) [45], in human-object interaction scenario.",
            "4": "We show ground truth and synthesized images of novel view for NeuralBody [45], and ST-NeRF [77] and our layered human-object representation.",
            "5": "Methods NB [45] ST-NeRF [77] Ours Scenes PSNR \u2191SSIM\u2191PSNR\u2191SSIM\u2191PSNR\u2191SSIM\u2191 Bigsofa 19."
        },
        "Occupancy planes for single-view RGB-d human reconstruction": {
            "authors": [
                "Xiaoming Zhao",
                "Ting Hu",
                "Zhongzheng Ren",
                "Alexander G. Schwing"
            ],
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/25474/25246",
            "ref_texts": "3640 Pavlakos, G.; Choutas, V .; Ghorbani, N.; Bolkart, T.; Osman, A. A.; Tzionas, D.; and Black, M. J. 2019. Expressive body capture: 3d hands, face, and body from a single image. In CVPR. Peng, S.; Zhang, Y .; Xu, Y .; Wang, Q.; Shuai, Q.; Bao, H.; and Zhou, X. 2021. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR. Qi, C. R.; Su, H.; Mo, K.; and Guibas, L. J. 2017. Pointnet: Deep learning on point sets for 3d classification and segmentation. In CVPR. Ren, Z.; Agarwala, A.; Russell, B.; Schwing, A. G.; and Wang, O. 2022. Neural V olumetric Object Selection. In CVPR. Ren, Z.; Zhao, X.; and Schwing, A. G. 2021. Classagnostic Reconstruction of Dynamic Objects from Videos. InNeurIPS. Saito, S.; Huang, Z.; Natsume, R.; Morishima, S.; Kanazawa, A.; and Li, H. 2019. PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization. In ICCV. Saito, S.; Simon, T.; Saragih, J. M.; and Joo, H. 2020. PIFuHD: Multi-Level Pixel-Aligned Implicit Function for HighResolution 3D Human Digitization. In CVPR. Sclaroff, S.; and Pentland, A. 1991. Generalized implicit functions for computer graphics. ACM Siggraph. Shade, J.; Gortler, S.; He, L.-w.; and Szeliski, R. 1998. Layered depth images. In Computer graphics and interactive techniques. Srinivasan, P. P.; Tucker, R.; Barron, J. T.; Ramamoorthi, R.; Ng, R.; and Snavely, N. 2019. Pushing the boundaries of view extrapolation with multiplane images. In CVPR. Tong, J.; Zhou, J.; Liu, L.; Pan, Z.; and Yan, H. 2012. Scanning 3d full human bodies using kinects. IEEE TVCG. Tucker, R.; and Snavely, N. 2020. Single-view view synthesis with multiplane images. In CVPR. Varol, G.; Ceylan, D.; Russell, B.; Yang, J.; Yumer, E.; Laptev, I.; and Schmid, C. 2018. Bodynet: V olumetric inference of 3d human body shapes. In ECCV. Vaswani, A.; Shazeer, N. M.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. Attention is All you Need. In NeurIPS. Wang, L.; Zhao, X.; Yu, T.; Wang, S.; and Liu, Y . 2020. Normalgan: Learning detailed 3d human from a single rgb-d image. In ECCV. Wang, N.; Zhang, Y .; Li, Z.; Fu, Y .; Liu, W.; and Jiang, Y .-G."
        },
        "Hvtr: Hybrid volumetric-textural rendering for human avatars": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2112.10203",
            "ref_texts": "[57] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans.",
            "ref_ids": [
                "57"
            ],
            "1": "Recent neural rendering methods [29, 34, 57, 61, 72, 80, 86] have made great progress in generating realistic images of humans, which are simple yet effective compared with traditional graphics pipelines [2, 4, 82].",
            "2": "Existing methods parameterize poses by global pose parameter conditioning [28, 39, 54, 84], 3D sparse points [57], or skinning weights [7, 29, 56].",
            "3": "Inspired by the recent neural scene representations [29, 44, 53, 57, 60], we model articulated humans with an implicit volumetric representation by constructing a dynamic pose-conditioned neural radiance field.",
            "4": "Constructing the radiance field is computationally heavy [29, 44, 57], however, hence we propose to learn only a rough volumetric representation by construct1arXiv:2112.",
            "5": "HVTR is about 52 \u0002 faster than Neural Body [57] in inference (see Tab.",
            "6": "2D : EDN [5], vid2vid [76] GAN 7 3\n2D Plus : SMPLpix [58], DNR [73], ANR[61]GAN 7 3\n3D : NB[57], AniNeRF[56] V olR 3 7\n3D : Ours Hybrid 3 3 Table 1: A set of recent human synthesis approaches classified by feature representations (2D/3D) and renderers.",
            "7": "For stable view synthesis, recent papers [7, 29, 48, 56, 57, 71, 83] propose to unify geometry reconstruction with view 2 Figure 1: We illustrate the differences between (left) GAN-based methods (DNR), (middle) our hybrid approach, and (right) NeRF methods (Neural Body [57]).",
            "8": ", Neural Body [57]) largely relies on the quality of geometry reconstruction, which is very challenging for dynamic humans, and imperfect geometry reconstruction will lead to blurry images (Fig.",
            "9": "First, Yim texis trained in 2D, which converges faster thanYim vol, since Yim volneeds to regress a geometry by optimizing downsampled images, and NeRF training generally converges more slowly for dynamic scenes [57].",
            "10": "Z1-3 from ZJU MoCap [57] have 24 cameras (1024\u00021024, 620-1400 frames each), and we use splits of 10/7, 12/8, 5/5 separately for training/test cameras.",
            "11": "We compare our method with GAN-based methods (DNR[73], SMPLpix[58], ANR[61]), and V olume Rendering method Neural Body [57].",
            "12": "Neural Body[57] was trained with the provided code and setup.",
            "13": ", [58, 61, 73]) and become more obvious for dynamic humans due to the uncertainties of 7 Figure 7: Comparisons with GAN-based methods (DNR[73], SMPLpix [58], ANR [61]), and Neural Body [57] on R2-4, Z1, and Z3.",
            "14": "We evaluate the results of image synthesis on novel views while the test poses are the same as training poses and follow the same protocol as Neural Body [57] on Z1 and Z3 sequences from ZJU MoCap [57].",
            "15": "Comparisons against SMPLpix [58] andNeural Body [57] on shape editing are shown in Fig.",
            "16": "We trained DNR [73], SMPLpix [58], ANR [61], and our method for 50,000 iterations, and 180,000 iterations for Neural Body [57].",
            "17": "FPis based on Pix2PixHD [78] architecture with Encoder blocks of [Conv2d, BatchNorm, ReLU], ResNet [17] blocks, and 11 Z1 (this example) LPIPS # FID# SSIM\"PSNR\" Neural Body [57] .",
            "18": "75 Z1 (mean) LPIPS# FID# SSIM\"PSNR\" Neural Body [57] .",
            "19": "16Z3 (this example) LPIPS FID SSIM PSNR Neural Body [57] .",
            "20": "09 Z3 (mean) LPIPS FID SSIM PSNR Neural Body [57] ."
        },
        "High-fidelity Facial Avatar Reconstruction from Monocular Video with Generative Priors": {
            "authors": [
                "Yunpeng Bai",
                "Yanbo Fan",
                "Xuan Wang",
                "Yong Zhang",
                "Jingxiang Sun",
                "Chun Yuan",
                "Ying Shan"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Bai_High-Fidelity_Facial_Avatar_Reconstruction_From_Monocular_Video_With_Generative_Priors_CVPR_2023_paper.pdf",
            "ref_texts": "[36] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021. 2",
            "ref_ids": [
                "36"
            ],
            "1": "Recently, Neural Radiance Field (NeRF) [7, 8, 11, 12, 26\u201330, 36, 38, 44, 49, 51] obtains impressive performance for novel view 4542\n synthesis of complex scenes."
        },
        "SteerNeRF: Accelerating NeRF Rendering via Smooth Viewpoint Trajectory": {
            "authors": [
                "Sicheng Li",
                "Hao Li",
                "Yue Wang",
                "Yiyi Liao",
                "Lu Yu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Li_SteerNeRF_Accelerating_NeRF_Rendering_via_Smooth_Viewpoint_Trajectory_CVPR_2023_paper.pdf",
            "ref_texts": "[32] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , 2021. 2",
            "ref_ids": [
                "32"
            ]
        },
        "Relightable Neural Human Assets from Multi-view Gradient Illuminations": {
            "authors": [
                "Taotao Zhou",
                "Kai He",
                "Di Wu",
                "Teng Xu",
                "Qixuan Zhang",
                "Kuixiang Shao",
                "Wenzheng Chen",
                "Lan Xu",
                "Jingyi Yu"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Relightable_Neural_Human_Assets_From_Multi-View_Gradient_Illuminations_CVPR_2023_paper.pdf",
            "ref_texts": "[67] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 4, 8",
            "ref_ids": [
                "67"
            ],
            "1": "[67] 21 Biet al.",
            "2": "Applying more advanced designs [67] will definitely improve the effects, which we leave for future works."
        },
        "AvatarReX: Real-time Expressive Full-body Avatars": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2305.04789",
            "ref_texts": "41, 4 (2022), 102:1\u2013102:15. Ahmed A. A. Osman, Timo Bolkart, and Michael J. Black. 2020. STAR: Sparse Trained Articulated Human Body Regressor. In ECCV (6) . 598\u2013613. Jeong Joon Park, Peter Florence, Julian Straub, Richard A. Newcombe, and Steven Lovegrove. 2019. DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation. In CVPR . 165\u2013174. Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. 2019. Expressive Body Capture: 3D Hands, Face, and Body From a Single Image. In CVPR . 10975\u201310985. Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021a. Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. In ICCV . 14294\u201314303. Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021b. Neural Body: Implicit Neural Representations With Structured Latent Codes for Novel View Synthesis of Dynamic Humans. In CVPR . 9054\u20139063. Gerard Pons-Moll, Sergi Pujades, Sonny Hu, and Michael J. Black. 2017. ClothCap: seamless 4D clothing capture and retargeting. ACM Trans. Graph. 36, 4 (2017), 73:1\u201373:15. Sergey Prokudin, Michael J. Black, and Javier Romero. 2021. SMPLpix: Neural Avatars from 3D Human Models. In WACV . 1809\u20131818. Amit Raj, Julian Tanke, James Hays, Minh Vo, Carsten Stoll, and Christoph Lassner."
        },
        "KeypointNeRF: Generalizing image-based volumetric avatars using relative spatial encoding of keypoints": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2205.04992.pdf?trk=public_post_comment-text",
            "ref_texts": "45. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021) 2, 4, 8, 9, 13, 14, 3",
            "ref_ids": [
                "45"
            ],
            "1": "be reduced to sparse camera setups [15, 45].",
            "2": "Recent approaches have incorporated priors specific to human faces [8, 9, 15, 17, 50, 61, 72] and human bodies [44,45,64,66,67,71,73,74] to reduce the dependence on multi-view captures.",
            "3": "For the experiments on human heads, we use a sphere with a radius of 30 centimeters centered around the keypoints, while for the human bodies we follow the prior work [29, 45] and use a 3D bounding box.",
            "4": "KeypointNeRF 9\n6 Experiments In this section, we validate our method on three different reconstruction tasks and datasets: 1)reconstruction of human heads from images captured in a multicamera studio, 2)reconstruction of human heads from in-the-wild images taken with the iPhone\u2019s camera, and 3)reconstruction of human bodies on the public ZJU-MoCap dataset [45].",
            "5": "We use the public ZJU [45] dataset in order to follow the experimental setup used in [29], so that we could closely compare our method\u2019s ability to reconstruct human bodies to the current state-of-the-art method without changing any experimental variables.",
            "6": "Comparison of NHP [29] and our method on unseen identities from the ZJU-MoCap dataset [45].",
            "7": "We re-implemented PVA [48] since their code is not public and we directly used the public results of NHP [29] for the experiments on the ZJU-MoCap dataset [45]."
        },
        "S-PIFu: Integrating Parametric Human Models with PIFu for Single-view Clothed Human Reconstruction": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/6f32db03ef5211f66101ec5972ea9da5-Paper-Conference.pdf",
            "ref_texts": "[18] Peng, S., Zhang, Y ., Xu, Y ., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.",
            "ref_ids": [
                "18"
            ],
            "1": "10975\u201310985 (2019)\n11\n[18] Peng, S."
        },
        "Dance in the wild: Monocular human animation with neural dynamic appearance synthesis": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2111.05916",
            "ref_texts": "[29] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 4322",
            "ref_ids": [
                "29"
            ],
            "1": "Another recent thread of work has explored the use of neural textures [39, 8] and neural rendering [38] to synthesize humans under different poses and viewpoints [35, 34, 32, 29]."
        },
        "Voltemorph: Realtime, controllable and generalisable animation of volumetric representations": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2208.00949",
            "ref_texts": "10.1145/1778765.1778803 Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 9054\u20139063. Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. 2020. D-NeRF: Neural Radiance Fields for Dynamic Scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . 10318\u201310327. Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. 2021. Deep Marching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape Synthesis. In Advances in Neural Information Processing Systems , M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (Eds.), Vol. 34. Curran Associates, Inc., 6087\u20136101. https://proceedings.neurips.cc/paper/2021/file/"
        },
        "Training and tuning generative neural radiance fields for attribute-conditional 3d-aware face generation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2208.12550.pdf?trk=public_post_comment-text",
            "ref_texts": "2009. A 3D face model for pose and illumination invariant face recognition. In 2009 sixth IEEE international conference on advanced video and signal based surveillance . Ieee, 296\u2013301. William Peebles, John Peebles, Jun-Yan Zhu, Alexei Efros, and Antonio Torralba. 2020. The hessian penalty: A weak prior for unsupervised disentanglement. In European Conference on Computer Vision . Springer, 581\u2013597. Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021a. Animatable Neural Radiance Fields for Human Body Modeling. In ICCV . Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021b. Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. In CVPR . Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. 2021. KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs. In ICCV . Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or. 2021. Encoding in style: a stylegan encoder for image-toimage translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 2287\u20132296. Daniel Roich, Ron Mokady, Amit H Bermano, and Daniel Cohen-Or. 2021. Pivotal tuning for latent-based editing of real images. arXiv preprint arXiv:2106.05744 (2021). Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. 2020. GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis. In NeurIPS . Katja Schwarz, Axel Sauer, Michael Niemeyer, Yiyi Liao, and Andreas Geiger. 2022. VoxGRAF: Fast 3D-Aware Image Synthesis with Sparse Voxel Grids. ARXIV (2022). Yujun Shen and Bolei Zhou. 2021. Closed-form factorization of latent semantics in gans. In CVPR . 1532\u20131540. Yichun Shi, Divyansh Aggarwal, and Anil K Jain. 2021. Lifting 2D StyleGAN for 3DAware Face Generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 6258\u20136266. Yichun Shi, Xiao Yang, Yangyue Wan, and Xiaohui Shen. 2022. SemanticStyleGAN: Learning Compositional Generative Priors for Controllable Image Synthesis and Editing. CVPR (2022). Ivan Skorokhodov, Sergey Tulyakov, Yiqun Wang, and Peter Wonka. 2022. EpiGRAF: Rethinking training of 3D GANs. arXiv preprint arXiv:2206.10535 (2022). Jingxiang Sun, Xuan Wang, Yichun Shi, Lizhen Wang, Jue Wang, and Yebin Liu. 2022a. IDE-3D: Interactive Disentangled Editing for High-Resolution 3D-aware Portrait Synthesis. TOG (2022). Jingxiang Sun, Xuan Wang, Yong Zhang, Xiaoyu Li, Qi Zhang, Yebin Liu, and Jue Wang.",
            "ref_ids": [
                "2009"
            ]
        },
        "BAD-NeRF: Bundle Adjusted Deblur Neural Radiance Fields": {
            "authors": [
                "Peng Wang",
                "Lingzhe Zhao",
                "Ruijie Ma",
                "Peidong Liu"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Wang_BAD-NeRF_Bundle_Adjusted_Deblur_Neural_Radiance_Fields_CVPR_2023_paper.pdf",
            "ref_texts": "[36] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. InComputer Vision and Pattern Recognition (CVPR) , pages 9050\u20139059, 2021. 2",
            "ref_ids": [
                "36"
            ]
        },
        "Mononhr: Monocular neural human renderer": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2210.00627",
            "ref_texts": "[36] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 1, 2, 3, 4, 5, 6, 11, 12, 13, 14",
            "ref_ids": [
                "36"
            ],
            "1": "These can generally be grouped into 2 main categories: those learning subject-specific Neural Radiance Fields (NeRF) [30] to represent the appearance of a particular human [25, 35, 36, 40, 46, 52] and approaches that estimate neural surface fields [44] using pixelaligned image features [13, 38, 39, 44, 60].",
            "2": "The first ones require a large number of input images [25, 52] or multiview video frames capturing the complete surface of the target [25,35,36,52], while the others rely on a detailed geometric ground-truth during training, and thus, require expensive, therefore small-scale, 3D scans datasets, preventing generalization to unseen human poses and appearances.",
            "3": "We study the efficacy of MonoNHR on ZJUMoCap [36], AIST [23, 48] and HUMBI [58] datasets.",
            "4": "method input supervisionunseen identity NB [36], Ani-NeRF [35] subject codemulti-view videos7 PIFu [38, 39] ARCH/ARCH++ [12, 15]monocular image3D scans 3 NHP [20]multi-view videosmulti-view images3 MonoNHR (Ours)monocular imagemulti-view images3 Table 1: Comparison of recent neural human rendering methods.",
            "5": "NB [36] utilizes a 3D human mesh model (i.",
            "6": "COLMAP [41, 42], are not accurate enough to provide supervision targets for PIFu and its variants, as discussed in [22, 36].",
            "7": "In consequence, PIFu and its variants are trained on small scale datasets and tend not to generalize well to unseen data, especially non-upright standing poses, as discussed in [20, 36].",
            "8": "The rays are bounded by the given mesh\u2019s 3D bounding box, following NB [36] and NHP [20].",
            "9": "To this aim, we leverage sparse 3D convolutions similarly to NB [36] and NHP [20].",
            "10": "In practice, we set Nto 64 following [20, 36].",
            "11": "We train and test MonoNHR on ZJUMoCap [36], AIST [23, 48], CAPE [27], and HUMBI [59].",
            "12": "We report peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) to evaluate the quality of novel view synthesis, following [20, 36, 57].",
            "13": "The Figure 3 and Table 2 show that MonoNHR produces the best results on both ZJUMoCap [36] and AIST [23, 48].",
            "14": "We also compare MonoNHR with NHP [20] on ZJU-MoCap [36] using estimated body meshes of SPIN [19] in Table 3.",
            "15": "Qualitative results We provide a video1showing more qualitative results and comparisons on the ZJU-MoCap dataset [36], as well as the HUMBI dataset [58].",
            "16": "The geometry branch first builds a sparse 3D volume with the mesh feature M, given a SMPL mesh similarly to NB [36] and NHP [20].",
            "17": "pixelNeRF We trained and tested pixelNeRF [57] on ZJUMoCap [36] and AIST [23, 48] using the official implementation.",
            "18": "We re-implemented NHP from the information available in the paper, the implementation of NB [36] and pixelNeRF [57], following the instructions of the authors of NHP [20].",
            "19": "The initial learning rate is set to 5\u000210\u00004and decays exponentially to 5\u000210\u00005following Neural Body [36]."
        },
        "Neural residual flow fields for efficient video representations": {
            "authors": [
                "Daniel Rho",
                "Junwoo Cho",
                "Jong Hwan",
                "Eunbyung Park"
            ],
            "url": "https://openaccess.thecvf.com/content/ACCV2022/papers/Rho_Neural_Residual_Flow_Fields_for_Efficient_Video_Representations_ACCV_2022_paper.pdf",
            "ref_texts": "27. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). (2021)",
            "ref_ids": [
                "27"
            ],
            "1": "Neural fields have also been applied to signals having both spatial and temporal dimensions, including video representation and novel view synthesis in 4D space [24,25,26,27,28]."
        },
        "Multi-view Shape Generation for a 3D Human-like Body": {
            "authors": [],
            "url": "https://scholar.archive.org/work/whbajfbxzfcvdlb6vyagmuprwu/access/wayback/https://dl.acm.org/doi/pdf/10.1145/3514248"
        },
        "Neural radiance fields from sparse RGB-D images for high-quality view synthesis": {
            "authors": [],
            "url": "https://orca.cardiff.ac.uk/id/eprint/155871/1/SparseNeRF_TPAMI.pdf",
            "ref_texts": "[61] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2021, pp. 9054\u20139063.",
            "ref_ids": [
                "61"
            ],
            "1": "In addition to the above extensions, NeRF has been extended for dynamic scenes [37], [38], better rendering effects [39], [40], generalization on multiple scenes [41], [42], [43], [44], [45], faster training or inference speed [46], [47], [48], [49], [50], [51], [52], re-lighting rendering [53], [54], [55], geometry or appearance editing [56], [57], [58], [59], [60] and specifically for processing human bodies [61], [62], [63] and faces [64], [65].",
            "2": "[61] S."
        },
        "Pixel2ISDF: Implicit Signed Distance Fields Based Human Body Model from Multi-view and Multi-pose Images": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2212.02765",
            "ref_texts": "19. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: CVPR (2021) 2, 3, 4, 5",
            "ref_ids": [
                "19"
            ],
            "1": "Because of the sparsity of the vertices, we utilize a SparseConvNet to generate effective features for any 3D point following [19].",
            "2": "NeuralBody [19] adopts the posed SMPLX mesh to construct the latent code volume that aims to extract the implicit pose code.",
            "3": "Different from the latent code in NeuralBody [19] which is initialized randomly for optimizing specific humans, our latent code is the feature vector learned by a network with the normal map as the input, which can generalize to humans unseen from training ones.",
            "4": "To overcome this challenge, we use a SparseConvNet [19] to form a latent feature volume V \u2208RH\u00d7H\u00d7H\u00d7dwhich diffuses the codes defined on the mesh surface to the nearby 3D space."
        },
        "Structured 3D Features for Reconstructing Controllable Avatars": {
            "authors": [
                "Enric Corona",
                "Mihai Zanfir",
                "Thiemo Alldieck",
                "Eduard Gabriel",
                "Andrei Zanfir",
                "Cristian Sminchisescu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Corona_Structured_3D_Features_for_Reconstructing_Controllable_Avatars_CVPR_2023_paper.pdf",
            "ref_texts": "[46] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 1, 3, 7",
            "ref_ids": [
                "46"
            ],
            "1": "Recently, research on implicit representations [6, 16, 22, 52, 53] and neural fields [24, 46, 63, 70] has made significant progress in improving the realism of avatars.",
            "2": "NeRFs have been recently explored for novel human view synthesis [11, 14, 25, 45, 46, 55, 60, 61, 63].",
            "3": "Here, we do not as-Chamfer \u2193IoU\u2191NC\u2191PSNR\u2191SSIM\u2191LPIPS\u2193Train time NeuralBody [46] 0.",
            "4": "Here, we compare against two such recent works, NeuralBody [46] and H-Nerf [63], on the GHS3D dataset [63]."
        },
        "Point2Pix: Photo-Realistic Point Cloud Rendering via Neural Radiance Fields": {
            "authors": [
                "Tao Hu",
                "Xiaogang Xu",
                "Shu Liu",
                "Jiaya Jia"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Hu_Point2Pix_Photo-Realistic_Point_Cloud_Rendering_via_Neural_Radiance_Fields_CVPR_2023_paper.pdf",
            "ref_texts": "[35] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 2",
            "ref_ids": [
                "35"
            ],
            "1": "For human reconstruction and synthesis, many methods, such as Neural-Body [35], Neural-Actor [25], and Anim-NeRF [6], introduce the parameterized human model SMPL [26] as a strong 3D prior and achieve impressive performance."
        },
        "PoseVocab: Learning Joint-structured Pose Embeddings for Human Avatar Modeling": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2304.13006",
            "ref_texts": "(2022). Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021a. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV . 14314\u201314323. Sida Peng, Shangzhan Zhang, Zhen Xu, Chen Geng, Boyi Jiang, Hujun Bao, and Xiaowei Zhou. 2022b. Animatable Neural Implicit Surfaces for Creating Avatars from Videos. arXiv preprint arXiv:2203.08133 (2022).Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021b. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR . 9054\u20139063. Edoardo Remelli, Timur Bagautdinov, Shunsuke Saito, Chenglei Wu, Tomas Simon, Shih-En Wei, Kaiwen Guo, Zhe Cao, Fabian Prada, Jason Saragih, et al .2022. Drivable volumetric avatars using texel-aligned features. In ACM SIGGRAPH 2022 Conference Proceedings . 1\u20139. Shunsuke Saito, Jinlong Yang, Qianli Ma, and Michael J Black. 2021. SCANimate: Weakly supervised learning of skinned clothed avatar networks. In CVPR . 2886\u2013"
        },
        "Dual-space nerf: Learning animatable avatars and scene lighting in separate spaces": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2208.14851",
            "ref_texts": "[20] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 1, 2, 3, 5, 6, 7, 11, 12",
            "ref_ids": [
                "20"
            ],
            "1": "Several recent works [20, 9, 19, 15, 24, 26, 27, 8] have adapted NeRF onto human body reconstruction and animation, but challenges remain in the following aspects: Recent methods that model the human body with NeRF [14] mostly learn the human body in a canonical space, but the lighting inconsistency in the canonical space lacks exploration [20, 9, 19, 15, 24].",
            "2": "Neural Body [20] binds features onto SMPL\u2019s vertices and diffuses them into the space before volumetric rendering.",
            "3": "6M [7] and the ZJU-MoCap [20] datasets.",
            "4": "[20] use a set of latent code to encode the local geometry and appearance of the human body and bind them onto SMPL[11] vertices.",
            "5": "To accelerate training and inference, we abandon the coarse-to-fine strategy [14, 20, 19].",
            "6": "ZJU-MoCap [20] is a multi-view dataset containing 9 performers captured by 21 synchronized cameras.",
            "7": "We follow the experimental settings of Neural Body [20] and AniNeRF [19].",
            "8": "We use the same protocol as Neural Body [20] to generate SMPL [11] parameters and masks.",
            "9": "Since we focus on the generalization ability of the model under novel poses, we compare our method with two state-of-the-art methods [20, 19] on novel pose synthesis.",
            "10": "Note that we cannot compare with some recent works [9, 27] since the official code 5 Ground Truth Ours Neural Body AniNeRF Ground Truth Ours Neural Body AniNeRF Ground Truth Ours Neural Body AniNeRF Figure 4: Results of novel pose synthesis on the ZJU-MoCap [20] dataset.",
            "11": "Neural Body [20] provides result images on both ZJU-MoCap and Human3.",
            "12": "5, Neural Body [20] tends to produce wrong body structures on Human3.",
            "13": "\u201cNB\u201d means Neural Body[20], and \u201cAN\u201d means AniNeRF [19].",
            "14": "044 Table 1: Comparison with baselines on novel pose synthesis on ZJU-MoCap [20], \u201cNB\u201d means Neural Body [20], and \u201cAN\u201d means AniNeRF [19].",
            "15": "\u201cNB\u201d means Neural Body [20], and \u201cAN\u201d means AniNeRF [19].",
            "16": "Ablation Studies To verify the effectiveness of our main components, we conduct ablation studies on the \u201cTwirl\u201d sequence of ZJUMoCap [20] in terms of novel pose and novel view synthesis.",
            "17": "We compare the methods on the ZJU-Mocap [20] dataset in Tab.",
            "18": "4, where Neural Body [20] exhibits superiority on PSNR and SSIM, and achieves comparable LPIPS to our method.",
            "19": "6M [7] is noisier with higher errors in the fitted SMPL parameters and unclear boundaries in foreground masks compared to ZJU-MoCap [20].",
            "20": "031 Table 4: Comparison with baselines on novel view synthesis on the ZJU-MoCap [20] dataset.",
            "21": "Our method outperforms AniNeRF [19] and is comparable to Neural Body [20] on the perceptual metric.",
            "22": "\u201cNB\u201d means Neural Body [20], and \u201cAN\u201d means AniNeRF [19].",
            "23": "Neural Body [20] can hardly generalize to extreme poses.",
            "24": "11 Ground Truth Neural Body Ours AniNeRF Ground Truth Neural Body Ours AniNeRF Figure 9: Results of novel view synthesis on the ZJU-MoCap [20] dataset.",
            "25": "The results of Neural Body [20] and our method exhibit fewer artifacts compared with AniNeRF [19]."
        },
        "RelightableHands: Efficient Neural Relighting of Articulated Hand Models": {
            "authors": [
                "Shun Iwase",
                "Shunsuke Saito",
                "Tomas Simon",
                "Stephen Lombardi",
                "Timur Bagautdinov",
                "Rohan Joshi",
                "Fabian Prada",
                "Takaaki Shiratori",
                "Yaser Sheikh",
                "Jason Saragih"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Iwase_RelightableHands_Efficient_Neural_Relighting_of_Articulated_Hand_Models_CVPR_2023_paper.pdf",
            "ref_texts": "[44] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 3",
            "ref_ids": [
                "44"
            ],
            "1": "Neural rendering approaches based on volumetric representation have been extended to articulation modeling, compensating for inaccurate geometry by using view-dependent appearance [40,44]."
        },
        "Efficient neural radiance fields for interactive free-viewpoint video": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2112.01517",
            "ref_texts": "(2022). Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas Kurz, Joerg H Mueller, Chakravarty R Alla Chaitanya, Anton Kaplanyan, and Markus Steinberger. 2021. DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks. In EGSR . Michael Oechsle, Songyou Peng, and Andreas Geiger. 2021. Unisurf: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction. In ICCV . Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Steven M. Seitz, and Ricardo Martin-Brualla. 2021a. Nerfies: Deformable Neural Radiance Fields. In ICCV . Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, and Steven M Seitz. 2021b. Hypernerf: A higher-dimensional representation for topologically varying neural radiance fields. arXiv preprint arXiv:2106.13228 (2021). Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021a. Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. In ICCV . Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021b. Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. In CVPR . Eric Penner and Li Zhang. 2017. Soft 3D reconstruction for view synthesis. ACM TOG"
        },
        "Free-viewpoint rgb-d human performance capture and rendering": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2112.13889",
            "ref_texts": "51. Peng, S., Zhang, Y ., Xu, Y ., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: CVPR (2021) 1, 3, 4, 11",
            "ref_ids": [
                "51"
            ],
            "1": "In addition, prior work usually relies on a large amount of cameras [5,42], expensive capture setups [51], or inference time on the order of several minutes per frame.",
            "2": "A key finding of our work is that it generalizes very well to real data captured by a 3dMD scanner system with a level of detail in the face or the clothes that are not seen in prior works [31,32,51].",
            "3": "Given multi-view input frames or videos, recent works on rendering animate humans from novel views show impressive results [46,50,51,66].",
            "4": "ments [48], multi-view input streams [31,36,51,66] and most importantly none of these works can generalize to new human identities (or for the case of Neural Body [51] not even new poses) at testing time which our proposed HVS-Net can accomplish.",
            "5": "Hence we stick to performing quantitative comparisons against LookingGood [40] and SynSin [64] and we do not compare it with NeRF-based approaches [31,36,51,66] as such comparisons are not applicable."
        },
        "UV Volumes for real-time rendering of editable free-view human performance": {
            "authors": [
                "Yue Chen",
                "Xuan Wang",
                "Xingyu Chen",
                "Qi Zhang",
                "Xiaoyu Li",
                "Yu Guo",
                "Jue Wang",
                "Fei Wang"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_UV_Volumes_for_Real-Time_Rendering_of_Editable_Free-View_Human_Performance_CVPR_2023_paper.pdf",
            "ref_texts": "[39] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021. 3, 5, 6, 8",
            "ref_ids": [
                "39"
            ],
            "1": "Recently, various neural representations have been employed in differentiable rendering to depict dynamic scenes, such as voxels [29], point clouds [55], textured meshes [1, 31, 50], and implicit functions [25,27,36,37,39,40].",
            "2": "Particularly, DyNeRF [25] takes the latent code as the condition for time-varying scenes, while NeuralBody [39] employs structured latent codes anchored to a posed human model.",
            "3": "(8) Benefiting from our memory-saving framework that disentangles appearance and geometry, we can render an entire image during training instead of sampling image patches [33, 39].",
            "4": "We use 26 and 20 training views on CMU Panoptic dataset [21] with 960\u00d7540resolution and ZJU Mocap dataset [39] with 512\u00d7512resolution, respectively.",
            "5": "To validate our method, we compare it againstseveral state-of-the-art free-view video synthesis techniques: 1) DN: DyNeRF [25], which takes time-varying latent codes as the conditions for dynamic scenes; and 2) NB: NeuralBody [39], which takes as input the posed human model with structured time-invariant latent codes and generates a pose-conditioned neural radiance field; 3) AN: Animatable-NeRF [38], which uses neural blend weight fields to generate correspondences between observation and canonical space.",
            "6": "Body [39] on ZJU Mocap performer 313, as shown in Table 4."
        },
        "Crosshuman: learning cross-guidance from multi-frame images for human reconstruction": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2207.09735",
            "ref_texts": "[41] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .",
            "ref_ids": [
                "41"
            ],
            "1": "For geometry feature extraction, inspired by [40,41,47], we choose the SparseConvNet [15] to process the SMPL model, divide the 3D bounding box of the SMPL into voxels and obtain a 352-channel geometry feature volume."
        },
        "Nerfcap: Human performance capture with dynamic neural radiance fields": {
            "authors": [],
            "url": "http://www.cad.zju.edu.cn/home/gfzhang/papers/NerfCap/NerfCap_TVCG_2022.pdf",
            "ref_texts": "[15] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural Body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in CVPR , 2021.",
            "ref_ids": [
                "15"
            ],
            "1": "Compared against polygon meshes, NeRFs can more flexibly represent temporally varying geometry and appearance without being concerned about the topological change, while being able to render more photo-realistic images with neural volume rendering [15], [16], [17], [18], [19].",
            "2": "To represent the deformation field, previous methods either adopt the linear blend skinning [15], [16] that cannot capture nonlinear local deformation or combine the linear blend skinning with a residual displacement [20], [21] that is unconstrained by any motion prior.",
            "3": "In addition, the prior works [15], [16], [20], [21] are primarily proposed to synthesize novel views for dynamic humans, while our goal is to not only achieve high-quality novel-view synthesis but also capture the dense space-time coherent geometry with frame-to-frame correspondences by integrating the human template tracking into the dynamic NeRF.",
            "4": "Neuralbody [15] and AniNeRF [16] represent a dynamic human NeRF based on the SMPL model [32] or by combining skeleton-driven deformation [52] with learned blend weights, which regularizes the learning of deformation fields and achieves impressive novel-view synthesis.",
            "5": "H-NeRF [54] unifies NeRF and signed distance field for recovering dynamic humans, which adopts the articulated deformation model similar to [15], [16].",
            "6": "However, the existing human NeRF methods [15], [16], [20], [21], [53], [54] build temporal correspondences among the video frames using the skeletondriven framework, which fails to represent the motion of loose clothes.",
            "7": "For a monocular video of300 frames, the fine-tuning takes less than 15minutes, which is very fast compared to the prior works [15], [16].",
            "8": "3 Comparison to state-of-the-art methods To validate our method, we compare with three stateof-the-art methods for human body reconstruction using NeRF, Neuralbody [15], AniNeRF [16], and AniSDF [53]\n(the extended work of AniNeRF).",
            "9": "93 Neuralbody [15] 89.",
            "10": "44 Neuralbody [15] 89.",
            "11": "(d) Neuralbody [15].",
            "12": "1 Comparison to novel-view synthesis methods of dynamic humans [15], [16], [17], [53] We perform both qualitative and quantitative comparisons on \u201cFranziRed\u201d from DynaCap and subject S4 of DeepCap.",
            "13": "The performance of Neuralbody [15], 8 AniNeRF [16], and AniSDF [53] degrades dramatically on humans in loose clothes, and AniNeRF even cannot converge on the \u201cFranziRed\u201d data.",
            "14": "This is because the skeleton-driven deformation adopted in [15], [16], [53] cannot represent the motion of loose skirts.",
            "15": "Instead of introducing a per-frame latent code as [15], [16], [53], our method extracts deep features from the input images, allowing us to train the networks end to end.",
            "16": ", [15], [16], [17], and [53]) because incorporating a template needs to estimate non-rigid deformations and requires large changes in the method.",
            "17": "Note that the methods of [15], [16], [53] use the SMPL template models.",
            "18": "Both NeuralActor [21] and HumanNeRF [20] model nonrigid human deformation with a residual displacement on top of the linear blend skinning so that they can represent the nonlinear local deformation that fails to be captured by the standard skinning adopted in [15], [16].",
            "19": "6 Test on ZJU-MoCap data [15] and our data To testify the effectiveness of our method on the data captured by general users, we first apply our method onZJU-MoCap data [15].",
            "20": "Results of our method on ZJU-MoCap data [15] (top) and our data (bottom).",
            "21": "[15] S."
        },
        "Artemis: articulated neural pets with appearance and motion synthesis": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2202.05628",
            "ref_texts": "2020. Convolutional occupancy networks. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part III 16 . Springer, 523\u2013540. Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021b. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 9054\u20139063. Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. 2021. D-nerf: Neural radiance fields for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 10318\u201310327. Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. 2021. Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 14335\u201314345. Riccardo Roveri, Lukas Rahmann, Cengiz Oztireli, and Markus Gross. 2018. A network architecture for point cloud classification via automatic depth images generation. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition .",
            "ref_ids": [
                "2020"
            ]
        },
        "Autonomous Manipulation Learning for Similar Deformable Objects via Only One Demonstration": {
            "authors": [
                "Yu Ren",
                "Ronghan Chen",
                "Yang Cong"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Ren_Autonomous_Manipulation_Learning_for_Similar_Deformable_Objects_via_Only_One_CVPR_2023_paper.pdf",
            "ref_texts": ""
        },
        "Hvh: Learning a hybrid neural volumetric representation for dynamic hair performance capture": {
            "authors": [
                "Ziyan Wang",
                "Giljoo Nam",
                "Tuur Stuyck",
                "Stephen Lombardi",
                "Michael Zollhofer",
                "Jessica Hodgins",
                "Christoph Lassner"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_HVH_Learning_a_Hybrid_Neural_Volumetric_Representation_for_Dynamic_Hair_CVPR_2022_paper.pdf",
            "ref_texts": "[44] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9054\u20139063, 2021. 2,3",
            "ref_ids": [
                "44"
            ],
            "1": "Some recent works [26, 44,53] have also 6144\n shown their effectiveness in modeling appearance.",
            "2": "Neural body [44] incorporates SMPL [28] with Neural V olumes [26] for body modeling."
        },
        "Implicit View-Time Interpolation of Stereo Videos using Multi-Plane Disparities and Non-Uniform Coordinates": {
            "authors": [
                "Avinash Paliwal",
                "Andrii Tsarov",
                "Nima Khademi"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Paliwal_Implicit_View-Time_Interpolation_of_Stereo_Videos_Using_Multi-Plane_Disparities_and_CVPR_2023_paper.pdf",
            "ref_texts": "[39] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 9054\u20139063, June 2021.",
            "ref_ids": [
                "39"
            ]
        },
        "Unbiased 4d: Monocular 4d reconstruction with a neural deformation model": {
            "authors": [
                "Erik C",
                "Marc Habermann",
                "Soshi Shimada",
                "Vladislav Golyanik",
                "Christian Theobalt"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023W/DynaVis/papers/Johnson_Unbiased_4D_Monocular_4D_Reconstruction_With_a_Neural_Deformation_Model_CVPRW_2023_paper.pdf",
            "ref_texts": "[28] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Computer Vision and Pattern Recognition (CVPR) , 2021. 2",
            "ref_ids": [
                "28"
            ]
        },
        "Instant-NVR: Instant Neural Volumetric Rendering for Human-Object Interactions From Monocular RGBD Stream": {
            "authors": [
                "Yuheng Jiang",
                "Kaixin Yao",
                "Zhuo Su",
                "Zhehao Shen",
                "Haimin Luo",
                "Lan Xu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_Instant-NVR_Instant_Neural_Volumetric_Rendering_for_Human-Object_Interactions_From_Monocular_CVPR_2023_paper.pdf",
            "ref_texts": "[36] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Ricardo MartinBrualla, and Steven M. Seitz. Hypernerf: A higherdimensional representation for topologically varying neural radiance fields. ACM Trans. Graph. , 40(6), dec 2021. 3, 5[37] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021. 5, 6, 7",
            "ref_ids": [
                "36",
                "37"
            ],
            "1": "fies [35] and HyperNeRF [36] use the SE(3) field.",
            "2": "For dynamic human, in contrast to recent approaches [35,36,38,50] which can\u2019t handle long sequences via pure MLP and human NeRFs [37, 55, 69] that heavily rely on SMPL [27] which do not align well with the surface and easily cause artifacts, we introduce a hybrid deformation module to efficiently leverage the motion priors.",
            "3": "Comparison We compare Instant-NVR against the fusion-based methods RobustFusion [44], NeuralHOFusion [17] and NeRF-based methods NeuralBody [37], HumanNerf [68], both in efficiency and rendering quality.",
            "4": "Both NeuralBody [37] and HumanNerf [68] give erroneous and blurry rendering results in the monocular setting (Fig.",
            "5": "(b) RobustFusion [44] (c) NeuralHOFusion [17] (d) NeuralBody [37] (e) HumanNerf [68] (f) Ours.",
            "6": "Note that both NeuralBody [37] and HumanNerf [68] take several hours to train, while training for our method is online.",
            "7": "151s NeuralBody [37] 19.",
            "8": "3, 5\n[36] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T."
        },
        "Envisioning a Next Generation Extended Reality Conferencing System With Efficient Photorealistic Human Rendering": {
            "authors": [
                "Chuanyue Shen",
                "Letian Zhang",
                "Zhangsihao Yang",
                "Masood Mortazavi",
                "Xiyun Song",
                "Liang Peng",
                "Heather Yu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Shen_Envisioning_a_Next_Generation_Extended_Reality_Conferencing_System_With_Efficient_CVPRW_2023_paper.pdf",
            "ref_texts": "[26] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural Body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 1, 2, 4, 5, 6",
            "ref_ids": [
                "26"
            ],
            "1": "Recent advances allow the use of sparse multi-view video acquisition [26].",
            "2": "It catalyzes a wave of human neural rendering methods that deliver high fidelity results [9,15,24,26,32,33].",
            "3": "Due to its high-quality performance while being simple and extendable, the use of NeRF as a core algorithm has been widely explored in a variety of scene representation and rendering tasks, such as pose estimation [24,26,29,32], lighting [1,2,37], scene labeling and understanding [30,39], and scene composition [23, 34].",
            "4": "Recent works attempt to realize free-viewpoint rendering with reduced hardware complexity, such as using videos acquired by sparse multiview [26] or even monocular camera systems [9, 29, 32].",
            "5": "Neural Body [26] achieves free-viewpoint synthesis from a sparse multi-view video by leveraging the arts from the NeRF model, a Skinned Multi-Person Linear (SMPL) model [18], and a latent variable model [16].",
            "6": "The illustration is made with ZJU-MoCap dataset [26].",
            "7": "The illustration is made with ZJU-MoCap dataset [26].",
            "8": "ZJU-MoCap dataset We use realistic dataset ZJU-MoCap [26] in our experiment for evaluating the system pipeline and algorithm performance in real-world conferencing scenarios."
        },
        "Learning Visibility Field for Detailed 3D Human Reconstruction and Relighting": {
            "authors": [
                "Ruichen Zheng",
                "Peng Li",
                "Haoqian Wang",
                "Tao Yu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Learning_Visibility_Field_for_Detailed_3D_Human_Reconstruction_and_Relighting_CVPR_2023_paper.pdf",
            "ref_texts": "[40] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021. 2",
            "ref_ids": [
                "40"
            ],
            "1": "Human templates such as SMPL [29] can serve as effective guidance [2,40,56,64], but introduce additional template alignment errors and therefore cannot guarantee complete occlusion awareness."
        },
        "Latenthuman: Shape-and-pose disentangled latent representation for human bodies": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2111.15113",
            "ref_texts": "[45] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural Body: Implicit Neural Representations With Structured Latent Codes for Novel View Synthesis of Dynamic Humans. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2021. 3",
            "ref_ids": [
                "45"
            ],
            "1": "Other works based on NeRF [36] rely on meshes for novel view synthesis [45] or rendering of avatars [48], although the work from Peng et al .",
            "2": "[45] can be seen as a hybrid: On one hand, using a mesh for anchoring of latent features and on the hand using those features as input for neural implicit functions."
        },
        "Learning Locally Editable Virtual Humans": {
            "authors": [
                "I Ho",
                "Lixin Xue",
                "Jie Song",
                "Otmar Hilliges"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Ho_Learning_Locally_Editable_Virtual_Humans_CVPR_2023_paper.pdf",
            "ref_texts": "[50] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 9054\u20139063, 2021. 2",
            "ref_ids": [
                "50"
            ],
            "1": "With the advances in neural rendering [63, 64] and the availability of large-scale human datasets [18,28,29,32,47,67,73,74], numerous approaches have been proposed to reconstruct [4, 56, 57] and explicitlycontrol [10, 30, 50] human avatars in a data-driven manner.",
            "2": "Other methods learn to model challenging pose-dependent deformations on avatars either by predicting LBS weights [8, 10, 14, 58, 59, 76] or improving the capabilities of body models [23, 30, 34, 35, 37, 50, 53] with the power of implicit neural fields."
        },
        "Geometry-contrastive transformer for generalized 3d pose transfer": {
            "authors": [
                "Haoyu Chen",
                "Hao Tang",
                "Zitong Yu",
                "Nicu Sebe",
                "Guoying Zhao"
            ],
            "url": "https://ojs.aaai.org/index.php/AAAI/article/download/19901/19660",
            "ref_texts": "Aumentado, T., Armstrong; Tsogkas, S.; Jepson, A.; and Dickinson, S. 2019. Geometric Disentanglement for Generative Latent Shape Models. In ICCV. Bhatnagar, B. L.; Tiwari, G.; Theobalt, C.; and Pons-Moll, G. 2019. Multi-Garment Net: Learning to Dress 3D People from Images. In ICCV. Bogo, F.; Kanazawa, A.; Lassner, C.; Gehler, P.; Romero, J.; and Black, M. J. 2016. Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image. In ECCV. Bogo, F.; Romero, J.; Loper, M.; and Black, M. J. 2014. FAUST: Dataset and evaluation for 3D mesh registration. In CVPR. Bogo, F.; Romero, J.; Pons-Moll, G.; and Black, M. J. 2017. Dynamic FAUST: Registering human bodies in motion. In CVPR. Chen, H.; Liu, X.; Li, X.; Shi, H.; and Zhao, G. 2019. Analyze Spontaneous Gestures for Emotional Stress State Recognition: A Micro-gesture Dataset and Analysis with Deep Learning. In FG. Chen, H.; Tang, H.; Henglin, S.; Peng, W.; Sebe, N.; and Zhao, G. 2021a. Intrinsic-Extrinsic Preserved GANs for Unsupervised 3D Pose Transfer. In ICCV. Chen, H.; Tang, H.; Sebe, N.; and Zhao, G. 2021b. AniFormer: Data-driven 3D Animation withTransformer. In BMVC. Cosmo, L.; Norelli, A.; Halimi, O.; Kimmel, R.; and Rodol `a, E. 2020. LIMP: Learning Latent Shape Representations with Metric Preservation Priors. ECCV. Crane, K.; Weischedel, C.; and Wardetzky, M. 2013. Geodesics in Heat: A New Approach to Computing Distance Based on Heat Flow. ACM TOG, 32. Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In ICLR. Engel, N.; Belagiannis, V .; and Dietmayer, K. 2020. Point Transformer. arXiv preprint arXiv:2011.00931. Groueix, T.; Fisher, M.; Kim, V . G.; Russell, B. C.; and Aubry, M. 2018. 3d-coded: 3d correspondences by deep deformation. In ECCV. Huang, X.; and Belongie, S. 2017. Arbitrary Style Transfer in Real-Time with Adaptive Instance Normalization. In ICCV. Li, W.; Liu, H.; Tang, H.; Wang, P.; and Van Gool, L. 2021. MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation. arXiv preprint arXiv:2111.12707. Lin, K.; Wang, L.; and Liu, Z. 2021. End-to-end human pose and mesh reconstruction with transformers. In CVPR. Mahmood, N.; Ghorbani, N.; F. Troje, N.; Pons-Moll, G.; and Black, M. J. 2019. AMASS: Archive of Motion Capture as Surface Shapes. In ICCV. Nash, C.; Ganin, Y .; Eslami, S. A.; and Battaglia, P. 2020. Polygen: An autoregressive generative model of 3d meshes. InICML.Park, T.; Liu, M.-Y .; Wang, T.-C.; and Zhu, J.-Y . 2019. Semantic image synthesis with spatially-adaptive normalization. In CVPR. Pavlakos, G.; Choutas, V .; Ghorbani, N.; Bolkart, T.; Osman, A. A. A.; Tzionas, D.; and Black, M. J. 2019. Expressive Body Capture: 3D Hands, Face, and Body from a Single Image. In CVPR. Peng, S.; Zhang, Y .; Xu, Y .; Wang, Q.; Shuai, Q.; Bao, H.; and Zhou, X. 2021. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 9054\u20139063. Qi, C. R.; Su, H.; Mo, K.; and Guibas, L. J. 2017a. Pointnet: Deep learning on point sets for 3d classification and segmentation. In CVPR. Qi, C. R.; Yi, L.; Su, H.; and Guibas, L. J. 2017b. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In NeurIPS. Romero, J.; Tzionas, D.; and Black, M. J. 2017. Embodied Hands: Modeling and Capturing Hands and Bodies Together. ACM TOG, 36. Sengupta, A.; Budvytis, I.; and Cipolla, R. 2020. Synthetic Training for Accurate 3D Human Pose and Shape Estimation in the Wild. In BMVC. Sorkine, O. 2005. Laplacian mesh processing. In Eurographics (State of the Art Reports). Sumner, R. W.; and Popovi \u00b4c, J. 2004. Deformation transfer for triangle meshes. ACM TOG, 23(3): 399\u2013405. Tan, Q.; Gao, L.; Lai, Y .-K.; and Xia, S. 2018. Variational autoencoders for deforming 3d mesh models. In CVPR. Tang, H.; Bai, S.; Torr, P. H.; and Sebe, N. 2020a. Bipartite graph reasoning gans for person image generation. In BMVC. Tang, H.; Bai, S.; Zhang, L.; Torr, P. H.; and Sebe, N. 2020b. XingGAN for Person Image Generation. In ECCV. Ulyanov, D.; Vedaldi, A.; and Lempitsky, V . 2016. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, u.; and Polosukhin, I. 2017. Attention is All You Need. In NeurIPS. Wang, J.; Wen, C.; Fu, Y .; Lin, H.; Zou, T.; Xue, X.; and Zhang, Y . 2020. Neural Pose Transfer by Spatially Adaptive Instance Normalization. In CVPR. Wang, N.; Zhang, Y .; Li, Z.; Fu, Y .; Liu, W.; and Jiang, Y .-G."
        },
        "Neural novel actor: Learning a generalized animatable neural representation for human actors": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2208.11905",
            "ref_texts": "[45] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings ofthe IEEE/CVF Conference onComputer Vision andPattern Recognition , pp.",
            "ref_ids": [
                "45"
            ],
            "1": "We evaluate our method on the ZJU-MoCap [45], DeepCap [21] and DynaCap [20] datasets.",
            "2": "Building on these representations, numerous studies [8], [16], [43], [45], [48], [56], [64], [66], [68] use neural representation to capture humans.",
            "3": "Qualitative comparison of identity generalization on the ZJU-MoCap [45] dataset.",
            "4": "Additionally, we provide the results of a person-specific model, Neural Body (NB) [45], in the task of seen poses for seen subjects as an upper-bound baseline.",
            "5": "In this setting, we compare our method with MPS-NeRF (MPS) [17], Keypoint NeRF (KN) [39], Neural Human Performer (NHP) [27] and Neural Body (NB) [45].",
            "6": "We also compare our method with Neural Body (NB) [45] which is a person-specific model ZJU-MoCap dataset consists of 10human subjects captured from 23 synchronized cameras.",
            "7": "All methods are trained on the ZJU-MoCap [45] and directly tested on the DeepCap [21] and DynaCap [20].",
            "8": "Moreover, Animatable NeRF (AN) [44] and Neural Body (NB) [45] do not account for selfocclusion in sparse view and lack specific designs for completing the missing parts during training.",
            "9": "We compared with three person-specific methods, Neural Body (NB)\n[45], Animatable Nerf (AN) [44], Neural Actor (NA) [31] and the generalized method, MPS-NeRF (MPS) [17].",
            "10": "Our method achieves the best performance in two metrics, compared to three person-specific animatable human models, NeuralBody (NB) [45], AnimatbleNerf (AN) [44], and Neural Actor (NA) [31] and MPS-NeRF (MPS) [17]\n.",
            "11": "MPS-NeRF and our method trained on the ZJU-MoCap [45] and directly tested on the DeepCap [21] and DynaCap [20].",
            "12": "[45] S."
        },
        "Clothed Human Performance Capture With a Double-Layer Neural Radiance Fields": {
            "authors": [
                "Kangkan Wang",
                "Guofeng Zhang",
                "Suxu Cong",
                "Jian Yang"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Clothed_Human_Performance_Capture_With_a_Double-Layer_Neural_Radiance_Fields_CVPR_2023_paper.pdf",
            "ref_texts": "[25] Sida Peng, Y uanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural Body:Implicit neural representations with structured latent codesfor novel view synthesis of dynamic humans. In CVPR , 2021. 2, 3,4",
            "ref_ids": [
                "25"
            ],
            "1": "Clothed humans can be reconstructed through implicit representation-based methods such as voxel representation [33,42\n], implicit function [27,28], or neural radiance fields (NeRFs) [20,24,25].",
            "2": "Neuralbody [25] and AniNeRF [24] extend NeRFs for a dynamic human with defor21099\n Color Loss Skeleton-Driven DeformationInverse Deformation Fields Input Color Rendered Color Non-Rigid DeformationGeometry Clothing Layer Input Mask Rendered Mask Simulated ClothSimulation Loss\u089e Predicted Cloth Clothed Human Performance CaptureMask Loss Combined Mesh\u0010\n\u072f\n\u072f\n\u0729 Optimization Losses\u0729\u0be6Canonical Space Cloth SimulationColor Body Layer Double-layer NeRF\u0bda\u0bdc\n\u0c25Composite Rendering Point Cloud RendererObserved Space \u0bd6\u0b65\u123d\n\u0bd7\u0b65\u123d Figure 1.",
            "3": "Double-layer NeRFs for Dynamic Humans Unlike a single NeRFs for clothed humans [17,24,25,34, 41], we represent the clothing with an independent NeRFs on the body, which forms a double-layer NeRFs.",
            "4": "The color network Fcin canonical frame is formulated as, ci(x)=Fc(\u03b3x(x),\u03d5i), (3) where\u03d5iis an appearance latent code [24,25] for frame i."
        },
        "One-Stage 3D Whole-Body Mesh Recovery with Component Aware Transformer": {
            "authors": [
                "Jing Lin",
                "Ailing Zeng",
                "Haoqian Wang",
                "Lei Zhang",
                "Yu Li"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_One-Stage_3D_Whole-Body_Mesh_Recovery_With_Component_Aware_Transformer_CVPR_2023_paper.pdf",
            "ref_texts": "[39] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 4",
            "ref_ids": [
                "39"
            ],
            "1": "1K Daily N N N N SMPL-X [37] ZJU-MoCap [39] \u2265237K Daily N N N Y SMPL-X [1] Pseudo3D LabelsPennAction [53] 77K Fitness N Y N Y SMPL [52] MSCOCO [27] 200K Daily Y Y N N SMPL-X [30] COCO-Wholebody [18] 200K Daily Y Y N N 2D KPT [18] MPII [3] 25K Daily Y Y N N SMPL-X [30] MTP [32] 3."
        },
        "Modiff: Action-conditioned 3d motion generation with denoising diffusion probabilistic models": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2301.03949",
            "ref_texts": "[23] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2021, pp. 9054\u20139063.",
            "ref_ids": [
                "23"
            ],
            "1": "[21], Variational Auto Encoder (V AE) [22], [16], [13], [2], Neural Distance Fields (NDF) [23], [24], [14] or diffusion models [25], [26], [27], [28], [29].",
            "2": "[23] presented an implicit neural representation for dynamic humans for the new-view RGB video synthesis and 3D human model reconstruction.",
            "3": "[23] S."
        },
        "Invertible Neural Skinning": {
            "authors": [
                "Yash Kant",
                "Aliaksandr Siarohin",
                "Riza Alp",
                "Menglei Chai",
                "Jian Ren",
                "Sergey Tulyakov",
                "Igor Gilitschenski"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Kant_Invertible_Neural_Skinning_CVPR_2023_paper.pdf",
            "ref_texts": "[44] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021. 2",
            "ref_ids": [
                "44"
            ],
            "1": "To this end, recent works adopt learningbased solutions for discovering LBS weights [10, 11, 14, 27, 35, 43, 44, 49]."
        },
        "Neural Kaleidoscopic Space Sculpting": {
            "authors": [
                "Byeongjoo Ahn",
                "Michael De",
                "Ioannis Gkioulekas",
                "Aswin C. Sankaranarayanan"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Ahn_Neural_Kaleidoscopic_Space_Sculpting_CVPR_2023_paper.pdf",
            "ref_texts": "[27] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2021. 2",
            "ref_ids": [
                "27"
            ],
            "1": "The most successful among these work use class-specific priors for faces and the human body[17,26,27,32,35]."
        },
        "Neural Residual Radiance Fields for Streamably Free-Viewpoint Videos": {
            "authors": [
                "Liao Wang",
                "Qiang Hu",
                "Qihan He",
                "Ziyu Wang",
                "Jingyi Yu",
                "Tinne Tuytelaars",
                "Lan Xu",
                "Minye Wu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Neural_Residual_Radiance_Fields_for_Streamably_Free-Viewpoint_Videos_CVPR_2023_paper.pdf",
            "ref_texts": ""
        },
        "DINER: Depth-aware Image-based NEural Radiance fields": {
            "authors": [
                "Malte Prinzler",
                "Otmar Hilliges",
                "Justus Thies"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Prinzler_DINER_Depth-Aware_Image-Based_NEural_Radiance_Fields_CVPR_2023_paper.pdf",
            "ref_texts": "[32] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 1",
            "ref_ids": [
                "32"
            ],
            "1": "Especially with the introduction of neural rendering and neural scene representations [43,44], we see 3D digital humans that can be rendered under novel views while being controlled via face and body tracking [3,11,14,15,24,32, 37,47,50,51,58]."
        },
        "Panoptic Compositional Feature Field for Editable Scene Rendering With Network-Inferred Labels via Metric Learning": {
            "authors": [
                "Xinhua Cheng",
                "Yanmin Wu",
                "Mengxi Jia",
                "Qian Wang",
                "Jian Zhang"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Cheng_Panoptic_Compositional_Feature_Field_for_Editable_Scene_Rendering_With_Network-Inferred_CVPR_2023_paper.pdf",
            "ref_texts": "[35] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 9054\u20139063, 2021. 2[36] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 10318\u201310327, 2021. 2",
            "ref_ids": [
                "35",
                "36"
            ],
            "1": "The following works extend NeRF to address its limitations in various aspects, including efficient rendering [2, 9, 25, 38, 41, 48], better generalization [3, 16, 44, 49], dynamic synthesis [10,23,35,36,45], appearance and shape editing [17, 26, 42], and scene stylization [7, 11, 15, 51].",
            "2": "2[36] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer."
        },
        "Learning Personalized High Quality Volumetric Head Avatars from Monocular RGB Videos": {
            "authors": [
                "Ziqian Bai",
                "Feitong Tan",
                "Zeng Huang",
                "Kripasindhu Sarkar",
                "Danhang Tang",
                "Di Qiu",
                "Abhimitra Meka",
                "Ruofei Du",
                "Mingsong Dou",
                "Sergio Orts",
                "Rohit Pandey",
                "Ping Tan",
                "Thabo Beeler",
                "Sean Fanello",
                "Yinda Zhang"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Bai_Learning_Personalized_High_Quality_Volumetric_Head_Avatars_From_Monocular_RGB_CVPR_2023_paper.pdf",
            "ref_texts": "[34] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural Body: Implicit Neural Representations With Structured Latent Codes for Novel View Synthesis of Dynamic Humans. pages 9054\u20139063, 2021. 3, 4",
            "ref_ids": [
                "34"
            ],
            "1": "Sparse local feature embedding attached on geometry has been demonstrated to be effective in improving the rendering quality of neural radiance field [26\u201328, 34, 54].",
            "2": "Inspired by local feature based neural radiance field [34, 47], we attach feature vectorszjon each 3DMM vertex vj ito encode the local radiance fields that can be decoded with MLPs, where idenotes frame index and jdenotes vertex index."
        },
        "CAT-NeRF: Constancy-Aware Tx2Former for Dynamic Body Modeling": {
            "authors": [
                "Haidong Zhu",
                "Zhaoheng Zheng",
                "Wanrong Zheng",
                "Ram Nevatia"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023W/DynaVis/papers/Zhu_CAT-NeRF_Constancy-Aware_Tx2Former_for_Dynamic_Body_Modeling_CVPRW_2023_paper.pdf",
            "ref_texts": "[33] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , pages 9054\u20139063, 2021. 2, 5, 8",
            "ref_ids": [
                "33"
            ],
            "1": "We assess our method on two public datasets, ZJU-MoCap [33] and H36M [13], and show state-of-the-art performance.",
            "2": "Recently some papers [20, 27,29, 32,33, 35] introduce decomposing the neural radiance from the observation space to canonical space for modeling the movement of an object.",
            "3": "To animate the body shapes and render them in the scene, animatable NeRF, different from the deformable methods, projects the body shape into a canonical space [18, 32, 33, 49] or a common shape shapes [34, 36] for projecting the human body shape from different frames into a shared shape or space.",
            "4": "Datasets In our experiments, we compare our methods with baseline methods on two different datasets: H36M [13] and ZJU-MoCap [33].",
            "5": "ZJU-MoCap [33] includes videos captured from 21 different cameras to collect different human poses.",
            "6": "We also report SMPLpix [34] on H36M, along with NeuralBody [33] and HumanNeRF [49] on ZJUMoCap, since the authors did not provide numbers on the other datasets.",
            "7": "Numerical Results We show our numerical results on H36M [13] and ZJUMoCap [33], along with the ablation study for Lcovand Tx2Former architecture, in this subsection.",
            "8": "Our method outperforms state-of-the-art methods on the public datasets, H36M [13] and ZJU-MoCap [33]."
        },
        "AutoAvatar: Autoregressive neural fields for dynamic avatar modeling": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2203.13817.pdf?trk=public_post_comment-text",
            "ref_texts": "37.Peng, S., Zhang, Y ., Xu, Y ., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proc. of Computer Vision and Pattern Recognition (CVPR). pp. 9054\u20139063",
            "ref_ids": [
                "37"
            ],
            "1": "Similarly, neural radiance fields [31] have been applied to body modeling to build animatable avatars from multi-view images [37,20].",
            "2": "The most exciting venue for future work is to extend the notion of dynamics to image-based avatars [37,20]."
        },
        "NIKI: Neural Inverse Kinematics with Invertible Neural Networks for 3D Human Pose and Shape Estimation": {
            "authors": [
                "Jiefeng Li",
                "Siyuan Bian",
                "Qi Liu",
                "Jiasheng Tang",
                "Fan Wang",
                "Cewu Lu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Li_NIKI_Neural_Inverse_Kinematics_With_Invertible_Neural_Networks_for_3D_CVPR_2023_paper.pdf",
            "ref_texts": "[43] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 1",
            "ref_ids": [
                "43"
            ],
            "1": "It has many applications [9, 31, 43, 56, 57, 59, 60]."
        },
        "PersonNeRF: Personalized Reconstruction from Photo Collections": {
            "authors": [
                "Yi Weng",
                "Pratul P. Srinivasan",
                "Brian Curless",
                "Ira Kemelmacher"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Weng_PersonNeRF_Personalized_Reconstruction_From_Photo_Collections_CVPR_2023_paper.pdf",
            "ref_texts": "[34] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021. 2",
            "ref_ids": [
                "34"
            ],
            "1": "Methods have reconstructed neural field representations of humans from a variety of different inputs, including 3D scans [4, 23, 31, 35, 45], multi-view RGB observations [18, 21, 34], RGB-D sequences [8], or monocular videos [13, 46]."
        },
        "HARP: Personalized Hand Reconstruction from a Monocular RGB Video": {
            "authors": [
                "Korrawe Karunratanakul",
                "Sergey Prokudin",
                "Otmar Hilliges",
                "Siyu Tang"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Karunratanakul_HARP_Personalized_Hand_Reconstruction_From_a_Monocular_RGB_Video_CVPR_2023_paper.pdf",
            "ref_texts": "[56] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 3",
            "ref_ids": [
                "56"
            ],
            "1": "Numerous methods have been proposed to learn and estimate appearance from images or videos for bodies with clothing [22,33,51,56,58,65] and faces [2, 15, 16, 19, 29, 53], including those that use NeRF [42] to implicitly represent appearance [53, 54]."
        },
        "CAISE: conversational agent for image search and editing": {
            "authors": [
                "Hyounghun Kim",
                "Doo Soon",
                "Seunghyun Yoon",
                "Franck Dernoncourt",
                "Trung Bui",
                "Mohit Bansal"
            ],
            "url": "https://ojs.aaai.org/index.php/AAAI/article/view/21337/21086"
        },
        "Floren: Real-time high-quality human performance rendering via appearance flow using sparse rgb cameras": {
            "authors": [],
            "url": "https://dl.acm.org/doi/pdf/10.1145/3550469.3555409",
            "ref_texts": "10.1145/3306346.3323020 Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J Black. 2015. SMPL: A skinned multi-person linear model. ACM Transactions on Graphics 34, 6 (2015), 248. Liqian Ma, Xu Jia, Qianru Sun, Bernt Schiele, Tinne Tuytelaars, and Luc Van Gool. 2017. Pose guided person image generation. Advances in neural information processing systems 30 (2017). William R. Mark, Leonard McMillan, and Gary Bishop. 1997. Post-Rendering 3D Warping. In Proceedings of the 1997 Symposium on Interactive 3D Graphics (Providence, Rhode Island, USA) (I3D \u201997) . Association for Computing Machinery, New York, NY, USA, 7\u2013ff. https://doi.org/10.1145/253284.253292 Ricardo Martin-Brualla, Rohit Pandey, Shuoran Yang, Pavel Pidlypenskyi, Jonathan Taylor, Julien Valentin, Sameh Khamis, Philip Davidson, Anastasia Tkach, Peter Lincoln, et al .2018. Lookingood: Enhancing performance capture with real-time neural re-rendering. arXiv preprint arXiv:1811.05029 (2018). Leonard McMillan and Gary Bishop. 1995. Plenoptic modeling: An image-based rendering system. In Proceedings of the 22nd annual conference on Computer graphics and interactive techniques . 39\u201346. Abhimitra Meka, Rohit Pandey, Christian Haene, Sergio Orts-Escolano, Peter Barnum, Philip David-Son, Daniel Erickson, Yinda Zhang, Jonathan Taylor, Sofien Bouaziz, et al.2020. Deep relightable textures: volumetric performance capture with neural rendering. ACM Transactions on Graphics (TOG) 39, 6 (2020), 1\u201321. Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. 2020. Nerf: Representing scenes as neural radiance fields for view synthesis. In European Conference on Computer Vision . Springer, 405\u2013421. Anqi Pang, Xin Chen, Haimin Luo, Minye Wu, Jingyi Yu, and Lan Xu. 2021. Few-shot Neural Human Performance Rendering from Sparse RGBD Videos. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021 , Zhi-Hua Zhou (Ed.). ijcai.org, 938\u2013944. Eunbyung Park, Jimei Yang, Ersin Yumer, Duygu Ceylan, and Alexander C. Berg. 2017. Transformation-Grounded Image Generation Network for Novel 3D View Synthesis. In2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017 . IEEE Computer Society, 702\u2013711. Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021. Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. In CVPR . Eric Penner and Li Zhang. 2017. Soft 3D reconstruction for view synthesis. ACM Transactions on Graphics (TOG) 36, 6 (2017), 1\u201311. Amit Raj, Julian Tanke, James Hays, Minh Vo, Carsten Stoll, and Christoph Lassner."
        },
        "An arbitrary scale super-resolution approach for 3d mr images via implicit neural representation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2110.14476",
            "ref_texts": "[38] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 9054\u20139063, 2021.",
            "ref_ids": [
                "38"
            ],
            "1": "Since then, a number of INRbased models [33]\u2013[38] for 3D shape and surface have been proposed.",
            "2": "[38] S."
        },
        "PREF: Predictability Regularized Neural Motion Fields": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2209.10691",
            "ref_texts": "40. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9054\u20139063 (2021)",
            "ref_ids": [
                "40"
            ],
            "1": "NeRF led to a series of breakthroughs in the fields of 3D scene understanding and rendering, such as relighting [2,46,3], human face and body capture [14,34,40,39,49,24], and city-scale reconstruction [50,58,53,43].",
            "2": "The following three datasets are used for evaluation: PREF: Predictability Regularized Neural Motion Fields 9\n\u2013 ZJU-MoCap [40] is a multi-camera dataset, with videos of one person performing different actions."
        },
        "CAMM: Building Category-Agnostic and Animatable 3D Models from Monocular Videos": {
            "authors": [
                "Tianshu Kuai",
                "Akash Karthikeyan",
                "Yash Kant",
                "Ashkan Mirzaei",
                "Igor Gilitschenski"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023W/DynaVis/papers/Kuai_CAMM_Building_Category-Agnostic_and_Animatable_3D_Models_From_Monocular_Videos_CVPRW_2023_paper.pdf",
            "ref_texts": "[40] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 1, 2",
            "ref_ids": [
                "40"
            ],
            "1": "They serve as base templates for methods that focus on reconstructing and animating 3D humans and animals [7,21,26,36,39,40,55,58,60].",
            "2": "Many works [7,12,13,21, 26,36,39,40,55,58,60\u201362,65] utilize these template shapes or pose priors from shape models to recover 3D shapes and perform animations."
        },
        "HOSNeRF: Dynamic Human-Object-Scene Neural Radiance Fields from a Single Video": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2304.12281",
            "ref_texts": "[36] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021. 1, 2",
            "ref_ids": [
                "36"
            ],
            "1": "Another line of research is specifically designed for dynamic neural human modeling that relies on estimated human poses as a priori information [36, 55].",
            "2": "Since the introduction of NeRF [30], neural human representation [36, 40, 46, 59, 26] has achieved remarkable progress on representing dynamic human bodies from sparse-view videos.",
            "3": "Among them, Neural Actor [26] and Neural Body [36] pioneer in combining NeRF [30] with SMPL deformable meshes to represent human bodies with complex motions."
        },
        "3D Cartoon Face Generation with Controllable Expressions from a Single GAN Image": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2207.14425",
            "ref_texts": ""
        },
        "HandNeRF: Neural Radiance Fields for Animatable Interacting Hands": {
            "authors": [
                "Zhiyang Guo",
                "Wengang Zhou",
                "Min Wang",
                "Li Li",
                "Houqiang Li"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Guo_HandNeRF_Neural_Radiance_Fields_for_Animatable_Interacting_Hands_CVPR_2023_paper.pdf",
            "ref_texts": "[26] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , pages 9054\u20139063, 2021. 1, 2",
            "ref_ids": [
                "26"
            ],
            "1": "To address the above issues and push the boundary of realistic human hand modeling, motivated by the recent success of NeRF [17] in modeling human body [11, 25, 26], we propose HandNeRF , a novel framework that unifiedly models the geometry and texture of animatable interacting This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.",
            "2": "Neural Body [26] signifies a breakthrough in low-cost human rendering by combining NeRF with the mesh-based SMPL model [15]."
        },
        "HDHumans: A hybrid approach for high-fidelity digital humans": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2210.12003",
            "ref_texts": "22 Marc Habermann, Lingjie Liu, Weipeng Xu, Gerard Pons-Moll, Michael Zollhoefer, and Christian Theobalt Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021b. Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. CVPR 1, 1"
        },
        "One-shot Implicit Animatable Avatars with Model-based Priors": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2212.02469",
            "ref_texts": "[46] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural Body: Implicit Neural Representations With Structured Latent Codes for Novel View Synthesis of Dynamic Humans. InComputer Vision and Pattern Recognition (CVPR) , pages 9054\u20139063, 2021. 2, 6, 7, 8, 14",
            "ref_ids": [
                "46"
            ],
            "1": "CV] 21 Aug 2023 tured by well-calibrated multi-camera systems [46, 69, 65, 44, 75], or long monocular videos [64] where almost all parts of the human body are visible.",
            "2": "Method Subject dataExtra training dataInvisible area completionAnimatable NeuralBody [46] Ani-NeRF [44] HumanNeRF [64]multi-view images, monocular videosdata-free \u2717 \u2713 PiFU [54] PaMIR [79] ARCH [20] ARCH++ [17] PHORHUM [1]monocular images3D scans \u2713 \u2713 MPS-NeRF [14] NHP [27]sparse videos, multi-view imagesmulti-view videos\u2717 \u2713 MonoNHR [8]monocular imagesmulti-view images\u2713 \u2717 EV A3D [18]monocular imagesmonocular images\u2713 \u2713 ELICIT (ours)monocular imagesdata-free \u2713 \u2713 Table 1: Recent human rendering methods that are most relevant to our work.",
            "3": "Among which [46] learns structured latent codes on SMPL [34] mesh vertices, other methods construct the representation in a canonical space by modeling pose-driven deformation [64, 69, 59, 78, 45, 44].",
            "4": "Datasets We conducted evaluations on two multi-view human video datasets: ZJU-MoCap [46] and Human3.",
            "5": "We selected three state-of-the-art methods as baselines: Neural Body [46] (NB) and Animatable NeRF [44] (AniNeRF) from per-subject optimization methods, and Neural Human Performer [27] (NHP) from generalizable methods.",
            "6": "Compared with state-of-the-art NeRF based methods[46, 27, 44] on novel view synthesis and novel pose synthesis, ELICIT generates human 3D renderings with more consistent appearance and realistic details from a single image.",
            "7": "As in previous works [46, 27, 32], we evaluated our results using two standard metrics: peak signal-tonoise ratio (PSNR) and structural similarity index (SSIM).",
            "8": "We followed the evaluation protocol of [46] and calculated the metrics only on the bounding box region, rather than the entire image.",
            "9": "For per-subject optimization methods NB [46] and Ani-NeRF [44], we optimized one model for each frame.",
            "10": "6m Subjects Methods PSNR \u2191SSIM\u2191LPIPS\u2193PSNR\u2191SSIM\u2191LPIPS\u2193 SallNB[46] 20.",
            "11": "143 StestNB[46] 19.",
            "12": "6m PSNR\u2191SSIM\u2191LPIPS\u2193PSNR\u2191SSIM\u2191LPIPS\u2193 NeuralBody[46] 20.",
            "13": "Initializing our implicit representation with the rendered views of SMPL mesh imparts /C6/C9/C8/C7/C4/C5/C6/C2/C1/C0/C3 /C0/C14/C17/C7/C9/C12/C15/C4/C14/C7/C4/C10/C27/C28/C17/C5/C24/C3/C2/C1/C9/C4/C6/C20/C5\n/C19/C17/C24/C24/C27/C28/C17/C5/C10/C44/C43/C14/C6/C12/C5\n/C24/C1/C2/C8/C19/C6/C9/C0/C54/C56/C53/C53/C5/C55/C50/C49/C52/C53\n/C6/C9/C8/C7/C4/C5/C6/C2/C1/C0/C3 /C0/C14/C17/C7/C9/C12/C15/C4/C14/C7/C4/C10/C27/C28/C17/C5/C0/C3/C17/C2/C3/C4/C14/C44/C5\n/C20/C17/C9/C24/C4/C14/C1/C6/C9/C4/C10/C1/C14/C12/C5/C0/C3/C17/C2/C3/C4/C14/C44/C5\n/C20/C17/C9/C24/C4/C14/C1/C6/C9/C4/C54/C56/C53/C53/C5/C55/C50/C49/C52/C53\n/C6/C9/C8/C7/C4/C5/C6/C2/C1/C0/C3\n/C61 /C14/C1/C6/C9/C5/C27/C28/C5/C12/C6/C60 /C60/C3/C14/C3/C9/C4/C5/C2/C17/C4/C6/C17/C9 /C61 /C14/C1/C6/C9/C5/C27/C28/C5/C6/C9/C8/C7/C4/C5/C8/C17/C24/C3/C0/C14/C17/C7/C9/C12/C15/C4/C14/C7/C4/C10/C89/C87/C84/C83 /C5/C2/C3/C24/C10/C3/C85\n/C80/C5/C3/C78/C8/C19/C6/C20/C6/C4/C5/C4/C3/C78/C4/C7/C14/C3/C27/C28/C17/C5/C24/C2/C8/C19/C5/C2/C3/C24/C10/C5\n/C6/C9/C6/C4/C6/C1/C19/C6/C90/C1/C4/C6/C17/C9/C54/C56/C53/C53/C5/C55/C50/C49/C52/C53/C107/C1/C105\n/C107/C20/C105\n/C107/C12/C105/C107/C43/C105\n/C61 /C14/C1/C6/C9/C5/C27/C28/C5/C4/C1/C14 /C0/C3/C4/C5/C2/C17/C4/C6/C17/C9 Figure 7: Qualitative results for the ablation studies of priors used in our method, selected from ZJU-MoCap [46] dataset.",
            "14": "1 Data splitting For per-subject optimization methods Animatable NeRF[44] (Ani-NeRF) and NeuralBody[46] (NB), we use all subjects of ZJU-MoCap data-set (313, 315, 377, 386, 387, 390, 392, 393, 394) and the \u201dPosing\u201d sequences of Human 3."
        },
        "Learning implicit body representations from double diffusion based neural radiance fields": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2112.12390",
            "ref_texts": "[Peng et al. , 2021 ]Sida Peng, Yuanqing Zhang, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , pages 9054\u20139063, 2021.[Saito et al. , 2019 ]Shunsuke Saito, Zeng Huang, and Hao Li. Pifu: Pixel-aligned implicit function for highresolution clothed human digitization. In CVPR , pages 2304\u20132314, 2019.",
            "ref_ids": [
                "Peng et al\\. , 2021 ",
                "Saito et al\\. , 2019 "
            ]
        },
        "Selfnerf: Fast training nerf for human from monocular self-rotating video": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2210.01651",
            "ref_texts": "[25] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2021. 1, 2, 4, 6, 8",
            "ref_ids": [
                "25"
            ],
            "1": "NeuralBody [25], AnimatableNeRF [24], H-NeRF [38] and other works [15, 16, 30, 41] are able to synthesize high-quality rendering images and extract rough body geometry from 1arXiv:2210.",
            "2": "Neuralbody [25] utilizes a set of latent codes anchored to a deformable mesh which is shared at different frames.",
            "3": "Following NeuralBody [25], an optimizable latent embedding `tfor each frametis employed to encode the temporally-varying factors.",
            "4": "We also employ ZJU-Mocap [25] dataset for comparing with state of the art methods.",
            "5": "Following NeuralBody [25], we only calculate these metrics on pixels inside the 2D bounding box, which is obtained per view from the input masks.",
            "6": "com/ashawkey/torch-ngpZJUMocap Custom Data Method PSNR\"SSIM\"PSNR\"SSIM\" NeuralBody [25] 23.",
            "7": "ZJUMocap Custom Data Method PSNR\"SSIM\"PSNR\"SSIM\" NeuralBody [25] 23.",
            "8": "2) NeuralBody [25] reconstructs per frame\u2019s neural radiance field conditioned at body structured latent codes, which are diffused to the whole space by the SparseConvNet.",
            "9": "We perform this experiment on ZJU-Mocap [25] and our custom data.",
            "10": "3 , our method with accurate SMPL meshes still outperforms NeuralBody [25] and AnimatableNeRF [24]."
        },
        "Implicit Epipolar Geometric Function Based Light Field Continuous Angular Representation": {
            "authors": [
                "Lin Zhong",
                "Bangcheng Zong",
                "Qiming Wang",
                "Junle Yu",
                "Wenhui Zhou"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023W/LFNAT/papers/Zhong_Implicit_Epipolar_Geometric_Function_Based_Light_Field_Continuous_Angular_Representation_CVPRW_2023_paper.pdf",
            "ref_texts": "[19] Ren Ng, Marc Levoy, Mathieu Br \u00b4edif, Gene Duval, Mark Horowitz, and Pat Hanrahan. Light field photography with a hand-held plenoptic camera . PhD thesis, Stanford University, 2005. 1[20] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021. 2",
            "ref_ids": [
                "19",
                "20"
            ],
            "1": "Introduction Light field imaging [12, 16, 19], recording 4D spatialangular information of incident light rays, is always an important research hotspot in the field of computational imaging [32].",
            "2": "[20] proposed a novel human body representation that assumes that the learned neural representations at different frames share the same set of latent codes anchored to a deformable mesh.",
            "3": "1, 2\n[19] Ren Ng, Marc Levoy, Mathieu Br \u00b4edif, Gene Duval, Mark Horowitz, and Pat Hanrahan."
        },
        "Boosting view synthesis with residual transfer": {
            "authors": [
                "Xuejian Rong",
                "Bin Huang",
                "Ayush Saraf",
                "Changil Kim",
                "Johannes Kopf"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Rong_Boosting_View_Synthesis_With_Residual_Transfer_CVPR_2022_paper.pdf",
            "ref_texts": "[27] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 2",
            "ref_ids": [
                "27"
            ]
        },
        "LiveHand: Real-time and Photorealistic Neural Hand Rendering": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2302.07672",
            "ref_texts": "[30] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021. 2, 3",
            "ref_ids": [
                "30"
            ],
            "1": "Some works have extended these formulations beyond static scenes to enable photorealistic renderings of articulated objects such as the human body [38, 30, 26, 18, 28, 42, 10, 9].",
            "2": "The literature contains works for modeling other animatable objects such as the human face [20, 4, 44, 8, 7, 22], human body [10, 41, 3, 18, 42, 30, 38], and animals [21].",
            "3": "For example, it has been used to model the geometry and appearance of clothed humans [38, 30, 26, 18, 28, 42, 9, 11, 29, 12]."
        },
        "Posesdf: Simultaneous 3d human shape reconstruction and gait pose estimation using signed distance functions": {
            "authors": [
                "Jianxin Yang",
                "Yuxuan Liu",
                "Xiao Gu",
                "Zhong Yang",
                "Yao Guo"
            ],
            "url": "https://spiral.imperial.ac.uk/bitstream/10044/1/97128/2/ICRA22_0952_FI.pdf",
            "ref_texts": "[16] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-nition (CVPR) , June 2021, pp. 9054\u20139063.",
            "ref_ids": [
                "16"
            ],
            "1": "Recently, extensive works built upon SDF havebeen developed for human shape reconstruction [16], [17].",
            "2": "Recent work has successfully applied implicit neural representations on reconstruction, view-synthesis, and registra-tion of humans [16], [25], [26], [27].",
            "3": "[16] S."
        },
        "Arah: Animatable volume rendering of articulated human sdfs": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2210.10036",
            "ref_texts": "60. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proc. of CVPR (2021) 2, 3, 4, 9, 10, 13, 21, 25, 27",
            "ref_ids": [
                "60"
            ],
            "1": "Neural Body [60] and Ani-NeRF [58], struggle with generalizing to unseen poses, our approach enables avatars that can be animated in extreme out-of-distribution poses.",
            "2": "(2) Existing approaches condition their NeRF networks [60] or canonicalization networks [58] on inputs in observation space.",
            "3": "We validate our approach on the ZJU-MoCap [60] and the H36M [26] dataset.",
            "4": "Clothed Humans as Implicit Functions: Neural implicit functions [13, 44, 45, 55, 61] have been used to model clothed humans from various sensor inputs including monocular images [22,23,25,33,64\u201366,72,80,93], multi-view videos [30, 38, 52, 58, 60, 81], sparse point clouds [6, 14, 16, 77, 78, 94], or 3D meshes [11, 12, 15,47,48,67,74].",
            "5": "[38, 52, 58, 60, 81] take multi-view videos as inputs and do not need ground-truth geometry during training.",
            "6": "Neural Rendering of Animatable Clothed Humans: Differentiable neural rendering has been extended to model animatable human bodies by a number of recent works [52, 58, 60, 63, 72, 81].",
            "7": "Neural Body [60] proposes to diffuse latent per-vertex codes associated with SMPL meshes in observation space and condition NeRF [49] on such latent codes.",
            "8": "4 Experiments We validate the generalization ability and reconstruction quality of our proposed method against several recent baselines [58, 60, 72].",
            "9": "As was done in [60], we consider a setup with 4 cameras positioned equally spaced around the human subject.",
            "10": "Datasets: We use the ZJU-MoCap [60] dataset as our primary testbed because its setup includes 23 cameras which allows us to extract pseudo-ground-truth geometry to evaluate our model.",
            "11": "We use the training/testing splits from Neural Body [60] for both the cameras and the poses.",
            "12": "Note that we refrain from using the masks provided by Neural Body [60] as these masks are noisy and insufficient for accurate static scene reconstruction.",
            "13": "Baselines: We compare against three major baselines: Neural Body [60](NB), Ani-NeRF [58](AniN), and A-NeRF [72](AN).",
            "14": "We report LPIPS [91] on synthesized images under unseen poses from the testset of the ZJU-MoCap dataset [60] (i.",
            "15": "We report L2 Chamfer Distance (CD) and Normal Consistency (NC) on the training poses of the ZJUMoCap dataset [60].",
            "16": "We report PSNR, SSIM, and LPIPS [91] for novel views of training poses of the ZJU-MoCap dataset [60].",
            "17": "Note that we crop slightly larger bounding boxes than Neural Body [60] to better capture loose clothes, e.",
            "18": "We use the resulting model as the initialization for our per-subject optimization on the ZJU-MoCap [60] dataset.",
            "19": "For inference, we follow [58, 60] and crop an enlarged bounding box around the projected SMPL mesh on the image plane and render only pixels inside the bounding box.",
            "20": "For unseen test poses we follow the practice of [58, 60] and use the latent code Zof the last training frame as the input.",
            "21": "Neural Body [60], Ani-NeRF [58], and A-NeRF [72].",
            "22": "A simple uniform sampling strategy (as used in [58, 60]) produces stratified artifacts due to the discretized sampling.",
            "23": "G Additional Quantitative Results We present complete evaluation metrics including PSNR, SSIM, LPIPS on the test poses of the ZJU-MoCap [60] dataset in Table G."
        },
        "DRaCoN--Differentiable Rasterization Conditioned Neural Radiance Fields for Articulated Avatars": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2203.15798",
            "ref_texts": "[26] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 2, 3, 5, 6, 7, 8",
            "ref_ids": [
                "26"
            ],
            "1": "The state-of-the-art methods in this direction use implicit functions [31, 33] to model human avatars and use volumetric rendering for image generation [14,25,26,39].",
            "2": "The most recent methods take inspiration from NeRF [21] or its variants [3,13,24,43,50] and represent human avatars using pose-conditioned implicit 3D representations [11, 14, 25, 26, 42].",
            "3": "NeuralBody [26] exploits the SMPL body model and learn latent codes corresponding to each vertex.",
            "4": "In contrast to existing methods [25, 26, 39] which encode all appearance and geometric details in the NeRF module using a highly non-linear mapping, our approach learns these details in a well-defined UV-space thereby simplifies neural avatar learning.",
            "5": "Note that using SDF to represent body surface has the advantage that it is interpretable and the geometry can be extracted easily without the need of a careful selection of threshold for density values as used in existing NeRF-based neural avatar methods [25, 26, 39].",
            "6": "Dataset ZJU MoCap [26]: The ZJU MoCap dataset consists of actors performing various activities captured in 23 videos.",
            "7": "In all our settings, we train our model on 4 views (as in [26]) on 300 frames.",
            "8": "NeuralBody [26].",
            "9": "5 show the performance of our method compared to above methods on the ZJU-MoCap and Hu6 A-NeRF Animatable Neural Ours Ground A-NeRF Animatable Neural Ours Ground [39] NeRF [25] Body [26] Truth [39] NeRF [25] Body [26] Truth Figure 3.",
            "10": "ZJU-315 ZJU-377 User Study SSIM\"PSNR\" LPIPS# SSIM\"PSNR\" LPIPS# Num V otes Mean preference Alexnet VGG Alexnet VGG A-NeRF [39] 0:930 17 :711 0 :094 0 :073 0:927 16 :528 0 :101 0 :077 53 7 :5\u00061:5% Anim-NeRF [25] 0:917 16 :774 0 :101 0 :083 0:940 17 :971 0 :075 0 :066 15 2 :5\u00060:64% NeuralBody [26] 0:947 18:608 0 :092 0 :090 0:950 20:072 0 :071 0 :060 168 24\u00063:25% Ours 0:922 21 :315 0 :053 0 :046 0:946 21 :084 0 :048 0 :044 464 66\u00064% Table 1.",
            "11": "7 NHR [45] NeuralBody [26] Animatable-NeRF [25] Ours GT Figure 5.",
            "12": "1) of the study indicate that our method is preferred 66 % of the time as compared to the the next highest rated method (NeuralBody [26]) at 24%.",
            "13": "MSE # SSIM \" PSNR \" LPIPS # Alexnet VGG A-NeRF [39] 2:34 0 :946 26 :47 0 :082 0 :058 NHR [45] 0:82 0 :976 27 :91 0 :026 0 :024 NeuralBody [26] 0:66 0 :977 27 :93 0 :039 0 :029 Anim-NeRF [25] 0:50 0 :979 28 :11 0 :035 0 :028 Ours 0:32 0 :985 29 :42 0 :023 0 :022 Table 2."
        },
        "Dynamic multi-view scene reconstruction using neural implicit surface": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2303.00050",
            "ref_texts": "[17] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in CVPR , 2021.",
            "ref_ids": [
                "17"
            ],
            "1": "Although these methods [13, 14, 15, 16, 17, 18] have demonstrated impressive efficacy in view synthesis and reconstruction in terms of the human body, the requirement of human priors th viewMLP Hyper -coordinates \n NetworkSE3 Deformation Network SDF Network MLPRadiance Network.",
            "2": "Comparison Since most of state-of-the-art multi-view dynamic reconstruction approaches are template-based, we have evaluated Neural Body [17], A-Nerf [13] and AniSDF [14] on both mentioned datasets as they are also human-driven dynamic scenes, following the official codes and instructions.",
            "3": "While Neural Body [17] fails to recover objects beyond human bodies and loose dresses, our method is robust to complicated motions with topological changes (e.",
            "4": "Following previous works [17, 14], we set the values of the background pixels as zero."
        },
        "Data-driven 3D reconstruction of dressed humans from sparse views": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2104.08013",
            "ref_texts": "[42] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE/CVF Conference on Computer Vision and Pattern Recognition , page to appear, 2021. 4323",
            "ref_ids": [
                "42"
            ],
            "1": "They are usually scene specific although some limitations have been explored in recent work [34, 62, 50, 42]."
        },
        "Progressively-connected Light Field Network for Efficient View Synthesis": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2207.04465",
            "ref_texts": "[41] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021. Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. In CVPR .",
            "ref_ids": [
                "41"
            ],
            "1": "Recent works [1,14,22,24,28\u201334,36,40, 41,48,53,54,58,61,64] show that we are able to learn neural scene representations for the NVS task."
        },
        "Structured 3d features for reconstructing relightable and animatable avatars": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2212.06820",
            "ref_texts": "[53] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 1, 3, 7",
            "ref_ids": [
                "53"
            ],
            "1": "Recently, research on implicit representations [6, 17, 26, 61, 62] and neural fields [29, 53, 72, 81] has made significant progress in improving the realism of avatars.",
            "2": "NeRFs have been recently explored for novel human view synthesis [11, 15, 30, 52, 53, 64, 69, 70, 72].",
            "3": "We evaluate our method for monoc-Chamfer \u2193IoU\u2191NC\u2191PSNR\u2191SSIM\u2191LPIPS\u2193Train time NeuralBody [53] 0.",
            "4": "Here, we compare against two such recent works, NeuralBody [53] and H-Nerf [72], on the GHS3D dataset [72]."
        },
        "Animatable implicit neural representations for creating realistic avatars from videos": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2203.08133",
            "ref_texts": "[17] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in CVPR , 2021.[18] M. Habermann, W. Xu, M. Zollhofer, G. Pons-Moll, and C. Theobalt, \u201cDeepcap: Monocular human performance capture using weak supervision,\u201d in CVPR , 2020.",
            "ref_ids": [
                "17",
                "18"
            ],
            "1": "6M [16], ZJUMoCap [17] and MonoCap [18], [19], [20] datasets.",
            "2": "Third, we compare with more recent methods and additionally perform experiments on a monocular dataset [18], [19].",
            "3": "However, they have difficulty in recovering reasonable 3D human shapes when the camera views are too sparse, as shown in [17].",
            "4": "In the field of human modeling, [17], [69], [70], [71], [72] represent 3D human models as implicit neural representations and optimize network parameters from images with differentiable volume rendering.",
            "5": "[17] combines neural radiance field with the SMPL model, allowing it to handle dynamic humans and synthesize photorealistic novel views from very sparse camera views.",
            "6": "MonoCap is created by [20], which consists of two videos from DeepCap dataset [18] and two videos from DynaCap dataset [19], which are captured by dense camera views and provide the human masks and 3D human poses.",
            "7": "ZJU-MoCap [17] records 9 multi-view videos with 21 cameras and collects human poses using the marker-less motion capture system.",
            "8": "Following the experimental protocol in [17], we select four uniformly distributed cameras as training input and test on the remaining cameras.",
            "9": "807 NB [17] 23.",
            "10": "2 Performance on image synthesis Our method is compared with [8], [17], [55], [89] that train a separate network for each video.",
            "11": "Table 1 compares our method with [8], [17], [55], [89] on image synthesis.",
            "12": "[17] anchors a set of latent codes to the SMPL model and regresses a neural radiance field from these latent codes.",
            "13": "For complex human poses, [17], [55] tend to generate distorted rendering results.",
            "14": "Although [17] synthesizes high-quality images on training poses, it struggles to give reasonable rendering results on novel human poses.",
            "15": "Table 2 summarizes the quantitative comparison of novel view synthesis between our methods and [89], [17], [55] under training and novel human pose.",
            "16": "And the pose-dependent deformationTraining poses Novel poses PSNR\" SSIM\" PSNR\" SSIM\" NB [17] 21.",
            "17": "field methods NeRF-PDF and SDF-PDF outperforms [17] on monocular novel view synthesis.",
            "18": "When trained on 4 camera views, our method is competitive to [17] on novel view synthesis of JOURNAL OF LATEX CLASS FILES, VOL.",
            "19": "P2S# CD# D-NeRF\n[8]NB [17] A-NeRF\n[89]NeRFNBWNeRFPDFSDFPDFD-NeRF\n[8]NB [17] A-NeRF\n[89]NeRFNBWNeRFPDFSDFPDF S1 3.",
            "20": "4 Views Single view Training poses Novel poses Training poses Novel poses PSNR\"SSIM\"PSNR\"SSIM\"PSNR\"SSIM\"PSNR\"SSIM\" NB [17] 28.",
            "21": "training poses and outperforms [17] on novel pose synthesis.",
            "22": "When trained on the single camera view, our method significantly outperforms [17] on both training and novel poses.",
            "23": "3 Performance on 3D reconstruction To validate our method on 3D reconstruction, we compare with [8], [17], [89].",
            "24": "Table 3 compares our method with [17], [21], [89] in terms of the P2S andCD metrics.",
            "25": "[17], [21], [89] and our \u201cNeRF-NBW\u201d, \u201cNeRFPDF\u201d representations model the human geometry with the volume density field, while our \u201cSDF-PDF\u201d representation adopt the signed distance field.",
            "26": "We empirically set the threshold of volume density to extract the geometry of [17], [21], [89] and our \u201cNeRF-NBW\u201d, \u201cNeRF-PDF\u201d.",
            "27": "Our representation \u201cSDF-PDF\u201d significantly outperforms baseline methods [17], [21], [89] by a margin of at least 0.",
            "28": "To explore the performance of our method on loose clothing, we collect two image sequences from [18] and [75].",
            "29": "The image sequence from [18] is selected from 700-th frame to 1000-th frame of the subject \u201cMagdalena\u201d, which performs arbitrary motions and has complex non-rigid cloth deformations.",
            "30": "[17] S.",
            "31": "[18] M.",
            "32": "1 Baseline methods We compare with state-of-the-art image synthesis methods [8], [17], [55], [89] that also utilize SMPL priors.",
            "33": "2) Neural body [17] anchors a set of latent codes on the vertices of SMPL and uses a network to regress neural radiance fields from the latent codes, which are then rendered into images using volume rendering.",
            "34": "It consists of two videos \u201cLan\u201d and \u201cMarc\u201d from DeepCap dataset [18], and two videos \u201cOlek\u201d and \u201cVlad\u201d from DynaCap dataset [19].",
            "35": "ZJU-MoCap [17].",
            "36": "3 Evaluation metrics We follow [17] to calculate the metrics of image synthesis.",
            "37": "Table 11 compares the number of network parameters ofNHR\n[55]D-NeRF\n[8]NB\n[17]A-NeRF\n[89]NeRFNBWNeRFPDFSDFPDF Params.",
            "38": "Our model has fewer parameters than [17], [55].",
            "39": "Our method has a smaller model size than NHR [55] and Neural Body [17]."
        },
        "3DMM-RF: Convolutional Radiance Fields for 3D Face Modeling": {
            "authors": [
                "Stathis Galanakis",
                "Baris Gecer",
                "Alexandros Lattas",
                "Stefanos Zafeiriou"
            ],
            "url": "https://openaccess.thecvf.com/content/WACV2023/papers/Galanakis_3DMM-RF_Convolutional_Radiance_Fields_for_3D_Face_Modeling_WACV_2023_paper.pdf",
            "ref_texts": "[62] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021.",
            "ref_ids": [
                "62"
            ],
            "1": "Other approaches focus on 3D shape reconstruction [89, 55, 83, 3], human bodies registration [62, 87, 61], dealing with non-static scenes [63, 56], scene editing [49, 36] and scene relighting [72, 9]."
        },
        "Multi-view Human Body Mesh Translator": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2210.01886",
            "ref_texts": "[33] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021.",
            "ref_ids": [
                "33"
            ]
        },
        "Recovery of continuous 3d refractive index maps from discrete intensity-only measurements using neural fields": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2112.00002",
            "ref_texts": "[38] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , DOI:10.1109/CVPR46437.2021.00894, 2021.",
            "ref_ids": [
                "38"
            ],
            "1": "Prior applications of NF include novel view synthesis [28, 31, 60], dynamic scene representation [24,35,38], object lightning [45,57], and computed tomography [40].",
            "2": "[38] S."
        },
        "Neural Puppeteer: Keypoint-Based Neural Rendering of Dynamic Shapes": {
            "authors": [
                "Simon Giebenhain",
                "Urs Waldmann",
                "Ole Johannsen",
                "Bastian Goldluecke"
            ],
            "url": "https://openaccess.thecvf.com/content/ACCV2022/papers/Giebenhain_Neural_Puppeteer_Keypoint-Based_Neural_Rendering_of_Dynamic_Shapes_ACCV_2022_paper.pdf",
            "ref_texts": "43. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: CVPR (2021)",
            "ref_ids": [
                "43"
            ],
            "1": "Neural Body [43]\n2832\n\n4 S."
        },
        "Coordx: Accelerating implicit neural representation with a split mlp architecture": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2201.12425",
            "ref_texts": "10 Ishit Mehta, Micha \u00a8el Gharbi, Connelly Barnes, Eli Shechtman, Ravi Ramamoorthi, and Manmohan Chandraker. Modulated periodic activations for generalizable local functional representations. arXiv preprint arXiv:2104.03960 , 2021. Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 4460\u20134470, 2019. Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European conference on computer vision , pp. 405\u2013421. Springer, 2020. Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas Kurz, Joerg H Mueller, Chakravarty R Alla Chaitanya, Anton Kaplanyan, and Markus Steinberger. Donerf: Towards real-time rendering of compact neural radiance fields using depth oracle networks. arXiv preprint arXiv:2103.03231 , 2021. Michael Niemeyer and Andreas Geiger. Giraffe: Representing scenes as compositional generative neural feature fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 11453\u201311464, 2021. Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 165\u2013174, 2019. Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Steven M Seitz, and Ricardo Martin-Brualla. Deformable neural radiance fields. arXiv preprint arXiv:2011.12948 , 2020. Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, and Steven M Seitz. Hypernerf: A higher-dimensional representation for topologically varying neural radiance fields. arXiv preprint arXiv:2106.13228 , 2021. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems , 32: 8026\u20138037, 2019. Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 9054\u20139063, 2021. Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. Convolutional occupancy networks. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part III 16 , pp. 523\u2013540. Springer, 2020. Stephan Rabanser, Oleksandr Shchur, and Stephan G \u00a8unnemann. Introduction to tensor decompositions and their applications in machine learning. arXiv preprint arXiv:1711.10781 , 2017. Ravi Ramamoorthi and Pat Hanrahan. An efficient representation for irradiance environment maps. InProceedings of the 28th annual conference on Computer graphics and interactive techniques , pp. 497\u2013500, 2001. Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps. arXiv preprint arXiv:2103.13744 , 2021. Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, and Hao Li. Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 2304\u20132314, 2019. Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. Graf: Generative radiance fields for 3d-aware image synthesis. arXiv preprint arXiv:2007.02442 , 2020."
        },
        "PhoMoH: Implicit Photorealistic 3D Models of Human Heads": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2212.07275",
            "ref_texts": "[50] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE Conf. Comput. Vis. Pattern Recog. , 2021. 3",
            "ref_ids": [
                "50"
            ],
            "1": "NeRF-based models have been also used for personalization, either to represent the full person [50, 67, 41, 39, 64] or by focusing on the head [24, 47, 5]."
        },
        "RANA: Relightable Articulated Neural Avatars": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2212.03237",
            "ref_texts": "[39] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural Body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 2, 3, 7",
            "ref_ids": [
                "39"
            ],
            "1": "More recent methods learn a neural representation of the person and use neural renderers [49] to directly generate photorealistic images in 2 novel view novel pose generalizable relightable Method X NeuralBody [39], HumanNeRF [54] X X AnimatableNeRf [38], NeuMan [26] X X X ANR [40], TNA [44], StylePeople [18] X X Relighting4D [13] X X X X RANA (Ours) Table 1.",
            "2": "The 3D neural rendering methods represent the person using neural radiance fields [35] and render the target images using volume rendering [26, 38, 39, 50, 54].",
            "3": "Thanks to the design of RANA, we can pretrain on as many subjects as available, which is not possible with most of the state-of-the-art methods for human synthesis [38,39,50]."
        },
        "OhMG: Zero-shot Open-vocabulary Human Motion Generation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2210.15929",
            "ref_texts": "[29] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021. 3",
            "ref_ids": [
                "29"
            ],
            "1": "This powerful representation ability of foundation model has led to 2 the emergence of zero-shot text-driven applications [?, 9, 26,29], including 3D meshes generation [15,16,22,28,39]."
        },
        "Dialoguenerf: Towards realistic avatar face-to-face conversation video generation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2203.07931",
            "ref_texts": "[36] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2021, pp. 9054\u20139063.",
            "ref_ids": [
                "36"
            ],
            "1": "[36] add human pose parameters as additional input to synthesize deformable human body.",
            "2": "[36] S."
        },
        "Deep Review and Analysis of Recent NeRFs": {
            "authors": [
                "Fang Zhu",
                "Shuai Guo",
                "Li Song",
                "Ke Xu",
                "Jiayu Hu"
            ],
            "url": "https://www.nowpublishers.com/article/OpenAccessDownload/SIP-2022-0062",
            "ref_texts": "[66]S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2021, 9054\u201363.",
            "ref_ids": [
                "66"
            ],
            "1": "In particular, implicit representation based on ML has become a hot topic of current research and has been widely discussed, such as in these works [48, 66, 73].",
            "2": "[66]S."
        },
        "Compositional 3D Human-Object Neural Animation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2304.14070",
            "ref_texts": "[45] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. In2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 9050\u20139059, Nashville, TN, USA, June 2021. IEEE. 1, 3, 5",
            "ref_ids": [
                "45"
            ],
            "1": "To explore dynamic non-rigid scenes and objects, vanilla NeRF has been recently extended to handle deforming scenes [42, 55, 45, 43] and motion modeling [29, 63, 46].",
            "2": "Animatable Avatars 3D Avatars [34, 45, 59, 28, 5, 72] have been through a significant progress.",
            "3": "[45] present to implicitly reconstruct human body from spare videos with neural radiance fields with a carefully designed skinning deformation.",
            "4": ", BEHA VE [4], ZJU-mocap [45] and CO3D [48] for the compositional human-object neural animation.",
            "5": "ZJU-mocap [45] consists of 10 sequences captured with 23 calibrated cameras."
        },
        "AG3D: Learning to Generate 3D Avatars from 2D Image Collections": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2305.02312",
            "ref_texts": "[53] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021. 3",
            "ref_ids": [
                "53"
            ],
            "1": "Several methods [13, 17, 27, 32, 53, 62, 63, 72] combine NeRF with human priors to enable 3D human reconstruction from multi-view data or even monocular videos."
        },
        "ActorsNeRF: Animatable Few-shot Human Rendering with Generalizable NeRFs": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2304.14401",
            "ref_texts": "[33] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , pages 9054\u20139063, 2021. 1, 2, 5",
            "ref_ids": [
                "33"
            ],
            "1": "However, to achieve high-quality rendering, existing approaches [33, 23, 32, 29, 42] require a combination of synchronized multi-view videos and an instance-level NeRF network, trained on a specific human video sequence.",
            "2": "We quantitatively and qualitatively demonstrate that ActorsNeRF outperforms the existing approaches by a large margin on various few-shot settings for both ZJU-MoCap Dataset [33] and AIST++ Dataset [21].",
            "3": "Recently, NeRF-based human representations have shown promise for high-quality view synthesis [33, 32, 50, 29, 19, 23, 42, 48, 55, 15, 10, 20, 45, 40, 41].",
            "4": "[33] propose to attach a set of latent codes to SMPL [24] and render novel views of a performer from sparse multi-view videos.",
            "5": "Similar ideas were used by [33, 19].",
            "6": ", ZJU-MoCap dataset [33] and AIST++ dataset [21], and ActorsNeRF significantly outperforms multiple representative state-of-the-art baselines.",
            "7": "We test ActorsNeRF on two datasets: ZJUMoCap [33] dataset and AIST++ dataset [21].",
            "8": "NeuralBody [33] is a representative method for rendering from observation space."
        },
        "Multimodal neural radiance field": {
            "authors": [],
            "url": "https://assets.amazon.science/a6/3d/ee31b6e24eec89e428f5ec4ad849/multimodal-neural-radiance-field.pdf",
            "ref_texts": "[9] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2021, pp. 9054\u20139063.",
            "ref_ids": [
                "9"
            ],
            "1": "Based on NeRF, researchers havedeveloped different methods for image and video-related tasks, such as video synthesis and animation [6], [7], [8], [9], model reconstruction [10], [11], etc.",
            "2": "[9] S."
        },
        "VINECS: Video-based Neural Character Skinning": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2307.00842",
            "ref_texts": "[45] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. CVPR , 1(1):9054\u20139063, 2021. 2",
            "ref_ids": [
                "45"
            ],
            "1": "The focus of this work is notto model deformable geometry [15, 14, 30, 19] or photorealistic appearance [32, 45, 44, 13] of humans but on learning 3D characters with pose-dependent skinning from 2D observations only."
        },
        "Generalizable Neural Voxels for Fast Human Radiance Fields": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2303.15387",
            "ref_texts": "[60] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 1, 2, 4, 5, 6, 7, 8, 12, 13",
            "ref_ids": [
                "60"
            ],
            "1": "Some works [60, 86, 72, 12, 59, 58, 15, 33, 89] successfully apply NeRF methods to human body rendering frameworks.",
            "2": "Neural Body [60] proposes to compute latent code volumes by inferring mesh vertices but performs poorly on unseen poses.",
            "3": "Representing Canonical Bodies with Neural Voxels Most previous NeRF-based methods for human bodies [60, 86, 72, 12, 59, 58, 15, 33, 89] adopt purely implicit representations.",
            "4": "Quantitative comparisons on the ZJU-MoCap dataset [60].",
            "5": "Subject 377 Subject 386 Subject 387 PSNR\"SSIM\"LPIPS (\u000210\u00002)#PSNR\"SSIM\"LPIPS (\u000210\u00002)#PSNR\"SSIM\"LPIPS (\u000210\u00002)# Neural Body [60] 29.",
            "6": "028 Subject 392 Subject 393 Subject 394 PSNR\"SSIM\"LPIPS (\u000210\u00002)#PSNR\"SSIM\"LPIPS (\u000210\u00002)#PSNR\"SSIM\"LPIPS (\u000210\u00002)# Neural Body [60] 30.",
            "7": "Comparisons about training cost and rendering quality on ZJU-MoCap dataset [60].",
            "8": "Method Pretrain Dataset Perscene Iterations Time PSNR\"SSIM\"LPIPS (\u000210\u00002)# Neural Body [60] 7 \u2013 \u2013 29.",
            "9": "Qualitative comparisons between HumanNeRF [98] and our GNeuV ox on the ZJU-MoCap [60] dataset.",
            "10": "The model is fine-tuned for 1kiterations when pretrained on the ZJU-MoCap dataset [60] and for 3kiterations when pretrained on Human3.",
            "11": "Evaluation Training from Scratch When training from scratch, We only use the images taken by \u201c camera 1 \u201d in ZJUMoCap [60] as training for the monocular setting.",
            "12": "The comparison results of our method, Neural body [60], and HumanNeRF are shown in Tab.",
            "13": "Change process of general voxels during pretraining on the ZJU-MoCap [60] dataset.",
            "14": "GNeuV ox-ZJU and GNeuV ox-H36m represent the finetuning results after pretraining on the ZJU-MoCap [60] and Human3.",
            "15": "Method Time PSNR\" Neural Body [60] \u001814 hours 24.",
            "16": "3, and the results of Neural Body [60] and Anim-NeRF [12] are from Anim-NeRF.",
            "17": "Ablation study about fine-tuning iterations with the pretrained model on the ZJU-MoCap dataset [60].",
            "18": "GNeuV ox-ZJU and GNeuV ox-H36m denote the fine-tuning results with pretraining on the ZJU-MoCap [60] and Human3.",
            "19": "6M [30] for different iterations and then fine-tune it on the ZJU-MoCap [60] dataset for 3kiterations to explore the impact of pretraining iterations on fine-tuning results."
        },
        "Text-guided 3D Human Generation from 2D Collections": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2305.14312",
            "ref_texts": "2018. Which Training Methods for GANs do actually Converge? In International Conference on Machine Learning (ICML) . Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. 2023. Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures. In Conference on Computer Vision and Pattern Recognition (CVPR) . Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and Rana Hanocka. 2022. Text2Mesh: Text-Driven Neural Stylization for Meshes. In Conference on Computer Vision and Pattern Recognition (CVPR) . Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. 2020. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. In European Conference on Computer Vision (ECCV) . MMHuman3D. 2021. OpenMMLab 3D Human Parametric Model Toolbox and Benchmark. https://github.com/ open-mmlab/mmhuman3d . Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. 2022. Instant Neural Graphics Primitives with a Multiresolution Hash Encoding. In Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH) . Gimin Nam, Mariem Khlifi, Andrew Rodriguez, Alberto Tono, Linqi Zhou, and Paul Guerrero. 2022. 3D-LDM: Neural Implicit 3D Shape Generation with Latent Diffusion Models. In arXiv:2212.00842 . Charlie Nash, Yaroslav Ganin, Ali Eslami, and Peter W. Battaglia. 2020. PolyGen: An Autoregressive Generative Model of 3D Meshes. In International Conference on Machine Learning (ICML) . Atsuhiro Noguchi, Xiao Sun, Stephen Lin, and Tatsuya Harada. 2022. Unsupervised Learning of Efficient Geometry-Aware Neural Articulated Representations. In European Conference on Computer Vision (ECCV) . Roy Or-El, Xuan Luo, Mengyi Shan, Eli Shechtman, Jeong Joon Park, and Ira Kemelmacher-Shlizerman. 2022. StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation. In Conference on Computer Vision and Pattern Recognition (CVPR) . Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. 2019. DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation. In Conference on Computer Vision and Pattern Recognition (CVPR) . Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Ahmed A Timo Bolkart, A. Osman, Dimitrios Tzionas, and Michael J. Black. 2019. Expressive Body Capture: 3D Hands, Face, and Body from a Single Image. In Conference on Computer Vision and Pattern Recognition (CVPR) . Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021a. Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. In International Conference on Computer Vision (ICCV) . Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021b. Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. InConference on Computer Vision and Pattern Recognition (CVPR) . Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. 2023. DreamFusion: Text-to-3D using 2D Diffusion. InInternational Conference on Learning Representations (ICLR) . Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From Natural Language Supervision. In International Conference on Machine Learning (ICML) . Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical Text-Conditional Image Generation with CLIP Latents. In arXiv:2204.06125 . Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-Shot Text-to-Image Generation. In International Conference on Machine Learning (ICML) . Ren\u00e9 Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. 2020. Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Crossdataset Transfer. In Transactions on Pattern Analysis and Machine Intelligence (TPAMI) . Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. 2016. Generative Adversarial Text to Image Synthesis. In International Conference on Machine Learning (ICML) .Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. 2023. TEXTure: Text-Guided Texturing of 3D Shapes. In Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH) . Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. 2022. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. In Conference on Neural Information Processing Systems (NeurIPS) . Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. 2021. GRAF: Generative Radiance Fields for 3DAware Image Synthesis. In Conference on Neural Information Processing Systems (NeurIPS) . Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon Ko, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung Lee, and Seungryong Kim. 2023. Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation. In arXiv:2303.07937 . Aliaksandra Shysheya, Egor Zakharov, Kara-Ali Aliev, Renat Bashirov, Egor Burkov, Karim Iskakov, Aleksei Ivakhnenko, Yury Malkov, Igor Pasechnik, Dmitry Ulyanov, Alexander Vakhitov, and Victor Lempitsky. 2019. Textured Neural Avatars. In Conference on Computer Vision and Pattern Recognition (CVPR) . Vincent Sitzmann, Julien N. P. Martel, Alexander W. Bergman, David B. Lindell, and Gordon Wetzstein. 2020. Implicit Neural Representations with Periodic Activation Functions. InConference on Neural Information Processing Systems (NeurIPS) . Ivan Skorokhodov, Aliaksandr Siarohin, Yinghao Xu, Jian Ren, Hsin-Ying Lee, Peter Wonka, and Sergey Tulyakov.",
            "ref_ids": [
                "2018"
            ]
        },
        "A Portable Multiscopic Camera for Novel View and Time Synthesis in Dynamic Scenes": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2208.14433",
            "ref_texts": "[28] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in CVPR , 2021.",
            "ref_ids": [
                "28"
            ],
            "1": "Many subsequent works were used to improve NeRF\u2019s representation ability and expand the scope of application in terms of generalization ability [23], dynamic and deformable scenes synthesis [24], applications under extreme environments [25], improving training and rendering efficiency [26], reconstruction on specific shape domains [27], [28] and so on.",
            "2": "[28] S."
        },
        "IntrinsicNGP: Intrinsic Coordinate based Hash Encoding for Human NeRF": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2302.14683"
        },
        "HM3D-ABO: A Photo-realistic Dataset for Object-centric Multi-view 3D Reconstruction": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2206.12356",
            "ref_texts": "[28] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2021. 1",
            "ref_ids": [
                "28"
            ],
            "1": "Introduction Reconstructing 3D object has been studied for decades [3, 19, 21, 27, 28, 39, 43\u201345, 50] and has been receiving more and more interest due to the recent popularity AR/VR applications."
        },
        "NeTO: Neural Reconstruction of Transparent Objects with Self-Occlusion Aware Refraction-Tracing": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2303.11219",
            "ref_texts": "[32] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 3",
            "ref_ids": [
                "32"
            ],
            "1": "Recently, implicit neural representations have been applied to a variety of applications, including novel view synthesis [24, 57], camera pose estimation [17, 48], human [18, 32] and multi-view 3D reconstruction [11, 19, 20, 27, 28, 42, 46, 47, 54, 55], and achieved impressive successes."
        },
        "INV: Towards Streaming Incremental Neural Videos": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2302.01532",
            "ref_texts": "[32] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV , 2021. 2[33] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 2",
            "ref_ids": [
                "32",
                "33"
            ],
            "1": "Some recent works [15, 21, 32, 33, 40] focus on animating clothed humans only."
        },
        "Dynamic cone-beam CT reconstruction using spatial and temporal implicit neural representation learning (STINR)": {
            "authors": [
                "Tielige Mengke"
            ],
            "url": "https://arxiv.org/pdf/2209.11392"
        },
        "SynBody: Synthetic Dataset with Layered Human Models for 3D Human Perception and Modeling": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2303.17368",
            "ref_texts": "[30] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 2, 3, 8",
            "ref_ids": [
                "30"
            ],
            "1": "Introduction The fields of 3D human perception [15\u201318,26,39,40] and human reconstruction [10, 19, 29, 30] have become increasingly important, but the lack of available data has limited their development.",
            "2": "6M [11] R X 4 312K 839 11 15 3DJ, SMPL MPI-INF-3DHP [23] M X X 14 96K 16 8 8 3DJ\n3DPW [36] R X X 1 32K 60 18 * SMPL Panoptic Studio [14] R X 480 736K 480 \u0018100 * 3DJ EFT [13] R X 1 129K NA Many NA SMPL ZJU-MoCap [30] R X 21 180K 9 9 9 SMPL,mask SURREAL [35] S X X 1 6.",
            "3": "NeuralBody [30] incorporates prior from a statistical body template to learn dynamic sequence, while Animatable NeRF [29] proposes to reconstruct an animatable human model that generalizes to new poses.",
            "4": "ZJU-MoCap [30] captures 9 human subjects with 21 synchronized cameras, providing fitted human body model parameters as well as the foreground mask.",
            "5": "201 NeuralBody [30] 28.",
            "6": "total, including the vanilla NeRF [24], NeuralBody [30] and HumanNeRF [37] for novel view synthesis, AnimNeRF [29] for novel pose synthesis, NHP [19] for generalizable human NeRF (novel identity synthesis)."
        },
        "AvatarBooth: High-Quality and Customizable 3D Human Avatar Generation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2306.09864",
            "ref_texts": "(2023). Suyi Jiang, Haoran Jiang, Ziyu Wang, Haimin Luo, Wenzheng Chen, and Lan Xu. 2023a. HumanGen: Generating Human Radiance Fields with Explicit Priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. InProceedings of International Conference on Learning Representations (ICLR) . Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. 2023. Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation. arXiv preprint arXiv:2305.01569 (2023). Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. 2022. Magic3D: HighResolution Text-to-3D Content Creation. arXiv preprint arXiv:2211.10440 (2022). Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. 2015. SMPL: A Skinned Multi-Person Linear Model. ACM Trans. Graphics (Proc. SIGGRAPH Asia) 34, 6 (Oct. 2015), 248:1\u2013248:16. Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. 2022. Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures. arXiv preprint arXiv:2211.07600 (2022). B Mildenhall, PP Srinivasan, M Tancik, JT Barron, R Ramamoorthi, and R Ng. 2020. Nerf: Representing scenes as neural radiance fields for view synthesis. In European conference on computer vision . Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. 2019. Deepsdf: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition . 165\u2013174. Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. 2019. Expressive Body Capture: 3D Hands, Face, and Body from a Single Image. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) . 10975\u201310985. Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 9054\u20139063. Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. 2023. Dreamfusion: Text-to-3d using 2d diffusion. In Proceedings of the International Conference on Learning Representations (ICLR) . Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al ."
        },
        "Real-time volumetric rendering of dynamic humans": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2303.11898",
            "ref_texts": "[29] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proc. CVPR , 2021. 1, 2, 6, 7",
            "ref_ids": [
                "29"
            ],
            "1": "This has motivated the development of specialised models that can account for the highly-deformable structure of such objects, but most of these still assume that scenes are captured from multiple cameras [20,22,29,44], which is incompatible with Figure 1.",
            "2": "We show that adopting a factorized radiance field representation and a simple LBS-based deformation model allows for fast reconstruction (24times faster than HumanNeRFUW) with comparable or better rendering quality on the ZJU-Mocap [29] scenes.",
            "3": "In order to obtain higher quality reconstructions, several methods [20, 22, 29, 44] leverage multiple synchronized videos.",
            "4": "Neural Body [29] attaches neural codes to the SMPL vertices, poses them, and converts them to a full volumetric representation of the radiance field using a 3D sparse CNN [9, 13].",
            "5": "5\n(a) Ground-truth (b) NeuralBody [29] (c) HumanNeRF-UW [41] (d) Ours reconstruction (e) Ours real-time Figure 4.",
            "6": "We first evaluate our method on the ZJU-Mocap dataset introduced in [29].",
            "7": "We follow the evaluation protocol from [29] and, for each sequence, use images from 19 unseen cameras for testing, sampled at a 30 frames interval for the first 300 frames, giving 190 test images per sequence.",
            "8": "4 we present the quantitative and qualitative results of our method compared to NeuralBody [29] and HumanNeRF-UW [41]."
        },
        "Human Image Generation: A Comprehensive Survey": {
            "authors": [
                "J Z"
            ],
            "url": "https://arxiv.org/pdf/2212.08896",
            "ref_texts": "[105] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 9054\u20139063.",
            "ref_ids": [
                "105"
            ],
            "1": "Human Image Generation: A Comprehensive Survey 111:5 Human Image Generation MethodsSourcesResearch Paradigms Model-Generated Contents Applications KnowledgeGuidedDataDrivenHybridAttributesBackgroundPosesData AugmentationVirtual Try-on RandPerson [150] ACM MM 2020 X X X X X BiGraphGAN [137] BMVC 2020 X X X Unpaired SPT [133] TPAMI 2020 X X X X X PoseFlow [183] TIP 2020 X X X MR-Net [160] TIP 2020 X X X GFLA 2 [115] TIP 2020 X X X DCTON [37] CVPR 2021 X X X MUST-GAN [95] CVPR 2021 X X X X PISE [176] CVPR 2021 X X X X SPGNet [92] CVPR 2021 X X X UnrealPerson [179] CVPR 2021 X X X X X PF-AFN [38] CVPR 2021 X X X VITON-HD [23] CVPR 2021 X X X CT-Net [166] CVPR 2021 X X X X Pose-Guided Animation [171] CVPR 2021 X X X ReAVAE [17] CVPR 2021 X X X Few-Shot Motion Transfer [55] CVPR 2021 X X X StylePeople [40] CVPR 2021 X X X X Neural Body [105] CVPR 2021 X X X DiOr [26] ICCV 2021 X X X X ZFlow [24] ICCV 2021 X X X X M3D-VTON [182] ICCV 2021 X X X FashionMirror [18] ICCV 2021 X X X X STTEN [162] ICCV 2021 X X X X PASTA-GAN [159] NeurIPS 2021 X X X HumanGAN [122] 3DV 2021 X X X X X APATN [194] TPAMI 2021 X X X X Text-Guided Manipulation [163] TPAMI 2021 X X X Attentional Liquid Warping GAN [88] TPAMI 2021 X X X X X SPATT [86] TIP 2021 X X X X PoT-GAN [76] TIP 2021 X X X Part-Based Representation[156] TIP 2021 X X X DRN [169] TIP 2021 X X X ShaTure [173] TIP 2022 X X X X X CPF-Net [155] TIP 2021 X X X Pose with Style [2] ACM TOG 2021 X X X X X TryOnGAN [73] ACM TOG 2021 X X X TightCap [21] ACM TOG 2021 X X X X Neural Actor [83] ACM TOG 2021 X X X DPTN [178] CVPR 2022 X X X NTED [114] CVPR 2022 X X X X X SCM-Net [152] CVPR 2022 X X X X X RT-VTON [167] CVPR 2022 X X X ClothFormer [63] CVPR 2022 X X X StyleGAN Appearance Flow [48] CVPR 2022 X X X wFlow [31] CVPR 2022 X X X X BodyGAN [164] CVPR 2022 X X X X CASD [190] ECCV 2022 X X X X X SDAFN [8] ECCV 2022 X X X\n3D-SGAN [177] ECCV 2022 X X X X HR-VITON [72] ECCV 2022 X X X For a brief summary, the data-driven methods introduced in this section are summarized in Table 2, where the used generative models, loss functions and characteristics are presented.",
            "2": "Human Virtual Rendering SURREAL [143] SMPL Virtual Rendering ClothCap [109] SMPL Virtual Rendering Human Appearance Transfer [175] SMPL Video Based Reconstruction [4] SMPL Virtual Rendering SyRI [9] Virtual Rendering SOMAnet [12] Virtual Rendering PersonX [135] Virtual Rendering Octopus [3] SMPL Virtual Rendering Multi-Garment Net [13] SMPL Virtual Rendering Pix2Surf [98] SMPL Virtual Rendering RandPerson [150] Virtual Rendering UnrealPerson [179] Virtual Rendering Neural Body [105] SMPL Virtual Rendering to generate human bodies."
        },
        "FlexNeRF: Photorealistic free-viewpoint rendering of moving humans from sparse views": {
            "authors": [
                "Vinoj Jayasundara",
                "Amit Agrawal",
                "Nicolas Heron",
                "Abhinav Shrivastava",
                "Larry S. Davis"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Jayasundara_FlexNeRF_Photorealistic_Free-Viewpoint_Rendering_of_Moving_Humans_From_Sparse_Views_CVPR_2023_paper.pdf",
            "ref_texts": "[28] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans.",
            "ref_ids": [
                "28"
            ],
            "1": "Hence, most methods [27, 28, 41] begin with assuming SMPL template as a prior [18].",
            "2": "This helps to eliminate the halo effects [28,40] and provide sharper boundaries.",
            "3": "9003 FullNeural Body [28] 57.",
            "4": "9043 ZJU-MoCap [7, 28]HumanNeRF [40] 36.",
            "5": "9685 FullNeural Body [28] 52.",
            "6": "9765 Neural Body [28] 48.",
            "7": "Benchmark Datasets and Metrics We evaluate the proposed method on two public datasets: ZJU-MoCap [7, 28] and People Snapshot [2], and one SelfCaptured Fashion (SCF) dataset.",
            "8": "Results and Analysis Quantitative Results: Table 1 compares our method against HumanNeRF [40] and Neural-Body [28] across the three datasets."
        },
        "Total-Recon: Deformable Scene Reconstruction for Embodied View Synthesis": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2304.12317",
            "ref_texts": "[34] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2021. 3",
            "ref_ids": [
                "34"
            ],
            "1": "One group of work leverages human-specific priors [33, 46, 27, 34, 19, 12, 15, 32] such as human body models (e."
        },
        "ModalNeRF: Neural Modal Analysis and Synthesis for Free-Viewpoint Navigation in Dynamically Vibrating Scenes": {
            "authors": [],
            "url": "https://hal.science/hal-04131503/document"
        },
        "SHERF: Generalizable Human NeRF from a Single Image": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2303.12791",
            "ref_texts": "[59] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021.",
            "ref_ids": [
                "59"
            ],
            "1": "Introduction Human NeRFs aim to recover high-quality 3D humans from 2D observations, avoiding the need to capture ground truth 3D geometry information [59,13,58,79,55,76,69, 30,31,74,40,23,15,85].",
            "2": "The first category focuses on reconstructing 3D humans from monocular or multi-view videos [59,13,58, 79,55,76,69,30,31,74].",
            "3": "Neural Body [59] applies sparse convolutions to model the radiance volume, while others model human NeRF in the canonical space [13,58,79,55,76,69,30,31,74] using SMPL LBS weights or optimizing LBS weights with appearance.",
            "4": ", THuman [87], RenderPeople [1], ZJU_MoCap [59] and HuMMan [9]."
        },
        "MoDA: Modeling Deformable 3D Objects from Casual Videos": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2304.08279",
            "ref_texts": "[34] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body:Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021. 2, 3",
            "ref_ids": [
                "34"
            ],
            "1": "Others requiring synchronized multi-view video inputs [34, 33] are also not available to general users.",
            "2": "With the popularity of neural radiance fields [24], there are many works [20, 34, 33, 27, 46, 51, 8, 31, 30, 62, 3, 45, 15] learning to reconstruct the shape and appearance from images or videos with a NeRF-based template.",
            "3": "[20, 34, 33, 27, 46, 51, 8] were proposed to solve this problem."
        },
        "You Only Train Once: Multi-Identity Free-Viewpoint Neural Human Rendering from Monocular Videos": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2303.05835",
            "ref_texts": "[24] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 2, 5, 6",
            "ref_ids": [
                "24"
            ],
            "1": "Wepresent our experimental results on ZJU-MoCap [24] and PeopleSnapshot [1] to demonstrate that YOTO can competently handle hard cases (e.",
            "2": "ZJU-MoCap [24]) to verify the effectiveness of the proposed approach for human rendering under free-viewpoints with monocular videos.",
            "3": "Datasets To thoroughly evaluate the performance of YOTO and to fairly compare against the baseline, we use ZJUMoCap [24] dataset for quantitative evaluation."
        },
        "3D Scene Creation and Rendering via Rough Meshes: A Lighting Transfer Avenue": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2211.14823",
            "ref_texts": "[42] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021. 3",
            "ref_ids": [
                "42"
            ],
            "1": "Recent advances show neural fields representations are promising to describe scenes, and support rendering photorealistic images of the fitted scenes under desired viewpoints [4,5,10,11,17,29,33,35,37,42,44,51,52,55,66,69]."
        },
        "SupeRVol: Super-Resolution Shape and Reflectance Estimation in Inverse Volume Rendering": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2212.04968",
            "ref_texts": "[39] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021. 2[40] David peter Capel. Image mosaicing and super-resolution.",
            "ref_ids": [
                "39",
                "40"
            ],
            "1": "Neural Inverse Rendering and View Synthesis Neural approaches for inverse rendering [21, 22, 31, 46, 49, 51, 50, 71] and novel view synthesis [9, 16, 24, 26, 30, 39, 42, 48, 59] have gained a lot of attention over the recent years.",
            "2": "In practice, we assume that the PSF is a Gaussian distribution, as [40] have shown the validity of such an approximation, and [11] successfully used it to achieve texture super-resolution.",
            "3": "2[40] David peter Capel."
        },
        "Dynamic Point Fields": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2304.02626",
            "ref_texts": "[68] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang,Qing Shuai, Hujun Bao, and Xiaowei Zhou. NeuralBody: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , pages 9054\u20139063, 2021. 10",
            "ref_ids": [
                "68"
            ],
            "1": "Early results on the sequence from [68]."
        },
        "HDhuman: High-quality Human Performance Capture with Sparse Views": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2201.08158",
            "ref_texts": "[5] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2021. 1, 1, 1, 2, 2, 3, 4.2, 4.3, 4.4, 4",
            "ref_ids": [
                "5"
            ],
            "1": "To render human performers from sparse views, neural body [5] presents an implicit neural representation of dynamic humans.",
            "2": "However, when the human performers wear clothes with complex texture patterns or ware loose clothes such as long dresses, the rendering quality of [5] remains limited as the SMPL model does not contain any geometry details, which is not suitable for the rendering of loose clothes or clothes with complex texture patterns.",
            "3": "Moreover, [5] needs to train an independent network for each human and the training procedure is extremely time-consuming (at least 10 hours for each subject), which further limits its applications.",
            "4": "Sparsity General High quality Neural body [5] \" % % pixelNerf [8] \" \" % IBRNet [7] \" \" % Soft3D [2] % \" \" Deep blending [1] % % \" Ours \" \" \" will suffer severe artifacts and blurring as many pixels or areas are invisible in the novel view.",
            "5": "Unlike most of the neural-network-based rendering works that always need to train or fine-tune an independent network for each scene or human [5], [9], [10], the proposed HDhuman is a general rendering framework.",
            "6": "For human rendering on sparse views, [5] uses latent codes that anchored on SMPL vertices to integrate observations from a multi-view video, thus enabling it to reconstruct and render humans from sparse views.",
            "7": "However, [5] is only able to render humans with relatively uniform textures.",
            "8": "P2S Chamfer Ours Neural body [5] Ours Neural body [5] Model1 0.",
            "9": "LPIPS (lower better) SSIM (higher better) PSNR (higher better) Ours NB [5] D+S3D [2] NT [10] Ours NB [5] D+S3D [2] NT [10] Ours NB [5] D+S3D [2] NT [10] Model1 0.",
            "10": "We compare our method with both generic and specific rendering methods [2], [5], [10].",
            "11": "3) Neural body [5] proposes a NeRF-based implicit neural representation for human novel view rendering, it needs to train a separate network for each human.",
            "12": "Note that reconstruction results are only available on neural body [5], so we will perform comparisons with it for reconstruction evaluation.",
            "13": "Table 2 shows the quantitative reconstruction comparisons of our method with neural body [5].",
            "14": "For both P2S distance and Chamfer distance, our method outperforms [5] by extremely large margins.",
            "15": "In contrast to the dataset of NeuralBody [5], which only captures humans with relatively uniform textures, our captured human performers with complex texture patterns or loose clothes.",
            "16": "LPIPS (lower better) SSIM (higher better) PSNR (higher better) Ours NB [5] D+S3D [2] NT [10] Ours NB [5] D+S3D [2] NT [10] Ours NB [5] D+S3D [2] NT [10] Sequence1 0.",
            "17": "1, 2\n[5] S."
        },
        "3D-GIF: 3D-Controllable Object Generation via Implicit Factorized Representations": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2203.06457",
            "ref_texts": "[39] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021. 3[40] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 32, 2018. 12",
            "ref_ids": [
                "39",
                "40"
            ],
            "1": "Several methods [15,39,42,55] extended this to texture representation and successfully extracted textured mesh from 2D images.",
            "2": "3[40] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville.",
            "3": "iandffiare the affine transformation parameters of the FiLM layer [40], which are the output of the mapping network conditioned on the latent vector z2R256."
        },
        "DreamEditor: Text-Driven 3D Scene Editing with Neural Fields": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2306.13455",
            "ref_texts": "[28] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 9054\u20139063.",
            "ref_ids": [
                "28"
            ],
            "1": "Another line of work uses pre-defined template models or skeletons to support re-posing or re-rendering [26,28], but is constrained in a specific category."
        },
        "Mesh Strikes Back: Fast and Efficient Human Reconstruction from RGB videos": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2303.08808",
            "ref_texts": "[43] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2021.",
            "ref_ids": [
                "43"
            ],
            "1": "Recently this approach has been extended to reconstruct clothed humans as well [43, 58, 12, 67, 24, 66, 30, 65, 23, 19, 62].",
            "2": "Unlike [43, 30], which have reported failure cases for out-of-distribution poses, our method\u2019s accuracy is only limited by the artefacts of the skinning function.",
            "3": "This Method RGB loss Mask loss KPS loss Representation Novel pose Training time GPU VideoAvatar [5] 7 3 3 SMPL+D 3 16 hours NA AnimNeRF [12] 3 3 7 NeRF 3 26 hours 48GB Neuralbody [43] 3 3 7 NeRF 7 14 hours 48GB HumanNeRF [67] 3 3 7 NeRF 3 72 hours 48GB NeuralActor [30] 3 3 7 NeRF 3 48 hours 256GB SCARF [19] 3 3 7 NeRF+SMPLX-D 3 40 hours 32GB Ours 3 3 3 SMPL+D 3 <1hour 5GB Table 1.",
            "4": "For experiments (1-2), we use People Snapshot [5] and ZJU Mocap [43] datasets.",
            "5": "We choose baselines across a spectrum of representation choices: SMPLPix [45] (SP) which uses deferred rendering, VideoAvatar [5] (V A) which performs SMPL+D optimization, NeuralBody [43] (NB), HumanNeRF [67] (HN) and AnimNeRF [12] (AN) which are SOTA NeRF methods.",
            "6": "38 NB [43] 1020.",
            "7": "[43] S."
        },
        "PatchShading: High-Quality Human Reconstruction by Patch Warping and Shading Refinement": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2211.14485",
            "ref_texts": "[44] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , 2021. 1, 2, 6, 7",
            "ref_ids": [
                "44"
            ],
            "1": "In order to model human avatars, some approaches [6, 40, 44, 58, 59] incorporate the estimated human skeleton and neural rendering to model animatable human avatars in an implicit manner.",
            "2": "[40, 44, 59] dynamically synthesize the human image.",
            "3": "We also qualitatively compare reconstruction results with NeuralBody [44], Animatable NeRF [40] and the point cloud reconstructed using Metashape [1] provided by the dataset.",
            "4": "We compare our method with NeuralBody [44], Animatable NeRF [40], and Metashape [1].",
            "5": "Moreover, we can synthesis relighting im-Sport 1 Sport 2 Sport 3 NeuralBody [44] 26."
        },
        "3DLatNav: Navigating Generative Latent Spaces for Semantic-Aware 3D Object Manipulation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2211.09770",
            "ref_texts": "33. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: CVPR (2021)",
            "ref_ids": [
                "33"
            ],
            "1": "The advancements of deep learning and the availability of large-scale 3D point cloud datasets have enabled fast progress in the 3D vision tasks such as classification [2, 22, 35, 46, 51, 54, 62, 63], segmentation [15,21,35,46,62,63], 3D reconstruction [11,17,25,55], and shape and view synthesis [18,26,30,33]."
        },
        "Zolly: Zoom Focal Length Correctly for Perspective-Distorted Human Mesh Reconstruction": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2303.13796",
            "ref_texts": "[41] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In IEEE Conf. Comput. Vis. Pattern Recog. , pages 9054\u20139063, 2021. 9",
            "ref_ids": [
                "41"
            ],
            "1": "Our results show significant value for human mesh reconstruction in perspective-distorted images and can empower many downstream tasks, such as monocular clothed human reconstruction [47, 41, 51] and human motion reconstruction in live shows, vlogs, and selfie videos."
        },
        "Bringing Telepresence to Every Desk": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2304.01197",
            "ref_texts": "[46] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 3",
            "ref_ids": [
                "46"
            ],
            "1": "Some recent works [46, 31, 54, 45, 26] exclusively focus on animating clothed humans."
        },
        "Human View Synthesis using a Single Sparse RGB-D Input.": {
            "authors": [],
            "url": "https://3dvar.com/Nguyen2021Human.pdf",
            "ref_texts": "[52] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021. 1,2",
            "ref_ids": [
                "52"
            ],
            "1": "In addition, prior work usually relies on a large amount of cameras [4, 43], expensive capture setups [52], or inference time on the order of several minutes per frame.",
            "2": "Given multi-view input frames or videos, recent works on rendering animatable humans from novel views show impressive results [49, 51,52,71]."
        },
        "Efficient 3D Reconstruction, Streaming and Visualization of Static and Dynamic Scene Parts for Multi-client Live-telepresence in Large-scale Environments": {
            "authors": [
                "Leif Van",
                "Patrick Stotko",
                "Stefan Krumpen",
                "Reinhard Klein",
                "Michael Weinmann"
            ],
            "url": "https://arxiv.org/pdf/2211.14310",
            "ref_texts": "[112] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, X. Zhou, Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans, in: Proceedings of the IEEE /CVF Conference on Computer Vision and Pattern Recognition, IEEE, 2021, pp. 9054\u20139063. doi:10.1109/cvpr46437.2021.",
            "ref_ids": [
                "112"
            ],
            "1": "[112] S."
        },
        "Representation learning of vertex heatmaps for 3D human mesh reconstruction from multi-view images": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2306.16615",
            "ref_texts": "[14] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2021.",
            "ref_ids": [
                "14"
            ],
            "1": "6M [13] and LightStage [14] datasets.",
            "2": "6M and LightStage [14] datasets."
        },
        "Learning Explicit Contact for Implicit Reconstruction of Hand-held Objects from Monocular Images": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2305.20089",
            "ref_texts": "[40] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021.",
            "ref_ids": [
                "40"
            ],
            "1": "Inspired by Neural Body [40], a sparse convolutional network [15] is used to process the contact code volumes V={Vi}L i=1atLdifferent resolutions."
        },
        "MA-NeRF: Motion-Assisted Neural Radiance Fields for Face Synthesis from Sparse Images": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2306.10350",
            "ref_texts": "[13] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou, \u201cNeural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2021.",
            "ref_ids": [
                "13"
            ],
            "1": "Inspired by [12], [13], we formulate a displacement volume by anchoring and diffusing the displacement of 3DMM with SparseConvNet [14].",
            "2": "OUR RESULTS OUTPERFORMED NEURAL BODY [13] (BASED ON SELF -REENACTMENT FOR FACIAL RECONSTRUCTION ), N ERFACE [4] ON MOST METRICS .",
            "3": "We qualitatively and quantitatively compare our method with two other methods that perform closely related tasks: (1) Neural Body [13], using a set of latent codes anchored on the SMPL vertices to build local implicit representations."
        },
        "The Cube Surface Light Field for Interactive Free-Viewpoint Rendering": {
            "authors": [
                "Xiaofei Ai",
                "Yigang Wang"
            ],
            "url": "https://www.mdpi.com/2076-3417/12/14/7212/pdf",
            "ref_texts": "30. Peng, S.; Zhang, Y.; Xu, Y.; Wang, Q.; Shuai, Q.; Bao, H.; Zhou, X. Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. In Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Nashville, TN, USA, 20\u201325 June 2021; pp. 9050\u20139059.",
            "ref_ids": [
                "30"
            ],
            "1": "Related works based on the NeRF, including NeRF in the wild [29], NeRF body [30], and Point-NeRF [31], Human-NeRF [32], extend the NeRF to various applications."
        },
        "Neural Rendering of Humans in Novel View and Pose from Monocular Video": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2204.01218",
            "ref_texts": "[35] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 1, 2, 3, 5, 6, 7",
            "ref_ids": [
                "35"
            ],
            "1": "Furthermore, some approaches [25, 35] introduce human pose as an additional input to serve as a geometric guidance for different frames.",
            "2": "In addition, we reconstruct fine-level details such as cloth wrinkles, hand details at a resolution and fidelity that several prior top-performing methods such as NeuralBody [35] or HumanNeRF [46] fail to recover (Figure 1).",
            "3": "Related works can be categorized into static [26, 29, 40, 41, 52, 51] and dynamic scenes [4, 8, 9, 10, 15, 17, 20, 22, 31, 32, 33, 34, 35, 36, 39, 44].",
            "4": "Dynamic NeRFs [36, 35] extend NeRF to dynamic scenes by introducing a latent deformation field or human poses.",
            "5": "NeuralBody [35] proposes a set of latent codes shared across all frames anchored to a human body model in order to replay character motions from arbitrary view points under training poses.",
            "6": "Pose-conditioned Representation Following [34, 35], we assume the 3D human model is given for each frame (i.",
            "7": "Here Nmdenotes the number of codes whereas the dimension of each pose code is set to 16 similar to [35].",
            "8": "For the videos without ground truth depths for training, we use the depth map predicted by the NeuralBody [35].",
            "9": "To train our method, we rely on the proposed dataset (four sequences of real humans in motion that captured with a 3dMD full-body scanner and a single sequence of a synthetic human in motion) and the public ZJU-MoCap dataset [35].",
            "10": "For the ZJU-MoCap dataset, we use the same training frames as [35].",
            "11": "\u2022 NeuralBody [35] models dynamic scenes using latent codes anchored to the human pose as an extra input besides the coordinate and viewing direction.",
            "12": "Following existing approaches [29, 35], we evaluate the performance on the proposed dataset using two metrics, including the peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM).",
            "13": "The qualitative comparisons are provided in Fig 4 where our approach captures fine-level details on the body (1st;3rdrows) and head (2nd row) better than prior works [34, 35, 46]."
        },
        "Efficient Meshy Neural Fields for Animatable Human Avatars": {
            "authors": [],
            "url": "https://xk-huang.github.io/ema/docs/arxiv_Efficient_Meshy_Neural_Fields_for_Animatable_Human_Avatars.small.pdf",
            "ref_texts": "[70] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, pages 9054\u20139063, 2021.",
            "ref_ids": [
                "70"
            ],
            "1": "Introduction Recent years have witnessed the rise of human digitization [25, 2, 70, 3, 72].",
            "2": "Particularly, volumetric-based implicit representations [58, 70] can reconstruct scenes or objects with much higher fidelity against previous neural renderer [83, 71], and is more user-friendly as it does not need any human templates, pre-set rigging, or UV coordinates.",
            "3": "We conduct extensive experiments on standards benchmarks H36M [30] and ZJU-MoCap [70].",
            "4": "[70, 81, 49, 69, 45, 31, 12, 88, 99, 63, 102] leveraged the radiance field for more photo-realistic human avatars from multiview images or single-view videos without any 3D supervision.",
            "5": "[70, 69] generates blurry textures compared with our method.",
            "6": "PSNR\" SSIM\" PSNR\" SSIM\" PSNR\" SSIM\" PSNR\" SSIM\" NB\n[70] NV\u001810 h 23.",
            "7": "The same data protocol and processing approaches are adopted following [70, 69].",
            "8": "We follow the typical protocol in [70, 69] using two metrics to measure image quality: peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM).",
            "9": "We compare our method with template-based methods [70, 92] and template-free methods [69, 88].",
            "10": "Neural Body (NB) [70] learns a set of latent codes anchored to a deformable template mesh to provide geometry guidance.",
            "11": "Therefore, we compute the `2norm between the masks and the preprocessed mattings (in both ZJU-MoCap and H36M benchmarks, we use the provided preprocessed subject masks from [70, 22]), akin to the traditional shape-from-silhouette [55] technique.",
            "12": "The indicative results with plausible quality appear after a few minutes, which is quite faster than our counterparts [70, 69, 88, 92]."
        },
        "Modularizing deep learning for geometry-aware registration and reconstruction": {
            "authors": [
                "Wei Jiang"
            ],
            "url": "https://open.library.ubc.ca/media/download/pdf/24/1.0427395/4",
            "ref_texts": "[196] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021. !pages 3, 80, 83, 93, 98, 99",
            "ref_ids": [
                "196"
            ],
            "1": "However, the established paradigm of classic camera registration and human reconstruction has been challenged by deep learning [6, 25, 28, 118, 134, 148, 172, 195, 196, 218, 236, 239, 298] in recent years.",
            "2": "Existing methods [148, 196] require multi-cameras setup, consistent lighting and exposure, clean backgrounds, and accurate human geometry to train the NeRF models.",
            "3": "While these methods have shown interesting and exciting results, they often require separate training of editable instances [89] or careful curation of training data [196].",
            "4": "Particularly related to our task of interest, various efforts have been made towards NeRF models conditioned by explicit human models, such as SMPL [153] or 3D skeleton [118, 148, 195, 196, 239].",
            "5": "Neural Body [196] associates a latent code to each SMPL vertex, and use sparse convolution to diffuse the latent code into the volume in observation space.",
            "6": "Generally, motion capture data [111, 196] is captured with a static multi-cameras system in a controlled environment which defeats the purpose of reconstructing from a single video.",
            "7": "5 Comparison with Previous Works We apply NeuralBody [196] to our dataset in a monocular setting.",
            "8": "NeuralBody [196] overfits to the training observations, and produce poor rendering on the back of the subject, while ours generalize better and 98 Figure 5.",
            "9": "11: Novel View Reconstructions on public ZJU Mocap dataset [196] \u2013 Ours and HumanNeRF [275] use only one camera view, NeuralBody [196] uses multiple camera views.",
            "10": "We also compare our method with HumanNeRF [275] and NeuralBody [196] on ZJU Mocap dataset, as shown qualitative comparisons in Figure 5.",
            "11": "!pages 3, 83, 86\n[196] S."
        },
        "Animal 3D reconstruction from videos": {
            "authors": [],
            "url": "https://era.library.ualberta.ca/items/40f35ca5-6f26-44ce-ac76-278cf967b9d5/download/76ec0180-9e7a-464f-92dc-c4a493e6459c",
            "ref_texts": ""
        },
        "M-NeRF: model-based human reconstruction from scratch with mirror-aware neural radiance fields": {
            "authors": [],
            "url": "https://open.library.ubc.ca/media/download/pdf/24/1.0423218/3",
            "ref_texts": "[59] S. Peng, Y . Zhang, Y . Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021.\u2192page 8",
            "ref_ids": [
                "59"
            ],
            "1": "Another line of work [59,67,69,70] focuses on extending neural radiance fields to articulated human body reconstruction which is related to this study.",
            "2": "\u2192page 5\n[59] S."
        },
        "Metaverse in the Wild: Modeling, Adapting, and Rendering of 3D Human Avatars from a Single Camera": {
            "authors": [],
            "url": "https://conservancy.umn.edu/bitstream/handle/11299/241632/Yoon_umn_0130E_23402.pdf?sequence=1",
            "ref_texts": "[57] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021.",
            "ref_ids": [
                "57"
            ],
            "1": ", 3D avatar reconstruction is possible only for a single subject from a speciffc scene [57, 58], whose application to unseen people and scenes produces signiffcant visual artifacts such 4 as unrealistic shape and appearance."
        },
        "Exploration in Light Field processing and editing": {
            "authors": [],
            "url": "http://www.tara.tcd.ie/bitstream/handle/2262/98933/Thesis_Matysiak_final.pdf?sequence=3",
            "ref_texts": "[72] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, \u201cNeuralbody:Implicitneuralrepresentationswithstructuredlatent codesfornovelviewsynthesisofdynamichumans,\u201din Proceedings oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR) ,2021.",
            "ref_ids": [
                "72"
            ],
            "1": "Inasimilarveinotherworklookedatgeneratingnovelviewsofcaptured videos[70,71,72,73],inwhichtheinputisamonocularvideoofthescene.",
            "2": "[72] S."
        },
        "SAgA-NeRF: Subject-agnostic and animatable neural radiance fields for human avatar": {
            "authors": [],
            "url": "https://summit.sfu.ca/_flysystem/fedora/2023-01/etd22141.pdf",
            "ref_texts": "[30] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proc. of Computer Vision and Pattern Recognition (CVPR) , pages 9054\u20139063, 2021.",
            "ref_ids": [
                "30"
            ],
            "1": "2 Quantitative comparison against animatable methods [30], [29], and [44].",
            "2": "Animatable NeRF-based human rendering methods propose conditioning NeRF on an extra human pose parameter to control the rendering [29, 21, 42, 44, 30].",
            "3": "Below we discuss three important works in this category: Animatable NeRF [29], Neural Body [30], and Structured Local Radiance Fields for Human Avatar Modeling [44].",
            "4": "Neural Body [30] also uses SMPL as the human body prior, but utilizes a very different approach overall.",
            "5": "A denser geometry feature volume \u02dcFis obtained by using a Sparse Convolution Network [12] similar to Neural Body [30].",
            "6": "Efforts have been made in adapting NeRF to rendering human subjects [30, 21, 18, 29, 42, 44, 6].",
            "7": "3 have less of an impact on our formulation, as their formulation are based on subject-specific learning, such as learning the canonical pose [29] or subject-specific latent code [30], or subject-specific embeddings and node offsets [44].",
            "8": "1 SMPL+D Optimization Inspired by previous Inspired by previous works [18, 30], we adopt the SMPL model [22] as a body prior to establish coarse correspondences across frames.",
            "9": "We perform our training and testing on the ZJUMoCap dataset [11, 30].",
            "10": "We compare against three methods; Structured Local Radiance Fields for Human Avatar Modeling (SLRF) [44], Animatable Nerf (AN) [29], Neural Body (NB) [30].",
            "11": "2: Quantitative comparison against animatable methods [30], [29], and [44]."
        },
        "MonoHuman: Animatable Human Neural Field from Monocular Video Supplementary Material": {
            "authors": [],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/supplemental/Yu_MonoHuman_Animatable_Human_CVPR_2023_supplemental.pdf",
            "ref_texts": "[3] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. CVPR , 2021. 3",
            "ref_ids": [
                "3"
            ],
            "1": "Subject 377 Subject 386 Subject 387 PSNR\u2191SSIM\u2191LPIPS* \u2193 PSNR\u2191SSIM\u2191LPIPS* \u2193 PSNR\u2191SSIM\u2191LPIPS* \u2193 Neural Body [3] 29.",
            "2": "45 Subject 392 Subject 393 Subject 394 PSNR\u2191SSIM\u2191LPIPS* \u2193 PSNR\u2191SSIM\u2191LPIPS* \u2193 PSNR\u2191SSIM\u2191LPIPS* \u2193 Neural Body [3] 28.",
            "3": "Subject 377 Subject 386 Subject 387 PSNR\u2191SSIM\u2191LPIPS* \u2193 PSNR\u2191SSIM\u2191LPIPS* \u2193 PSNR\u2191SSIM\u2191LPIPS* \u2193 Neural Body [3] 29.",
            "4": "06 Subject 392 Subject 393 Subject 394 PSNR\u2191SSIM\u2191LPIPS* \u2193 PSNR\u2191SSIM\u2191LPIPS* \u2193 PSNR\u2191SSIM\u2191LPIPS* \u2193 Neural Body [3] 29.",
            "5": "And follow NeuralBody [3] to evaluate the subject in a 3d bounding box to avoid getting an inflated PSNR value."
        },
        "Supplementary Materials for GM-NeRF: Learning Generalizable Model-based Neural Radiance Fields from Multi-view Images": {
            "authors": [],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/supplemental/Chen_GM-NeRF_Learning_Generalizable_CVPR_2023_supplemental.pdf",
            "ref_texts": "[8] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , pages 9054\u20139063, 2021. 2, 3",
            "ref_ids": [
                "8"
            ],
            "1": "Qualitative results of our method on ZJU Mocap [8] and GeneBody [2] datasets .",
            "2": "Quantitative results of novel view synthesis on ZJUMocap [8] dataset .",
            "3": "Quantitative results of novel pose synthesis on ZJUMocap [8] dataset .",
            "4": "we also train our model from scratch without any pretraining, Compared to per-scene optimization methods such as NT [10], NHR [12], NB [8], which often spend hours or even a day on training, our method can quickly produce more realistic and detailed images.",
            "5": "Genebody [2] has a broad distribution across different clothing styles and poses, even containing professional occasions such as traditional opera costumes, which is more challenging than ZJUMocap [8].",
            "6": "155 NB [8] 20."
        },
        "FlexNeRF: Photorealistic Free-Viewpoint Rendering of Moving Humans from Sparse Views (Supplementary Materials)": {
            "authors": [],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/supplemental/Jayasundara_FlexNeRF_Photorealistic_Free-Viewpoint_CVPR_2023_supplemental.pdf",
            "ref_texts": "[4] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 9050\u20139059, 2021. 1, 5, 6",
            "ref_ids": [
                "4"
            ],
            "1": "Dataset # of Videos# of Views Full Sparse ZJU-MoCap [4] 6 556 43 People Snapshot [2] 7 517 39 SCF 7 40 Table 1.",
            "2": "Quantitative Results (ZJU-MoCap Dataset) We present the performance breakdown of FlexNeRF on individual videos of the ZJU-MoCap dataset [4] for the full view setting in Table 2.",
            "3": "Method LPIPS\u00d7103\u2193PSNR\u2191SSIM\u2191 Neural Body [4] 57.",
            "4": "MethodSubject 377 Subject 392 LPIPS\u00d7103\u2193PSNR\u2191SSIM\u2191LPIPS\u00d7103\u2193PSNR\u2191SSIM\u2191 Neural Body [4] 40.",
            "5": "9769 MethodSubject 386 Subject 393 LPIPS\u00d7103\u2193PSNR\u2191SSIM\u2191LPIPS\u00d7103\u2193PSNR\u2191SSIM\u2191 Neural Body [4] 46.",
            "6": "9711 MethodSubject 387 Subject 394 LPIPS\u00d7103\u2193PSNR\u2191SSIM\u2191LPIPS\u00d7103\u2193PSNR\u2191SSIM\u2191 Neural Body [4] 59.",
            "7": "Comparison of various performance metrics on the ZJU-MoCap [4] dataset."
        },
        "Regularized Coordinate-based Neural Representation Learning for Optical Tomography": {
            "authors": [
                "Renhao Liu"
            ],
            "url": "https://openscholarship.wustl.edu/cgi/viewcontent.cgi?article=1642&context=eng_etds",
            "ref_texts": "[40] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021.",
            "ref_ids": [
                "40"
            ]
        },
        "Neural Articulated Radiance Field Supplemental Material": {
            "authors": [],
            "url": "https://openaccess.thecvf.com/content/ICCV2021/supplemental/Noguchi_Neural_Articulated_Radiance_ICCV_2021_supplemental.pdf",
            "ref_texts": "[5]Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 5, 6",
            "ref_ids": [
                "5"
            ],
            "1": "Experiment on Real Human Images In this section, we test our approach on a real human dataset ZJU-MOCAP [5].",
            "2": "This issue can be considered in future work, for example, by learning latent variables to account for both pose-dependent and pose-independent deformations similar to Neural Body [5]."
        },
        "Human gesture and micro-gesture analysis: datasets, methods, and applications": {
            "authors": [],
            "url": "https://oulurepo.oulu.fi/bitstream/handle/10024/36725/isbn978-952-62-3239-3.pdf?sequence=1",
            "ref_texts": "(2019). Pytorch: An imperative style, high-performance deep learning library. Conference on Neural Information Processing Systems. Pavlakos, G., Choutas, V ., Ghorbani, N., Bolkart, T., Osman, A. A. A., Tzionas, D., & Black, M. J. (2019). Expressive body capture: 3d hands, face, and body from a single image. IEEE Conference on Computer Vision and Pattern Recognition. Peng, S., Zhang, Y ., Xu, Y ., Wang, Q., Shuai, Q., Bao, H., & Zhou, X. (2021). Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. IEEE Conference on Computer Vision and Pattern 154 Recognition, 9054\u20139063. Peng, W., Hong, X., Chen, H., & Zhao, G. (2020). Learning graph convolutional network for skeleton-based human action recognition by neural searching. AAAI Conference on Artificial Intelligence. Peng, X. B., Ma, Z., Abbeel, P., Levine, S., & Kanazawa, A. (2021). Amp: Adversarial motion priors for stylized physics-based character control. ACM Transactions on Graphics. Pentland, A. (2008). Honest signals: How they shape our world. MIT Press, Cambridge, MA. Petrovich, M., Black, M. J., & Varol, G. (2021). Action-conditioned 3d human motion synthesis with transformer vae. IEEE Conference on Computer Vision and Pattern Recognition. Portilla, J., & Simoncelli, E. (2000). Parametric texture model based on joint statistics of complex wavelet coefficients. International Journal of Computer Vision, 40, 49\u201371. Praharaj, S., Scheffel, M., Drachsler, H., & Specht, M. (2018). Multimodal analytics for real-time feedback in co-located collaboration. European Conference on Technology Enhanced Learning, 187\u2013201. Qi, C. R., Su, H., Mo, K., & Guibas, L. J. (2017). Pointnet: Deep learning on point sets for 3d classification and segmentation. IEEE Conference on Computer Vision and Pattern Recognition. Qi, C. R., Yi, L., Su, H., & Guibas, L. J. (2017). Pointnet++: Deep hierarchical feature learning on point sets in a metric space. Conference on Neural Information Processing Systems. Radeta, M., & Maiocchi, M. (2013). Towards automatic and unobtrusive recognition of primary-process emotions in body postures. Humaine Association Conference on Affective Computing and Intelligent Interaction, 695-700. Rhodin, H., Salzmann, M., & Fua, P. (2018). Unsupervised geometry-aware representation for 3d human pose estimation. European Conference on Computer Vision. Romero, J., Tzionas, D., & Black, M. J. (2017). Embodied hands: Modeling and capturing hands and bodies together. ACM Transactions on Graphics, 36. Ros\u00e9, C. P., McLaughlin, E. A., Liu, R., & Koedinger, K. R. (2019). Explanatory learner models: Why machine learning (alone) is not the answer. British Journal of Educational Technology, 50(6), 2943\u20132958. Russell, J. A. (1980). A circumplex model of affect. Journal of personality and social psychology, 39(6), 1161."
        },
        "Supplementary for ARAH: Animatable Volume Rendering of Articulated Human SDFs": {
            "authors": [],
            "url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136920001-supp.pdf",
            "ref_texts": "13. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proc. of CVPR (2021) 3, 7, 8, 9",
            "ref_ids": [
                "13"
            ],
            "1": "We use the resulting model as the initialization for our per-subject optimization on the ZJU-MoCap [13] dataset.",
            "2": "For inference, we follow [12, 13] and crop an enlarged bounding box around the projected SMPL mesh on the image plane and render only pixels inside the bounding box.",
            "3": "For unseen test poses we follow the practice of [12, 13] and use the latent code Zof the last training frame as the input.",
            "4": "Neural Body [13], Ani-NeRF [12], and A-NeRF [17].",
            "5": "A simple uniform sampling strategy (as used in [12, 13]) produces stratified artifacts due to the discretized sampling.",
            "6": "G Additional Quantitative Results We present complete evaluation metrics including PSNR, SSIM, LPIPS on the test poses of the ZJU-MoCap [13] dataset in Table 1."
        },
        "Learning Neural Volumetric Representations of Dynamic Humans in Minutes Supplemental Material": {
            "authors": [],
            "url": "https://chen-geng.com/instant_nvr/files/supp.pdf",
            "ref_texts": "[11] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9054\u20139063, 2021. 3",
            "ref_ids": [
                "11"
            ],
            "1": "Details of baselines Neural Body [11], Ani-SDF [10], HumanNeRF [14] and Ani-NeRF [9] We use the released code and conduct experiments on a single NVIDIA RTX 3090 GPU."
        },
        "Supplemental Materials for TAVA: Template-free Animatable Volumetric Actors": {
            "authors": [],
            "url": "http://pages.iai.uni-bonn.de/gall_juergen/download/tava_eccv22_supp.pdf",
            "ref_texts": "6. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9054\u20139063 (2021)",
            "ref_ids": [
                "6"
            ],
            "1": "Thus the previous way [5,6] of splitting the dataset into two chunks with consecutive frames will cover similar poses in both sets, which is not suitable for evaluating the pose generalization ability.",
            "2": "2, for the template-based baselines Animatable-NeRF [5] and NeuralBody [6], we use their official implementations.",
            "3": "The ZJU-Mocap dataset has become an increasingly popular dataset to study human performance capture, reconstruction, and neural rendeirng [5,6,8].",
            "4": "952 NeuralBody [6] 33.",
            "5": "949 NeuralBody [6] 31.",
            "6": "969 NeuralBody [6] 33.",
            "7": "974 NeuralBody [6] 36."
        },
        "Real-time Reposable Rendering of Volumetric Digitalized Humans": {
            "authors": [],
            "url": "https://www.ocf.berkeley.edu/~sxyu/184proj/report.pdf",
            "ref_texts": "[12] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021. Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. arXiv:2012.15838 [cs.CV]",
            "ref_ids": [
                "12",
                "cs\\.CV"
            ],
            "1": "Several recent works can recover rigged models under some constraints [6, 9, 11, 12]."
        },
        "\u5355\u89c6\u89d2\u4e09\u7ef4\u670d\u88c5\u91cd\u5efa\u7684\u6c11\u65cf\u98ce\u683c\u8868\u5f81\u5b66\u4e60": {
            "authors": [],
            "url": "https://www.jcad.cn/cn/article/pdf/preview/f53f564f-f731-4917-a118-7b6b4ea1b94f.pdf"
        }
    }
}