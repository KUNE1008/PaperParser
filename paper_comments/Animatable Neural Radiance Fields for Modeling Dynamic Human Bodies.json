{
    "title": "Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies",
    "id": 11,
    "valid_pdf_number": "137/145",
    "matched_pdf_number": "115/137",
    "matched_rate": 0.8394160583941606,
    "citations": {
        "Humannerf: Free-viewpoint rendering of moving people from monocular video": {
            "authors": [
                "Yi Weng",
                "Brian Curless",
                "Pratul P. Srinivasan",
                "Jonathan T. Barron",
                "Ira Kemelmacher"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Weng_HumanNeRF_Free-Viewpoint_Rendering_of_Moving_People_From_Monocular_Video_CVPR_2022_paper.pdf",
            "ref_texts": "[47] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. ICCV , 2021. 1, 2, 4",
            "ref_ids": [
                "47"
            ],
            "1": "Human-specific methods typically assume a SMPL template [33] as a prior, which helps constrain the motion space but also introduces artifacts in clothing and complex motions that are not captured by the SMPL model [47, 48].",
            "2": "Other than free-viewpoint rendering, there is another related active research field that focus on human motion retargeting either in 2D [1, 6, 34, 41, 52, 65, 66] or 3D [18, 19, 24, 31, 47, 51, 67, 72].",
            "3": "Similar approaches have been applied to human modeling [3, 9, 13, 24, 39, 47, 50, 61, 72]."
        },
        "Nerf: Neural radiance field in 3d vision, a comprehensive review": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2210.00379",
            "ref_texts": "[184] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , October 2021, pp. 14 314\u2013",
            "ref_ids": [
                "184"
            ],
            "1": "[181],DoubleField [182], LISA [183], Animatable NeRF [184], TAVA [185], NeuMan [60] Fig.",
            "2": "State of the art models from top tier conferences in 2021/2022 such as A-NeRF [190] (Feb 2021), Animatable NeRF [184] (May 2021), DoubleField [182] (Jun 2021), HumanNeRF [180] (Jan 2022), Zheng et al.",
            "3": "[184] S."
        },
        "Ref-nerf: Structured view-dependent appearance for neural radiance fields": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2112.03907",
            "ref_texts": "[27] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. ICCV , 2021. 3",
            "ref_ids": [
                "27"
            ],
            "1": "NeRF has inspired many subsequent works, which extend its neural volumetric scene representation to application domains including dynamic and deformable scenes [26], avatar animation [11, 27], and even phototourism [21]."
        },
        "Nerf-editing: geometry editing of neural radiance fields": {
            "authors": [
                "Jie Yuan",
                "Tian Sun",
                "Kun Lai",
                "Yuewen Ma",
                "Rongfei Jia",
                "Lin Gao"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Yuan_NeRF-Editing_Geometry_Editing_of_Neural_Radiance_Fields_CVPR_2022_paper.pdf",
            "ref_texts": "[46] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 2",
            "ref_ids": [
                "46"
            ],
            "1": "However, they either limit the edits to human bodies [46,82], or can only learn motion information from the recorded videos, and cannot perform active editing [48]."
        },
        "Generative neural articulated radiance fields": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/7dbafa7d2051218f364c9a38ef1150de-Paper-Conference.pdf",
            "ref_texts": "[10] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In IEEE International Conference on Computer Vision (ICCV) , 2021.",
            "ref_ids": [
                "10"
            ]
        },
        "Banmo: Building animatable 3d neural models from many casual videos": {
            "authors": [
                "Gengshan Yang",
                "Minh Vo",
                "Natalia Neverova",
                "Deva Ramanan",
                "Andrea Vedaldi",
                "Hanbyul Joo"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_BANMo_Building_Animatable_3D_Neural_Models_From_Many_Casual_Videos_CVPR_2022_paper.pdf",
            "ref_texts": "[36] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV , 2021. 2",
            "ref_ids": [
                "36"
            ],
            "1": "Similar to our goal, some recent works [24, 32, 36, 37, 44] produce posecontrollable NeRFs, but they rely on a human body model, or synchronized multi-view video inputs."
        },
        "Fenerf: Face editing in neural radiance fields": {
            "authors": [
                "Jingxiang Sun",
                "Xuan Wang",
                "Yong Zhang",
                "Xiaoyu Li",
                "Qi Zhang",
                "Yebin Liu",
                "Jue Wang"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Sun_FENeRF_Face_Editing_in_Neural_Radiance_Fields_CVPR_2022_paper.pdf",
            "ref_texts": "[41] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 14314\u201314323, 2021. 2",
            "ref_ids": [
                "41"
            ],
            "1": "Various follow-ups extend NeRF to faster training and testing [8,13,17,43,49], pose-free [26,31], dynamic scenes [5,50] and animating avatars [15,27,41]."
        },
        "Neural human performer: Learning generalizable radiance fields for human performance rendering": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper/2021/file/cf866614b6b18cda13fe699a3a65661b-Paper.pdf",
            "ref_texts": "[32] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 14314\u201314323, 2021.",
            "ref_ids": [
                "32"
            ],
            "1": "Despite the promising results, these general deformable NeRF [17,53] and human-specific NeRF [11,9, 33,32] methods must be optimized for each new video separately, and generalize poorly on unseen scenarios."
        },
        "Neuman: Neural human radiance field from a single video": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2203.12575.pdf?trk=public_post_comment-text",
            "ref_texts": "32. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. In: ICCV (2021) 1, 3, 4, 6",
            "ref_ids": [
                "32"
            ],
            "1": "Recent efforts also focus on animation of these radiance field models [19,33,32,12,40] of human, with the aid of large controlled datasets, further extending the application domain of radiance-field-based modeling to enable augmented reality experiences.",
            "2": "Particularly related to our task of interest, various efforts have been made towards NeRF models conditioned by explicit human models, such as SMPL [22] or 3D skeleton [19,33,32,12,40].",
            "3": "NeRF [32] learns a blending weight field in both observation space and canonical space, and optimize for a new blending weight field for novel poses.",
            "4": "Therefore, we define a canonical space based on the \u5927-pose (Da-pose) SMPL [22] mesh, similar to [32,19]."
        },
        "Selfrecon: Self reconstruction your digital avatar from monocular video": {
            "authors": [
                "Boyi Jiang",
                "Yang Hong",
                "Hujun Bao",
                "Juyong Zhang"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Jiang_SelfRecon_Self_Reconstruction_Your_Digital_Avatar_From_Monocular_Video_CVPR_2022_paper.pdf",
            "ref_texts": "[38] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 14314\u201314323, 2021. 1",
            "ref_ids": [
                "38"
            ],
            "1": "To this end, technologies such as poserelated skinning weights prediction [24,38,47] and specific inverse articulated deformation design [13] are proposed at the cost of high complexity and poor generalization."
        },
        "Fourier plenoctrees for dynamic radiance field rendering in real-time": {
            "authors": [
                "Liao Wang",
                "Jiakai Zhang",
                "Xinhang Liu",
                "Fuqiang Zhao",
                "Yanshun Zhang",
                "Yingliang Zhang",
                "Minye Wu",
                "Jingyi Yu",
                "Lan Xu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Fourier_PlenOctrees_for_Dynamic_Radiance_Field_Rendering_in_Real-Time_CVPR_2022_paper.pdf",
            "ref_texts": "[40] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV , 2021. 2",
            "ref_ids": [
                "40"
            ],
            "1": "skeleton [40] or parametric models [27, 41]) to explicitly calculate stable motion flows from model animations."
        },
        "CelebV-HQ: A large-scale video facial attributes dataset": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2207.12393",
            "ref_texts": "62. Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance ffelds for modeling dynamic human bodies. In ICCV , 2021. 16",
            "ref_ids": [
                "62"
            ],
            "1": "These features on video modality could not only be further exploited to improve the quality of current models, but also stimulate the emerging of several budding topics, such as Dynamic NeRF [63] and Animatable NeRF [62]."
        },
        "Efficientnerf efficient neural radiance fields": {
            "authors": [
                "Tao Hu",
                "Shu Liu",
                "Yilun Chen",
                "Tiancheng Shen",
                "Jiaya Jia"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Hu_EfficientNeRF__Efficient_Neural_Radiance_Fields_CVPR_2022_paper.pdf",
            "ref_texts": "[22] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV , 2021. 1, 2",
            "ref_ids": [
                "22"
            ],
            "1": "With Neural Radiance Fields (NeRF) [17] proposed, NVS tasks [20, 24], like large-scale or dynamic synthesis [21,22,25], were successfully dealt with in high quality.",
            "2": "Neural Actor [13] and Animatable-NeRF [22] also adopt similar functions to synthesize human body with novel poses."
        },
        "Structured local radiance fields for human avatar modeling": {
            "authors": [
                "Zerong Zheng",
                "Han Huang",
                "Tao Yu",
                "Hongwen Zhang",
                "Yandong Guo",
                "Yebin Liu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_Structured_Local_Radiance_Fields_for_Human_Avatar_Modeling_CVPR_2022_paper.pdf",
            "ref_texts": "[54] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 1,2,3,4,6,7",
            "ref_ids": [
                "54"
            ],
            "1": "Recently, neural radiance representations, which implicitly encode shape and appearance using neural networks, are also applied in pursuit of higher-fidelity results [35, 49,54].",
            "2": "Even in state-of-the-art methods based on 15893\n implicit fields [35, 49, 54], researchers still assumed that skin motions can be propagated to approximate the cloth deformations, which, unfortunately, only holds for tightfitting clothes.",
            "3": "Recently,neural scene representations and rendering techniques are adopted for higher-fidelity results [35, 54,55].",
            "4": "Instead, our method allows more degrees of freedom for motion and geometry modeling, enabling avatar creation for different cloth topologies, which is a significant departure from the existing works [35, 54,58,65].",
            "5": "to transform the points in the posed space to a global canonical space, and has been the basis of previous methods [35, 54, 63].",
            "6": "Comparison We mainly compare our method with Animatable NeRF [54] and Neural Body [55].",
            "7": "We omit other related methods since they have been compared in [54].",
            "8": "We first compare with Animatable NeRF [54] on the dataset of [22] and our own data.",
            "9": "Compared to [54], our method can produce more appearance details, and generate the non-rigid mo15898\n Figure 6.",
            "10": "Comparison against Animatable Nerf [54] on novel pose synthesis.",
            "11": "1also prove that our method can achieve higher-quality results than [54].",
            "12": "Quantitative comparison with Animatable NeRF [54] in terms of novel pose synthesis.",
            "13": "PSNR (\u2191) SSIM (\u2191) Case\\Method [54] Ours [54] Ours Hoody 22.",
            "14": "Quantitative comparison with Neural Body [55] and Animatable NeRF [54] on ZJU-MoCap dataset.",
            "15": "PSNR (\u2191) SSIM (\u2191) ID Pose Type [55] [54] Ours [55] [54] Ours 387Seen 25.",
            "16": "We also report the numeric results of Animatable NeRF [54] in Tab."
        },
        "Learning neural light fields with ray-space embedding": {
            "authors": [
                "Benjamin Attal",
                "Bin Huang",
                "Michael Zollhofer",
                "Johannes Kopf",
                "Changil Kim"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Attal_Learning_Neural_Light_Fields_With_Ray-Space_Embedding_CVPR_2022_paper.pdf",
            "ref_texts": "[36] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. InICCV , 2021. 3",
            "ref_ids": [
                "36"
            ],
            "1": "Others leverage coordinate embedding for modeling articulated objects such as the human body [24, 36]."
        },
        "Rignerf: Fully controllable neural 3d portraits": {
            "authors": [
                "Rukh Athar",
                "Zexiang Xu",
                "Kalyan Sunkavalli",
                "Eli Shechtman",
                "Zhixin Shu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Athar_RigNeRF_Fully_Controllable_Neural_3D_Portraits_CVPR_2022_paper.pdf",
            "ref_texts": "[37] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. InICCV , 2021. 3",
            "ref_ids": [
                "37"
            ],
            "1": "The photorealism of volumetric and implicit representations have encouraged works that combine them with classical representations in order improve reconstruction [5] or lend control over foreground [12, 30, 37].",
            "2": "Similarly, [37] learns a 3D skinning field to accurately deform points according to the target pose.",
            "3": "Neither [30] nor [37] model the full 3D scene."
        },
        "Tava: Template-free animatable volumetric actors": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2206.08929",
            "ref_texts": "34.Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: International Conference on Computer Vision (2021) TAVA: Template-free Animatable Volumetric Actors 23",
            "ref_ids": [
                "34"
            ],
            "1": "TAVA: Template-free Animatable Volumetric Actors 3 Methods Template-freeNo Per-frame Latent Code3D Canonical SpaceDeformation NARF [29] \u2714 \u2714 \u2718 Inverse A-NeRF [42] \u2714 \u2718 \u2718 Inverse Animatable-NeRF [34] \u2718 \u2718 \u2714 Inverse HumanNeRF [45] \u2718 \u2714 \u2714 Inverse NeuralBody [35] \u2718 \u2718 \u2714\u2020Forward Ours (TAVA) \u2714 \u2714 \u2714 Forward Table 1.",
            "2": "The follow-up work Animatable-NeRF [34] establishes a transformation between view and canonical space through optimizing the inverse deformation field.",
            "3": "Some [22,34,35,45] are built on top of the SMPL [25] body template, which prohibits them to be applied to creatures beyond humans.",
            "4": "Moreover, most of the aforementioned methods either introduce latent codes to better memorize the seen poses [42,34,35], or represent the deformation in the inverse direction from view space to the canonical space [29,42,34,45].",
            "5": "Prior works [34,35] create the train andvalsets on the ZJU-Mocap dataset by simply splitting each video with 500 \u223c2200 frames into two splits, where the training set has 60 \u223c300 frames and the validation set has 300 \u223c1000 frames.",
            "6": "For the view splits, we follow the protocol from [34,35] for ZJU-Mocap, where 4 views are used for training and 17 views for testing.",
            "7": "Novel-view Novel-pose (ind) Novel-pose (ood) PSNR \u2191SSIM \u2191PSNR \u2191SSIM \u2191PSNR \u2191SSIM \u2191 SMPL-based Methods Animatable-NeRF [34] 30.",
            "8": "We compare our work with two types of previous methods: 1) Templatefree methods, including NARF [29] and A-NeRF [42], as well as 2) SMPL-based methods, including Animatable-NeRF [34] and NeuralBody [35].",
            "9": "Thus the previous way [34,35] of splitting the dataset into two chunks with consecutive frames will cover similar poses in both sets, which is not suitable for evaluating the pose generalization ability.",
            "10": "2, for the template-based baselines Animatable-NeRF [34] and NeuralBody [35], we use their official implementations.",
            "11": "The ZJU-Mocap dataset has become an increasingly popular dataset to study human performance capture, reconstruction, and neural rendeirng [34,35,45].",
            "12": "TAVA: Template-free Animatable Volumetric Actors 19 Novel-view Novel-pose (ind) Novel-pose (ood) PSNR \u2191SSIM \u2191PSNR \u2191SSIM \u2191PSNR \u2191SSIM \u2191 Subject 313 Animatable-NeRF [34] 29.",
            "13": "957 Subject 315 Animatable-NeRF [34] 27.",
            "14": "960 Subject 377 Animatable-NeRF [34] 32.",
            "15": "980 Subject 386 Animatable-NeRF [34] 34."
        },
        "Humannerf: Efficiently generated human radiance field from sparse inputs": {
            "authors": [
                "Fuqiang Zhao",
                "Wei Yang",
                "Jiakai Zhang",
                "Pei Lin",
                "Yingliang Zhang",
                "Jingyi Yu",
                "Lan Xu"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Zhao_HumanNeRF_Efficiently_Generated_Human_Radiance_Field_From_Sparse_Inputs_CVPR_2022_paper.pdf",
            "ref_texts": "[32] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 14314\u201314323, 2021. 2",
            "ref_ids": [
                "32"
            ],
            "1": "More recent implicit manner based work [23, 29, 32,40,42,49,56] achieves impressive results for novel view synthesis for a specific scene."
        },
        "Reconstructing personalized semantic facial nerf models from monocular video": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2210.06108",
            "ref_texts": "2022. Neural 3D Video Synthesis From Multi-View Video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 5521\u20135531. Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. 2020. Neural Sparse Voxel Fields. NeurIPS (2020). Xian Liu, Yinghao Xu, Qianyi Wu, Hang Zhou, Wayne Wu, and Bolei Zhou. 2022. Semantic-Aware Implicit Neural Audio-Driven Video Portrait Generation. arXiv preprint arXiv:2201.07786 (2022). Stephen Lombardi, Tomas Simon, Gabriel Schwartz, Michael Zollhoefer, Yaser Sheikh, and Jason Saragih. 2021. Mixture of volumetric primitives for efficient neural rendering. ACM Transactions on Graphics (TOG) 40, 4 (2021), 1\u201313. Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. 2020. Nerf: Representing scenes as neural radiance fields for view synthesis. In European conference on computer vision . Springer, 405\u2013421. Thomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. 2022. Instant Neural Graphics Primitives with a Multiresolution Hash Encoding. ACM Trans. Graph. 41, 4 (July 2022), 102:1\u2013102:15. Michael Niemeyer, Jonathan T Barron, Ben Mildenhall, Mehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. 2022. Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 5480\u20135490. Michael Niemeyer and Andreas Geiger. 2021. Giraffe: Representing scenes as compositional generative neural feature fields. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . 11453\u201311464. Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Steven M. Seitz, and Ricardo Martin-Brualla. 2021a. Nerfies: Deformable Neural Radiance Fields. ICCV (2021). Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, and Steven M. Seitz. 2021b. HyperNeRF: A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields. ACM Trans. Graph. 40, 6, Article 238 (dec 2021). Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al .2019. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems 32 (2019). Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021a. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 14314\u201314323. Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021b. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 9054\u20139063. A. Pumarola, A. Agudo, A.M. Martinez, A. Sanfeliu, and F. Moreno-Noguer. 2019. GANimation: One-Shot Anatomically Consistent Facial Animation. (2019). Anurag Ranjan, Timo Bolkart, Soubhik Sanyal, and Michael J Black. 2018. Generating 3D faces using convolutional mesh autoencoders. In Proceedings of the European Conference on Computer Vision (ECCV) . 704\u2013720. Sara Fridovich-Keil and Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. 2022. Plenoxels: Radiance Fields without Neural Networks. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. 2020. Graf: Generative radiance fields for 3d-aware image synthesis. Advances in Neural Information Processing Systems 33 (2020), 20154\u201320166. Aliaksandr Siarohin, St\u00e9phane Lathuili\u00e8re, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe.",
            "ref_ids": [
                "2022"
            ]
        },
        "Anifacegan: Animatable 3d-aware face image generation for video avatars": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/eae78bf2712f222f101bd7d12f875a57-Paper-Conference.pdf",
            "ref_texts": "[48] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In IEEE/CVF International Conference on Computer Vision , pages 14314\u201314323, 2021.",
            "ref_ids": [
                "48"
            ],
            "1": "The original NeRF and most of its successors [43,33,45,49,48,40,32,73] focus on learning scene-specific representation using a set of posed images or a video sequence of a static or dynamic scene."
        },
        "Lisa: Learning implicit shape and appearance of hands": {
            "authors": [
                "Enric Corona",
                "Tomas Hodan",
                "Minh Vo",
                "Francesc Moreno",
                "Chris Sweeney",
                "Richard Newcombe",
                "Lingni Ma"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Corona_LISA_Learning_Implicit_Shape_and_Appearance_of_Hands_CVPR_2022_paper.pdf",
            "ref_texts": "[50] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the International Conference on Computer Vision (ICCV) , 2021. 2",
            "ref_ids": [
                "50"
            ],
            "1": "Similar ideas are proposed in [50] and NARF [41]."
        },
        "Deforming radiance fields with cages": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2207.12298",
            "ref_texts": "37. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14314\u201314323 (2021)",
            "ref_ids": [
                "37"
            ],
            "1": "Harada For some specific object categories, such as the human body or articulated objects, recent studies [24,32,33,37,38,41,46] enable the generation of the unseen scene by controlling the body shape or bone pose.",
            "2": "For the specific task of human body modeling, various works proposed to combine NeRF with a parametric human model to enable human body reposing [37, 38], shape control [24] or even clothing changes [46]."
        },
        "Streaming radiance fields for 3d video synthesis": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/57c2cc952f388f6185db98f441351c96-Paper-Conference.pdf",
            "ref_texts": "[54] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision , 2021.",
            "ref_ids": [
                "54"
            ],
            "1": ", human bodies [54,55] by integrating with domain-specific priors, e."
        },
        "gdna: Towards generative detailed neural avatars": {
            "authors": [
                "Xu Chen",
                "Tianjian Jiang",
                "Jie Song",
                "Jinlong Yang",
                "Michael J. Black",
                "Andreas Geiger",
                "Otmar Hilliges"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Chen_gDNA_Towards_Generative_Detailed_Neural_Avatars_CVPR_2022_paper.pdf",
            "ref_texts": "[49] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proc. of the IEEE International Conf. on Computer Vision (ICCV) , 2021. 2",
            "ref_ids": [
                "49"
            ],
            "1": "To overcome the topology and resolution limitations of meshes, other representations, including point clouds [35, 37,65], implicit surfaces [12,47,52,55,58,60], and radiance fields [32, 45, 49, 56, 63], have been explored."
        },
        "Unsupervised learning of efficient geometry-aware neural articulated representations": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2204.08839",
            "ref_texts": "56. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV (2021)",
            "ref_ids": [
                "56"
            ],
            "1": "We demonstrate this approach by modeling the pose prior as a skeletal distribution [51,63], while noting that other models like meshes [57,56] may bring potential performance benefits.",
            "2": "Photorealistic rendering of articulated objects, especially for humans, is also achieved with 3D implicit representations [56,51,63,2,69,39].",
            "3": "They have achieved the state-of-the art in learning 3D shape [12,43,53], static [62,44,4] and dynamic scenes [58,37,54], articulated objects [57,14,7,65,11,56,51,63,2,69,39], and image synthesis [60,10].",
            "4": "Recently, articulated representations based on NeRF have been proposed [57,51,63,56,69,70,39].",
            "5": "In order to train a single tri-plane for all parts, the second change is to further transform the local coordinates xl kinto a canonical space defined by a canonical pose oc, similar to Animatable NeRF [56].",
            "6": "1, we compare the proposed Efficient NARF (ENARF) with the state-of-the-art methods [51,56] in terms of both efficiency and effectiveness, and we conduct ablation studies on the deformation modeling and the design choices for the selector.",
            "7": "1 Training on a Dynamic Scene Following the training setting in Animatable NeRF [56], we train our ENARF model on synchronized multi-view videos of a single moving articulated object.",
            "8": "Cost Novel view Novel pose #Memory #FLOPS Time(s) PSNR\u2191SSIM\u2191LPIPS \u2193PSNR\u2191SSIM\u2191LPIPS \u2193 Animatable NeRF [56] 0.",
            "9": "First, we compare our method with the state-of-the-art supervised methods NARF [51] and Animatable NeRF [56]."
        },
        "PINA: Learning a personalized implicit neural avatar from a single RGB-D video sequence": {
            "authors": [
                "Zijian Dong",
                "Chen Guo",
                "Jie Song",
                "Xu Chen",
                "Andreas Geiger",
                "Otmar Hilliges"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Dong_PINA_Learning_a_Personalized_Implicit_Neural_Avatar_From_a_Single_CVPR_2022_paper.pdf",
            "ref_texts": "[47] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pages 14314\u201314323, October 2021. 2",
            "ref_ids": [
                "47"
            ],
            "1": "Furthermore, several methods that learn a neural avatar for a specific outfit from watertight meshes [13,16,24,32,47,53,56] have been proposed.",
            "2": "These methods either require complete full-body scans with accurate surface normals and registered poses [13, 16, 53, 56] or rely on complex and intrusive multi-view setups [24,32,47]."
        },
        "Learning implicit templates for point-based clothed human modeling": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2207.06955",
            "ref_texts": "49. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV (2021)",
            "ref_ids": [
                "49"
            ],
            "1": "Recently, based on neural radiance fields (NeRF) [43], attempts have been made to bypass the underlying geometry and synthesize rendered images of clothed humans directly [49,50,62,67]."
        },
        "Devrf: Fast deformable voxel radiance fields for dynamic scenes": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/eeb57fdf745eb31a3c7ef22c59a4661d-Paper-Conference.pdf",
            "ref_texts": "[26] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 14314\u2013",
            "ref_ids": [
                "26"
            ],
            "1": "Lastly, several NeRF-based approaches have been proposed for modeling dynamic humans [8,41,17,26,32] but can not directly generalize to other scenes."
        },
        "Dressing avatars: Deep photorealistic appearance for physically simulated clothing": {
            "authors": [
                "Donglai Xiang"
            ],
            "url": "https://dl.acm.org/doi/pdf/10.1145/3550454.3555456",
            "ref_texts": "(2007), 109\u2013118. Atsuhiro Noguchi, Xiao Sun, Stephen Lin, and Tatsuya Harada. 2021. Neural articulated radiance field. In Proceedings of the IEEE/CVF International Conference on Computer Vision . Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. 2017. Colored point cloud registration revisited. In Proceedings of the IEEE International Conference on Computer Vision . Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al .2019. Pytorch: An imperative style, high-performance deep learning library. Advances in Neural Information Processing Systems 32 (2019). Chaitanya Patel, Zhouyingcheng Liao, and Gerard Pons-Moll. 2020. Tailornet: Predicting clothing in 3d as a function of human pose, shape and garment style. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021a. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision . Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021b. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter Battaglia. 2021. Learning Mesh-Based Simulation with Graph Networks. In International Conferenceon Learning Representations . Gerard Pons-Moll, Sergi Pujades, Sonny Hu, and Michael J Black. 2017. ClothCap: Seamless 4D clothing capture and retargeting. ACM Transactions on Graphics (ToG)"
        },
        "Learning neural volumetric representations of dynamic humans in minutes": {
            "authors": [
                "Chen Geng",
                "Sida Peng",
                "Zhen Xu",
                "Hujun Bao",
                "Xiaowei Zhou"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Geng_Learning_Neural_Volumetric_Representations_of_Dynamic_Humans_in_Minutes_CVPR_2023_paper.pdf",
            "ref_texts": "[56] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies.",
            "ref_ids": [
                "56"
            ],
            "1": "We extend the technique of displacement map [14, 15] to represent human motions by restricting the originally 3D deformation field [40, 56, 59] on the 2D surface of a parametric human model, such as SMPL [43].",
            "2": "Another line of works [28, 30, 34, 36, 40, 56, 58, 65, 92, 93, 95, 101 \u2013103, 105] exploits dynamic implicit neural representations and differentiable renderers to reconstruct 3D human models from 8760\n\n(b) Part-based voxelized human representation (a) Motion parameterization on 2D surface domainCanonical spaceDeformation space Lower resolutionHigher resolution 3D parameterization 4D space-time motionTime Time Query pointBody pose Point Density and colorFigure 2.",
            "3": "In contrast to [40, 56] which use a single neural radiance field (NeRF) to represent the canonical human model, we decompose the human body into multiple parts with different complexity and adopt a structured set of MHE-augmented NeRF with varying resolutions as the body representation.",
            "4": "(8) In contrast to [40, 56] that represent the body with a single NeRF network, our part-based voxelized human representation can assign different densities of model parameters to different human parts with different complexity, thereby enabling us to efficiently distribute the representational power of the network.",
            "5": "Our method is implemented purely with the PyTorch framework [53] to demonstrate the effectiveness of our representation It also enables us to fairly compare with baseline methods [34, 56, 58] implemented in PyTorch.",
            "6": "Animatable NeRF (AN) [56] deforms the canonical NeRF with the skeleton-driven framework and models non-rigid deformations by learning blend weight fields.",
            "7": "[57] extend [56] with a signed distance field and pose-dependent deformation field to better model the residual deformation and geometric 8763\n Ours ~5 min Ground-truth ~10 hNB\n~10 hAS\n~10 hHumanNeRF\n~1h fine-tuningNHP\n~1h fine-tuningPixelNeRF\n~10 hAN Figure 3.",
            "8": "Table 1 compares our method with NB [58], AN [56], PixelNeRF [100], NHP [34], HN [93] and AS [57] on novel view synthesis.",
            "9": "[57, 93] exhibit better results than [56, 58].",
            "10": "Although [56,58] have shown impressive rendering results given 4-view videos, they do not perform well on monocular inputs.",
            "11": "18 AN [56] \u02dc10 h 29.",
            "12": "[56] uses a learnable blend weight field to model human motion, which has a higher dimension and could be hard to converge well given single-view supervision."
        },
        "Watch it move: Unsupervised discovery of 3D joints for re-posing of articulated objects": {
            "authors": [
                "Atsuhiro Noguchi",
                "Umar Iqbal",
                "Jonathan Tremblay",
                "Tatsuya Harada",
                "Orazio Gallo"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Noguchi_Watch_It_Move_Unsupervised_Discovery_of_3D_Joints_for_Re-Posing_CVPR_2022_paper.pdf",
            "ref_texts": "[40] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic humanbodies. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2021. 2",
            "ref_ids": [
                "40"
            ],
            "1": "They allow novel view and pose synthesis, but require ground truth poses [8,36,49], or dense 3D meshes [24, 30, 40, 41, 53] annotations for the training image."
        },
        "Nerfplayer: A streamable dynamic scene representation with decomposed neural radiance fields": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2210.15947",
            "ref_texts": "[61] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 14314\u201314323, October 2021. 2",
            "ref_ids": [
                "61"
            ],
            "1": "The scene representation in NeRF inspired a number of works focusing on 3D modeling, such as human face and body capture [24, 42,55, 61,62, 72], relighting [4,5, 71] and 3D content generation [9, 10, 22, 25, 31, 69, 78]."
        },
        "Avatarcap: Animatable avatar conditioned monocular human volumetric capture": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2207.02031",
            "ref_texts": "53. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance ffelds for modeling dynamic human bodies. In: ICCV. pp. 14314{",
            "ref_ids": [
                "53"
            ],
            "1": "They create animatable avatars from various inputs, including scans [9, 58, 44, 46, 8], multi-view RGB videos [53, 39] and monocular depth measurements [7, 71].",
            "2": "Recent works proposed to directly learn an animatable avatar from the database, including scans [45, 58, 44, 46, 8], multi-view RGB videos [39, 53] and depth frames [7, 71, 10]."
        },
        "Instantavatar: Learning avatars from monocular video in 60 seconds": {
            "authors": [
                "Tianjian Jiang",
                "Xu Chen",
                "Jie Song",
                "Otmar Hilliges"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_InstantAvatar_Learning_Avatars_From_Monocular_Video_in_60_Seconds_CVPR_2023_paper.pdf",
            "ref_texts": "[48] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proc. of the IEEE International Conf. on Computer Vision (ICCV) , 2021. 2",
            "ref_ids": [
                "48"
            ],
            "1": "Recently, neural representations [37, 41, 45, 46] have emerged as a powerful tool to model 3D humans [3,6,8,10, 11, 13, 14, 22\u201326, 30, 31, 34, 38, 39, 39, 43, 44, 48, 49, 52, 53, 57, 59\u201363, 63, 64, 67, 69, 70].",
            "2": "Using neural representations, many works [6, 18, 26, 27, 30, 34, 43, 48, 49, 61, 62, 64, 69] can directly reconstruct high fidelity neural human avatars from a sparse set of views or a monocular video without prescanning personalized template."
        },
        "Nerfacc: A general nerf acceleration toolbox": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2210.04847",
            "ref_texts": "[6] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang , Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human b odies. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 14314\u201314323, 2021.",
            "ref_ids": [
                "6"
            ],
            "1": "In the past two years, they have been proven to be quite powerful in many downstream appl ications in 3D such as static/dynamic scene reconstruction [3, 4, 5, 6], relighting [7, 8, 9, 10, 11] and conten t generation [12, 13, 14]."
        },
        "Imface: A nonlinear 3d morphable face model with implicit neural representations": {
            "authors": [
                "Mingwu Zheng",
                "Hongyu Yang",
                "Di Huang",
                "Liming Chen"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_ImFace_A_Nonlinear_3D_Morphable_Face_Model_With_Implicit_Neural_CVPR_2022_paper.pdf",
            "ref_texts": "[41] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV , 2021. 4",
            "ref_ids": [
                "41"
            ],
            "1": "Such design is inspired by the recent INRs study [41] on human body, which introduces the linear blend skinning algorithm [30] to make the network learn from separate transformations of body parts."
        },
        "Humangen: Generating human radiance fields with explicit priors": {
            "authors": [
                "Suyi Jiang",
                "Haoran Jiang",
                "Ziyu Wang",
                "Haimin Luo",
                "Wenzheng Chen",
                "Lan Xu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_HumanGen_Generating_Human_Radiance_Fields_With_Explicit_Priors_CVPR_2023_paper.pdf",
            "ref_texts": "[56] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14314\u201314323, 2021. 3",
            "ref_ids": [
                "56"
            ],
            "1": "Embracing the developing of NeRF techniques [8, 9,40\u201342, 44,45,48,69,71,73,82,86], the human shape prior augmented NeRFs achieve modeling realistic human bodies [32, 37,50,57,89], learning animatable avatars [34, 56,74] and generalizing across different persons [32, 70,89] from temporal data."
        },
        "Vid2avatar: 3d avatar reconstruction from videos in the wild via self-supervised scene decomposition": {
            "authors": [
                "Chen Guo",
                "Tianjian Jiang",
                "Xu Chen",
                "Jie Song",
                "Otmar Hilliges"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Guo_Vid2Avatar_3D_Avatar_Reconstruction_From_Videos_in_the_Wild_via_CVPR_2023_paper.pdf",
            "ref_texts": "[41] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV , 2021. 3",
            "ref_ids": [
                "41"
            ],
            "1": "Recent works [20, 22, 29, 39, 41, 54, 59, 60] attempt to reconstruct humans from more sparse settings by deploying neural rendering."
        },
        "Surface-aligned neural radiance fields for controllable 3d human synthesis": {
            "authors": [
                "Tianhan Xu",
                "Yasuhiro Fujita",
                "Eiichi Matsumoto"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Xu_Surface-Aligned_Neural_Radiance_Fields_for_Controllable_3D_Human_Synthesis_CVPR_2022_paper.pdf",
            "ref_texts": "[35] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV , 2021. 1, 2, 3, 5, 6, 7",
            "ref_ids": [
                "35"
            ],
            "1": "Because manually designing high-quality 3D human models is usually labor-intensive, increasing studies [1\u20133,24,27,30,35,36] have proposed the reconstruction of 3D human models using only 2D observations.",
            "2": "Several approaches [24, 30, 35, 36] have been proposed to incorporate knowledge from a statistical 3D human model and its pose estimation with NeRF.",
            "3": "Deformationbased approaches [24, 35] use a deformation field to transform the query point from the observation space to a poseindependent canonical space and then build NeRF in the canonical space.",
            "4": "For modeling a dynamic human body, recent studies [24, 30, 35, 36] have proposed the use of prior knowledge of human pose and skinning weights of SMPL [26] to ease the learning of a deformation field.",
            "5": "Animatable NeRF [35] uses the skinning weight of SMPL [26] to predict the neural blend shape fields.",
            "6": "Animatable NeRF [35] and Neural Actor [24] rely on this way of projection for learning a deformation field and/or utilizing a texture map.",
            "7": "Following the previous studies [28,35,36], we minimize the per-pixel mean squared error (MSE) between the rendered image and the ground truth image.",
            "8": "For detailed settings, we refer to [35].",
            "9": "6M dataset, we compare with Animatable NeRF [35], which learns the neural blend weight field and builds a NeRF within a canonical space.",
            "10": "6M dataset, our approach outperforms both [30] and [35] by a large margin.",
            "11": "For both datasets, the performance of our approach almost consistently outperforms [36], [35], and [30].",
            "12": "Also, while we do not explicitly model the time-varying deformation components (such as using per-frame embedding in [35,36]), neural networks could implicitly model such components by inferring time from the skeleton pose information.",
            "13": "Training pose Unseen pose PSNR SSIM PSNR SSIM NARF [30] AN [35] Ours NARF [30] AN [35] Ours NARF [30] AN [35] Ours NARF [30] AN [35] Ours S1 21."
        },
        "Unsupervised volumetric animation": {
            "authors": [
                "Aliaksandr Siarohin",
                "Willi Menapace",
                "Ivan Skorokhodov",
                "Kyle Olszewski",
                "Jian Ren",
                "Ying Lee",
                "Menglei Chai",
                "Sergey Tulyakov"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Siarohin_Unsupervised_Volumetric_Animation_CVPR_2023_paper.pdf",
            "ref_texts": "[43] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE International Conference on Computer Vision , 2021. 3",
            "ref_ids": [
                "43"
            ],
            "1": "Initially, a dataset with multiview videos was required to train animatable radiance fields [43]."
        },
        "Human performance modeling and rendering via neural animated mesh": {
            "authors": [
                "Fuqiang Zhao"
            ],
            "url": "https://dl.acm.org/doi/pdf/10.1145/3550454.3555451",
            "ref_texts": "3530127 Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao, Wenzheng Chen, Alex Evans, Thomas Mueller, and Sanja Fidler. 2021. Extracting Triangular 3D Models, Materials, and Lighting From Images. arXiv:2111.12503 (2021). Richard A. Newcombe, Dieter Fox, and Steven M. Seitz. 2015. DynamicFusion: Reconstruction and Tracking of Non-Rigid Scenes in Real-Time. In CVPR . Richard A. Newcombe, Shahram Izadi, Otmar Hilliges, David Molyneaux, David Kim, Andrew J. Davison, Pushmeet Kohli, Jamie Shotton, Steve Hodges, and Andrew Fitzgibbon. 2011. KinectFusion: Real-Time Dense Surface Mapping and Tracking. In Proc. of ISMAR . 127\u2013136. Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. 2019. Occupancy flow: 4d reconstruction by learning particle dynamics. In Proceedings of the IEEE/CVF international conference on computer vision . 5379\u20135389. Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. 2020. Differentiable volumetric rendering: Learning implicit 3d representations without 3d supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 3504\u20133515. Michael Oechsle, Songyou Peng, and Andreas Geiger. 2021. Unisurf: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 5589\u20135599. Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. 2019. Deepsdf: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 165\u2013174. Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Steven M Seitz, and Ricardo-Martin Brualla. 2020. Deformable Neural Radiance Fields. arXiv preprint arXiv:2011.12948 (2020). Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Steven M Seitz, and Ricardo Martin-Brualla. 2021. Nerfies: Deformable neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 5865\u20135874. Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021a. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 14314\u201314323. Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021b. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 9054\u20139063. PhotoScan 2019. AgiSoft PhotoScan Professional. http://www.agisoft.com/downloads/installer/. Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. 2021. D-nerf: Neural radiance fields for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 10318\u201310327. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention . Springer, 234\u2013241. Shunsuke Saito, Jinlong Yang, Qianli Ma, and Michael J Black. 2021. SCANimate: Weakly supervised learning of skinned clothed avatar networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 2886\u20132897. Sara Fridovich-Keil and Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. 2022. Plenoxels: Radiance Fields without Neural Networks. In CVPR . Johannes Lutz Sch\u00f6nberger and Jan-Michael Frahm. 2016. Structure-from-Motion Revisited. In Conference on Computer Vision and Pattern Recognition (CVPR) . Soumyadip Sengupta, Vivek Jayaram, Brian Curless, Steven M Seitz, and Ira Kemelmacher-Shlizerman. 2020. Background matting: The world is your green screen. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 2291\u20132300. Aliaksandra Shysheya, Egor Zakharov, Kara-Ali Aliev, Renat Bashirov, Egor Burkov, Karim Iskakov, Aleksei Ivakhnenko, Yury Malkov, Igor Pasechnik, Dmitry Ulyanov, et al.2019. Textured neural avatars. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 2387\u20132397. Miroslava Slavcheva, Maximilian Baust, Daniel Cremers, and Slobodan Ilic. 2017. Killingfusion: Non-rigid 3d reconstruction without correspondences. In Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition . 1386\u20131395. Miroslava Slavcheva, Maximilian Baust, and Slobodan Ilic. 2018. Sobolevfusion: 3d reconstruction of scenes undergoing free non-rigid motion. In Proceedings of the IEEE conference on computer vision and pattern recognition . 2646\u20132655. Robert W Sumner, Johannes Schmid, and Mark Pauly. 2007. Embedded deformation for shape manipulation. ACM Transactions on Graphics (TOG) 26, 3 (2007), 80. Guoxing Sun, Xin Chen, Yizhang Chen, Anqi Pang, Pei Lin, Yuheng Jiang, Lan Xu, Jingya Wang, and Jingyi Yu. 2021. Neural Free-Viewpoint Performance Rendering under Complex Human-object Interactions. In Proceedings of the 29th ACM International Conference on Multimedia . Xin Suo, Yuheng Jiang, Pei Lin, Yingliang Zhang, Minye Wu, Kaiwen Guo, and Lan Xu."
        },
        "TotalSelfScan: Learning Full-body Avatars from Self-Portrait Videos of Faces, Hands, and Bodies": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper_files/paper/2022/file/589c5bd0aa4322e37813e8e41ddf8034-Paper-Conference.pdf",
            "ref_texts": "[39] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV , 2021.",
            "ref_ids": [
                "39"
            ],
            "1": "Recently, optimizing a network to represent a person-specific model shows impressive results [14,46,10,41,39].",
            "2": "Furthermore, in order to achieve better animatable effects, learning blend weights automatically from data [39] and incorporating articulated structures [36] are explored.",
            "3": "98 AniNeRF [39] 1.",
            "4": "2 Comparison with the baselines Since most previous methods only focus on body modeling, we extend the state-of-the-art methods AniNeRF [39] and AniSDF [40] with hands to compare with our method.",
            "5": "239 AniNeRF [39] 20."
        },
        "NeRF-Art: Text-Driven Neural Radiance Fields Stylization": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2212.08070",
            "ref_texts": "2021. Styleclip: Text-driven manipulation of stylegan imagery. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 2085\u20132094. Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 14314\u201314323. Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. 2021. D-nerf: Neural radiance fields for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 10318\u201310327. Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. 2017. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition . 652\u2013660. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al .2021. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020 (2021). Eduard Ramon, Gil Triginer, Janna Escur, Albert Pumarola, Jaime Garcia, Xavier Giro-i Nieto, and Francesc Moreno-Noguer. 2021. H3D-Net: Few-Shot High-Fidelity 3D Head Reconstruction. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 5620\u20135629. Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. 2021. Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 14335\u201314345. Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or. 2021. Encoding in style: a stylegan encoder for image-toimage translation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition . 2287\u20132296. Manuel Ruder, Alexey Dosovitskiy, and Thomas Brox. 2016. Artistic style transfer for videos. In German conference on pattern recognition . Springer, 26\u201336. Johannes L Schonberger and Jan-Michael Frahm. 2016. Structure-from-motion revisited. InProceedings of the IEEE conference on computer vision and pattern recognition . 4104\u2013",
            "ref_ids": [
                "2021"
            ]
        },
        "Mps-nerf: Generalizable 3d human rendering from multiview images": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2203.16875",
            "ref_texts": "[6] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in IEEE/CVF International Conference on Computer Vision , 2021, pp. 14 314\u201314 323. 1, 2, 3, 5, 6, 7, 10",
            "ref_ids": [
                "6"
            ],
            "1": "While traditional methods [1], [2], [3] use dense multiview camera rigs or depth sensors to accomplish this task, recent neural rendering approaches [4], [5], [6], [7] have shown that free-view rendering and animation can be achieved using sparse color cameras, which could significantly reduce the device setup and capture cost.",
            "2": "In particular, promising results have been shown by methods [5], [6], [7] that are based on the neural radiance field (NeRF) [8] representation.",
            "3": "However, due to the high complexity of human motion and appearance, existing methods [5], [6], [7] are typically trained in a person-specific setup, i.",
            "4": "Some methods [6], [40], [41] also leverage SMPL to deform the 3D space to a canonical pose, where posedependent residual deformation can be considered.",
            "5": "To render the target image, we follow recent works [5], [6], [7] and base our rendering scheme on NeRF [8], which is a compact yet powerful representation for neural rendering.",
            "6": "The notion of a canonical space has been used in previous deformable NeRF schemes such as [6], [22], [23].",
            "7": "Following [6], [38], [49], for each point in the volume, we assign the skinning weights of its closest body vertex.",
            "8": ", a residual skinning weight field is learned in the personspecific model of [6]).",
            "9": "In [6], a per-frame latent code is jointly learned to alleviate this issue, and a test-time optimization is further needed to optimize the latent code for a novel pose.",
            "10": "Following [6], we conduct experiments on 7 subject: S1, S5, S6, S7, S8, S9, and S11.",
            "11": "As in AniNeRF [6], we use three views (#0, #1, #2) out of the four as the input to our method and all four views for supervision during training.",
            "12": "We train and test our method using fitted SMPL parameters and image masks provided by [6] which are obtained using [30] and [59], respectively.",
            "13": "Instead of directly calculating PSNR and SSIM for the whole image, we follow AniNeRF [6] and NeuralBody [5] to project the 3D bounding box of a body onto image plane to obtain a 2D mask and only calculate PSNR and SSIM in the masked region.",
            "14": "TABLE 2: Comparison of our method with NB [5], AniNeRF [6] on the Human3.",
            "15": "NeuralBody [5] and AniNeRF [6] are person-specific models which only need camera parameters to render a novel view of these trained subjects.",
            "16": "Hence, for reference purpose, we compare our method with recent person-specific models NeuralBody (NB) [5] and Animat-able NeRF (AniNeRF) [6].",
            "17": "NeuralBody [5] and AniNeRF [6] are person-specific models which only need camera and pose parameters to render these trained subjects.",
            "18": "TABLE 3: Comparison of NB [5], AniNeRF [6], and our method on the THuman dataset.",
            "19": "Note that we did notapply any post-processing on the extracted 3D shapes such as the Gaussian smoothing used in AniNeRF [6].",
            "20": "1, 2, 3, 5, 6, 7, 10\n[6] S."
        },
        "Danbo: Disentangled articulated neural body representations via graph neural networks": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2205.01666",
            "ref_texts": "40. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV (2021)",
            "ref_ids": [
                "40"
            ],
            "1": "Not using an explicit surface poses a major difficulty as surface-based solutions exploit surface points to anchor neural features locally as vertex attributes [41], and leverage skinning weights to associate points on or close to the surface to nearby body parts [29,40].",
            "2": "Another line of work makes use of SMPL blend skinning weights as initialization for learning deformation fields [40].",
            "3": "While the skinning weights in SMPL provide an initialization, [40] showed that fine-tuning the deformation fields via self-supervision helps rendering unseen poses.",
            "4": "4 Experiments In the following, we evaluate the improvements upon the most recent surface-free neural body model A-NeRF [46], and compare against recent model-based solutions NeuralBody [41] and Anim-NeRF [40].",
            "5": "NeuralBody [41] Anim-NeRF [40] A-NeRF [46] DANBO (Ours) PSNR \u2191SSIM \u2191KID \u2193LPIPS \u2193PSNR \u2191SSIM \u2191KID \u2193LPIPS \u2193PSNR \u2191SSIM \u2191KID \u2193LPIPS \u2193PSNR \u2191SSIM \u2191KID \u2193LPIPS \u2193 S122.",
            "6": "Notethat,unlikeAnim-NeRF[40], we do not require test-time finetuning for unseen poses.",
            "7": "6M3[21,22]: We follow the same evaluation protocol as in AnimNeRF [40], with a total of 7 subjects for evaluation.",
            "8": "Table 1 verifies these improvements on the test set of Anim-NeRF [40].",
            "9": "NeuralBody [41] Anim-NeRF [40] A-NeRF [46] DANBO (Ours) PSNR \u2191SSIM \u2191KID \u2193LPIPS \u2193PSNR \u2191SSIM \u2191KID \u2193LPIPS \u2193PSNR \u2191SSIM \u2191KID \u2193LPIPS \u2193PSNR \u2191SSIM \u2191KID \u2193LPIPS \u2193 S1 22."
        },
        "Editablenerf: Editing topologically varying neural radiance fields by key points": {
            "authors": [
                "Chengwei Zheng",
                "Wenbin Lin",
                "Feng Xu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_EditableNeRF_Editing_Topologically_Varying_Neural_Radiance_Fields_by_Key_Points_CVPR_2023_paper.pdf",
            "ref_texts": "[29] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 14314\u201314323, 2021. 3",
            "ref_ids": [
                "29"
            ],
            "1": "By using hu-man body parametric models and skinning techniques suchas SMPL [22], neural radiance fields have been extended to model the human body and can be animated by controllingskeleton poses [3,19,25,29,36]."
        },
        "Hand avatar: Free-pose hand animation and rendering from monocular video": {
            "authors": [
                "Xingyu Chen",
                "Baoyuan Wang",
                "Yeung Shum"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Hand_Avatar_Free-Pose_Hand_Animation_and_Rendering_From_Monocular_Video_CVPR_2023_paper.pdf",
            "ref_texts": "[39] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV , 2021.",
            "ref_ids": [
                "39"
            ],
            "1": "8683\n\n3D point query [9, 10, 20, 25, 37, 39, 40, 48, 51, 58, 62\u201364, 69, 72, 75] .",
            "2": ", self-contact), previous skinning-based methods can hardly find accurate skinning weights for an arbitrary query [3, 6, 18, 19, 31, 35, 39, 47, 62, 74], while part-aware methods usually suffer from across-part inconsistency issue [17, 21, 30, 60].",
            "3": "As reported in [3, 35, 39, 62], a posed-space query can be transformed back to canonical space with linear blend skinning and inverse skinning weights."
        },
        "UNIF: United neural implicit functions for clothed human reconstruction and animation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2207.09835",
            "ref_texts": "[28] Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14314\u201314323 (2021)",
            "ref_ids": [
                "28"
            ],
            "1": "Although recent methods use another neural implicit function to learn the weights [31, 21, 28, 34], they generalize poorly under unseen poses because the LBS weights of a point vary along with the pose.",
            "2": "2 Human Body Reconstruction and Animation As the most popular mesh-based human body model, SMPL [17] and its variations [12, 30, 27] dominate the area of human body reconstruction for its expressiveness and flexibility, supporting innumerable downstream task [15, 26, 2, 9, 29, 28, 14].",
            "3": "Besides the minimal-clothed human body, later works also use neural implicit functions to model clothed humans [31, 34, 24, 29, 28].",
            "4": "However, since a neural implicit function lacks point-wise correspondences, a forward and a backward skinning network are introduced [31, 21, 28, 34] to save LBS weights for the bidirectional mapping between a posed shape and the canonical shape.",
            "5": "10975\u201310985 (2019)\n[28] Peng, S."
        },
        "MonoHuman: Animatable Human Neural Field from Monocular Video": {
            "authors": [
                "Zhengming Yu",
                "Wei Cheng",
                "Xian Liu",
                "Wayne Wu",
                "Yee Lin"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_MonoHuman_Animatable_Human_Neural_Field_From_Monocular_Video_CVPR_2023_paper.pdf",
            "ref_texts": "[32] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. ICCV , 2021. 1, 2, 3, 4",
            "ref_ids": [
                "32"
            ],
            "1": "However, previous methods [32, 33, 58] usually require carefully-collected multi-view videos with complicated systems and controlled studios, which limits the usage in general and personalized scenarios applications.",
            "2": "To address this, some recent methods deform the neural radiance fields (NeRF [27])\n[32, 49] to learn a backward skinning weight of parametric models depending on the pose or individual frame index.",
            "3": "Some other works [16,32,42] leverage the blend weight from the template model like SMPL [25].",
            "4": "NeuralBody [32] uses structured pose features generated from SMPL [25] to condition the radiance field, enabling it to recover human performers and produce free-viewpoint images from sparse multi-view videos.",
            "5": "[32] create an animatable model by conditioning the NeRF with pose-dependent latent code.",
            "6": "Preliminaries and Problem Setting Given an image sequence of a person and corresponding poses, segmentation masks, as well as the camera\u2019s intrinsic and extrinsic parameters, following [32,49] we use a neural field to represent the human from observation space in one canonical space."
        },
        "Gravitationally lensed black hole emission tomography": {
            "authors": [
                "Aviad Levis",
                "Pratul P. Srinivasan",
                "Andrew A. Chael",
                "Ren Ng",
                "Katherine L. Bouman"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Levis_Gravitationally_Lensed_Black_Hole_Emission_Tomography_CVPR_2022_paper.pdf",
            "ref_texts": "[39] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. ICCV , 2021. 2",
            "ref_ids": [
                "39"
            ],
            "1": "Our work is related to extensions of NeRF to dynamic scenes [17, 32, 38, 39, 45].",
            "2": "Previous methods use priors on deformation sparsity and rigidity [38, 45], pre-trained monocular depth estimation [32], or explicit models of human faces and bodies [17,39] to relate 3D points across time."
        },
        "Capturing and animation of body and clothing from monocular video": {
            "authors": [],
            "url": "https://dl.acm.org/doi/pdf/10.1145/3550469.3555423",
            "ref_texts": "3504\u20133515. Mohamed Omran, Christoph Lassner, Gerard Pons-Moll, Peter Gehler, and Bernt Schiele. 2018. Neural body fitting: Unifying deep learning and model based human pose and shape estimation. In International Conference on Computer Vision (ICCV) . IEEE, 484\u2013494. Ahmed A. A. Osman, Timo Bolkart, and Michael J. Black. 2020. STAR: Sparse trained articulated human body regressor. In European Conference on Computer Vision (ECCV) . 598\u2013613. Chaitanya Patel, Zhouyingcheng Liao, and Gerard Pons-Moll. 2020. Tailornet: Predicting clothing in 3d as a function of human pose, shape and garment style. In Conference on Computer Vision and Pattern Recognition (CVPR) . 7365\u20137375. Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. 2019. Expressive Body Capture: 3D Hands, Face, and Body From a Single Image. In Conference on Computer Vision and Pattern Recognition (CVPR) . 10975\u201310985. Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021a. Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. In ICCV . Sida Peng, Shangzhan Zhang, Zhen Xu, Chen Geng, Boyi Jiang, Hujun Bao, and Xiaowei Zhou. 2022. Animatable Neural Implicit Surfaces for Creating Avatars from Videos. arXiv preprint arXiv:2203.08133 (2022). Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021b. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Conference on Computer Vision and Pattern Recognition (CVPR) . 9054\u20139063. Gerard Pons-Moll, Sergi Pujades, Sonny Hu, and Michael J Black. 2017. ClothCap: Seamless 4D clothing capture and retargeting. Transactions on Graphics (TOG) 36, 4 (2017), 1\u201315. Sergey Prokudin, Michael J. Black, and Javier Romero. 2021. SMPLpix: Neural Avatars from 3D Human Models. In Winter Conference on Applications of Computer Vision (WACV) . 1810\u20131819. Nikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia Gkioxari. 2020. Accelerating 3D Deep Learning with PyTorch3D. arXiv:2007.08501 (2020). Capturing and Animation of Body and Clothing from Monocular Video SA \u201922 Conference Papers, December 6\u20139, 2022, Daegu, Republic of Korea Yu Rong, Takaaki Shiratori, and Hanbyul Joo. 2021. FrankMocap: A Monocular 3D Whole-Body Pose Estimation System via Regression and Integration. In International Conference on Computer Vision Workshops (ICCV-W) . Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, and Hao Li. 2019. PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization. In International Conference on Computer Vision (ICCV) . Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul Joo. 2020. PIFuHD: MultiLevel Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization. InConference on Computer Vision and Pattern Recognition (CVPR) . Igor Santesteban, Miguel A Otaduy, and Dan Casas. 2019. Learning-based animation of clothing for virtual try-on. In Computer Graphics Forum , Vol. 38. Wiley Online Library, 355\u2013366. Shih-Yang Su, Frank Yu, Michael Zollh\u00f6fer, and Helge Rhodin. 2021. A-NeRF: Articulated neural radiance fields for learning human shape, appearance, and pose. Advances in Neural Information Processing Systems (NeurIPS) 34 (2021). Yating Tian, Hongwen Zhang, Yebin Liu, and Limin Wang. 2022. Recovering 3D Human Mesh from Monocular Images: A Survey. arXiv preprint arXiv:2203.01923 (2022). Garvita Tiwari, Bharat Lal Bhatnagar, Tony Tung, and Gerard Pons-Moll. 2020. SIZER: A dataset and model for parsing 3d clothing and learning size sensitive 3d clothing. InEuropean Conference on Computer Vision (ECCV) . Springer, 1\u201318. Raquel Vidaurre, Igor Santesteban, Elena Garces, and Dan Casas. 2020. Fully Convolutional Graph Neural Networks for Parametric Virtual Try-On. In Computer Graphics Forum , Vol. 39. Wiley Online Library, 145\u2013156. Yi Wang, Xin Tao, Xiaojuan Qi, Xiaoyong Shen, and Jiaya Jia. 2018. Image inpainting via generative multi-column convolutional neural networks. In Advances in Neural Information Processing Systems (NeurIPS) . 331\u2013340. Chung-Yi Weng, Brian Curless, Pratul P. Srinivasan, Jonathan T. Barron, and Ira Kemelmacher-Shlizerman. 2022. HumanNeRF: Free-Viewpoint Rendering of Moving People From Monocular Video. In Conference on Computer Vision and Pattern Recognition (CVPR) . 16210\u201316220. Donglai Xiang, Hanbyul Joo, and Yaser Sheikh. 2019. Monocular Total Capture: Posing Face, Body, and Hands in the Wild. In Conference on Computer Vision and Pattern Recognition (CVPR) . 10965\u201310974. Donglai Xiang, Fabian Prada, Timur Bagautdinov, Weipeng Xu, Yuan Dong, He Wen, Jessica Hodgins, and Chenglei Wu. 2021. Modeling clothing as a separate layer for an animatable human avatar. Transactions on Graphics (TOG) 40, 6 (2021), 1\u201315. Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, and Michael J. Black. 2022. ICON: Implicit Clothed humans Obtained from Normals. In Conference on Computer Vision andPattern Recognition (CVPR) . Hongyi Xu, Thiemo Alldieck, and Cristian Sminchisescu. 2021. H-NeRF: Neural radiance fields for rendering and temporal reconstruction of humans in motion. Advances in Neural Information Processing Systems (NeurIPS) 34 (2021). Hongyi Xu, Eduard Gabriel Bazavan, Andrei Zanfir, William T Freeman, Rahul Sukthankar, and Cristian Sminchisescu. 2020. GHUM & GHUML: Generative 3d human shape and articulated pose models. In Conference on Computer Vision and Pattern Recognition (CVPR) . 6184\u20136193. Lu Yang, Qing Song, Zhihui Wang, Mengjie Hu, Chun Liu, Xueshi Xin, Wenhe Jia, and Songcen Xu. 2020. Renovating Parsing R-CNN for Accurate Multiple Human Parsing. In European Conference on Computer Vision (ECCV) (Lecture Notes in Computer Science, Vol. 12357) . Springer, 421\u2013437. Ze Yang, Shenlong Wang, Sivabalan Manivasagam, Zeng Huang, Wei-Chiu Ma, Xinchen Yan, Ersin Yumer, and Raquel Urtasun. 2021. S3: Neural shape, skeleton, and skinning fields for 3D human modeling. In Conference on Computer Vision and Pattern Recognition (CVPR) . 13284\u201313293. Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. 2021. Volume rendering of neural implicit surfaces. Advances in Neural Information Processing Systems (NeurIPS) 34"
        },
        "Representing Volumetric Videos as Dynamic MLP Maps": {
            "authors": [
                "Sida Peng",
                "Yunzhi Yan",
                "Qing Shuai",
                "Hujun Bao",
                "Xiaowei Zhou"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Peng_Representing_Volumetric_Videos_As_Dynamic_MLP_Maps_CVPR_2023_paper.pdf",
            "ref_texts": "[44] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV , 2021. 2",
            "ref_ids": [
                "44"
            ],
            "1": "To model high-resolution scenes, [14, 16, 31, 42, 44, 46, 47, 71] extend NeRF to represent dynamic scenes."
        },
        "Real-time neural radiance talking portrait synthesis via audio-spatial decomposition": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2211.12368",
            "ref_texts": "[40] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV , 2021. 2",
            "ref_ids": [
                "40"
            ]
        },
        "Ndf: Neural deformable fields for dynamic human modelling": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2207.09193",
            "ref_texts": "24. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14314\u201314323 (2021)",
            "ref_ids": [
                "24"
            ],
            "1": "To extend NeRF to dynamic scenes, an effective idea is to aggregate all observations over different video frames [26,22,24,23,12].",
            "2": "To further simplify the learning of the deformation fields, Animatable NeRF [24] resorts to a parametric human body model as a strong geometry prior to the deformation fields.",
            "3": "Animatable NeRF [24] can animate the performer to novel poses however it requires fine-tuning on the novel pose frames.",
            "4": "1 SMPL as Projection Reference with Non-linear Deformation To decrease NeRF\u2019s high requirement of camera numbers, a typical solution is to learn a deformation function \u03a6t(x) :R37\u2192R3to map sample points xin frame t to a shared canonical space [24] [26].",
            "5": "Following typical protocols [20] and works most related to us [24] [25], we evaluate our method on image synthesis using two metrics: peak signal-tonoise ratio (PSNR) and structural similarity index (SSIM).",
            "6": "2 Performance on NVS and NPS We compare our method with state-of-the-art view synthesis methods [25,24] that also use SMPL models and can handle dynamic scenes.",
            "7": "Animatable NeRF [24] predicts the blend weights for each sample point and aggregates observations across frames to a shared canonical representation and further improves on novel pose synthesis by fine-tuning on novel pose images.",
            "8": "Table 1 shows the comparison of our method with Neural Body [25] and Animatable NeRF [24] on ZJU-MoCap dataset.",
            "9": "Our method outperforms Animatable NeRF [24] by a margin of 0.",
            "10": "Figure 3 presents the qualitative comparison of our method with [25,24] on the ZJU-MoCap dataset.",
            "11": "Both [25] and [24] have difficulty in recovering fine details of the dynamic scene.",
            "12": "Animatable NeRF [24] shows more artifacts as the blur of the first person\u2019s face and the second person\u2019s clothes.",
            "13": "For the second row, Neural Body [25] losses wrinkles on the back and Animatable NeRF [24] suffers from artifacts.",
            "14": "Table 2 shows the comparison of our method with Neural Body [25] and Animatable NeRF [24] on novel pose synthesis.",
            "15": "Note that Animatable NeRF [24] needs to be fine-tuned on novel pose images while our method can be directly applied to novel pose synthesis.",
            "16": "Though fine-tuned on novel pose images, Animatable NeRF [24] has difficulty in modelling large movements and also leads to blur result.",
            "17": "PSNR SSIM NB [25] AN [24] OURS NB [25] AN [24] OURS\n313 30.",
            "18": "Results of novel pose synthesis on the ZJU-MoCap dataset in terms of PSNR and SSIM (higher is better) PSNR SSIM NB [25] AN [24] OURS NB [25] AN [24] OURS\n313 23.",
            "19": "PSNR SSIM NB [25] AN [24] OURS NB [25] AN [24] OURS novel view 23."
        },
        "Vmrf: View matching neural radiance fields": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2207.02621",
            "ref_texts": "[25] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 14314\u201314323.",
            "ref_ids": [
                "25"
            ],
            "1": "Due to the multi-view consistency of the generated images, NeRF as well as its variants [1,3,9,17,20,28,42] has attracted increasing attention in different tasks such as dynamic scene representation [7,8,10,24,31], color and shape editing [18,35], compositional scene modeling [23], scene relighting [2,30] and skeleton-driven synthesis [25]."
        },
        "Canonical Fields: Self-Supervised Learning of Pose-Canonicalized Neural Fields": {
            "authors": [
                "Rohith Agaram",
                "Shaurya Dewan",
                "Rahul Sajnani",
                "Adrien Poulenard",
                "Madhava Krishna",
                "Srinath Sridhar"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Agaram_Canonical_Fields_Self-Supervised_Learning_of_Pose-Canonicalized_Neural_Fields_CVPR_2023_paper.pdf",
            "ref_texts": "[33] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 14314\u201314323, 2021. 1",
            "ref_ids": [
                "33"
            ],
            "1": "In particular, neural radiance fields (NeRFs) [24], have been successfully used in problems such as novel view synthesis [3, 4, 66], scene geometry extraction [55, 61], capturing dynamic scenes [19, 29, 30, 33, 52], 3D semantic segmentation [53, 68], and robotics [1, 13, 21]."
        },
        "GM-NeRF: Learning Generalizable Model-based Neural Radiance Fields from Multi-view Images": {
            "authors": [
                "Jianchuan Chen",
                "Wentao Yi",
                "Liqian Ma",
                "Xu Jia",
                "Huchuan Lu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_GM-NeRF_Learning_Generalizable_Model-Based_Neural_Radiance_Fields_From_Multi-View_Images_CVPR_2023_paper.pdf",
            "ref_texts": "[30] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV , pages 14294\u201314303. IEEE, 2021. 2, 6, 8",
            "ref_ids": [
                "30"
            ],
            "1": "To alleviate this limitation, some works [6, 30, 31, 41, 48, 51] combine neural radiance fields [27] with SMPL [23] to represent the human body, which can be rendered to 2D images by differentiable rendering.",
            "2": "1427 Ani-N [30] 26.",
            "3": "We also compare with per-scene optimization methods NB [31], Ani-NeRF [30], A-NeRF [41], ARAH [48]."
        },
        "Layered-garment net: Generating multiple implicit garment layers from a single image": {
            "authors": [
                "Alakh Aggarwal",
                "Jikai Wang",
                "Steven Hogue",
                "Saifeng Ni",
                "Madhukar Budagavi",
                "Xiaohu Guo"
            ],
            "url": "https://openaccess.thecvf.com/content/ACCV2022/papers/Aggarwal_Layered-Garment_Net_Generating_Multiple_Implicit_Garment_Layers_from_a_Single_ACCV_2022_paper.pdf",
            "ref_texts": "39. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. (2021) 14314\u201314323",
            "ref_ids": [
                "39"
            ]
        },
        "NeuralDome: A Neural Modeling Pipeline on Multi-View Human-Object Interactions": {
            "authors": [
                "Juze Zhang",
                "Haimin Luo",
                "Hongdi Yang",
                "Xinru Xu",
                "Qianyang Wu",
                "Ye Shi",
                "Jingyi Yu",
                "Lan Xu",
                "Jingya Wang"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_NeuralDome_A_Neural_Modeling_Pipeline_on_Multi-View_Human-Object_Interactions_CVPR_2023_paper.pdf",
            "ref_texts": "[44] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 14314\u201314323, 2021. 2",
            "ref_ids": [
                "44"
            ],
            "1": "Existing works equip NeRF with pose-embeddings [23, 27, 40, 45, 80], learnable skinning weights [25, 44, 70] and even generalization across individuals [23, 66, 80]."
        },
        "Hvtr: Hybrid volumetric-textural rendering for human avatars": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2112.10203",
            "ref_texts": "[56] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 14314\u201314323, 2021. 1, 2, 3, 6, 11, 13",
            "ref_ids": [
                "56"
            ],
            "1": "Existing methods parameterize poses by global pose parameter conditioning [28, 39, 54, 84], 3D sparse points [57], or skinning weights [7, 29, 56].",
            "2": "2D : EDN [5], vid2vid [76] GAN 7 3\n2D Plus : SMPLpix [58], DNR [73], ANR[61]GAN 7 3\n3D : NB[57], AniNeRF[56] V olR 3 7\n3D : Ours Hybrid 3 3 Table 1: A set of recent human synthesis approaches classified by feature representations (2D/3D) and renderers.",
            "3": "For stable view synthesis, recent papers [7, 29, 48, 56, 57, 71, 83] propose to unify geometry reconstruction with view 2 Figure 1: We illustrate the differences between (left) GAN-based methods (DNR), (middle) our hybrid approach, and (right) NeRF methods (Neural Body [57]).",
            "4": "Furthermore, most animatable NeRF methods [7, 29, 48, 56, 71] parameterize clothes with skinning by learning backward warping, and they cannot handle skirts.",
            "5": "However, most animatable NeRF methods [7, 29, 48, 56] parameterize clothes with skinning in a backward warping step, can only handle clothing types that roughly follow the topological structure of the SMPL, but cannot handle skirts.",
            "6": "AniNeRF [56] parameterizes clothes with skinning weights by learning backward skinning, which generally requires a large training dataset, and may suffer from one-to-many backward correspondences [8] as shown in Fig.",
            "7": "The geometry-guided ray marching algorithm and UV conditioned architecture enable us to train a PD-NeRF\n12 Figure 14: Comparisons of backward skinning method (AniNeRF [56]) and forward skinning method (ours).",
            "8": "LPIPS# FID# SSIM\"PSNR\" AniNeRF [56] ."
        },
        "SteerNeRF: Accelerating NeRF Rendering via Smooth Viewpoint Trajectory": {
            "authors": [
                "Sicheng Li",
                "Hao Li",
                "Yue Wang",
                "Yiyi Liao",
                "Lu Yu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Li_SteerNeRF_Accelerating_NeRF_Rendering_via_Smooth_Viewpoint_Trajectory_CVPR_2023_paper.pdf",
            "ref_texts": "[31] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proc. of the IEEE International Conf. on Computer Vision (ICCV) , 2021. 2",
            "ref_ids": [
                "31"
            ]
        },
        "Fast-SNARF: A fast deformer for articulated neural fields": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2211.15601",
            "ref_texts": "[45] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proc. of the IEEE International Conf. on Computer Vision (ICCV) , 2021.",
            "ref_ids": [
                "45"
            ],
            "1": "These methods serve as a foundation for many tasks such as generative modeling of articulated objects or humans [3, 8, 12, 20, 40, 63], and reconstructing animatable avatars from scans [9, 13, 27, 34, 35, 51, 56], depth [14, 42, 57], videos [7, 24, 25, 26, 29, 39, 45, 47, 58, 59, 64] or a single image [18, 21, 60]."
        },
        "AvatarReX: Real-time Expressive Full-body Avatars": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2305.04789",
            "ref_texts": "41, 4 (2022), 102:1\u2013102:15. Ahmed A. A. Osman, Timo Bolkart, and Michael J. Black. 2020. STAR: Sparse Trained Articulated Human Body Regressor. In ECCV (6) . 598\u2013613. Jeong Joon Park, Peter Florence, Julian Straub, Richard A. Newcombe, and Steven Lovegrove. 2019. DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation. In CVPR . 165\u2013174. Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. 2019. Expressive Body Capture: 3D Hands, Face, and Body From a Single Image. In CVPR . 10975\u201310985. Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021a. Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. In ICCV . 14294\u201314303. Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021b. Neural Body: Implicit Neural Representations With Structured Latent Codes for Novel View Synthesis of Dynamic Humans. In CVPR . 9054\u20139063. Gerard Pons-Moll, Sergi Pujades, Sonny Hu, and Michael J. Black. 2017. ClothCap: seamless 4D clothing capture and retargeting. ACM Trans. Graph. 36, 4 (2017), 73:1\u201373:15. Sergey Prokudin, Michael J. Black, and Javier Romero. 2021. SMPLpix: Neural Avatars from 3D Human Models. In WACV . 1809\u20131818. Amit Raj, Julian Tanke, James Hays, Minh Vo, Carsten Stoll, and Christoph Lassner."
        },
        "KeypointNeRF: Generalizing image-based volumetric avatars using relative spatial encoding of keypoints": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2205.04992.pdf?trk=public_post_comment-text",
            "ref_texts": "44. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14314\u201314323 (2021)",
            "ref_ids": [
                "44"
            ],
            "1": "Recent approaches have incorporated priors specific to human faces [8, 9, 15, 17, 50, 61, 72] and human bodies [44,45,64,66,67,71,73,74] to reduce the dependence on multi-view captures."
        },
        "Vaxnerf: Revisiting the classic for voxel-accelerated neural radiance field": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2111.13112",
            "ref_texts": "[51] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV , 2021. 1",
            "ref_ids": [
                "51"
            ],
            "1": "Many extensions and applications quickly ensued, including dynamic scene rendering [16, 31, 47, 50, 51, 67], controllable relighting [3, 61, 77, 87], latent appearance and shape priors [6, 57, 68], scene composition [21, 45, 49, 79], and pose estimation [24, 32, 38, 62, 82]."
        },
        "BAD-NeRF: Bundle Adjusted Deblur Neural Radiance Fields": {
            "authors": [
                "Peng Wang",
                "Lingzhe Zhao",
                "Ruijie Ma",
                "Peidong Liu"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Wang_BAD-NeRF_Bundle_Adjusted_Deblur_Neural_Radiance_Fields_CVPR_2023_paper.pdf",
            "ref_texts": "[35] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In International Conference on Computer Vision (ICCV) , 2021. 2",
            "ref_ids": [
                "35"
            ]
        },
        "Structured 3D Features for Reconstructing Controllable Avatars": {
            "authors": [
                "Enric Corona",
                "Mihai Zanfir",
                "Thiemo Alldieck",
                "Eduard Gabriel",
                "Andrei Zanfir",
                "Cristian Sminchisescu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Corona_Structured_3D_Features_for_Reconstructing_Controllable_Avatars_CVPR_2023_paper.pdf",
            "ref_texts": "[45] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In CVPR , 2021. 3",
            "ref_ids": [
                "45"
            ],
            "1": "NeRFs have been recently explored for novel human view synthesis [11, 14, 25, 45, 46, 55, 60, 61, 63]."
        },
        "PoseVocab: Learning Joint-structured Pose Embeddings for Human Avatar Modeling": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2304.13006",
            "ref_texts": "(2022). Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021a. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV . 14314\u201314323. Sida Peng, Shangzhan Zhang, Zhen Xu, Chen Geng, Boyi Jiang, Hujun Bao, and Xiaowei Zhou. 2022b. Animatable Neural Implicit Surfaces for Creating Avatars from Videos. arXiv preprint arXiv:2203.08133 (2022).Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021b. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR . 9054\u20139063. Edoardo Remelli, Timur Bagautdinov, Shunsuke Saito, Chenglei Wu, Tomas Simon, Shih-En Wei, Kaiwen Guo, Zhe Cao, Fabian Prada, Jason Saragih, et al .2022. Drivable volumetric avatars using texel-aligned features. In ACM SIGGRAPH 2022 Conference Proceedings . 1\u20139. Shunsuke Saito, Jinlong Yang, Qianli Ma, and Michael J Black. 2021. SCANimate: Weakly supervised learning of skinned clothed avatar networks. In CVPR . 2886\u2013"
        },
        "Dual-space nerf: Learning animatable avatars and scene lighting in separate spaces": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2208.14851",
            "ref_texts": "[19] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV , 2021. 1, 2, 3, 5, 6, 7, 11, 12",
            "ref_ids": [
                "19"
            ],
            "1": "Several recent works [20, 9, 19, 15, 24, 26, 27, 8] have adapted NeRF onto human body reconstruction and animation, but challenges remain in the following aspects: Recent methods that model the human body with NeRF [14] mostly learn the human body in a canonical space, but the lighting inconsistency in the canonical space lacks exploration [20, 9, 19, 15, 24].",
            "2": "Animatable NeRF [19] resolves novel-pose synthesis by mapping observed points into a canonical space with inverse linear blend skinning (LBS).",
            "3": "Since the LBS weight of a spatial point varies for different poses, Animatable NeRF [19] learns a neural blending weight network conditioned on the pose, which requires additional training for novel poses.",
            "4": "AniNeRF [19] learns a neural blending weight field to learn the LBS weights for each particular pose.",
            "5": "Although the per-frame (or per-pose) lighting embeddings used by previous methods [19, 17] do help relieve the inconsistency of lighting in the training frames, they cannot generalize to novel poses.",
            "6": "To accelerate training and inference, we abandon the coarse-to-fine strategy [14, 20, 19].",
            "7": "We follow the experimental settings of Neural Body [20] and AniNeRF [19].",
            "8": "Since we focus on the generalization ability of the model under novel poses, we compare our method with two state-of-the-art methods [20, 19] on novel pose synthesis.",
            "9": "AniNeRF [19] releases results only on Human3.",
            "10": "5, our method produces fewer artifacts than AniNeRF [19], indicating a better correspondences across frames.",
            "11": "\u201cNB\u201d means Neural Body[20], and \u201cAN\u201d means AniNeRF [19].",
            "12": "044 Table 1: Comparison with baselines on novel pose synthesis on ZJU-MoCap [20], \u201cNB\u201d means Neural Body [20], and \u201cAN\u201d means AniNeRF [19].",
            "13": "\u201cNB\u201d means Neural Body [20], and \u201cAN\u201d means AniNeRF [19].",
            "14": "For the blending weights, we follow the strategy of AniNeRF [19], which interpolates the blending weights from nearby SMPL vertices.",
            "15": "Our method exceeds AniNeRF [19], which explicitly builds correspondences across frames like our method, by a large margin in all metrics.",
            "16": "\u201cNB\u201d means Neural Body, and \u201cAN\u201d means AniNeRF [19].",
            "17": "Our method outperforms AniNeRF [19] and is comparable to Neural Body [20] on the perceptual metric.",
            "18": "\u201cNB\u201d means Neural Body [20], and \u201cAN\u201d means AniNeRF [19].",
            "19": "AniNeRF [19] lacks high-fidelity details such as wrinkles and lighting.",
            "20": "The results of Neural Body [20] and our method exhibit fewer artifacts compared with AniNeRF [19]."
        },
        "Neural Transformation Fields for Arbitrary-Styled Font Generation": {
            "authors": [
                "Bin Fu",
                "Junjun He",
                "Jianjun Wang",
                "Yu Qiao"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Fu_Neural_Transformation_Fields_for_Arbitrary-Styled_Font_Generation_CVPR_2023_paper.pdf",
            "ref_texts": "[29] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 14314\u201314323, 2021. 3",
            "ref_ids": [
                "29"
            ],
            "1": "For example, Animatable NeRF [29] employs NeRF to embed 3D static human bodies and utilizes a deformation field to model body movement by transforming observation-space points to the canonical space."
        },
        "Efficient neural radiance fields for interactive free-viewpoint video": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2112.01517",
            "ref_texts": "(2022). Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas Kurz, Joerg H Mueller, Chakravarty R Alla Chaitanya, Anton Kaplanyan, and Markus Steinberger. 2021. DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks. In EGSR . Michael Oechsle, Songyou Peng, and Andreas Geiger. 2021. Unisurf: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction. In ICCV . Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Steven M. Seitz, and Ricardo Martin-Brualla. 2021a. Nerfies: Deformable Neural Radiance Fields. In ICCV . Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, and Steven M Seitz. 2021b. Hypernerf: A higher-dimensional representation for topologically varying neural radiance fields. arXiv preprint arXiv:2106.13228 (2021). Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021a. Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. In ICCV . Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021b. Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. In CVPR . Eric Penner and Li Zhang. 2017. Soft 3D reconstruction for view synthesis. ACM TOG"
        },
        "Free-viewpoint rgb-d human performance capture and rendering": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2112.13889",
            "ref_texts": "50. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: ICCV (2021) 1, 4",
            "ref_ids": [
                "50"
            ],
            "1": "However, synthesizing novel views of humans in motion requires methods to handle dynamic scenes with various deformations which is a challenging task [62,67]; especially in those regions with fine details such as the face or the clothes [46,50,63,66].",
            "2": "Given multi-view input frames or videos, recent works on rendering animate humans from novel views show impressive results [46,50,51,66]."
        },
        "UV Volumes for real-time rendering of editable free-view human performance": {
            "authors": [
                "Yue Chen",
                "Xuan Wang",
                "Xingyu Chen",
                "Qi Zhang",
                "Xiaoyu Li",
                "Yu Guo",
                "Jue Wang",
                "Fei Wang"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_UV_Volumes_for_Real-Time_Rendering_of_Editable_Free-View_Human_Performance_CVPR_2023_paper.pdf",
            "ref_texts": "[38] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE International Conference on Computer Vision , 2021. 3, 6",
            "ref_ids": [
                "38"
            ],
            "1": "Some methods [38, 54, 57] decompose a dynamic human into a canonical neural radiance field and a skeleton-driven warp field that backward maps observation-space points to canonical space.",
            "2": "To validate our method, we compare it againstseveral state-of-the-art free-view video synthesis techniques: 1) DN: DyNeRF [25], which takes time-varying latent codes as the conditions for dynamic scenes; and 2) NB: NeuralBody [39], which takes as input the posed human model with structured time-invariant latent codes and generates a pose-conditioned neural radiance field; 3) AN: Animatable-NeRF [38], which uses neural blend weight fields to generate correspondences between observation and canonical space."
        },
        "Nerfcap: Human performance capture with dynamic neural radiance fields": {
            "authors": [],
            "url": "http://www.cad.zju.edu.cn/home/gfzhang/papers/NerfCap/NerfCap_TVCG_2022.pdf",
            "ref_texts": "[16] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in ICCV , 2021.",
            "ref_ids": [
                "16"
            ],
            "1": "Compared against polygon meshes, NeRFs can more flexibly represent temporally varying geometry and appearance without being concerned about the topological change, while being able to render more photo-realistic images with neural volume rendering [15], [16], [17], [18], [19].",
            "2": "To represent the deformation field, previous methods either adopt the linear blend skinning [15], [16] that cannot capture nonlinear local deformation or combine the linear blend skinning with a residual displacement [20], [21] that is unconstrained by any motion prior.",
            "3": "In addition, the prior works [15], [16], [20], [21] are primarily proposed to synthesize novel views for dynamic humans, while our goal is to not only achieve high-quality novel-view synthesis but also capture the dense space-time coherent geometry with frame-to-frame correspondences by integrating the human template tracking into the dynamic NeRF.",
            "4": "3 Neural implicit representation-based methods The 3D surfaces can be reconstructed by learning implicit representations such as voxel representation [28], [51], implicit function [25], [26], or neural radiance fields [14], [16], [17], [18], [19].",
            "5": "Neuralbody [15] and AniNeRF [16] represent a dynamic human NeRF based on the SMPL model [32] or by combining skeleton-driven deformation [52] with learned blend weights, which regularizes the learning of deformation fields and achieves impressive novel-view synthesis.",
            "6": "The extended work, AniSDF [53] of AniNeRF [16] utilizes signed distance fields to model the human geometry, and achieves better geometry reconstruction accuracy compared to the density fields [16].",
            "7": "H-NeRF [54] unifies NeRF and signed distance field for recovering dynamic humans, which adopts the articulated deformation model similar to [15], [16].",
            "8": "However, the existing human NeRF methods [15], [16], [20], [21], [53], [54] build temporal correspondences among the video frames using the skeletondriven framework, which fails to represent the motion of loose clothes.",
            "9": "For each vertex on the 3D template, we compute its skinning weight by finding the closest surface point on the SMPL model as [16].",
            "10": "Some recent works extend the original NeRF to dynamic humans, but they are limited to small and slow motions [18], [19] or cannot recover detailed deformations for humans in loose clothes [16], [17], [20].",
            "11": "In [16], all the sampled points along the ray within a human bounding box are contributed to the volume rendering (as illustrated in Figure 4 (a)).",
            "12": "(a) The volume rendering of [16] integrates all the space points along the ray.",
            "13": "Following [16], we evaluate the novel-view image synthesis using two metrics: peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM).",
            "14": "We use the same color network as [16] without outputting the density and extract the appearance features from input images instead of the per-frame latent code [16].",
            "15": "For a monocular video of300 frames, the fine-tuning takes less than 15minutes, which is very fast compared to the prior works [15], [16].",
            "16": "3 Comparison to state-of-the-art methods To validate our method, we compare with three stateof-the-art methods for human body reconstruction using NeRF, Neuralbody [15], AniNeRF [16], and AniSDF [53]\n(the extended work of AniNeRF).",
            "17": "Based on the human deformationMethodSilhouette Alignment Image Synthesis AMVIoU RVIoU SVIoU PSNR SSIM AniNeRF [16] 88.",
            "18": "MethodSilhouette Alignment Image Synthesis AMVIoU RVIoU SVIoU PSNR SSIM AniNeRF [16] 80.",
            "19": "(b) AniNeRF [16].",
            "20": "1 Comparison to novel-view synthesis methods of dynamic humans [15], [16], [17], [53] We perform both qualitative and quantitative comparisons on \u201cFranziRed\u201d from DynaCap and subject S4 of DeepCap.",
            "21": "The performance of Neuralbody [15], 8 AniNeRF [16], and AniSDF [53] degrades dramatically on humans in loose clothes, and AniNeRF even cannot converge on the \u201cFranziRed\u201d data.",
            "22": "This is because the skeleton-driven deformation adopted in [15], [16], [53] cannot represent the motion of loose skirts.",
            "23": "Instead of introducing a per-frame latent code as [15], [16], [53], our method extracts deep features from the input images, allowing us to train the networks end to end.",
            "24": ", [15], [16], [17], and [53]) because incorporating a template needs to estimate non-rigid deformations and requires large changes in the method.",
            "25": "Note that the methods of [15], [16], [53] use the SMPL template models.",
            "26": "The volume rendering method adopted in [16] use all the sampled points along the ray (similar to the original method in NeRF [14]).",
            "27": "(c,f) The novel-view synthesis result using the original point sampling [16].",
            "28": "We can see that there are clear artifacts in the synthesized images using the rendering method of [16].",
            "29": "Both NeuralActor [21] and HumanNeRF [20] model nonrigid human deformation with a residual displacement on top of the linear blend skinning so that they can represent the nonlinear local deformation that fails to be captured by the standard skinning adopted in [15], [16].",
            "30": "[16] S."
        },
        "Artemis: articulated neural pets with appearance and motion synthesis": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2202.05628",
            "ref_texts": "2021. DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks. Computer Graphics Forum 40, 4 (2021). https://doi.org/10.1111/cgf.14340 Sylvain Paris, Hector M. Brice\u00f1o, and Fran\u00e7ois X. Sillion. 2004. Capture of Hair Geometry from Multiple Images. ACM Trans. Graph. 23, 3 (aug 2004), 712\u2013719. https://doi.org/10.1145/1015706.1015784 Sylvain Paris, Will Chang, Oleg I. Kozhushnyan, Wojciech Jarosz, Wojciech Matusik, Matthias Zwicker, and Fr\u00e9do Durand. 2008. Hair Photobooth: Geometric and Photometric Acquisition of Real Hairstyles. ACM Trans. Graph. 27, 3 (aug 2008), 1\u20139. https://doi.org/10.1145/1360612.1360629 Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. 2019. Deepsdf: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 165\u2013174. Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Steven M Seitz, and Ricardo Martin-Brualla. 2021a. Nerfies: Deformable neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 5865\u20135874. Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, and Steven M. Seitz. 2021b. HyperNeRF: A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields. ACM Trans. Graph. 40, 6, Article 238 (dec 2021). Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. 2019. Expressive Body Capture: 3D Hands, Face, and Body from a Single Image. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) . Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021a. Animatable Neural Radiance Fields for Modeling ACM Trans. Graph., Vol. 41, No. 4, Article 164. Publication date: July 2022. Artemis: Articulated Neural Pets with Appearance and Motion Synthesis \u2022164:19 Dynamic Human Bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 14314\u201314323. Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger.",
            "ref_ids": [
                "2021"
            ]
        },
        "General neural gauge fields": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2305.03462",
            "ref_texts": "11 Published as a conference paper at ICLR 2023 Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Steven M. Seitz, and Ricardo Martin-Brualla. Deformable neural radiance fields. arXiv preprint arXiv:2011.12948 , 2020. Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 14314\u201314323, 2021. Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. Convolutional occupancy networks. In European Conference on Computer Vision , pp. 523\u2013540. Springer, 2020. Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes, 2020. Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 14335\u201314345, 2021. Yossi Rubner, Carlo Tomasi, and Leonidas J Guibas. The earth mover\u2019s distance as a metric for image retrieval. International journal of computer vision , 40(2):99\u2013121, 2000. Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. Graf: Generative radiance fields for 3d-aware image synthesis. Advances in Neural Information Processing Systems , 33:20154\u2013"
        },
        "Frequency-Modulated Point Cloud Rendering with Easy Editing": {
            "authors": [
                "Yi Zhang",
                "Xiaoyang Huang",
                "Bingbing Ni",
                "Teng Li",
                "Wenjun Zhang"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Frequency-Modulated_Point_Cloud_Rendering_With_Easy_Editing_CVPR_2023_paper.pdf",
            "ref_texts": "[30] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 14314\u201314323, 2021. 2",
            "ref_ids": [
                "30"
            ]
        },
        "Unbiased 4d: Monocular 4d reconstruction with a neural deformation model": {
            "authors": [
                "Erik C",
                "Marc Habermann",
                "Soshi Shimada",
                "Vladislav Golyanik",
                "Christian Theobalt"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023W/DynaVis/papers/Johnson_Unbiased_4D_Monocular_4D_Reconstruction_With_a_Neural_Deformation_Model_CVPRW_2023_paper.pdf",
            "ref_texts": "[27] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In International Conference on Computer Vision (ICCV) , 2021. 2",
            "ref_ids": [
                "27"
            ]
        },
        "Envisioning a Next Generation Extended Reality Conferencing System With Efficient Photorealistic Human Rendering": {
            "authors": [
                "Chuanyue Shen",
                "Letian Zhang",
                "Zhangsihao Yang",
                "Masood Mortazavi",
                "Xiyun Song",
                "Liang Peng",
                "Heather Yu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023W/ECV/papers/Shen_Envisioning_a_Next_Generation_Extended_Reality_Conferencing_System_With_Efficient_CVPRW_2023_paper.pdf",
            "ref_texts": "[24] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV , 2021. 1, 2",
            "ref_ids": [
                "24"
            ],
            "1": "It catalyzes a wave of human neural rendering methods that deliver high fidelity results [9,15,24,26,32,33].",
            "2": "Due to its high-quality performance while being simple and extendable, the use of NeRF as a core algorithm has been widely explored in a variety of scene representation and rendering tasks, such as pose estimation [24,26,29,32], lighting [1,2,37], scene labeling and understanding [30,39], and scene composition [23, 34].",
            "3": "Animatable NeRF [24] introduces a per-frame neural blend weight field to be combined with NeRF, while using human priors from SMPL to regularize the learned blend weight."
        },
        "Neural novel actor: Learning a generalized animatable neural representation for human actors": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2208.11905",
            "ref_texts": "[44] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao. Animatable neural radiance fields for modeling dynamic human bodies. InProceedings oftheIEEE/CVF International Conference onComputer Vision, pp. 14314\u201314323, 2021.",
            "ref_ids": [
                "44"
            ],
            "1": "Some methods [6], [31], [44], [56] can learn an animatable human representation from multi-view imagery in a person-specific setting, but they are not able to generalize to new persons.",
            "2": "Leveraging the Skinned Multi-Person Linear (SMPL) model [36], several works [3], [6], [31], [44], [56] manage to obtain an animatable neural representation for humans.",
            "3": "To evaluate the performance of this task, we compare our method with Neural Body (NB), Animatable Nerf (AN) [44], Neural Actor (NA) [31] and MPS-NeRF (MPS) [17] on the ZJU-MoCap dataset.",
            "4": "Moreover, Animatable NeRF (AN) [44] and Neural Body (NB) [45] do not account for selfocclusion in sparse view and lack specific designs for completing the missing parts during training.",
            "5": "We compared with three person-specific methods, Neural Body (NB)\n[45], Animatable Nerf (AN) [44], Neural Actor (NA) [31] and the generalized method, MPS-NeRF (MPS) [17].",
            "6": "Our method achieves the best performance in two metrics, compared to three person-specific animatable human models, NeuralBody (NB) [45], AnimatbleNerf (AN) [44], and Neural Actor (NA) [31] and MPS-NeRF (MPS) [17]\n.",
            "7": "[44] S."
        },
        "Clothed Human Performance Capture With a Double-Layer Neural Radiance Fields": {
            "authors": [
                "Kangkan Wang",
                "Guofeng Zhang",
                "Suxu Cong",
                "Jian Yang"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Clothed_Human_Performance_Capture_With_a_Double-Layer_Neural_Radiance_Fields_CVPR_2023_paper.pdf",
            "ref_texts": "[24] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Ani-matable neural radiance fields for modeling dynamic humanbodies. In ICCV , 2021. 2,3,4,5",
            "ref_ids": [
                "24"
            ],
            "1": "Clothed humans can be reconstructed through implicit representation-based methods such as voxel representation [33,42\n], implicit function [27,28], or neural radiance fields (NeRFs) [20,24,25].",
            "2": "Neuralbody [25] and AniNeRF [24] extend NeRFs for a dynamic human with defor21099\n Color Loss Skeleton-Driven DeformationInverse Deformation Fields Input Color Rendered Color Non-Rigid DeformationGeometry Clothing Layer Input Mask Rendered Mask Simulated ClothSimulation Loss\u089e Predicted Cloth Clothed Human Performance CaptureMask Loss Combined Mesh\u0010\n\u072f\n\u072f\n\u0729 Optimization Losses\u0729\u0be6Canonical Space Cloth SimulationColor Body Layer Double-layer NeRF\u0bda\u0bdc\n\u0c25Composite Rendering Point Cloud RendererObserved Space \u0bd6\u0b65\u123d\n\u0bd7\u0b65\u123d Figure 1.",
            "3": "Double-layer NeRFs for Dynamic Humans Unlike a single NeRFs for clothed humans [17,24,25,34, 41], we represent the clothing with an independent NeRFs on the body, which forms a double-layer NeRFs.",
            "4": "The color network Fcin canonical frame is formulated as, ci(x)=Fc(\u03b3x(x),\u03d5i), (3) where\u03d5iis an appearance latent code [24,25] for frame i.",
            "5": ", peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) as [24,34]."
        },
        "Generalizing Neural Human Fitting to Unseen Poses With Articulated SE (3) Equivariance": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2304.10528",
            "ref_texts": "[36] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In International Conference on Computer Vision (ICCV) , pages 14314\u201314323, 2021. 1",
            "ref_ids": [
                "36"
            ],
            "1": "Introduction The three-dimensional (3D) capture of humans in varied poses is increasingly common and has many applications including synthetic data generation [35], human health analysis [57], apparel design and sizing [51], and avatar creation [10, 36, 50, 55]."
        },
        "Neural Kaleidoscopic Space Sculpting": {
            "authors": [
                "Byeongjoo Ahn",
                "Michael De",
                "Ioannis Gkioulekas",
                "Aswin C. Sankaranarayanan"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Ahn_Neural_Kaleidoscopic_Space_Sculpting_CVPR_2023_paper.pdf",
            "ref_texts": "[26] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2021. 2",
            "ref_ids": [
                "26"
            ],
            "1": "The most successful among these work use class-specific priors for faces and the human body[17,26,27,32,35]."
        },
        "Implicit Neural Head Synthesis via Controllable Local Deformation Fields": {
            "authors": [
                "Chuhan Chen",
                "Matthew O",
                "Gaurav Bharaj",
                "Pablo Garrido"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Implicit_Neural_Head_Synthesis_via_Controllable_Local_Deformation_Fields_CVPR_2023_paper.pdf",
            "ref_texts": "[37] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. InICCV , pages 14294\u201314303. IEEE, 2021. 4",
            "ref_ids": [
                "37"
            ],
            "1": "Local Deformation Fields Inspired by advances in part-based implicit rigging [33, 37, 67, 70], we overcome limitations of previous work by decomposing the global deformation field into multiple local fields, each centered around a pre-defined facial landmark location, to model non-linear local expression deformations with higher level of details, as shown in Fig."
        },
        "CAT-NeRF: Constancy-Aware Tx2Former for Dynamic Body Modeling": {
            "authors": [
                "Haidong Zhu",
                "Zhaoheng Zheng",
                "Wanrong Zheng",
                "Ram Nevatia"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023W/DynaVis/papers/Zhu_CAT-NeRF_Constancy-Aware_Tx2Former_for_Dynamic_Body_Modeling_CVPRW_2023_paper.pdf",
            "ref_texts": "[32] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV , pages 14314\u201314323, 2021. 1, 2, 3, 4, 5, 6, 7, 11, 12",
            "ref_ids": [
                "32"
            ],
            "1": "To align the points between these two spaces, researchers use SE(3) [29] or a translation vector field [32,35] for building correspondences between the canonical and observation spaces.",
            "2": "To solve this problem, we separate appearance constancy and uniqueness between the frames based on the neural blend weight fields [32] with Constancy Awareness Tx2Former, abbreviated as CAT-NeRF.",
            "3": "Recently some papers [20, 27,29, 32,33, 35] introduce decomposing the neural radiance from the observation space to canonical space for modeling the movement of an object.",
            "4": "By predicting the cor-respondences in the canonical space [29,32,35], deformable NeRF finds the connection between every point in the observation space and the canonical space and uses the moving object in the video for the construction of a unique object.",
            "5": "To animate the body shapes and render them in the scene, animatable NeRF, different from the deformable methods, projects the body shape into a canonical space [18, 32, 33, 49] or a common shape shapes [34, 36] for projecting the human body shape from different frames into a shared shape or space.",
            "6": "SNARF [4] and Animatable NeRF [32] utilize the neural blend weight field with frame-level correction for building such correspondence with statistical methods, while TA V A [18] uses the linear blend skinning (LBS) and apply a constant change for modeling muscles and clothing dynamics.",
            "7": "We first briefly review Animatable NeRF [32] in 3.",
            "8": "Animatable NeRF for Human Modeling To represent a dynamic scene or object in the video, Animatable NeRF [32] constructs two spaces: one observation space representing the shape we observe for each individual frame and one canonical space shared by all the frames describing the same object with a default pose.",
            "9": "Objective To train the model, we follow [32] to build the objective function for training \u03c8c,\u03c8u i,Fconst \u2206w,Funique \u2206w ,G(\u00b7),F\u03c3and Fcjointly.",
            "10": "3, we follow [32] for establishing Lnsfto minimize the difference.",
            "11": "We follow [32] to select the frames and generate the splits for training and inference in our experiment.",
            "12": "In our experiment, we follow [32] to select the videos from subjects S1, S5, S6, S7, S8, S9 and S11.",
            "13": "We follow [32] to use the videos in four categories, \u201cTwirl\u201d, \u201cTaichi\u201d, \u201cWarmup\u201d, and \u201cPunch1\u201d in our experiment.",
            "14": "To train the network for novel camera viewpoints, we follow [32] to implement our network.",
            "15": "For both training steps, we use the Adam optimizer [17] and set the initial learning rate as 5e\u22124and decay it to1\n10 after 1,000 epochs with an exponential training scheduler following [32].",
            "16": "In our experiment, we compare our method with Neural Texture [44], NHR [50] and Animated NeRF [32].",
            "17": "We follow [32] to use the body shape reconstruction from SMPL [22] as the input for Neural Texture [44] as the coarse mesh to render the image and use the points sampled from the SMPL body shape reconstruction as the input for NHR.",
            "18": "For the results in the tables, NT is the result for Neural Texture [44] and AN is the result from Animatable NeRF [32].",
            "19": "For the results on the novel view setting shown in Table 1, we outperform the state-of-the-art baseline method, Animatable NeRF [32], on all splits in the dataset for both metrics we assessed.",
            "20": "In addition to the Neural Texture [44], NHR [50] and Animatable NeRF [32], we also compared with D-NeRF [35] for reconstruction."
        },
        "CAISE: conversational agent for image search and editing": {
            "authors": [
                "Hyounghun Kim",
                "Doo Soon",
                "Seunghyun Yoon",
                "Franck Dernoncourt",
                "Trung Bui",
                "Mohit Bansal"
            ],
            "url": "https://ojs.aaai.org/index.php/AAAI/article/view/21337/21086"
        },
        "Collaborative neural rendering using anime character sheets": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2207.05378",
            "ref_texts": "[Peng et al. , 2021 ]Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In International Conference on Computer Vision (ICCV) , 2021. 3",
            "ref_ids": [
                "Peng et al\\. , 2021 "
            ]
        },
        "Placing Human Animations into 3D Scenes by Learning Interaction-and Geometry-Driven Keyframes": {
            "authors": [
                "James F. Mullen",
                "Divya Kothandaraman",
                "Aniket Bera",
                "Dinesh Manocha"
            ],
            "url": "https://openaccess.thecvf.com/content/WACV2023/papers/Mullen_Placing_Human_Animations_Into_3D_Scenes_by_Learning_Interaction-_and_WACV_2023_paper.pdf",
            "ref_texts": "[38] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 14294\u201314303, Montreal, QC, Canada, Oct. 2021. IEEE.[39] Viraj Prabhu, Arjun Chandrasekaran, Kate Saenko, and Judy Hoffman. Active domain adaptation via clustering uncertainty-weighted embeddings. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8505\u20138514, 2021.",
            "ref_ids": [
                "38",
                "39"
            ],
            "1": "[38, 34, 58, 47] worked towards extending NeRF to articulated objects like humans.",
            "2": "To do so, it computes the relevance of each sample based on a variety of parameters such as uncertainty/ entropy, diversity [3], and a careful trade-off of uncertainty and diversity [39].",
            "3": "[39] Viraj Prabhu, Arjun Chandrasekaran, Kate Saenko, and Judy Hoffman."
        },
        "PREF: Predictability Regularized Neural Motion Fields": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2209.10691",
            "ref_texts": "39. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14314\u201314323",
            "ref_ids": [
                "39"
            ],
            "1": "NeRF led to a series of breakthroughs in the fields of 3D scene understanding and rendering, such as relighting [2,46,3], human face and body capture [14,34,40,39,49,24], and city-scale reconstruction [50,58,53,43]."
        },
        "CAMM: Building Category-Agnostic and Animatable 3D Models from Monocular Videos": {
            "authors": [
                "Tianshu Kuai",
                "Akash Karthikeyan",
                "Yash Kant",
                "Ashkan Mirzaei",
                "Igor Gilitschenski"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023W/DynaVis/papers/Kuai_CAMM_Building_Category-Agnostic_and_Animatable_3D_Models_From_Monocular_Videos_CVPRW_2023_paper.pdf",
            "ref_texts": "[39] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV , 2021. 1, 2",
            "ref_ids": [
                "39"
            ],
            "1": "They serve as base templates for methods that focus on reconstructing and animating 3D humans and animals [7,21,26,36,39,40,55,58,60].",
            "2": "Many works [7,12,13,21, 26,36,39,40,55,58,60\u201362,65] utilize these template shapes or pose priors from shape models to recover 3D shapes and perform animations."
        },
        "HOSNeRF: Dynamic Human-Object-Scene Neural Radiance Fields from a Single Video": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2304.12281",
            "ref_texts": "[35] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 14314\u201314323, 2021. 2",
            "ref_ids": [
                "35"
            ],
            "1": "Subsequent works have further improved on the generalizability [18, 4, 9] and animatability [35, 20] of human bodies."
        },
        "HandNeRF: Neural Radiance Fields for Animatable Interacting Hands": {
            "authors": [
                "Zhiyang Guo",
                "Wengang Zhou",
                "Min Wang",
                "Li Li",
                "Houqiang Li"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Guo_HandNeRF_Neural_Radiance_Fields_for_Animatable_Interacting_Hands_CVPR_2023_paper.pdf",
            "ref_texts": "[25] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV , pages 14314\u201314323, 2021. 1, 2, 4, 6, 8",
            "ref_ids": [
                "25"
            ],
            "1": "To address the above issues and push the boundary of realistic human hand modeling, motivated by the recent success of NeRF [17] in modeling human body [11, 25, 26], we propose HandNeRF , a novel framework that unifiedly models the geometry and texture of animatable interacting This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.",
            "2": "Similar LBS-based pipelines are adopted by a lot of works [11,12,25,32,37,40].",
            "3": "Most methods [11,37] regress an extra point-wise offset for samples, while some works like Animatable-NeRF [25] try to jointly optimize NeRF with the LBS weights for deformation.",
            "4": "Therefore, we follow previous works on NeRF for dynamic human body [11, 25, 37, 40] to leverage the parameterized human priors.",
            "5": "(5) Note that different from previous works [25, 32] relying on per-pose latent code to guide the deformation, we use pose representation instead, ensuring robustness to unseen poses.",
            "6": "1) Pose-NeRF: we modify Mip-NeRF [1] to learn a NeRF conditioned on pose; 2) Ani-NeRF: we adapt [25] to the setup of human hands; 3) NeuMan: we re-implement the \u201cHuman NeRF\u201d module of [11] on the settings of hands while preserving its various training losses.",
            "7": "Since AniNeRF [25] cannot directly generalize to unseen poses, we report its pose adaptation performance after re-training with blend weight consistency."
        },
        "DP-NeRF: Deblurred Neural Radiance Field With Physical Scene Priors": {
            "authors": [
                "Dogyoon Lee",
                "Minhyeok Lee",
                "Chajin Shin",
                "Sangyoun Lee"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_DP-NeRF_Deblurred_Neural_Radiance_Field_With_Physical_Scene_Priors_CVPR_2023_paper.pdf",
            "ref_texts": "[31] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 14314\u201314323, 2021. 2",
            "ref_ids": [
                "31"
            ],
            "1": "Due to the success of the NeRF in neural rendering, several studies have applied NeRF to other areas such as dynamic scenes [17\u201319, 29, 30, 33, 49, 55], generative models [28, 38], relighting [3, 23, 32, 42], human avatars [31, 45, 58], and 3D reconstruction [47, 50]."
        },
        "One-shot Implicit Animatable Avatars with Model-based Priors": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2212.02469",
            "ref_texts": "[44] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. In International Conference on Computer Vision (ICCV) , pages 14314\u201314323, 2021. 2, 6, 7, 14, 16",
            "ref_ids": [
                "44"
            ],
            "1": "CV] 21 Aug 2023 tured by well-calibrated multi-camera systems [46, 69, 65, 44, 75], or long monocular videos [64] where almost all parts of the human body are visible.",
            "2": "Method Subject dataExtra training dataInvisible area completionAnimatable NeuralBody [46] Ani-NeRF [44] HumanNeRF [64]multi-view images, monocular videosdata-free \u2717 \u2713 PiFU [54] PaMIR [79] ARCH [20] ARCH++ [17] PHORHUM [1]monocular images3D scans \u2713 \u2713 MPS-NeRF [14] NHP [27]sparse videos, multi-view imagesmulti-view videos\u2717 \u2713 MonoNHR [8]monocular imagesmulti-view images\u2713 \u2717 EV A3D [18]monocular imagesmonocular images\u2713 \u2713 ELICIT (ours)monocular imagesdata-free \u2713 \u2713 Table 1: Recent human rendering methods that are most relevant to our work.",
            "3": "Among which [46] learns structured latent codes on SMPL [34] mesh vertices, other methods construct the representation in a canonical space by modeling pose-driven deformation [64, 69, 59, 78, 45, 44].",
            "4": "We selected three state-of-the-art methods as baselines: Neural Body [46] (NB) and Animatable NeRF [44] (AniNeRF) from per-subject optimization methods, and Neural Human Performer [27] (NHP) from generalizable methods.",
            "5": "Compared with state-of-the-art NeRF based methods[46, 27, 44] on novel view synthesis and novel pose synthesis, ELICIT generates human 3D renderings with more consistent appearance and realistic details from a single image.",
            "6": "For per-subject optimization methods NB [46] and Ani-NeRF [44], we optimized one model for each frame.",
            "7": "264 Ani-NeRF[44] 20.",
            "8": "1 Data splitting For per-subject optimization methods Animatable NeRF[44] (Ani-NeRF) and NeuralBody[46] (NB), we use all subjects of ZJU-MoCap data-set (313, 315, 377, 386, 387, 390, 392, 393, 394) and the \u201dPosing\u201d sequences of Human 3.",
            "9": "NeuralBody [44].",
            "10": "Animatable NeRF [44].",
            "11": "For example, as shown in Figure 13, we replaced the HumanNeRF model used in ELICIT with an SDF-based model from Animatable NeRF[45, 44]."
        },
        "Skinned Motion Retargeting with Residual Perception of Motion Semantics & Geometry": {
            "authors": [
                "Jiaxu Zhang",
                "Junwu Weng",
                "Di Kang",
                "Fang Zhao",
                "Shaoli Huang",
                "Xuefei Zhe",
                "Linchao Bao",
                "Ying Shan",
                "Jue Wang",
                "Zhigang Tu"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Skinned_Motion_Retargeting_With_Residual_Perception_of_Motion_Semantics__CVPR_2023_paper.pdf",
            "ref_texts": "[19] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 14314\u201314323, 2021. 2",
            "ref_ids": [
                "19"
            ],
            "1": "[19] introduced the neural blend weight fields to reconstruct an animatable human model from a multi-view video."
        },
        "LiveHand: Real-time and Photorealistic Neural Hand Rendering": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2302.07672",
            "ref_texts": "[28] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 14314\u201314323, 2021. 2, 3",
            "ref_ids": [
                "28"
            ],
            "1": "Some works have extended these formulations beyond static scenes to enable photorealistic renderings of articulated objects such as the human body [38, 30, 26, 18, 28, 42, 10, 9].",
            "2": "For example, it has been used to model the geometry and appearance of clothed humans [38, 30, 26, 18, 28, 42, 9, 11, 29, 12]."
        },
        "Arah: Animatable volume rendering of articulated human sdfs": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2210.10036",
            "ref_texts": "58. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proc. of ICCV",
            "ref_ids": [
                "58"
            ],
            "1": "Neural Body [60] and Ani-NeRF [58], struggle with generalizing to unseen poses, our approach enables avatars that can be animated in extreme out-of-distribution poses.",
            "2": "(2) Existing approaches condition their NeRF networks [60] or canonicalization networks [58] on inputs in observation space.",
            "3": "Clothed Humans as Implicit Functions: Neural implicit functions [13, 44, 45, 55, 61] have been used to model clothed humans from various sensor inputs including monocular images [22,23,25,33,64\u201366,72,80,93], multi-view videos [30, 38, 52, 58, 60, 81], sparse point clouds [6, 14, 16, 77, 78, 94], or 3D meshes [11, 12, 15,47,48,67,74].",
            "4": "[38, 52, 58, 60, 81] take multi-view videos as inputs and do not need ground-truth geometry during training.",
            "5": "Neural Rendering of Animatable Clothed Humans: Differentiable neural rendering has been extended to model animatable human bodies by a number of recent works [52, 58, 60, 63, 72, 81].",
            "6": "Several recent works [52, 58, 72] propose to model the radiance field in canonical space and use a pre-defined or learned backward mapping to map query points from observation space to this canonical space.",
            "7": "AniNeRF [58] trains a backward LBS network that does not generalize well to outof-distribution poses, even when fine-tuned with a cycle consistency loss for its backward LBS network for each test pose.",
            "8": "4 Experiments We validate the generalization ability and reconstruction quality of our proposed method against several recent baselines [58, 60, 72].",
            "9": "For completeness, we also tested our approach on the H36M dataset [26] and report a quantitative comparison to [52,58] in Appendix G.",
            "10": "Baselines: We compare against three major baselines: Neural Body [60](NB), Ani-NeRF [58](AniN), and A-NeRF [72](AN).",
            "11": "For inference, we follow [58, 60] and crop an enlarged bounding box around the projected SMPL mesh on the image plane and render only pixels inside the bounding box.",
            "12": "For unseen test poses we follow the practice of [58, 60] and use the latent code Zof the last training frame as the input.",
            "13": "Neural Body [60], Ani-NeRF [58], and A-NeRF [72].",
            "14": "A simple uniform sampling strategy (as used in [58, 60]) produces stratified artifacts due to the discretized sampling.",
            "15": "We also report quantitative results on the H36M dataset [26], following the testing protocols proposed by [58] in Table G.",
            "16": "Numbers of NARF [52] and Ani-N [58] are reported in [84]."
        },
        "DRaCoN--Differentiable Rasterization Conditioned Neural Radiance Fields for Articulated Avatars": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2203.15798",
            "ref_texts": "[25] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In CVPR , 2021. 2, 3, 5, 6, 7, 8",
            "ref_ids": [
                "25"
            ],
            "1": "The state-of-the-art methods in this direction use implicit functions [31, 33] to model human avatars and use volumetric rendering for image generation [14,25,26,39].",
            "2": "The most recent methods take inspiration from NeRF [21] or its variants [3,13,24,43,50] and represent human avatars using pose-conditioned implicit 3D representations [11, 14, 25, 26, 42].",
            "3": "A-NeRF [39] and AnimatableNeRF [25] use body pose information to canonicalize the sampled rays and learn neural radiance fields in the canonical space, which helps the learned avatar to generalize across different poses.",
            "4": "In contrast to existing methods [25, 26, 39] which encode all appearance and geometric details in the NeRF module using a highly non-linear mapping, our approach learns these details in a well-defined UV-space thereby simplifies neural avatar learning.",
            "5": "Note that using SDF to represent body surface has the advantage that it is interpretable and the geometry can be extracted easily without the need of a careful selection of threshold for density values as used in existing NeRF-based neural avatar methods [25, 26, 39].",
            "6": "6M [10]: We follow the protocol defined in [25] and train our model on 3 cameras and evaluate the performance of our method on the remaining cameras for novel view and poses on a selected subset of of Human3.",
            "7": "Baselines We compare our approach with the following baselines: Animatable NeRF [25].",
            "8": "The main difference of [25] and our approach involves the residual calculation.",
            "9": "The generated images used for evaluation of the state-of-the-art methods are provided by [25].",
            "10": "5 show the performance of our method compared to above methods on the ZJU-MoCap and Hu6 A-NeRF Animatable Neural Ours Ground A-NeRF Animatable Neural Ours Ground [39] NeRF [25] Body [26] Truth [39] NeRF [25] Body [26] Truth Figure 3.",
            "11": "ZJU-315 ZJU-377 User Study SSIM\"PSNR\" LPIPS# SSIM\"PSNR\" LPIPS# Num V otes Mean preference Alexnet VGG Alexnet VGG A-NeRF [39] 0:930 17 :711 0 :094 0 :073 0:927 16 :528 0 :101 0 :077 53 7 :5\u00061:5% Anim-NeRF [25] 0:917 16 :774 0 :101 0 :083 0:940 17 :971 0 :075 0 :066 15 2 :5\u00060:64% NeuralBody [26] 0:947 18:608 0 :092 0 :090 0:950 20:072 0 :071 0 :060 168 24\u00063:25% Ours 0:922 21 :315 0 :053 0 :046 0:946 21 :084 0 :048 0 :044 464 66\u00064% Table 1.",
            "12": "7 NHR [45] NeuralBody [26] Animatable-NeRF [25] Ours GT Figure 5.",
            "13": "MSE # SSIM \" PSNR \" LPIPS # Alexnet VGG A-NeRF [39] 2:34 0 :946 26 :47 0 :082 0 :058 NHR [45] 0:82 0 :976 27 :91 0 :026 0 :024 NeuralBody [26] 0:66 0 :977 27 :93 0 :039 0 :029 Anim-NeRF [25] 0:50 0 :979 28 :11 0 :035 0 :028 Ours 0:32 0 :985 29 :42 0 :023 0 :022 Table 2."
        },
        "Structured 3d features for reconstructing relightable and animatable avatars": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2212.06820",
            "ref_texts": "[52] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In CVPR , 2021. 3",
            "ref_ids": [
                "52"
            ],
            "1": "NeRFs have been recently explored for novel human view synthesis [11, 15, 30, 52, 53, 64, 69, 70, 72]."
        },
        "Animatable implicit neural representations for creating realistic avatars from videos": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2203.08133",
            "ref_texts": "[21] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in ICCV , 2021.",
            "ref_ids": [
                "21"
            ],
            "1": "A preliminary version of this work appeared in ICCV\n2021 [21].",
            "2": "Similar to [21], [75], [76], [78], [80] leverage the LBS model to establish observation-to-canonical correspondences, which enables them to aggregate temporal observations in the input video.",
            "3": "We decompose the human motion into articulated and non-rigid deformations, which is represented by the LBS model [5], [21] and a neural displacement field, respectively.",
            "4": "Table 3 compares our method with [17], [21], [89] in terms of the P2S andCD metrics.",
            "5": "[17], [21], [89] and our \u201cNeRF-NBW\u201d, \u201cNeRFPDF\u201d representations model the human geometry with the volume density field, while our \u201cSDF-PDF\u201d representation adopt the signed distance field.",
            "6": "We empirically set the threshold of volume density to extract the geometry of [17], [21], [89] and our \u201cNeRF-NBW\u201d, \u201cNeRF-PDF\u201d.",
            "7": "Our representation \u201cSDF-PDF\u201d significantly outperforms baseline methods [17], [21], [89] by a margin of at least 0.",
            "8": "Our method adopts the network of [26], while the preliminary version of this work [21] uses the network of NeRF [6].",
            "9": "[21] S.",
            "10": "Following [21], we use three camera views for training and test on the remaining view.",
            "11": "[21] select video clips from the action \u201cPosing\u201d of S1, S5, S6, S7, S8, S9, and S11."
        },
        "3DMM-RF: Convolutional Radiance Fields for 3D Face Modeling": {
            "authors": [
                "Stathis Galanakis",
                "Baris Gecer",
                "Alexandros Lattas",
                "Stefanos Zafeiriou"
            ],
            "url": "https://openaccess.thecvf.com/content/WACV2023/papers/Galanakis_3DMM-RF_Convolutional_Radiance_Fields_for_3D_Face_Modeling_WACV_2023_paper.pdf",
            "ref_texts": "[61] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV , 2021.",
            "ref_ids": [
                "61"
            ],
            "1": "Other approaches focus on 3D shape reconstruction [89, 55, 83, 3], human bodies registration [62, 87, 61], dealing with non-static scenes [63, 56], scene editing [49, 36] and scene relighting [72, 9]."
        },
        "Get3DHuman: Lifting StyleGAN-Human into a 3D Generative Model using Pixel-aligned Reconstruction Priors": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2302.01162",
            "ref_texts": "[45] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV , 2021. 3",
            "ref_ids": [
                "45"
            ],
            "1": "Some other representations are also proposed for dealing with more complex clothed bodies, like point clouds [33, 35, 58], radiance fields [29, 41, 45, 54] and implicit fields [7, 37, 43, 57]."
        },
        "Neural Puppeteer: Keypoint-Based Neural Rendering of Dynamic Shapes": {
            "authors": [
                "Simon Giebenhain",
                "Urs Waldmann",
                "Ole Johannsen",
                "Bastian Goldluecke"
            ],
            "url": "https://openaccess.thecvf.com/content/ACCV2022/papers/Giebenhain_Neural_Puppeteer_Keypoint-Based_Neural_Rendering_of_Dynamic_Shapes_ACCV_2022_paper.pdf",
            "ref_texts": "42. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance ffelds for modeling dynamic human bodies. In: ICCV. pp. 14314{",
            "ref_ids": [
                "42"
            ],
            "1": "In addition we compare NePu to AniNeRF [42] on Human3.",
            "2": "In addition we compare NePu to AniNeRF [42] on Human3.",
            "3": "We trained NePu using the same training regime as AniNeRF [42]: We only trained on a single subject, using every 5 thof the ffrst 1300 frames of sequence \"Posing\" of subject \"S9\" for training and every 5 thof the 2840\n\n12 S.",
            "4": "The results of the novel view and pose comparison with AniNeRF [42] on Human3.",
            "5": "Compared to A-NeRF [52] and AniNeRF [42] the fundamental difierences in the rendering pipeline result in a signiffcant speed increase."
        },
        "RANA: Relightable Articulated Neural Avatars": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2212.03237",
            "ref_texts": "[38] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV , 2021. 2, 3, 7",
            "ref_ids": [
                "38"
            ],
            "1": "More recent methods learn a neural representation of the person and use neural renderers [49] to directly generate photorealistic images in 2 novel view novel pose generalizable relightable Method X NeuralBody [39], HumanNeRF [54] X X AnimatableNeRf [38], NeuMan [26] X X X ANR [40], TNA [44], StylePeople [18] X X Relighting4D [13] X X X X RANA (Ours) Table 1.",
            "2": "The 3D neural rendering methods represent the person using neural radiance fields [35] and render the target images using volume rendering [26, 38, 39, 50, 54].",
            "3": "Thanks to the design of RANA, we can pretrain on as many subjects as available, which is not possible with most of the state-of-the-art methods for human synthesis [38,39,50]."
        },
        "Deblurred Neural Radiance Field with Physical Scene Priors": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2211.12046",
            "ref_texts": "[31] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 14314\u201314323, 2021. 2",
            "ref_ids": [
                "31"
            ],
            "1": "Due to the success of the NeRF in neural rendering, several studies have applied NeRF to other areas such as dynamic scenes [17\u201319, 29, 30, 33, 49, 55], generative models [28, 38], relighting [3, 23, 32, 42], human avatars [31, 45, 58], and 3D reconstruction [47, 50]."
        },
        "MoRF: Mobile Realistic Fullbody Avatars from a Monocular Video": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2303.10275",
            "ref_texts": "[43] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV , 2021. 1, 2, 6",
            "ref_ids": [
                "43"
            ],
            "1": "Specifically, some rely on multi-view videos [33, 46], or a depth sensor [28, 55], some leverage implicit representations whose rendering process is far from realtime especially on mobile devices [4, 44, 37, 43, 30, 18], whilst others take days to converge on multiple GPUs [57, 61].",
            "2": "Some of the monocular fullbody avatar methods modelthe human geometry implicitly [30, 43, 59], others output the classical mesh+static texture format [3, 1, 2, 64, 15, 47].",
            "3": "66 N/A Comparison with state-of-the-art We compare our MoRF method against Anim-NeRF [43], HF-Avatar [64],Table 2: Mesh fitting used in training and validation."
        },
        "MARF: The Medial Atom Ray Field object representation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2307.00037",
            "ref_texts": "[31] Peng S, Dong J, Wang Q, Zhang S, Shuai Q, Zhou X, et al. Animatable neural radiance fields for modeling dynamic human bodies. ICCV, 2021.",
            "ref_ids": [
                "31"
            ],
            "1": "[31] Peng S, Dong J, Wang Q, Zhang S, Shuai Q, Zhou X, et al."
        },
        "ActorsNeRF: Animatable Few-shot Human Rendering with Generalizable NeRFs": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2304.14401",
            "ref_texts": "[32] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV , pages 14294\u201314303, 2021. 1, 2",
            "ref_ids": [
                "32"
            ],
            "1": "However, to achieve high-quality rendering, existing approaches [33, 23, 32, 29, 42] require a combination of synchronized multi-view videos and an instance-level NeRF network, trained on a specific human video sequence.",
            "2": "Recently, NeRF-based human representations have shown promise for high-quality view synthesis [33, 32, 50, 29, 19, 23, 42, 48, 55, 15, 10, 20, 45, 40, 41].",
            "3": "To better animate the human actor, subsequent works [23, 32] introduce a canonical space to align different body poses."
        },
        "FLNeRF: 3D Facial Landmarks Estimation in Neural Radiance Fields": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2211.11202",
            "ref_texts": "[53] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In IEEE/CVF International Conference on Computer Vision (ICCV) , 2021. 2",
            "ref_ids": [
                "53"
            ],
            "1": "Further studies [32, 67, 68, 30, 77, 59, 83, 39, 92, 56, 38, 72, 44, 23, 58, 88, 28, 12] have been done to improve the performance, efficiency and generalization of NeRF, with its variants quickly and widely adopted in dynamic scene reconstruction [52, 82, 37, 55, 18], novel scene composition [51, 89, 48, 25, 41, 85, 46, 35], articulated 3D shape reconstruction [86, 61, 78, 93, 31, 94, 84, 11, 49, 53], and various computer vision tasks, including face NeRFs [21, 5, 50, 69, 16, 29], the focus of this paper."
        },
        "Multimodal neural radiance field": {
            "authors": [],
            "url": "https://assets.amazon.science/a6/3d/ee31b6e24eec89e428f5ec4ad849/multimodal-neural-radiance-field.pdf",
            "ref_texts": "[8] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, \u201cAnimatable neural radiance fields for modeling dynamic human bodies,\u201d in ICCV , 2021.",
            "ref_ids": [
                "8"
            ],
            "1": "Based on NeRF, researchers havedeveloped different methods for image and video-related tasks, such as video synthesis and animation [6], [7], [8], [9], model reconstruction [10], [11], etc.",
            "2": "[8] S."
        },
        "Generalizable Neural Voxels for Fast Human Radiance Fields": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2303.15387",
            "ref_texts": "[58] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV , 2021. 1, 2, 4",
            "ref_ids": [
                "58"
            ],
            "1": "Some works [60, 86, 72, 12, 59, 58, 15, 33, 89] successfully apply NeRF methods to human body rendering frameworks.",
            "2": "To solve this problem, Animatable NeRF [58, 59] maps human poses from the observation space to the predefined canonical space.",
            "3": "Representing Canonical Bodies with Neural Voxels Most previous NeRF-based methods for human bodies [60, 86, 72, 12, 59, 58, 15, 33, 89] adopt purely implicit representations."
        },
        "Text-guided 3D Human Generation from 2D Collections": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2305.14312",
            "ref_texts": "2018. Which Training Methods for GANs do actually Converge? In International Conference on Machine Learning (ICML) . Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. 2023. Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures. In Conference on Computer Vision and Pattern Recognition (CVPR) . Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and Rana Hanocka. 2022. Text2Mesh: Text-Driven Neural Stylization for Meshes. In Conference on Computer Vision and Pattern Recognition (CVPR) . Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. 2020. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. In European Conference on Computer Vision (ECCV) . MMHuman3D. 2021. OpenMMLab 3D Human Parametric Model Toolbox and Benchmark. https://github.com/ open-mmlab/mmhuman3d . Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. 2022. Instant Neural Graphics Primitives with a Multiresolution Hash Encoding. In Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH) . Gimin Nam, Mariem Khlifi, Andrew Rodriguez, Alberto Tono, Linqi Zhou, and Paul Guerrero. 2022. 3D-LDM: Neural Implicit 3D Shape Generation with Latent Diffusion Models. In arXiv:2212.00842 . Charlie Nash, Yaroslav Ganin, Ali Eslami, and Peter W. Battaglia. 2020. PolyGen: An Autoregressive Generative Model of 3D Meshes. In International Conference on Machine Learning (ICML) . Atsuhiro Noguchi, Xiao Sun, Stephen Lin, and Tatsuya Harada. 2022. Unsupervised Learning of Efficient Geometry-Aware Neural Articulated Representations. In European Conference on Computer Vision (ECCV) . Roy Or-El, Xuan Luo, Mengyi Shan, Eli Shechtman, Jeong Joon Park, and Ira Kemelmacher-Shlizerman. 2022. StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation. In Conference on Computer Vision and Pattern Recognition (CVPR) . Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. 2019. DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation. In Conference on Computer Vision and Pattern Recognition (CVPR) . Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Ahmed A Timo Bolkart, A. Osman, Dimitrios Tzionas, and Michael J. Black. 2019. Expressive Body Capture: 3D Hands, Face, and Body from a Single Image. In Conference on Computer Vision and Pattern Recognition (CVPR) . Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. 2021a. Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. In International Conference on Computer Vision (ICCV) . Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021b. Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. InConference on Computer Vision and Pattern Recognition (CVPR) . Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. 2023. DreamFusion: Text-to-3D using 2D Diffusion. InInternational Conference on Learning Representations (ICLR) . Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From Natural Language Supervision. In International Conference on Machine Learning (ICML) . Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical Text-Conditional Image Generation with CLIP Latents. In arXiv:2204.06125 . Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-Shot Text-to-Image Generation. In International Conference on Machine Learning (ICML) . Ren\u00e9 Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. 2020. Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Crossdataset Transfer. In Transactions on Pattern Analysis and Machine Intelligence (TPAMI) . Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. 2016. Generative Adversarial Text to Image Synthesis. In International Conference on Machine Learning (ICML) .Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. 2023. TEXTure: Text-Guided Texturing of 3D Shapes. In Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH) . Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. 2022. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. In Conference on Neural Information Processing Systems (NeurIPS) . Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. 2021. GRAF: Generative Radiance Fields for 3DAware Image Synthesis. In Conference on Neural Information Processing Systems (NeurIPS) . Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon Ko, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung Lee, and Seungryong Kim. 2023. Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation. In arXiv:2303.07937 . Aliaksandra Shysheya, Egor Zakharov, Kara-Ali Aliev, Renat Bashirov, Egor Burkov, Karim Iskakov, Aleksei Ivakhnenko, Yury Malkov, Igor Pasechnik, Dmitry Ulyanov, Alexander Vakhitov, and Victor Lempitsky. 2019. Textured Neural Avatars. In Conference on Computer Vision and Pattern Recognition (CVPR) . Vincent Sitzmann, Julien N. P. Martel, Alexander W. Bergman, David B. Lindell, and Gordon Wetzstein. 2020. Implicit Neural Representations with Periodic Activation Functions. InConference on Neural Information Processing Systems (NeurIPS) . Ivan Skorokhodov, Aliaksandr Siarohin, Yinghao Xu, Jian Ren, Hsin-Ying Lee, Peter Wonka, and Sergey Tulyakov.",
            "ref_ids": [
                "2018"
            ]
        },
        "NPC: Neural Point Characters from Video": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2304.02013",
            "ref_texts": "[30] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV , 2021. 1, 2, 3, 4, 6, 7",
            "ref_ids": [
                "30"
            ],
            "1": "A prominent line of work follows the traditional animation pipeline\u2014rigging a surface mesh to an underlying skeleton and equipping the mesh either with a neural texture [2, 20] or learnable vertex features [14, 30, 31].",
            "2": "Recent work applies NeRF to reconstruct dynamic human appearance [14, 31, 33, 46] and learning animatable human models [17, 20, 26, 30, 37, 38, 44] from video sequences.",
            "3": ", T-pose) [6, 12, 20, 24, 30, 35, 41].",
            "4": "While learning backward mapping allows for high-quality reconstructions of dynamic human geometry and appearance, these methods typically 2 require 3D scans for training [6, 24, 35, 41], need a texture map [20], or depend on a surface prior [12, 30].",
            "5": "This is distinct from prior work using template meshes with artist-created LBS weights [20, 30, 31, 44].",
            "6": "Prior work tried to approximate it with another network [26, 30, 41], but this requires conditioning on the position in posed space and leads to poor generalization.",
            "7": "6M [10, 11, 30] test split .",
            "8": "6M [10, 11, 30] test split .",
            "9": "Experiments We quantify the improvements our NPC brings over the most recent surface-free approach TA V A [17], DANBO [37] and A-NeRF [38], as well as most recent and established approaches that leverage template or scan-based prior, including ARAH [44], Anim-NeRF [30] and NeuralBody [31].",
            "10": "6M [10, 11, 30] test split.",
            "11": "6M [10, 11, 30] test split.",
            "12": "202 NeRF [30], evaluating on a total of 7 subjects using the foreground maps provided by the authors."
        },
        "PACE: Data-Driven Virtual Agent Interaction in Dense and Cluttered Environments": {
            "authors": [
                "James F. Mullen",
                "Dinesh Manocha"
            ],
            "url": "https://arxiv.org/pdf/2303.14255",
            "ref_texts": "[47] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao. Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. In2021 IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 14294\u201314303. IEEE, Montreal, QC, Canada, Oct. 2021. doi: 10.1109/ ICCV48922.2021.01405 3",
            "ref_ids": [
                "47"
            ],
            "1": "[5,14,28,41 \u201343,47,57,67,68,72] worked towards extending NeRF to scenes with multiple objects and arbitrary backgrounds, or to articulated objects like humans.",
            "2": "2\n[47] S."
        },
        "IntrinsicNGP: Intrinsic Coordinate based Hash Encoding for Human NeRF": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2302.14683"
        },
        "INV: Towards Streaming Incremental Neural Videos": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2302.01532",
            "ref_texts": "[32] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV , 2021. 2[33] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR , 2021. 2",
            "ref_ids": [
                "32",
                "33"
            ],
            "1": "Some recent works [15, 21, 32, 33, 40] focus on animating clothed humans only."
        },
        "Evaluate Geometry of Radiance Field with Low-frequency Color Prior": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2304.04351",
            "ref_texts": "[26] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. In ICCV , pages 14314\u201314323, 2021. 1, 2",
            "ref_ids": [
                "26"
            ],
            "1": "From this path-breaking work, many works aimed at improving NeRF from several aspects, such as various challenge scenarios [47, 17, 26, 2, 19], inference speed [43, 12, 28, 9, 7], training efficiency [8, 13, 42, 32, 33, 22, 5], generalization *Equal contribution.",
            "2": "From this path-breaking work, there have been many works aiming at improving NeRF from several aspects, such as various challenge scenarios [47, 17, 2, 26, 14, 19, 35, 39, 48, 29, 24], inference speed [43, 12, 28, 9, 7], training efficiency [8, 13, 42, 32, 33, 22, 5], generalization ability [44, 37, 6], and so on."
        },
        "SynBody: Synthetic Dataset with Layered Human Models for 3D Human Perception and Modeling": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2303.17368",
            "ref_texts": "[29] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV , 2021. 2, 3, 8",
            "ref_ids": [
                "29"
            ],
            "1": "Introduction The fields of 3D human perception [15\u201318,26,39,40] and human reconstruction [10, 19, 29, 30] have become increasingly important, but the lack of available data has limited their development.",
            "2": "NeuralBody [30] incorporates prior from a statistical body template to learn dynamic sequence, while Animatable NeRF [29] proposes to reconstruct an animatable human model that generalizes to new poses.",
            "3": "108 AnimNeRF [29] 27.",
            "4": "total, including the vanilla NeRF [24], NeuralBody [30] and HumanNeRF [37] for novel view synthesis, AnimNeRF [29] for novel pose synthesis, NHP [19] for generalizable human NeRF (novel identity synthesis)."
        },
        "MEIL-NeRF: Memory-Efficient Incremental Learning of Neural Radiance Fields": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2212.08328",
            "ref_texts": "[43] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV , 2021. 2",
            "ref_ids": [
                "43"
            ],
            "1": "Among the attempts, neural radiance fields (NeRF) [36] has emerged as one of promising methods for its high reconstruction quality, spurring a large number of research works on improvements [3, 4, 9, 18, 22, 27, 28, 33, 37, 44, 52\u201355, 57\u201359] and a wide range of applications [56]: controllable human avatar [10, 29, 43], realistic game [17], robotics [20, 41, 50, 62], 3D-aware image generation [6, 7], and data compression [13]."
        },
        "UVA: Towards Unified Volumetric Avatar for View Synthesis, Pose rendering, Geometry and Texture Editing": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2304.06969",
            "ref_texts": "[31] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. InProceedings of the IEEE/CVF International Conference on Computer Vision , pages 14314\u201314323, 2021.",
            "ref_ids": [
                "31"
            ],
            "1": "NeRF [26] 4 8 8 8 Neumesh [51] 4 8 4 4 Ani-NeRF [31] 4 4 8 8 UV-V olumes [6] 4 4 4 8 Ours (UV A) 4 4 4 4 Table 1: Comparison of the rendering and editing abilities of different methods.",
            "2": "Typically, a motion field such as a displacement field [27,28] or a skinning field based on human prior [31] is used to align the dynamic human with a template in canonical space, where the canonical radiance field captures the human in coordinate-based networks.",
            "3": "While implicit LBS weights can provide detailed deformation, joint training of the radiance field and skinning field can result in a local optimum [31].",
            "4": "Novel view and Novel pose synthesis For novel view synthesis and novel pose rendering, we compare our method against five existing approaches: 1) Neural Body (NB) [32] diffuses per-SMPL-vertex latent codes in observation space to condition the NeRF model and achieves high-quality novel view synthesis results on training poses; 2) Ani-NeRF [31] learns a backward LBS weight field and a canonical NeRF to reconstruct the human avatar;\n3) A-NeRF [41] employs skeleton-relative embedding to predict the radiance field; 4) we implement TA V A-NN based on TA V A [18], but replace the learnable deformation field 6 MethodsNovel View Nove Pose PSNR\"SSIM\"LPIPS#PSNR\"SSIM\"LPIPS# NB [32] 28.",
            "5": "142 Ani-NeRF [31] 26."
        },
        "Total-Recon: Deformable Scene Reconstruction for Embodied View Synthesis": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2304.12317",
            "ref_texts": "[33] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In IEEE International Conference on Computer Vision (ICCV) , 2021. 3",
            "ref_ids": [
                "33"
            ],
            "1": "One group of work leverages human-specific priors [33, 46, 27, 34, 19, 12, 15, 32] such as human body models (e."
        },
        "AniPixel: Towards Animatable Pixel-Aligned Human Avatar": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2302.03397",
            "ref_texts": "[Peng et al. , 2021a ]Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV , pages 14314\u201314323, 2021.",
            "ref_ids": [
                "Peng et al\\. , 2021a "
            ]
        },
        "Human 3D Avatar Modeling with Implicit Neural Representation: A Brief Survey": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2306.03576",
            "ref_texts": "[38] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 14314\u201314323, 2021. 3, 4, 7",
            "ref_ids": [
                "38"
            ],
            "1": "[38] \u2713 \u2713 DD-NeRF [69] \u2713 \u2713 Neural Body [40] \u2713 \u2713 \u2713 HandHALO [23] \u2713 \u2713 LISA [14] \u2713 \u2713 \u2713 Grasping Field [24] \u2713 \u2713 HeadNeuralHDHair [58] \u2713 \u2713 ImFace [70] \u2713 \u2713 Yang et al.",
            "2": "In the modeling of a human 3D avatar, researchers design a vertex velocity field [33], blend skinning weight field [11, 30, 38], hair growth field [58], and so on to describe the human body in detail.",
            "3": "[38] develop NeRF for the re-animation of an avatar."
        },
        "Neural Radiance Fields: Past, Present, and Future": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2304.10050",
            "ref_texts": "169. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance ffelds for modeling dynamic human bodies. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 14314{14323",
            "ref_ids": [
                "169"
            ],
            "1": "\u000fAnimatable Neural Radiance Fields [169] is an extension of NeRF that focuses on modeling dynamic human bodies by incorporating a parametric human body model into the neural radiance ffeld framework."
        },
        "Decoupling Dynamic Monocular Videos for Dynamic View Synthesis": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2304.01716",
            "ref_texts": "[33] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In IEEE/CVF International Conference on Computer Vision (ICCV) , pages 14314\u201314323, 2021.",
            "ref_ids": [
                "33"
            ],
            "1": "Some recent methods [12], [29], [33], [47] focus on specific domains, such as the face or human synthesis."
        },
        "DreamWaltz: Make a Scene with Complex 3D Animatable Avatars": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2305.12529",
            "ref_texts": "[29] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 14314\u2013",
            "ref_ids": [
                "29"
            ],
            "1": "The advancement of deep learning methods has enabled promising methods which can reconstruct 3D human models from monocular images [35,44] or videos [43,13,45,40,11,29]."
        },
        "Bringing Telepresence to Every Desk": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2304.01197",
            "ref_texts": "[45] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV , 2021. 3",
            "ref_ids": [
                "45"
            ],
            "1": "Some recent works [46, 31, 54, 45, 26] exclusively focus on animating clothed humans."
        },
        "Human View Synthesis using a Single Sparse RGB-D Input.": {
            "authors": [],
            "url": "https://3dvar.com/Nguyen2021Human.pdf",
            "ref_texts": "[51] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, 2021. 1,2",
            "ref_ids": [
                "51"
            ],
            "1": "However, synthesizing novel views of humans in motion requires methods to handle dynamic scenes with various deformations which is a challenging task [66, 72]; especially in those regions with fine details such as the face or the clothes [49, 51,71].",
            "2": "Given multi-view input frames or videos, recent works on rendering animatable humans from novel views show impressive results [49, 51,52,71]."
        },
        "Efficient 3D Reconstruction, Streaming and Visualization of Static and Dynamic Scene Parts for Multi-client Live-telepresence in Large-scale Environments": {
            "authors": [
                "Leif Van",
                "Patrick Stotko",
                "Stefan Krumpen",
                "Reinhard Klein",
                "Michael Weinmann"
            ],
            "url": "https://arxiv.org/pdf/2211.14310",
            "ref_texts": "[105] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, H. Bao, Animatable neural radiance fields for modeling dynamic human bodies, in: Proceedings of the IEEE /CVF International Conference on Computer Vision, IEEE, 2021, pp. 14314\u201314323. doi:10.1109/iccv48922.",
            "ref_ids": [
                "105"
            ],
            "1": "[105] S."
        },
        "Neural Rendering of Humans in Novel View and Pose from Monocular Video": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2204.01218",
            "ref_texts": "[34] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV , 2021. 1, 2, 3, 6, 7",
            "ref_ids": [
                "34"
            ],
            "1": "Given a monocular video, we predict novel views with body poses unseen from training with fine-level details (wrinkles) that works such as NeuralBody [34] or HumanNeRF [46] struggle to obtain.",
            "2": "Related works can be categorized into static [26, 29, 40, 41, 52, 51] and dynamic scenes [4, 8, 9, 10, 15, 17, 20, 22, 31, 32, 33, 34, 35, 36, 39, 44].",
            "3": "Pose-conditioned Representation Following [34, 35], we assume the 3D human model is given for each frame (i.",
            "4": "\u2022 Ani-NeRF [34] combines NeRF and 3D human skeletons by learning the blend weight field to recover animatable human models.",
            "5": "The qualitative comparisons are provided in Fig 4 where our approach captures fine-level details on the body (1st;3rdrows) and head (2nd row) better than prior works [34, 35, 46]."
        },
        "Efficient Meshy Neural Fields for Animatable Human Avatars": {
            "authors": [],
            "url": "https://xk-huang.github.io/ema/docs/arxiv_Efficient_Meshy_Neural_Fields_for_Animatable_Human_Avatars.small.pdf",
            "ref_texts": "[69] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In ICCV, pages 14294\u201314303, 2021.",
            "ref_ids": [
                "69"
            ],
            "1": "[70, 81, 49, 69, 45, 31, 12, 88, 99, 63, 102] leveraged the radiance field for more photo-realistic human avatars from multiview images or single-view videos without any 3D supervision.",
            "2": "[70, 69] generates blurry textures compared with our method.",
            "3": "We follow [69] data protocol which includes subject S1, S5S9, and S11.",
            "4": "We use the same data preprocessing as [69].",
            "5": "902 Ani-NeRF\n[69]X NV\u001810 h 23.",
            "6": "The same data protocol and processing approaches are adopted following [70, 69].",
            "7": "We follow the typical protocol in [70, 69] using two metrics to measure image quality: peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM).",
            "8": "We compare our method with template-based methods [70, 92] and template-free methods [69, 88].",
            "9": "Animatable NeRF (Ani-NeRF) [69] introduces neural blend weight fields to produce the deformation fields instead of explicit template control.",
            "10": "The same phenomenon has been stated in [69].",
            "11": "Notice that the reconstruction quality saturated after using a certain amount of training frames, the same results can be observed in [69] as well.",
            "12": "The indicative results with plausible quality appear after a few minutes, which is quite faster than our counterparts [70, 69, 88, 92].",
            "13": "Limitations and Further Discussions Our method is biased for shape-material ambiguity [88, 45, 81, 63, 69, 87, 98]."
        },
        "A Roadmap for Craft Understanding, Education, Training, and Preservation": {
            "authors": [],
            "url": "https://www.preprints.org/manuscript/202306.1214/download/final_file",
            "ref_texts": "119. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H. (2021). Animatable neural radiance fields for modeling dynamic human bodies. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 14314 -14323. ",
            "ref_ids": [
                "119"
            ],
            "1": "Generative learning methods, such as Variational Autoencoders and Generative Adversarial Networks [116, 117, 118, 119], creat e novel text, images, and videos, from training data."
        },
        "Modularizing deep learning for geometry-aware registration and reconstruction": {
            "authors": [
                "Wei Jiang"
            ],
            "url": "https://open.library.ubc.ca/media/download/pdf/24/1.0427395/4",
            "ref_texts": "[195] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao. Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies. InICCV , 2021.!pages 3, 83, 86",
            "ref_ids": [
                "195"
            ],
            "1": "However, the established paradigm of classic camera registration and human reconstruction has been challenged by deep learning [6, 25, 28, 118, 134, 148, 172, 195, 196, 218, 236, 239, 298] in recent years.",
            "2": "Particularly related to our task of interest, various efforts have been made towards NeRF models conditioned by explicit human models, such as SMPL [153] or 3D skeleton [118, 148, 195, 196, 239].",
            "3": "Animatable NeRF [195] learns a blending weight field in both observation space and canonical space, and optimize for a new blending weight field for novel poses.",
            "4": "Therefore, we define a canonical space based on the \u5927-pose (Dapose) SMPL [153] mesh, similar to [148, 195].",
            "5": "!page 102\n[195] S."
        },
        "SAgA-NeRF: Subject-agnostic and animatable neural radiance fields for human avatar": {
            "authors": [],
            "url": "https://summit.sfu.ca/_flysystem/fedora/2023-01/etd22141.pdf",
            "ref_texts": "[29] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. In Proc. of International Conference on Computer Vision (ICCV) , pages 14314\u201314323, 2021.",
            "ref_ids": [
                "29"
            ],
            "1": "2 Quantitative comparison against animatable methods [30], [29], and [44].",
            "2": "Animatable NeRF-based human rendering methods propose conditioning NeRF on an extra human pose parameter to control the rendering [29, 21, 42, 44, 30].",
            "3": "Below we discuss three important works in this category: Animatable NeRF [29], Neural Body [30], and Structured Local Radiance Fields for Human Avatar Modeling [44].",
            "4": "Animatable NeRF [29] renders a novel view of a human model in a target pose, by leveraging SMPL as human body prior and using deformations to canonical pose (which in this case is a T-pose).",
            "5": "Efforts have been made in adapting NeRF to rendering human subjects [30, 21, 18, 29, 42, 44, 6].",
            "6": "3, prior works have been proposed to achieve either subjectagnostic [18, 6] or animatable [21, 29, 42, 44] human avatar NeRF-based rendering.",
            "7": "3 have less of an impact on our formulation, as their formulation are based on subject-specific learning, such as learning the canonical pose [29] or subject-specific latent code [30], or subject-specific embeddings and node offsets [44].",
            "8": "We compare against three methods; Structured Local Radiance Fields for Human Avatar Modeling (SLRF) [44], Animatable Nerf (AN) [29], Neural Body (NB) [30].",
            "9": "2: Quantitative comparison against animatable methods [30], [29], and [44]."
        },
        "Supplementary for ARAH: Animatable Volume Rendering of Articulated Human SDFs": {
            "authors": [],
            "url": "https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136920001-supp.pdf",
            "ref_texts": "12. Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: Proc. of ICCV",
            "ref_ids": [
                "12"
            ],
            "1": "For inference, we follow [12, 13] and crop an enlarged bounding box around the projected SMPL mesh on the image plane and render only pixels inside the bounding box.",
            "2": "For unseen test poses we follow the practice of [12, 13] and use the latent code Zof the last training frame as the input.",
            "3": "Neural Body [13], Ani-NeRF [12], and A-NeRF [17].",
            "4": "A simple uniform sampling strategy (as used in [12, 13]) produces stratified artifacts due to the discretized sampling.",
            "5": "We also report quantitative results on the H36M dataset [5], following the testing protocols proposed by [12] in Table 2.",
            "6": "Numbers of NARF [11] and Ani-N [12] are reported in [21]."
        },
        "Learning Neural Volumetric Representations of Dynamic Humans in Minutes Supplemental Material": {
            "authors": [],
            "url": "https://chen-geng.com/instant_nvr/files/supp.pdf",
            "ref_texts": "[9]Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable neural radiance fields for modeling dynamic human bodies. InProceedings of the IEEE/CVF International Conference on Computer Vision , pages 14314\u201314323, 2021. 1, 3",
            "ref_ids": [
                "9"
            ],
            "1": "Specifically, MHE-encoded (x, y, z )coordinates in canonical space are first fed into the first MLP to output a 16D geometric feature z, which is concatenated with positional encoded [6] canonical view direction, zand an 8D time-varying latent code [9].",
            "2": "Details of baselines Neural Body [11], Ani-SDF [10], HumanNeRF [14] and Ani-NeRF [9] We use the released code and conduct experiments on a single NVIDIA RTX 3090 GPU."
        },
        "Supplemental Materials for TAVA: Template-free Animatable Volumetric Actors": {
            "authors": [],
            "url": "http://pages.iai.uni-bonn.de/gall_juergen/download/tava_eccv22_supp.pdf",
            "ref_texts": "5.Peng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Zhou, X., Bao, H.: Animatable neural radiance fields for modeling dynamic human bodies. In: International Conference on Computer Vision (2021)",
            "ref_ids": [
                "5"
            ],
            "1": "Thus the previous way [5,6] of splitting the dataset into two chunks with consecutive frames will cover similar poses in both sets, which is not suitable for evaluating the pose generalization ability.",
            "2": "2, for the template-based baselines Animatable-NeRF [5] and NeuralBody [6], we use their official implementations.",
            "3": "The ZJU-Mocap dataset has become an increasingly popular dataset to study human performance capture, reconstruction, and neural rendeirng [5,6,8].",
            "4": "Novel-view Novel-pose (ind) Novel-pose (ood) PSNR \u2191SSIM \u2191PSNR \u2191SSIM \u2191PSNR \u2191SSIM \u2191 Subject 313 Animatable-NeRF [5] 29.",
            "5": "957 Subject 315 Animatable-NeRF [5] 27.",
            "6": "960 Subject 377 Animatable-NeRF [5] 32.",
            "7": "980 Subject 386 Animatable-NeRF [5] 34."
        }
    }
}