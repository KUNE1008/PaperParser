{
    "title": "Gift: Learning transformation-invariant dense visual descriptors via group cnns",
    "id": 24,
    "valid_pdf_number": "25/48",
    "matched_pdf_number": "20/25",
    "matched_rate": 0.8,
    "citations": {
        "LoFTR: Detector-free local feature matching with transformers": {
            "authors": [
                "Jiaming Sun",
                "Zehong Shen",
                "Yuang Wang",
                "Hujun Bao",
                "Xiaowei Zhou"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2021/papers/Sun_LoFTR_Detector-Free_Local_Feature_Matching_With_Transformers_CVPR_2021_paper.pdf",
            "ref_texts": "[25] Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou. GIFT: Learning transformation-invariant dense visual descriptors via group cnns. NeurIPS , 2019.",
            "ref_ids": [
                "25"
            ],
            "1": "Many learning-based local features along this line [32,11,25,28,47] also adopt the detectorbased design."
        },
        "You only hypothesize once: Point cloud registration with rotation-equivariant descriptors": {
            "authors": [
                "Haiping Wang"
            ],
            "url": "https://dl.acm.org/doi/pdf/10.1145/3503161.3548023",
            "ref_texts": ""
        },
        "A case for using rotation invariant features in state of the art feature matchers": {
            "authors": [
                "Georg Bokman",
                "Fredrik Kahl"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022W/IMW/papers/Bokman_A_Case_for_Using_Rotation_Invariant_Features_in_State_of_CVPRW_2022_paper.pdf",
            "ref_texts": "[31] Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou. GIFT: Learning TransformationInvariant Dense Visual Descriptors via Group CNNs. arXiv:1911.05932 [cs] , Nov. 2019. 2",
            "ref_ids": [
                "31",
                "cs"
            ],
            "1": "CNN-based approaches for detection and description of keypoints include for instance D2-Net, R2D2-Net and DISK [15, 36, 44], but most related to our approach are the ones that use rotation invariant features such as LIFT, GIFT and others [26, 31, 49, 51].",
            "2": "05400 [cs, math, stat] , May 2021.",
            "3": "03393 [cs, math] , June 2018.",
            "4": "13478 [cs, stat], May 2021.",
            "5": "07629 [cs] , Apr.",
            "6": "00667 [cs] , Feb.",
            "7": "03385 [cs] , Dec.",
            "8": "03690 [cs, stat] , Nov.",
            "9": "10952 [cs] , Jan.",
            "10": "01224 [cs] , Sept.",
            "11": "05932 [cs] , Nov.",
            "12": "11763\n[cs], Mar.",
            "13": "02545 [cs] , Jan.",
            "14": "06020 [cs, stat] , June 2021.",
            "15": "02547 [cs, stat] , Oct.",
            "16": "09114 [cs] , July 2016.",
            "17": "05971 [cs] , May 2018.",
            "18": "04273 [cs] , Apr."
        },
        "HyNet: Learning local descriptor with hybrid similarity measure and triplet loss": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper/2020/file/52d2752b150f9c35ccb6869cbf074e48-Paper.pdf",
            "ref_texts": "[22]Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou. Gift: Learning transformation-invariant dense visual descriptors via group cnns. In Advances in Neural Information Processing Systems (NeurIPS) , pages 6990\u20137001, 2019.",
            "ref_ids": [
                "22"
            ],
            "1": "Recently, joint detection and description [51,30,10,11,34,22,25,13,45,5] has drawn significant attention."
        },
        "Vs-net: Voting with segmentation for visual localization": {
            "authors": [
                "Zhaoyang Huang",
                "Han Zhou",
                "Yijin Li",
                "Bangbang Yang",
                "Yan Xu",
                "Xiaowei Zhou",
                "Hujun Bao",
                "Guofeng Zhang",
                "Hongsheng Li"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Huang_VS-Net_Voting_With_Segmentation_for_Visual_Localization_CVPR_2021_paper.pdf",
            "ref_texts": "[28] Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou. Gift: Learning transformation-invariant dense visual descriptors via group cnns. In Advances in Neural Information Processing Systems , pages 6990\u20137001, 2019.",
            "ref_ids": [
                "28"
            ],
            "1": "Traditional visual localization frameworks [4,16,26,61,44,12] build a map by SfM techniques [62,1,67,46,55] with general feature detectors and descriptors [30,6,43,35,15,17,28,41]."
        },
        "Pump: Pyramidal and uniqueness matching priors for unsupervised learning of local descriptors": {
            "authors": [
                "Jerome Revaud",
                "Vincent Leroy",
                "Philippe Weinzaepfel",
                "Boris Chidlovskii"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Revaud_PUMP_Pyramidal_and_Uniqueness_Matching_Priors_for_Unsupervised_Learning_of_CVPR_2022_paper.pdf",
            "ref_texts": "[33] Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou. GIFT: learning transformation-invariant dense visual descriptors via group cnns. In NeurIPS , 2019.",
            "ref_ids": [
                "33"
            ],
            "1": "A few works tried to improve the capability of CNNs to describe non-planar regions by introducing robust [33] or dynamic convolutional kernels [37], yet in a supervised training scenario."
        },
        "Learnable motion coherence for correspondence pruning": {
            "authors": [
                "Yuan Liu",
                "Lingjie Liu",
                "Cheng Lin",
                "Zhen Dong",
                "Wenping Wang"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Learnable_Motion_Coherence_for_Correspondence_Pruning_CVPR_2021_paper.pdf",
            "ref_texts": "[24] Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou. Gift: Learning transformation-invariant dense visual descriptors via group cnns. In Advances in Neural Information Processing Systems , pages 6992\u20137003, 2019.",
            "ref_ids": [
                "24"
            ],
            "1": "Image matching Previously, works about image matching mainly focus on learning repetitive detectors [25, 58, 16, 40, 4, 17, 46] or discriminative descriptors [28, 27, 24, 49, 50, 30, 53, 15]."
        },
        "Self-supervised equivariant learning for oriented keypoint detection": {
            "authors": [
                "Jongmin Lee",
                "Byungjin Kim",
                "Minsu Cho"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Lee_Self-Supervised_Equivariant_Learning_for_Oriented_Keypoint_Detection_CVPR_2022_paper.pdf",
            "ref_texts": "[26] Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou. Gift: Learning transformation-invariant dense visual descriptors via group cnns. Advances in Neural Information Processing Systems , 32:6992\u20137003, 2019. 2, 7",
            "ref_ids": [
                "26"
            ],
            "1": "The most similar work, GIFT [26], use equivariant networks to obtain dense local descriptors, but [26] constructs the group representation with augmented images, whereas we construct the representation through 4848\n steerable kernels [67] without rotating images at runtime.",
            "2": "3 SPoint [14] GIFT [26] 47.",
            "3": "3 ours GIFT [26] 57.",
            "4": "We additionally compare the joint detection and description methods [14,15, 41,45] and the integration of the rotation-invariant dense descriptors [26].",
            "5": "Our model with GIFT descriptor [26] achieves better MMAs compared to the Su-Det.",
            "6": "perPoint [14] detector of the cases with SuperPoint descriptor [14] and GIFT [26].",
            "7": "In particular, our model with the rotation-invariant descriptors [26] achieves the best MMAs, which shows that the rotation-invariant representation contributes to improving the accuracy of correspondences."
        },
        "Matching in the dark: A dataset for matching image pairs of low-light scenes": {
            "authors": [
                "Wenzheng Song",
                "Masanori Suganuma",
                "Xing Liu",
                "Noriyuki Shimobayashi",
                "Daisuke Maruta",
                "Takayuki Okatani"
            ],
            "url": "http://openaccess.thecvf.com/content/ICCV2021/papers/Song_Matching_in_the_Dark_A_Dataset_for_Matching_Image_Pairs_ICCV_2021_paper.pdf",
            "ref_texts": "[30] Y . Liu, Z. Shen, Z. Lin, S. Peng, H. Bao, and X. Zhou. Gift: Learning transformation-invariant dense visual descriptors via group cnns. In Proc. NeurIPS , 2019. https: //github.com/zju3dv/GIFT . 6",
            "ref_ids": [
                "30"
            ],
            "1": "We use SuperPoint [17], Reinforced SuperPoint [9], GIFT\n[30], R2D2 [26], and RF-Net [50] as representative neural network-based methods.",
            "2": "2\n[30] Y ."
        },
        "P2-net: Joint description and detection of local features for pixel and point matching": {
            "authors": [
                "Bing Wang",
                "Changhao Chen",
                "Zhaopeng Cui",
                "Jie Qin",
                "Chris Xiaoxuan",
                "Zhengdi Yu",
                "Peijun Zhao",
                "Zhen Dong",
                "Fan Zhu",
                "Niki Trigoni",
                "Andrew Markham"
            ],
            "url": "http://openaccess.thecvf.com/content/ICCV2021/papers/Wang_P2-Net_Joint_Description_and_Detection_of_Local_Features_for_Pixel_ICCV_2021_paper.pdf",
            "ref_texts": "[30] Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou. Gift: Learning transformation-invariant dense visual descriptors via group cnns. In NeurIPS , 2019.",
            "ref_ids": [
                "30"
            ],
            "1": "2D Local Features Description and Detection Previous learning-based methods in 2D domain simply replaced the descriptor [50, 51, 30, 19, 38] or detector [43, 59, 4] with a learnable alternative.",
            "2": "1 Image Matching In the image matching experiment, we use the HPatches dataset [3], which has been widely adopted to evaluate the quality of image matching [33, 16, 40, 30, 51, 38, 53]."
        },
        "Scalenet: A shallow architecture for scale estimation": {
            "authors": [
                "Axel Barroso",
                "Yurun Tian",
                "Krystian Mikolajczyk"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2022/papers/Barroso-Laguna_ScaleNet_A_Shallow_Architecture_for_Scale_Estimation_CVPR_2022_paper.pdf",
            "ref_texts": "[22] Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou. Gift: Learning transformationinvariant dense visual descriptors via group cnns. arXiv preprint arXiv:1911.05932 , 2019. 2",
            "ref_ids": [
                "22"
            ],
            "1": "The rotation has been addressed by correcting input patches [13, 23, 30] before extracting local descriptors [28,48,49], or by designing robust architectures [4, 22, 24]."
        },
        "Planar object tracking via weighted optical flow": {
            "authors": [
                "Jonas Serych",
                "Jiri Matas"
            ],
            "url": "https://openaccess.thecvf.com/content/WACV2023/papers/Serych_Planar_Object_Tracking_via_Weighted_Optical_Flow_WACV_2023_paper.pdf",
            "ref_texts": "[25] Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou. Gift: Learning transformation-invariant dense visual descriptors via group cnns. Advances in Neural Information Processing Systems , 32, 2019.",
            "ref_ids": [
                "25"
            ],
            "1": "The best ones use the SIFT keypoint detector, a deep learning descriptor such as GIFT [25], M ATCH NET[13], SOSN ET[43], or LISRD [32], followed by RANSAC."
        },
        "Homography decomposition networks for planar object tracking": {
            "authors": [
                "Xinrui Zhan",
                "Yueran Liu",
                "Jianke Zhu",
                "Yang Li"
            ],
            "url": "https://ojs.aaai.org/index.php/AAAI/article/view/20232/19991",
            "ref_texts": ""
        },
        "Learning Rotation-Equivariant Features for Visual Correspondence": {
            "authors": [
                "Jongmin Lee",
                "Byungjin Kim",
                "Seungwook Kim",
                "Minsu Cho"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Learning_Rotation-Equivariant_Features_for_Visual_Correspondence_CVPR_2023_paper.pdf",
            "ref_texts": "[1]Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Si-mon, Brian Curless, Steven M Seitz, and Richard Szeliski.Building rome in a day.Communications of the ACM,54(10):105\u2013112, 2011.1[2]Vassileios Balntas, Karel Lenc, Andrea Vedaldi, and Krys-tian Mikolajczyk. Hpatches: A benchmark and evaluationof handcrafted and learned local descriptors. InProceed-ings of the IEEE Conference on Computer Vision and PatternRecognition, pages 5173\u20135182, 2017.2,5,6,7[3]Axel Barroso-Laguna, Yurun Tian, and Krystian Mikola-jczyk. Scalenet: A shallow architecture for scale estimation.InProceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 12808\u201312818, 2022.2[4]Fabio Bellavia and Carlo Colombo. Rethinking the sglohdescriptor.IEEE Transactions on Pattern Analysis and Ma-chine Intelligence, 40(4):931\u2013944, 2017.2[5]Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-offrey Hinton. A simple framework for contrastive learningof visual representations. InInternational conference on ma-chine learning, pages 1597\u20131607. PMLR, 2020.5[6]Taco Cohen and Max Welling. Group equivariant convo-lutional networks. InInternational conference on machinelearning, pages 2990\u20132999. PMLR, 2016.2[7]Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-novich. Toward geometric deep slam.arXiv preprintarXiv:1707.07410, 2017.1[8]Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-novich. Superpoint: Self-supervised interest point detectionand description. InCVPR Deep Learning for Visual SLAMWorkshop, 2018.1,2,3,5,6,7[9]Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Polle-feys, Josef Sivic, Akihiko Torii, and Torsten Sattler. D2-net: A trainable cnn for joint description and detection oflocal features. InProceedings of the ieee/cvf conference oncomputer vision and pattern recognition, pages 8092\u20138101,2019.2,6,7[10]Patrick Ebel, Anastasiia Mishchuk, Kwang Moo Yi, PascalFua, and Eduard Trulls. Beyond cartesian representationsfor local descriptors. InProceedings of the IEEE/CVF In-ternational Conference on Computer Vision, pages 253\u2013262,2019.2[11]Bin Fan, Fuchao Wu, and Zhanyi Hu. Rotationally invariantdescriptors using intensity order pooling.IEEE transactionson pattern analysis and machine intelligence, 34(10):2031\u20132045, 2011.1,2[12]Jiaming Han, Jian Ding, Nan Xue, and Gui-Song Xia. Redet:A rotation-equivariant detector for aerial object detection. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 2786\u20132795, 2021.2,5[13]Bharath Hariharan, Pablo Arbel\u00b4aez, Ross Girshick, and Ji-tendra Malik. Hypercolumns for object segmentation andfine-grained localization. InProceedings of the IEEE con-ference on computer vision and pattern recognition, pages447\u2013456, 2015.3[14]Chris Harris, Mike Stephens, et al. A combined corner andedge detector. InAlvey vision conference, volume 15, pages10\u20135244. Citeseer, 1988.3,5[15]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.Deep residual learning for image recognition. InProceed-ings of the IEEE conference on computer vision and patternrecognition, pages 770\u2013778, 2016.2[16]Jared Heinly, Johannes L. Sch\u00a8onberger, Enrique Dunn, andJan-Michael Frahm. Reconstructing the world* in six days.In2015 IEEE Conference on Computer Vision and PatternRecognition (CVPR), pages 3287\u20133295, 2015.1[17]Yuhe Jin, Dmytro Mishkin, Anastasiia Mishchuk, Jiri Matas,Pascal Fua, Kwang Moo Yi, and Eduard Trulls. Imagematching across wide baselines: From paper to practice.International Journal of Computer Vision, 129(2):517\u2013547,2021.1[18]Seungwook Kim, Yoonwoo Jeong, Chunghyun Park, JaesikPark, and Minsu Cho. SeLCA: Self-supervised learning ofcanonical axis. InNeurIPS 2022 Workshop on Symmetry andGeometry in Neural Representations, 2022.2[19]Seungwook Kim, Juhong Min, and Minsu Cho. Trans-formatcher: Match-to-match attention for semantic corre-spondence. InProceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 8697\u20138707, 2022.3[20]Axel Barroso Laguna and Krystian Mikolajczyk. Key. net:Keypoint detection by handcrafted and learned cnn filters re-visited.IEEE Transactions on Pattern Analysis and MachineIntelligence, 2022.3,6[21]Jongmin Lee, Yoonwoo Jeong, and Minsu Cho. Self-supervised learning of image scale and orientation. In31stBritish Machine Vision Conference 2021, BMVC 2021, Vir-tual Event, UK. BMV A Press, 2021.1,2,4[22]Jongmin Lee, Yoonwoo Jeong, Seungwook Kim, JuhongMin, and Minsu Cho. Learning to distill convolutional fea-tures into compact local descriptors. InProceedings of theIEEE/CVF Winter Conference on Applications of ComputerVision, pages 898\u2013908, 2021.2[23]Jongmin Lee, Byungjin Kim, and Minsu Cho. Self-supervised equivariant learning for oriented keypoint detec-tion. InProceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 4847\u20134857,2022.1,2,4[24]Kunhong Li, Longguang Wang, Li Liu, Qing Ran, Kai Xu,and Yulan Guo. Decoupling makes weakly supervised localfeature better. InProceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 15838\u201315848, 2022.2,6[25]Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,Pietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C LawrenceZitnick. Microsoft coco: Common objects in context. InEuropean conference on computer vision, pages 740\u2013755.Springer, 2014.5[26]Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao,and Xiaowei Zhou. Gift: Learning transformation-invariantdense visual descriptors via group cnns.Advances in NeuralInformation Processing Systems, 32:6992\u20137003, 2019.1,2,5,6,7",
            "ref_ids": [
                "1",
                "2",
                "3",
                "4",
                "5",
                "6",
                "7",
                "8",
                "9",
                "10",
                "11",
                "12",
                "13",
                "14",
                "15",
                "16",
                "17",
                "18",
                "19",
                "20",
                "21",
                "22",
                "23",
                "24",
                "25",
                "26"
            ],
            "1": "IntroductionExtracting local descriptors is an essential step for vi-sual correspondence across images, which is used for awide range of computer vision problems such as visual lo-calization [29,47,48], simultaneous localization and map-ping [7,8,39], and 3D reconstruction [1,16,17,49,66].",
            "2": "Since the remarkable success of deep learning for visualrecognition, deep neural networks have also been adoptedto learn local descriptors, showing enhanced performanceson visual correspondence [44,45,64].",
            "3": "Learningrotation-invariant local descriptors, however, remains challenging;the classical techiniques [11,27,46] for rotation-invariantdescriptors, which are used for shallow gradient-based fea-ture maps, cannot be applied to feature maps from stan-dard deep neural networks, in which rotation of input in-duces unpredictable feature variations.",
            "4": "First, weuse group-equivariant CNNs [60] to jointly extract rotation-equivariant local features and their orientations from an im-age.",
            "5": "To extract reliable orientations, we use an orientationalignment loss [21,23,63], which trains the network to pre-dict the dominant orientation robustly against other imag-ing variations, including illumination or viewpoint changes.",
            "6": "Using group-equivariant CNNs enables the local featuresto be empowered with explicitly encoded rotation equiv-ariance without having to perform rigorous data augmen-tations [58,60].",
            "7": ",max-pooling or bilinear-pooling [26], resulting in a drop in feature discriminabil-ity and quality.",
            "8": "21887\n our group-aligning scheme against group-pooling methodson various image matching benchmarks to demonstrate theefficacy of our method.",
            "9": "\u2022We demonstrate state-of-the-art performances undervarying rotations on the Roto-360 dataset and showcompetitive transferability on the HPatches dataset [2]and the MVS dataset [53].",
            "10": "2.",
            "11": ", histogram, from which the estimated dominantorientation is subtracted to obtain rotation-invariant fea-tures [27,46].",
            "12": "Several studies [4,11,59] suggest extractinglocal descriptors by invariant mapping of the order-basedgradient histogram of a patch.",
            "13": "Therefore, we propose a deep end-to-end pipeline toobtain orientation-normalized local descriptors by utilizingrotation-equivariant CNNs [60] with additional losses.",
            "14": "GIFT [26] constructs group-equivariant features by rotating or rescaling the images, andthen collapses the group dimension using bilinear pooling toobtain invariant local descriptors.",
            "15": "However, their groups arelimited to non-cyclic discrete rotations ranging from\u000090\u0000to90\u0000.",
            "16": "LISRD [42] jointly learns meta de-scriptors with different levels of regional variations and se-lects the most appropriate level of invariance given the con-text.",
            "17": "These methods are either patch-based [10,36,54,56]or image-based [8,9,22,24,28,37,40,44,50,57].",
            "18": "We con-struct group-equivariant local features using the steerablenetworks [60], which explicitly encodes cyclic rotationalequivariance to the features without having to rely on dataaugmentation.",
            "19": "There has been aconstant pursuit to learn equivariant representations by ex-plicitly incorporating group equivariance into the modelarchitecture design [30\u201332,51,60,65].",
            "20": "For example, G-CNNs [6] use group equivariant convolutions that reducesample complexity by exploiting symmetries on discreteisometric groups; SFCNNs [61] and H-Nets [62] extractfeatures from more diverse groups and continuous domainsby using harmonics as filters.",
            "21": "There are also studies that fo-cus on scale-equivariant representation learning [3,21,52].",
            "22": "[12,18,23,38,43] leverage equivariant neural networks totackle vision taskse.",
            "23": "3.",
            "24": "3.",
            "25": "3.",
            "26": "2), how group-aligning is per-formed to yield rotation-invariant yet discriminative de-scriptors (Sec.",
            "27": "3.",
            "28": "3), how we formulate our orientation align-ment loss (Sec.",
            "29": "3.",
            "30": "4) and contrastive descriptor loss (Sec.",
            "31": "3.",
            "32": "5)to train our network to extract descriptors which are robustto not only rotation but also other imaging transformations,and finally how we obtain scale-invariant descriptors at testtime using image pyramids (Sec.",
            "33": "3.",
            "34": "6).",
            "35": "3.",
            "36": "Rotation-equivariant feature extractionAs the feature extractor, we use ReResNet18 [12], whichhas the same structure as ResNet18 [15] but is constructedusing rotation-equivariant convolutional layers [60].",
            "37": "The 21888\n\n{\"!\u2208\u211d\"!\u00d7|%|\u00d7&!\u00d7'!}!()*Bilinear Interpolation& Concatenation&\u2208\u211d(\"\",\"#,\"$,\"%)\u00d7|%|\u00d7&\u00d7.",
            "38": "Flatten& L2 Norm'\u2208\u211d|%|\u00d7&\u00d7.",
            "39": "!=#&\u2112/01('2,'3,\u220645)\u21126789(-2,-3,.",
            "40": ":;){/}\u2208\u211d\"\u00d7|%|\u00d7<{0}\u2208\u211d\"|%|\u00d7<{1}\u2208\u211d|%|\u00d7<Forward passAssign local features to keypointsCopy the first channel of the input feature Rotation-equivariant feature mapOrientationmapGroup aligning2\u2206=456max=1=Rotation-equivariant feature extractionGroup aligning for invariant mappingSelf-supervised trainingFigure 1.",
            "41": "At the first layer, thescalar field of the input image is lifted to the vector field ofthe group representation [60].",
            "42": "We leverage feature pyramidsfrom the intermediate layers of the ReResNet18 backboneto construct output features as follows:F=Mi2l\u2318(fi),fi=[\u21e7ij=1Lj](I),(1)wherefi2RCi\u21e5|G|\u21e5Hi\u21e5Wiis an intermediate feature fromLi,Liis thei-th layer of the equivariant network,\u2318denotesbilinear interpolation toH\u21e5W, andLdenotes concate-nation along theCdimension.",
            "43": "We utilize the multi-layerfeature maps to exploit the low-level geometry informationand high-level semantics in the local descriptors [13,19,35].",
            "44": "The output featuresF2RC\u21e5|G|\u21e5H\u21e5Wcontains rotation-equivariant features with multiple layers containing differ-ent semantics and receptive fields.",
            "45": "We setH=H1andW=W1, which are12of the input image size.",
            "46": "3.",
            "47": "2.",
            "48": "Assigning local features to keypointsDuring training, we extractKkeypoints from the sourceimage using Harris corner detection [14].",
            "49": "Also, we allocate a local featurep2RC\u21e5|G|\u21e5Kto each keypoint, using the interpolated lo-cation of the equivariant feature mapF.",
            "50": "We experiment ourdescriptor with SIFT [27], LF-Net [40], SuperPoint [8], andKeyNet [20] as the keypoint detector during inference time.",
            "51": "3.",
            "52": "3.",
            "53": "Figure2il-lustrates the difference between group pooling and groupaligning on an equivariant representation.",
            "54": "We obtain the orientation histogram mapO2R|G|\u21e5H\u21e5W=F0by selecting the first channel of therotation-equivariant tensorFas an orientation histogrammap.",
            "55": "We first select an ori-entation vectoro2R|G|of a keypoint from the orienta-tion histogram mapOusing the coordinates of the keypoint.",
            "56": "Next, we estimate the dominant orientation value\u02c6\u2713from theorientation vectoroby selecting the index of the maximumscore,\u02c6\u2713=360|G|arg maxgo.",
            "57": "Using the dominant orientationvalue\u02c6\u2713, we obtain the shifting value\u02c6\u0000=|G|360\u02c6\u2713inG-dim.",
            "58": "Given a keypoint-allocated feature ten-sorp2RC\u21e5|G|from the equivariant representationF, weobtain the rotation-invariant local descriptord2RC|G|bygroup aligning using\u0000.",
            "59": "After computing the dominant ori-entation\u02c6\u2713and the shifting value\u02c6\u0000fromo, we obtain theorientation-normalized descriptord02RC|G|by shiftingp 21889\n\n!\u2208\u211d!|#|Flatten$|&|(b) Group aligning\u205d\u205dCyclic Shift'\u2206\n)\u2208\u211d$\u00d7|&|!\u2208\u211d$Pooling$(a) Group pooling\u205d\u205d*\u2208\u211d|&| Figure 2.",
            "60": "We finally obtain the L2-normalizeddescriptordfrom the orientation-normalized descriptord0,such that||d||2=1.",
            "61": "Formally, this process can be definedas:p0:,i=T0r(p:,i,\u02c6\u0000) =p:,(i+\u02c6\u0000)mod|G|,d0|G|i:|G|(i+1)=p0i,d=d0||d0||2,(2)whereT0ris shifting operator in vector space, andp0is agroup-aligned descriptor before flattening.",
            "62": "This process is conceptually similar to subtracting thedominant orientation value of the orientation histogram inthe classical descriptor SIFT [27], but we apply this con-cept to the equivariant neural features.",
            "63": "3.",
            "64": "4.",
            "65": "Orientation alignment lossTo learn to obtain the dominant orientations from the ori-entation vectors, we use an orientation alignment loss [21,23,63] to supervise the orientation histograms inOto berotation equivariant under the photometric/geometric trans-formations.",
            "66": "Figure3shows the illustration of orientation!!\u2208\u211d\"\u00d7|%|\u205d\u2112!\"#(#$,%\u2032%(#&,\u2206'())$!=!&!$'=!&' Cyclic Shift\u2206%(=3!'\u2208\u211d\"\u00d7|%|\u205d Figure 3.",
            "67": "Given tworotation-equivariant tensorspA,pB2RC\u21e5|G|obtained from twodifferent rotated versions of the same image, we apply cyclic shifton one of the descriptors in the group dimension using the GTdifference in rotation.",
            "68": "The cyclic shift of an orientation histogrammap at the training time is formulated as follows:T0r(Oi,\u0000GT)=O(i+\u0000GT)mod|G|,(3)where\u0000GT=|G|360\u2713GTis the shifting value calculated fromthe ground-truth rotation\u2713GT.",
            "69": "We formulate the orientationalignment loss in the form of a cross-entropy as follows:Lori(OA,OB,\u0000GT)=\u0000Xk2KXg2G\u0000(OAg,k) log(\u0000(T0r(OBg,k,\u0000GT))),(4)whereOAis the source orientation histogram map andOBis the target orientation histogram map obtained from a syn-thetically warped source image,\u0000is a softmax function ap-plied to theG-dimension of the orientation histogram mapto represent the orientation vector as a probability distribu-tion for the cross-entropy loss to be applicable.",
            "70": "Using Equa-tion4, the network learns to predict the characteristic orien-tations robustly against different imaging variations, suchas photometric transformations and geometric transforma-tions beyond rotation, as these transformations cannot behandled by equivariance to discrete rotations alone.",
            "71": "21890\n MMApred.",
            "72": "@1pxAlign97.",
            "73": "5484.",
            "74": "90Avg33.",
            "75": "7233.",
            "76": "72Max57.",
            "77": "9257.",
            "78": "92None23.",
            "79": "9723.",
            "80": "97Bilinear43.",
            "81": "6026.",
            "82": "42Table 1.",
            "83": "Evaluation with GT keypoint pairs on Roto-360 with-out training.",
            "84": "We use anaverage of 111 keypoint pairs extracted using SuperPoint [8].",
            "85": "3.",
            "86": "5.",
            "87": "Contrastive descriptor lossWe propose to use a descriptor similarity loss motivatedby contrastive learning [5] to further empower the descrip-tors to be robust against variations apart from rotation,e.",
            "88": "The descriptor loss is formulatedin a contrastive manner as follows:Ldesc(DA,DB)=X(dAi,dBi)2(DA,DB)\u0000logexp(sim(dAi,dBi)/\u2327)Pk2K\\iexp(sim(dAi,dBk))/\u2327),(5)where sim is cosine similarity and\u2327is the softmax temper-ature.",
            "89": "This contrastive loss with InfoNCE [41] maxi-mizes the mutual information between the encoded featuresand effectively reduces the low-level noise.",
            "90": "3.",
            "91": "6.",
            "92": "Thus, at inference time, we construct an image pyramid us-ing a scale factor of21/4from a maximum of 1,024 pixelsto a minimum of 256 pixels as in R2D2 [44].",
            "93": "After con-structing the scale-wise descriptors2RS\u21e5C|G|\u21e5KwithSvarying scales, we finally generate the scale-invariant localdescriptors2RC|G|\u21e5Kby max-pooling in the scale dimen-sion inspired by scale-space maxima as in SIFT [27,33], forimproved robustness to scale changes.",
            "94": "4.",
            "95": "We use rotation-equivariantResNet-18 (ReResNet-18) [12] implemented using therotation-equivariant layers ofE(2)-CNN [60] as our back-bone.",
            "96": "@10px @5px @3pxAlign93.",
            "97": "08 91.",
            "98": "35 90.",
            "99": "18688.",
            "100": "3Avg85.",
            "101": "84 82.",
            "102": "12 81.",
            "103": "05705.",
            "104": "9Max82.",
            "105": "61 78.",
            "106": "00 77.",
            "107": "79686.",
            "108": "0None19.",
            "109": "68 18.",
            "110": "81 18.",
            "111": "57349.",
            "112": "1Bilinear42.",
            "113": "69 41.",
            "114": "03 40.",
            "115": "51332.",
            "116": "5Table 2.",
            "117": "Evaluation with predicted keypoint pairs on Roto-360with training.",
            "118": "We use an averageof 1161 keypoint pairs extracted using SuperPoint [8].",
            "119": "equivariant featureFisH=H02andW=W02, whereH0andW0are the height and width of an input image.",
            "120": "We use16 for the order of cyclic groupG.",
            "121": "We use a batch size of 8,a learning rate of10\u00004, and a weight decay of 0.",
            "122": "We trainour model for 12 epochs with 1,000 iterations using a ma-chine with an Intel i7-8700 CPU and an NVIDIA GeForceRTX 3090 GPU.",
            "123": "07.",
            "124": "The loss balancing factor\u21b5is 10.",
            "125": "The final output descrip-tor size is 1,024, withC= 64,|G|= 16.",
            "126": "We use Super-Point [8] as the keypoint detector to evaluate our methodexcept Table4.",
            "127": "4.",
            "128": "We evaluate our model on theRoto-360 dataset and show the transferability on real imagebenchmarks,i.",
            "129": ",HPatches [2] and MVS [53] datasets.",
            "130": "We generate a synthetic dataset for self-supervised training from the MS-COCO dataset [25].",
            "131": "As we need the ground-truthrotation\u2713GTfor our orientation alignment loss, we de-compose the synthetic homographyHas follows:\u2713GT=arctan(H21H11), where we assume that a3\u21e53homographymatrixHwith no significant tilt can be approximated to anaffine matrix.",
            "132": "We sampleK= 512keypoints for an im-age using Harris corner detector [14], obtaining 512 corre-sponding keypoint pairs for each image pair using homogra-phy and rotation.",
            "133": "Note that this dataset generation protocolis the same as that of GIFT [26] for a fair comparison.",
            "134": "Roto-360is an evaluation dataset that consists of 360 imagepairs with in-plane rotation ranging from0\u0000to350\u0000at10\u0000intervals, created using ten randomly sampled images fromHPatches [2].",
            "135": "Roto-360 is more suitable to evaluate the ro-tation invariance of our descriptors, as the extreme rotation(ER) dataset [26] only covers180\u0000, and includes photomet-ric variations.",
            "136": "We use mean matching accuracy (MMA) asthe evaluation metric with pixel thresholds of 3/5/10 pixels 21891\n MethodMMApred.",
            "137": "@10px @5px @3pxSIFT [27]78.",
            "138": "86 78.",
            "139": "59 78.",
            "140": "23774.",
            "141": "1 1500.",
            "142": "0ORB [46]86.",
            "143": "78 85.",
            "144": "29 78.",
            "145": "73607.",
            "146": "6 1005.",
            "147": "2SuperPoint [8]22.",
            "148": "85 22.",
            "149": "10 21.",
            "150": "83462.",
            "151": "6 1161.",
            "152": "0LF-Net [40]75.",
            "153": "05 74.",
            "154": "30 72.",
            "155": "61386.",
            "156": "7 1024.",
            "157": "0RF-Net [50]15.",
            "158": "64 15.",
            "159": "18 14.",
            "160": "581602.",
            "161": "5 5000.",
            "162": "0D2-Net [9]15.",
            "163": "56 9.",
            "164": "30 5.",
            "165": "21386.",
            "166": "9 1474.",
            "167": "5R2D2 [44]15.",
            "168": "80 14.",
            "169": "97 13.",
            "170": "50197.",
            "171": "9 1500.",
            "172": "0GIFT [26]42.",
            "173": "35 42.",
            "174": "05 41.",
            "175": "59589.",
            "176": "2 1161.",
            "177": "0LISRD [42]16.",
            "178": "96 16.",
            "179": "04 15.",
            "180": "64323.",
            "181": "6 1781.",
            "182": "1ASLFeat [28]19.",
            "183": "34 16.",
            "184": "38 13.",
            "185": "131366.",
            "186": "9 6764.",
            "187": "2DISK [57]13.",
            "188": "22 12.",
            "189": "43 12.",
            "190": "04359.",
            "191": "1 2048.",
            "192": "0PosFeat [24]13.",
            "193": "76 11.",
            "194": "79 9.",
            "195": "82717.",
            "196": "2 7623.",
            "197": "5ours93.",
            "198": "08 91.",
            "199": "35 90.",
            "200": "18688.",
            "201": "3 1161.",
            "202": "0ours*94.",
            "203": "35 92.",
            "204": "82 91.",
            "205": "691333.",
            "206": "0 2340.",
            "207": "4Table 3.",
            "208": "Comparison to existing local descriptors on Roto-360.",
            "209": "We use Super-Point keypoint detector [8] same to the GIFT descriptor [26].",
            "210": "and the number of predicted matches following [9,34].",
            "211": "HPatches[2] has 57 scenes with illumination variations and59 scenes with viewpoint variations.",
            "212": "Weuse the same evaluation metrics to Roto-360 to show thetransferability of our local descriptors.",
            "213": "MVS dataset[53] has six image sequences of outdoorscenes with GT camera poses.",
            "214": "We evaluate the relativepose estimation accuracy at5\u0000/10\u0000/20\u0000angular differencethresholds.",
            "215": "4.",
            "216": "2.",
            "217": "Comparison to other invariant mappingsTable1compares group aligning to various group pool-ing methods on the Roto-360 dataset using ground-truthkeypoint pairs,i.",
            "218": "We use\u0000GTto shift theequivariant features, and group aligning shows almost per-fect keypoint correspondences with 97.",
            "219": "54% matching ac-curacy.",
            "220": "Note that the bilinear pool-ing [26] does not guarantee the rotation-invariant matching.",
            "221": "Table2compares the proposed group aligning to the ex-isting group pooling methods on the Roto-360 dataset, thistime with predicted keypoint pairs and with training.",
            "222": "@10px @5px @3pxSIFT [27]SIFT [27]78.",
            "223": "86 78.",
            "224": "59 78.",
            "225": "23774.",
            "226": "1 1500GIFT [26]37.",
            "227": "97 36.",
            "228": "82 36.",
            "229": "09531.",
            "230": "2 1500ours84.",
            "231": "6779.",
            "232": "8577.",
            "233": "96558.",
            "234": "3 1500ours*84.",
            "235": "91 80.",
            "236": "09 78.",
            "237": "18759.",
            "238": "8 2219LF-Net [40]LF-Net [40]75.",
            "239": "0574.",
            "240": "30 72.",
            "241": "61386.",
            "242": "7 1024GIFT [26]35.",
            "243": "56 33.",
            "244": "82 32.",
            "245": "29426.",
            "246": "3 1024ours79.",
            "247": "9071.",
            "248": "63 67.",
            "249": "39431.",
            "250": "8 1024ours*80.",
            "251": "3271.",
            "252": "9967.",
            "253": "62591.",
            "254": "4 1503SuperPoint [8]SuperPoint [8]22.",
            "255": "85 22.",
            "256": "10 21.",
            "257": "83462.",
            "258": "6 1161GIFT [26]42.",
            "259": "35 42.",
            "260": "05 41.",
            "261": "59589.",
            "262": "2 1161ours93.",
            "263": "0891.",
            "264": "3590.",
            "265": "18688.",
            "266": "3 1161ours*94.",
            "267": "35 92.",
            "268": "82 91.",
            "269": "691333 234KeyNet [20]HyNet [55]24.",
            "270": "43 22.",
            "271": "82 20.",
            "272": "64288.",
            "273": "7 995GIFT [26]34.",
            "274": "08 32.",
            "275": "31 29.",
            "276": "17275.",
            "277": "7 995ours72.",
            "278": "9561.",
            "279": "3641.",
            "280": "33257.",
            "281": "2 995ours*72.",
            "282": "48 60.",
            "283": "69 40.",
            "284": "95356.",
            "285": "6 1484Table 4.",
            "286": "Comparison to existing local descriptors when usingthe same keypoint detector on Roto-360.",
            "287": "Overall, incorporatinggroup aligning demonstrates the best results in terms ofMMA compared to average pooling, max pooling or bilin-ear pooling [26].",
            "288": "4.",
            "289": "3.",
            "290": "Comparison to existing local descriptorsTable3shows the matching accuracy compared to ex-isting local descriptors on the Roto-360 dataset.",
            "291": "We evalu-ate the descriptors using their own keypoint detectors [8,9,27,39,40,44,50], or combined with off-the-shelf de-tectors [24,26,42].",
            "292": "While the classical methods [27,46]achieve better matching accuracy than the existing learning-based methods, our method achieves the best results over-all.",
            "293": "Table4shows the performance of our method in com-parison to existing local descriptors when using the samekeypoint detector, where our method shows consistent per-formance improvement.",
            "294": "In particular, our rotation-invariantdescriptor shows consistently higher matching accuracythan GIFT [26], which is a representative learning-basedgroup-invariant descriptor.",
            "295": "While our model shows a lowerMMA than the LF-Net [40] descriptor when using the LF-Net detector at 5px and 3px thresholds, we conjecture thatthis is due to the better integrity of the detector and descrip-tor of LF-Net due to their joint training scheme.",
            "296": "21892\n\n0\"/4\"/23\"/4\"5\"/43\"/27\"/460%100%2\"Figure 4.",
            "297": "Thedistribution is an orientation histogramo2R16, and the scores areconfidence values for each bin from group-equivariant features.",
            "298": "Group aligning can extract multiple descriptors withdifferent alignments by using multiple orientation candi-dates, denoted by \u2019ours*\u2019, whose scores are at least 60%of the maximum score in the orientation histogram.",
            "299": "Figure4illustrates an example of multipledescriptor extraction with a score ratio threshold of 0.",
            "300": "6.",
            "301": "Figure5illustrates how the matching ac-curacy changes with respect to varying degrees of rota-tion.",
            "302": "While MMAof SIFT [27] and ORB [46] are high at the upright rota-tions, they tend to fluctuate significantly with varying rota-tions.",
            "303": "The existing learning-based group-invariant descrip-tor, GIFT [26], fails to find correspondences beyond60\u0000.",
            "304": "4.",
            "305": "4.",
            "306": "Transferability to real image benchmarksTable5shows the matching performance of local de-scriptors on HPatches illumination/viewpoint [2] and poseestimation [53].",
            "307": "The performancegain of ours becomes smaller compared to the Roto-360dataset due to the absence of extreme rotations in HPatches.",
            "308": "While GIFT shows a higher performance under illumina-tion changes that only contain identity mappings, ours\u2020,which uses a larger backbone network (ReWRN), improves020406080100\n0306090120150180210240270300330360MMA @3pxAngle of rotation (degrees) oursGIFTSuperPointSIFTORB Figure 5.",
            "309": "Matching accuracies according to varying degree ofrotations on Roto-360.",
            "310": "MethodHP-allHP-illuHP-viewPose@5px @3px@5px @3px@5px @3px20\u000010\u00005\u0000SIFT [27]51.",
            "311": "36 46.",
            "312": "3249.",
            "313": "08 44.",
            "314": "6253.",
            "315": "57 47.",
            "316": "960.",
            "317": "02 0.",
            "318": "00ORB [46]52.",
            "319": "22 47.",
            "320": "4050.",
            "321": "85 46.",
            "322": "2953.",
            "323": "55 48.",
            "324": "470.",
            "325": "06 0.",
            "326": "00SuperPoint [8]69.",
            "327": "71 61.",
            "328": "7574.",
            "329": "63 67.",
            "330": "5364.",
            "331": "96 56.",
            "332": "170.",
            "333": "20 0.",
            "334": "07 0.",
            "335": "01LF-Net [40]56.",
            "336": "45 52.",
            "337": "2262.",
            "338": "21 57.",
            "339": "6350.",
            "340": "88 47.",
            "341": "06 0.",
            "342": "03 0.",
            "343": "01RF-Net [50]59.",
            "344": "08 54.",
            "345": "4261.",
            "346": "63 57.",
            "347": "4656.",
            "348": "62 51.",
            "349": "490.",
            "350": "10 0.",
            "351": "04 0.",
            "352": "01D2-Net [9]50.",
            "353": "18 32.",
            "354": "5463.",
            "355": "80 44.",
            "356": "0937.",
            "357": "02 21.",
            "358": "380.",
            "359": "11 0.",
            "360": "05 0.",
            "361": "01GIFT [26]76.",
            "362": "0367.",
            "363": "3179.",
            "364": "71 71.",
            "365": "8972.",
            "366": "48 62.",
            "367": "880.",
            "368": "600.",
            "369": "28 0.",
            "370": "09LISRD [42]62.",
            "371": "16 56.",
            "372": "1270.",
            "373": "09 63.",
            "374": "6454.",
            "375": "50 48.",
            "376": "850.",
            "377": "05 0.",
            "378": "02 0.",
            "379": "00oursavgpool64.",
            "380": "10 57.",
            "381": "9462.",
            "382": "28 56.",
            "383": "2765.",
            "384": "85 59.",
            "385": "550.",
            "386": "27 0.",
            "387": "10 0.",
            "388": "05oursmaxpool61.",
            "389": "57 55.",
            "390": "8159.",
            "391": "66 53.",
            "392": "9163.",
            "393": "42 57.",
            "394": "640.",
            "395": "27 0.",
            "396": "11 0.",
            "397": "03oursbilinearpool[26]45.",
            "398": "59 41.",
            "399": "9045.",
            "400": "13 41.",
            "401": "5746.",
            "402": "03 42.",
            "403": "220.",
            "404": "35 0.",
            "405": "17 0.",
            "406": "09oursbilinearpool\u2020[26]58.",
            "407": "72 53.",
            "408": "7757.",
            "409": "32 52.",
            "410": "6760.",
            "411": "06 54.",
            "412": "830.",
            "413": "24 0.",
            "414": "11 0.",
            "415": "03oursgroupalign70.",
            "416": "69 63.",
            "417": "4270.",
            "418": "39 62.",
            "419": "8870.",
            "420": "97 63.",
            "421": "950.",
            "422": "580.",
            "423": "26 0.",
            "424": "12oursgroupalign*73.",
            "425": "92 66.",
            "426": "3773.",
            "427": "13 65.",
            "428": "3374.",
            "429": "6967.",
            "430": "380.",
            "431": "56 0.",
            "432": "300.",
            "433": "12oursgroupalign\u202078.",
            "434": "00 69.",
            "435": "7077.",
            "436": "9469.",
            "437": "3578.",
            "438": "06 70.",
            "439": "030.",
            "440": "560.",
            "441": "33 0.",
            "442": "14Table 5.",
            "443": "\u2018ours*\u2019 denotes the extraction ofmultiple descriptors using the orientation candidates, whose scoresare at least 60% of the maximum score in the orientation his-togram.",
            "444": "\u2018ours\u2020\u2019 denotes our method using the rotation-equivariantWideResNet16-8 (ReWRN) backbone for feature extraction.",
            "445": "Weuse SuperPoint [8] keypoint detector to evaluate ours.",
            "446": "matching accuracy by 7.",
            "447": "15%p at 3px and 5.",
            "448": "58%p at 5px,and ours* improves by 4.",
            "449": "5%p at 3px, 2.",
            "450": "21%p at 5px un-der viewpoint changes compared to GIFT [26].",
            "451": "It shouldbe noted that the core difference between oursbilinearpooland GIFT is the usage of explicit rotation-equivariantCNNs [60], which clearly shows that bilinear pooling isnot well-compatible with the equivariant CNNs in compar-ison to group aligning.",
            "452": "Using the same network with bilin-ear pooling (oursbilinearpool\u2020) proposed in [26] shows signifi-cantly lower results compared to oursgroupalign\u2020.",
            "453": "In the MVS dataset [53] to evaluate relative camera poseestimation, our model shows a higher performance thanGIFT at finer error thresholds of10\u0000and5\u0000.",
            "454": "This showsthat our model can find more precise correspondences un-der 3D viewpoint changes.",
            "455": "Overall, these results show thatour descriptors using rotation-equivariant representation ex21893\n HP-allRoto-360params.",
            "456": "@5px @3px@5px @3px(millions)ours (proposed|G|= 16)70.",
            "457": "69 63.",
            "458": "4291.",
            "459": "3590.",
            "460": "180.",
            "461": "62Mw/o orientation loss66.",
            "462": "41 58.",
            "463": "6185.",
            "464": "29 83.",
            "465": "260.",
            "466": "62Mw/o descriptor loss27.",
            "467": "49 24.",
            "468": "8325.",
            "469": "64 24.",
            "470": "980.",
            "471": "62Mw/o image scale pyramid68.",
            "472": "7762.",
            "473": "2591.",
            "474": "47 90.",
            "475": "430.",
            "476": "62Mw/o equivariant backbone47.",
            "477": "25 42.",
            "478": "528.",
            "479": "65 8.",
            "480": "5111.",
            "481": "18M|G|= 6463.",
            "482": "96 57.",
            "483": "3585.",
            "484": "12 83.",
            "485": "320.",
            "486": "16M|G|= 3668.",
            "487": "17 60.",
            "488": "9587.",
            "489": "78 85.",
            "490": "890.",
            "491": "26M|G|= 3269.",
            "492": "44 62.",
            "493": "0889.",
            "494": "10 87.",
            "495": "310.",
            "496": "31M|G|= 2469.",
            "497": "72 62.",
            "498": "2190.",
            "499": "27 88.",
            "500": "340.",
            "501": "39M|G|=865.",
            "502": "74 58.",
            "503": "9287.",
            "504": "16 85.",
            "505": "571.",
            "506": "24MTable 6.",
            "507": "Ablation test on HPatches and Roto-360.",
            "508": "4.",
            "509": "5.",
            "510": "Ablation study and design choiceTable6shows the results of ablation studies on theHPatches and Roto-360 datasets.",
            "511": "Not us-ing the image pyramid at inference time results in a slightdrop in HPatches, but the performance on Roto-360 remainsnearly unchanged.",
            "512": "When training without equivariant lay-ers, ResNet-18 with conventional convolutional layers wasused this results in a drastic drop in performance espe-cially on Roto-360, with a rapid increase in the number ofmodel parameters.",
            "513": "We also demonstrate the effect of the order of cyclicgroupGon the performance of our method in the secondgroup of Table6.",
            "514": "We fix the computational costC\u21e5|G|=1,024, and vary the order of group to show the parameterefficiency of the group equivariant networks.",
            "515": "Our designchoice|G|= 16yields the best results, and the perfor-mance drops gracefully asGincreases.",
            "516": "Reducing the order of group to|G|=8reduces the MMAin both benchmarks as well, which we suspect is becausethe range of rotation covered by one group action becomestoo wide, leading to increased approximation errors.",
            "517": "4.",
            "518": "6.",
            "519": "Qualitative resultsFigure6visualizes the consistency of dominant orienta-tion estimation.",
            "520": "The green and red arrows (middle, (b) LF-Net(62/114)\n(0/89)(c) RF-Net (a) Ours (89/106)SourceTargetAligned viewFigure 6.",
            "521": "right) represent the consistent and inconsistent orientationpredictions with respect to the initial estimations (left) at a30\u0000threshold.",
            "522": "Compared to LF-Net [40] and RF-Net [50], our method pre-dicts more consistent dominant orientations of keypoints.",
            "523": "5.",
            "524": "This work was supported by SamsungResearch Funding & Incubation Center of Samsung Elec-tronics under Project Number SRFC-TF2103-02 and alsoby the NRF grant (NRF-2021R1A2C3012728) funded bythe Korea government (MSIT).",
            "525": "21894\n References.",
            "526": "[1]Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Si-mon, Brian Curless, Steven M Seitz, and Richard Szeliski.",
            "527": "Communications of the ACM,54(10):105\u2013112, 2011.",
            "528": "1[2]Vassileios Balntas, Karel Lenc, Andrea Vedaldi, and Krys-tian Mikolajczyk.",
            "529": "InProceed-ings of the IEEE Conference on Computer Vision and PatternRecognition, pages 5173\u20135182, 2017.",
            "530": "2,5,6,7[3]Axel Barroso-Laguna, Yurun Tian, and Krystian Mikola-jczyk.",
            "531": "InProceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 12808\u201312818, 2022.",
            "532": "2[4]Fabio Bellavia and Carlo Colombo.",
            "533": "IEEE Transactions on Pattern Analysis and Ma-chine Intelligence, 40(4):931\u2013944, 2017.",
            "534": "2[5]Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-offrey Hinton.",
            "535": "InInternational conference on ma-chine learning, pages 1597\u20131607.",
            "536": "PMLR, 2020.",
            "537": "5[6]Taco Cohen and Max Welling.",
            "538": "InInternational conference on machinelearning, pages 2990\u20132999.",
            "539": "PMLR, 2016.",
            "540": "2[7]Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-novich.",
            "541": "arXiv preprintarXiv:1707.",
            "542": "07410, 2017.",
            "543": "1[8]Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-novich.",
            "544": "InCVPR Deep Learning for Visual SLAMWorkshop, 2018.",
            "545": "1,2,3,5,6,7[9]Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Polle-feys, Josef Sivic, Akihiko Torii, and Torsten Sattler.",
            "546": "D2-net: A trainable cnn for joint description and detection oflocal features.",
            "547": "InProceedings of the ieee/cvf conference oncomputer vision and pattern recognition, pages 8092\u20138101,2019.",
            "548": "2,6,7[10]Patrick Ebel, Anastasiia Mishchuk, Kwang Moo Yi, PascalFua, and Eduard Trulls.",
            "549": "InProceedings of the IEEE/CVF In-ternational Conference on Computer Vision, pages 253\u2013262,2019.",
            "550": "2[11]Bin Fan, Fuchao Wu, and Zhanyi Hu.",
            "551": "IEEE transactionson pattern analysis and machine intelligence, 34(10):2031\u20132045, 2011.",
            "552": "1,2[12]Jiaming Han, Jian Ding, Nan Xue, and Gui-Song Xia.",
            "553": "InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 2786\u20132795, 2021.",
            "554": "2,5[13]Bharath Hariharan, Pablo Arbel\u00b4aez, Ross Girshick, and Ji-tendra Malik.",
            "555": "InProceedings of the IEEE con-ference on computer vision and pattern recognition, pages447\u2013456, 2015.",
            "556": "3[14]Chris Harris, Mike Stephens, et al.",
            "557": "InAlvey vision conference, volume 15, pages10\u20135244.",
            "558": "Citeseer, 1988.",
            "559": "3,5[15]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.",
            "560": "InProceed-ings of the IEEE conference on computer vision and patternrecognition, pages 770\u2013778, 2016.",
            "561": "2[16]Jared Heinly, Johannes L.",
            "562": "In2015 IEEE Conference on Computer Vision and PatternRecognition (CVPR), pages 3287\u20133295, 2015.",
            "563": "1[17]Yuhe Jin, Dmytro Mishkin, Anastasiia Mishchuk, Jiri Matas,Pascal Fua, Kwang Moo Yi, and Eduard Trulls.",
            "564": "International Journal of Computer Vision, 129(2):517\u2013547,2021.",
            "565": "1[18]Seungwook Kim, Yoonwoo Jeong, Chunghyun Park, JaesikPark, and Minsu Cho.",
            "566": "InNeurIPS 2022 Workshop on Symmetry andGeometry in Neural Representations, 2022.",
            "567": "2[19]Seungwook Kim, Juhong Min, and Minsu Cho.",
            "568": "InProceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 8697\u20138707, 2022.",
            "569": "3[20]Axel Barroso Laguna and Krystian Mikolajczyk.",
            "570": "IEEE Transactions on Pattern Analysis and MachineIntelligence, 2022.",
            "571": "3,6[21]Jongmin Lee, Yoonwoo Jeong, and Minsu Cho.",
            "572": "In31stBritish Machine Vision Conference 2021, BMVC 2021, Vir-tual Event, UK.",
            "573": "BMV A Press, 2021.",
            "574": "1,2,4[22]Jongmin Lee, Yoonwoo Jeong, Seungwook Kim, JuhongMin, and Minsu Cho.",
            "575": "InProceedings of theIEEE/CVF Winter Conference on Applications of ComputerVision, pages 898\u2013908, 2021.",
            "576": "2[23]Jongmin Lee, Byungjin Kim, and Minsu Cho.",
            "577": "InProceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 4847\u20134857,2022.",
            "578": "1,2,4[24]Kunhong Li, Longguang Wang, Li Liu, Qing Ran, Kai Xu,and Yulan Guo.",
            "579": "InProceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 15838\u201315848, 2022.",
            "580": "2,6[25]Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,Pietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C LawrenceZitnick.",
            "581": "InEuropean conference on computer vision, pages 740\u2013755.",
            "582": "Springer, 2014.",
            "583": "Advances in NeuralInformation Processing Systems, 32:6992\u20137003, 2019.",
            "584": "1,2,5,6,7\n21895\n\n[27]David G Lowe.",
            "585": "International journal of computer vi-sion, 60(2):91\u2013110, 2004.",
            "586": "1,2,3,4,5,6,7[28]Zixin Luo, Lei Zhou, Xuyang Bai, Hongkai Chen, JiahuiZhang, Yao Yao, Shiwei Li, Tian Fang, and Long Quan.",
            "587": "InProceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pages 6589\u20136598,2020.",
            "588": "2,6[29]Simon Lynen, Bernhard Zeisl, Dror Aiger, Michael Bosse,Joel Hesch, Marc Pollefeys, Roland Siegwart, and TorstenSattler.",
            "589": "The International Journal of Robotics Research,39(9):1061\u20131084, 2020.",
            "590": "1[30]Diego Marcos, Michele Volpi, Nikos Komodakis, and DevisTuia.",
            "591": "InPro-ceedings of the IEEE International Conference on ComputerVision, pages 5048\u20135057, 2017.",
            "592": "2[31]Roland Memisevic.",
            "593": "InICML, 2012.",
            "594": "2[32]Roland Memisevic and Geoffrey E Hinton.",
            "595": "Neural computation, 22(6):1473\u20131492, 2010.",
            "596": "2[33]Krystian Mikolajczyk and Cordelia Schmid.",
            "597": "International journal ofcomputer vision, 60(1):63\u201386, 2004.",
            "598": "5[34]Krystian Mikolajczyk and Cordelia Schmid.",
            "599": "IEEE transactions on patternanalysis and machine intelligence, 27(10):1615\u20131630, 2005.",
            "600": "6[35]Juhong Min, Jongmin Lee, Jean Ponce, and Minsu Cho.",
            "601": "InICCV, 2019.",
            "602": "3[36]Anastasiia Mishchuk, Dmytro Mishkin, Filip Radenovic,and Jiri Matas.",
            "603": "InAdvances in NeuralInformation Processing Systems, pages 4826\u20134837, 2017.",
            "604": "2[37]Dmytro Mishkin, Filip Radenovic, and Jiri Matas.",
            "605": "InProceedings of the European Conference onComputer Vision (ECCV), pages 284\u2013300, 2018.",
            "606": "2[38]Daniel Moyer, Esra Abaci Turk, P Ellen Grant, William MWells, and Polina Golland.",
            "607": "Equivariant filters for efficienttracking in 3d imaging.",
            "608": "InInternational Conference on Med-ical Image Computing and Computer-Assisted Intervention,pages 193\u2013202.",
            "609": "Springer, 2021.",
            "610": "2[39]Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan DTardos.",
            "611": "IEEE transactions on robotics, 31(5):1147\u20131163,2015.",
            "612": "1,6[40]Yuki Ono, Eduard Trulls, Pascal Fua, and Kwang Moo Yi.",
            "613": "InAdvancesin neural information processing systems, pages 6234\u20136244,2018.",
            "614": "2,3,6,7,8[41]Aaron van den Oord, Yazhe Li, and Oriol Vinyals.",
            "615": "arXivpreprint arXiv:1807.",
            "616": "03748, 2018.",
            "617": "5[42]R\u00b4emi Pautrat, Viktor Larsson, Martin R Oswald, and MarcPollefeys.",
            "618": "InEuropean Conference on Computer Vision,pages 707\u2013724.",
            "619": "Springer, 2020.",
            "620": "2,6,7[43]Nicolas Pielawski, Elisabeth Wetzer, Johan\u00a8Ofverstedt, Ji-ahao Lu, Carolina W\u00a8ahlby, Joakim Lindblad, and Nata\u02c7saSladoje.",
            "621": "Lin, editors,Advances in NeuralInformation Processing Systems, volume 33, pages 18433\u201318444.",
            "622": ", 2020.",
            "623": "2[44]Jerome Revaud, Cesar De Souza, Martin Humenberger, andPhilippe Weinzaepfel.",
            "624": "R2d2: Reliable and repeatable detec-tor and descriptor.",
            "625": "Advances in neural information process-ing systems, 32:12405\u201312415, 2019.",
            "626": "1,2,5,6[45]J\u00b4erome Revaud, Vincent Leroy, Philippe Weinzaepfel, andBoris Chidlovskii.",
            "627": "InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 3926\u20133936, 2022.",
            "628": "1[46]Ethan Rublee, Vincent Rabaud, Kurt Konolige, and GaryBradski.",
            "629": "In2011International conference on computer vision, pages 2564\u20132571.",
            "630": "Ieee, 2011.",
            "631": "1,2,6,7[47]Torsten Sattler, Bastian Leibe, and Leif Kobbelt.",
            "632": "InEuropean conference on computer vision, pages 752\u2013765.",
            "633": "Springer, 2012.",
            "634": "1[48]Torsten Sattler, Will Maddern, Carl Toft, Akihiko Torii,Lars Hammarstrand, Erik Stenborg, Daniel Safari, MasatoshiOkutomi, Marc Pollefeys, Josef Sivic, et al.",
            "635": "Benchmark-ing 6dof outdoor visual localization in changing conditions.",
            "636": "InProceedings of the IEEE Conference on Computer Visionand Pattern Recognition, pages 8601\u20138610, 2018.",
            "637": "1[49]Johannes L Schonberger and Jan-Michael Frahm.",
            "638": "InProceedings of the IEEE con-ference on computer vision and pattern recognition, pages4104\u20134113, 2016.",
            "639": "1[50]Xuelun Shen, Cheng Wang, Xin Li, Zenglei Yu, JonathanLi, Chenglu Wen, Ming Cheng, and Zijian He.",
            "640": "InProceedings of the IEEE Conference on Computer Visionand Pattern Recognition, pages 8132\u20138140, 2019.",
            "641": "2,6,7,8[51]Kihyuk Sohn and Honglak Lee.",
            "642": "InICML, 2012.",
            "643": "2[52]Ivan Sosnovik, Artem Moskalev, and Arnold Smeulders.",
            "644": "InPro-ceedings of the IEEE/CVF International Conference onComputer Vision, pages 1092\u20131097, 2021.",
            "645": "2[53]Christoph Strecha, Wolfgang Von Hansen, Luc Van Gool,Pascal Fua, and Ulrich Thoennessen.",
            "646": "In2008 IEEE conference on computer vision and pat-tern recognition, pages 1\u20138.",
            "647": "Ieee, 2008.",
            "648": "2,5,6,7[54]Yurun Tian, Axel Barroso Laguna, Tony Ng, Vassileios Bal-ntas, and Krystian Mikolajczyk.",
            "649": "Ad21896\n vances in Neural Information Processing Systems, 33:7401\u20137412, 2020.",
            "650": "2[55]Yurun Tian, Axel Barroso Laguna, Tony Ng, Vassileios Bal-ntas, and Krystian Mikolajczyk.",
            "651": "InNeurIPS, 2020.",
            "652": "6[56]Yurun Tian, Xin Yu, Bin Fan, Fuchao Wu, Huub Heijnen,and Vassileios Balntas.",
            "653": "InProceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 11016\u201311025, 2019.",
            "654": "2[57]Michal Jan Tyszkiewicz, Pascal Fua, and Eduard Trulls.",
            "655": "Advancesin Neural Information Processing Systems, 33, 2020.",
            "656": "2,6[58]Rui Wang, Robin Walters, and Rose Yu.",
            "657": "arXiv preprint arXiv:2206.",
            "658": "09450, 2022.",
            "659": "1[59]Zhenhua Wang, Bin Fan, Gang Wang, and Fuchao Wu.",
            "660": "IEEE transactions on pattern analysis andmachine intelligence, 38(11):2198\u20132211, 2015.",
            "661": "2[60]Maurice Weiler and Gabriele Cesa.",
            "662": "General e (2)-equivariantsteerable cnns.",
            "663": "Advances in Neural Information ProcessingSystems, 32:14334\u201314345, 2019.",
            "664": "1,2,3,5,7[61]Maurice Weiler, Fred A Hamprecht, and Martin Storath.",
            "665": "InProceedings of the IEEE Conference on Computer Visionand Pattern Recognition, pages 849\u2013858, 2018.",
            "666": "2[62]Daniel E Worrall, Stephan J Garbin, Daniyar Turmukham-betov, and Gabriel J Brostow.",
            "667": "InProceedings of theIEEE Conference on Computer Vision and Pattern Recogni-tion, pages 5028\u20135037, 2017.",
            "668": "2[63]Pei Yan, Yihua Tan, Shengzhou Xiong, Yuan Tai, and Yan-sheng Li.",
            "669": "InProceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 19406\u201319415, 2022.",
            "670": "1,4[64]Kwang Moo Yi, Eduard Trulls, Vincent Lepetit, and PascalFua.",
            "671": "InEuropeanconference on computer vision, pages 467\u2013483.",
            "672": "Springer,2016.",
            "673": "1[65]Yanzhao Zhou, Qixiang Ye, Qiang Qiu, and Jianbin Jiao.",
            "674": "InProceedings of the IEEEConference on Computer Vision and Pattern Recognition,pages 519\u2013528, 2017.",
            "675": "2[66]Siyu Zhu, Runze Zhang, Lei Zhou, Tianwei Shen, TianFang, Ping Tan, and Long Quan.",
            "676": "InProceedings of theIEEE conference on computer vision and pattern recogni-tion, pages 4568\u20134577, 2018.",
            "677": "1\n21897\n\n."
        },
        "Recurrent Homography Estimation Using Homography-Guided Image Warping and Focus Transformer": {
            "authors": [
                "Yuan Cao",
                "Runmin Zhang",
                "Lun Luo",
                "Beinan Yu",
                "Zehua Sheng",
                "Junwei Li",
                "Liang Shen"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Cao_Recurrent_Homography_Estimation_Using_Homography-Guided_Image_Warping_and_Focus_Transformer_CVPR_2023_paper.pdf",
            "ref_texts": "[25] Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou. Gift: Learning transformation-invariant dense visual descriptors via group CNNs. Advances in Neural Information Processing Systems , 32, 2019. 1, 3, 4",
            "ref_ids": [
                "25"
            ],
            "1": "However, besides translation, homography is composed of rotation, scaling, shearing, aspect ratio, and perspective transformations [37, 43], which leads to the inconsistency of the features from corresponding points [25].",
            "2": "Many efforts have been made to acquire the transformation-equivariance by either applying group convolutions in the network [9] or pre-warping [16, 20, 25, 43] the input image.",
            "3": "Many efforts [9\u201311, 16, 25, 39] have been made to deal with this problem.",
            "4": "However, the model becomes inefficient as it applies group convolutions directly on a large group [25].",
            "5": "Other methods [16, 20, 25, 43] achieve transformation-equivariant by the predefined warping of the input image by different transformation dimensions and degrees.",
            "6": "For example, GIFT [25] produces rotation and scale invariant features using warped images with predefined rotation angles and scale ratios.",
            "7": "To cope with the above problem, many strategies have been proposed [9, 10, 20, 25] to either employ the group convolution or pre-warp the image by multiple times."
        },
        "TransVLAD: Multi-scale attention-based global descriptors for visual geo-localization": {
            "authors": [
                "Yifan Xu",
                "Pourya Shamsolmoali",
                "Eric Granger",
                "Claire Nicodeme",
                "Laurent Gardes",
                "Jie Yang"
            ],
            "url": "https://openaccess.thecvf.com/content/WACV2023/papers/Xu_TransVLAD_Multi-Scale_Attention-Based_Global_Descriptors_for_Visual_Geo-Localization_WACV_2023_paper.pdf",
            "ref_texts": "[24] Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou. Gift: Learning transformation-invariant dense visual descriptors via group cnns. In Adv. Neural Inform. Process. Syst. , 2019.",
            "ref_ids": [
                "24"
            ]
        },
        "Learning soft estimator of keypoint scale and orientation with probabilistic covariant loss": {
            "authors": [
                "Pei Yan",
                "Yihua Tan",
                "Shengzhou Xiong",
                "Yuan Tai",
                "Yansheng Li"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Yan_Learning_Soft_Estimator_of_Keypoint_Scale_and_Orientation_With_Probabilistic_CVPR_2022_paper.pdf",
            "ref_texts": "[21] Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou. GIFT: learning transformation-invariant dense visual descriptors via group cnns. In Proceedings of the Advances in Neural Information Processing Systems , pages 6990\u20137001, 2019. 1",
            "ref_ids": [
                "21"
            ],
            "1": "However, it is challenging to maintain the invariance under significant geometric changes [21, 35]."
        },
        "A robust learned feature-based visual odometry system for UAV pose estimation in challenging indoor environments": {
            "authors": [],
            "url": "https://pureportal.strath.ac.uk/files/164587189/Yu_etal_IEEE_TIM_2023_A_robust_learned_feature_based_visual_odometry.pdf",
            "ref_texts": "[18] Y . Liu, Z. Shen, Z. Lin, S. Peng, H. Bao, and X. Zhou, \u201cGift: Learning transformation-invariant dense visual descriptors via group cnns,\u201d Advances in Neural Information Processing Systems , vol. 32, 2019.",
            "ref_ids": [
                "18"
            ],
            "1": "Compared to traditional features, the features extracted or described by Convolutional Neural Networks (CNNs), such as the ASLFeat [17] and GIFT [18], show significant improvements in accuracy and robustness in challenging environments.",
            "2": "[18] Y ."
        },
        "A Pseudoinverse Siamese Convolutional Neural Network of Transformation Invariance Feature Detection and Description for a SLAM System": {
            "authors": [
                "Chaofeng Yuan",
                "Yuelei Xu",
                "Jingjing Yang",
                "Zhaoxiang Zhang",
                "Qing Zhou"
            ],
            "url": "https://www.mdpi.com/2075-1702/10/11/1070/pdf",
            "ref_texts": "20. Liu, Y.; Shen, Z.; Lin, Z.; Peng, S.; Bao, H.; Zhou, X. Gift: Learning transformation-invariant dense visual descriptors via group cnns. arXiv 2019 , arXiv:1911.05932.",
            "ref_ids": [
                "20"
            ],
            "1": "GIFT [20] introduces a novel dense descriptor, with a provable invariance to a certain group of transformations.",
            "2": "Group features [20,26] and the bilinear model [27] are merged to generate descriptors, as shown in Figure 3: Gmap(i) =y(B(ti\u0001I),ti(P(B(I)))) (2) where yis the bilinear interpolation function, Bis the backbone network feature detection function, tiis the transformation and Pis the feature point detector.",
            "3": "In the feature point descriptor subnetwork, we adopt group convolutions [20] to extract description features, and the group convolutions are divided into two branches."
        },
        "Feature-Point Matching for Aerial and Ground Images by Exploiting Line Segment-Based Local-Region Constraints": {
            "authors": [
                "Min Chen",
                "Tong Fang",
                "Qing Zhu",
                "Xuming Ge",
                "Zhanhao Zhang",
                "Xin Zhang"
            ],
            "url": "https://www.ingentaconnect.com/contentone/asprs/pers/2021/00000087/00000010/art00012?crawler=true&mimetype=application/pdf",
            "ref_texts": "5394 in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , held in Las Vegas, NV, 27 30 June 2016. Location: IEEE or https://doi.org/10.1109/CVPR.2016.581. Li, J., Q. Hu and M. Ai. 2017. 4FP-structure: A robust local region feature descriptor. Photogrammetric Engineering and Remote Sensing 83(12):813\u2013826. Li, K. and J. Yao. 2017. Line segment matching and reconstruction via exploiting coplanar cues. ISPRS Journal of Photogrammetry and Remote Sensing 125:33\u201349. Li, K., J. Yao, M. Lu, Y. Heng, T. Wu and Y. Li. 2016a. Line segment matching: A benchmark. Pages 1 9 in 2016 IEEE Winter Conference on Applications of Computer Vision (WACV) , held in Lake Placid, NY, 7\u201310 March 2016. Location: IEEE or https://doi. org/10.1109/WACV.2016.7477726. Li, K., J. Yao, X. Lu, L. Li and Z. Zhang. 2016b. Hierarchical line matching based on Line\u2013Junction\u2013Line structure descriptor and local homography estimation. Neurocomputing 184:207\u2013220.Liu, Y., Z. Shen, Z. Lin, S. Peng, H. Bao and X. Zhou. 2019. GIFT: Learning Transformation Invariant Dense Visual Descriptors via Group CNNs. <https://arxiv.org/abs/1911.05932> Accessed 14 Nov 2019. L\u00f3pez, J., R. Santos, X. R. Fdez-Vidal and X. M. Pardo. 2015. Twoview line matching algorithm based on context and appearance in low-textured images. Pattern Recognition 48(7):2164\u20132184. Lowe, D. G. 2004. Distinctive image features from scale-invariant keypoints. International Journal of Computer Vision 60(2):91\u2013110. Luo, Z., T. Shen, L. Zhou, J. Zhang, Y. Yao, S. Li, T. Fang and L. Quan. 2019. ContextDesc: Local descriptor augmentation with cross modality context. Pages 2527\u20132536 in 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , held in Long Beach, CA, 15\u201320 June 2019. Location: IEEE or https://doi.org/10.1109/CVPR.2019.00263. Luo, Z., T. Shen, L. Zhou, S. Zhu, R. Zhang, Y. Yao, T. Fang and L. Quan. "
        },
        "A Stricter Constraint Produces Outstanding Matching: Learning Reliable Image Matching with a Quadratic Hinge Triplet Loss Network": {
            "authors": [],
            "url": "https://openreview.net/pdf?id=3baVPWLmUiO",
            "ref_texts": "[15] Y . Liu, Z. Shen, Z. Lin, S. Peng, H. Bao, and X. Zhou. Gift: Learning transformation-invariant dense visual descriptors via group cnns. arXiv preprint arXiv:1911.05932 , 2019.",
            "ref_ids": [
                "15"
            ],
            "1": "Traditionally, detectors and descriptors are separately applied in the pipeline, SIFT [16] (and RootSIFT [3]) and SURF [4] are most popular detectors while some descriptors are followed, in which LogPolar [12] shows better performance than ContextDesc [17] from the relative pose error, while SOSNet [26] and HardNet [18] surpass GIFT [15] in the public validation set.",
            "2": "167\n[15] Y ."
        },
        "Object Pose Estimation without Direct Supervision": {
            "authors": [],
            "url": "https://kilthub.cmu.edu/ndownloader/files/39009038",
            "ref_texts": "[143] Y . Liu, Z. Shen, Z. Lin, S. Peng, H. Bao, and X. Zhou, \u201cGift: Learning transformationinvariant dense visual descriptors via group cnns,\u201d NeurIPS , vol. 32, 2019.",
            "ref_ids": [
                "143"
            ],
            "1": "Dense descriptors [142,143] and descriptor fields [144] achieve generalization across classes by predicting a dense embedding over the full image/point cloud space.",
            "2": "[143] Y ."
        },
        "Learning Rotation-Equivariant Features for Visual Correspondence-supplementary material": {
            "authors": [
                "N L"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/supplemental/Lee_Learning_Rotation-Equivariant_Features_CVPR_2023_supplemental.pdf",
            "ref_texts": "[1]Vassileios Balntas, Karel Lenc, Andrea Vedaldi, and Krys-tian Mikolajczyk. Hpatches: A benchmark and evaluationof handcrafted and learned local descriptors. InProceed-ings of the IEEE Conference on Computer Vision and PatternRecognition, pages 5173\u20135182, 2017.1,5[2]Taco Cohen and Max Welling. Group equivariant convo-lutional networks. InInternational conference on machinelearning, pages 2990\u20132999. PMLR, 2016.1[3]Taco S Cohen, Mario Geiger, and Maurice Weiler. A gen-eral theory of equivariant cnns on homogeneous spaces. InProceedings of the 33rd International Conference on NeuralInformation Processing Systems, pages 9145\u20139156, 2019.1[4]Taco S Cohen and Max Welling. Steerable cnns.arXivpreprint arXiv:1612.08498, 2016.1[5]Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-novich. Superpoint: Self-supervised interest point detectionand description. InCVPR Deep Learning for Visual SLAMWorkshop, 2018.2,3,4,7[6]Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Polle-feys, Josef Sivic, Akihiko Torii, and Torsten Sattler. D2-net: A trainable cnn for joint description and detection oflocal features. InProceedings of the ieee/cvf conference oncomputer vision and pattern recognition, pages 8092\u20138101,2019.2,3[7]Martin A Fischler and Robert C Bolles. Random sampleconsensus: a paradigm for model fitting with applications toimage analysis and automated cartography.Communicationsof the ACM, 24(6):381\u2013395, 1981.2[8]Yuhe Jin, Dmytro Mishkin, Anastasiia Mishchuk, Jiri Matas,Pascal Fua, Kwang Moo Yi, and Eduard Trulls. Imagematching across wide baselines: From paper to practice.International Journal of Computer Vision, 129(2):517\u2013547,2021.1,3[9]Axel Barroso Laguna and Krystian Mikolajczyk. Key. net:Keypoint detection by handcrafted and learned cnn filters re-visited.IEEE Transactions on Pattern Analysis and MachineIntelligence, 2022.2[10]Jongmin Lee, Yoonwoo Jeong, and Minsu Cho. Self-supervised learning of image scale and orientation. In31stBritish Machine Vision Conference 2021, BMVC 2021, Vir-tual Event, UK. BMV A Press, 2021.5[11]Jongmin Lee, Byungjin Kim, and Minsu Cho. Self-supervised equivariant learning for oriented keypoint detec-tion. InProceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 4847\u20134857,2022.5[12]Kunhong Li, Longguang Wang, Li Liu, Qing Ran, Kai Xu,and Yulan Guo. Decoupling makes weakly supervised localfeature better. InProceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 15838\u201315848, 2022.3[13]Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,Pietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C LawrenceZitnick. Microsoft coco: Common objects in context. InEuropean conference on computer vision, pages 740\u2013755.Springer, 2014.3[14]Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao,and Xiaowei Zhou. Gift: Learning transformation-invariantdense visual descriptors via group cnns.Advances in NeuralInformation Processing Systems, 32:6992\u20137003, 2019.1,2,3,4,5,9[15]David G Lowe. Distinctive image features from scale-invariant keypoints.International journal of computer vi-sion, 60(2):91\u2013110, 2004.2,4[16]Anastasiia Mishchuk, Dmytro Mishkin, Filip Radenovic,and Jiri Matas. Working hard to know your neighbor\u2019s mar-gins: Local descriptor learning loss. InAdvances in NeuralInformation Processing Systems, pages 4826\u20134837, 2017.3[17]Yuki Ono, Eduard Trulls, Pascal Fua, and Kwang Moo Yi.Lf-net: learning local features from images. InAdvancesin neural information processing systems, pages 6234\u20136244,2018.3,5,7[18]R\u00b4emi Pautrat, Viktor Larsson, Martin R Oswald, and MarcPollefeys. Online invariance selection for local feature de-scriptors. InEuropean Conference on Computer Vision,pages 707\u2013724. Springer, 2020.1,2,3,6[19]Jerome Revaud, Cesar De Souza, Martin Humenberger, andPhilippe Weinzaepfel. R2d2: Reliable and repeatable detec-tor and descriptor.Advances in neural information process-ing systems, 32:12405\u201312415, 2019.2,3[20]Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,Aditya Khosla, Michael Bernstein, et al. Imagenet largescale visual recognition challenge.International journal ofcomputer vision, 115(3):211\u2013252, 2015.4[21]Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz,and Andrew Rabinovich. Superglue: Learning featurematching with graph neural networks. InProceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 4938\u20134947, 2020.4[22]Xuelun Shen, Cheng Wang, Xin Li, Zenglei Yu, JonathanLi, Chenglu Wen, Ming Cheng, and Zijian He. Rf-net: Anend-to-end image matching network based on receptive field.InProceedings of the IEEE Conference on Computer Visionand Pattern Recognition, pages 8132\u20138140, 2019.3,5,7[23]Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, andXiaowei Zhou. Loftr: Detector-free local feature matchingwith transformers. InProceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages8922\u20138931, 2021.4[24]Yurun Tian, Axel Barroso Laguna, Tony Ng, Vassileios Bal-ntas, and Krystian Mikolajczyk. Hynet: Learning local de-scriptor with hybrid similarity measure and triplet loss.Ad-vances in Neural Information Processing Systems, 33:7401\u20137412, 2020.2[25]Yurun Tian, Axel Barroso Laguna, Tony Ng, Vassileios Bal-ntas, and Krystian Mikolajczyk. Hynet: Learning local de-scriptor with hybrid similarity measure and triplet loss. InNeurIPS, 2020.3[26]Maurice Weiler and Gabriele Cesa. General e (2)-equivariantsteerable cnns.Advances in Neural Information ProcessingSystems, 32:14334\u201314345, 2019.1,3,5[27]Maurice Weiler, Fred A Hamprecht, and Martin Storath.Learning steerable filters for rotation equivariant cnns. In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition, pages 849\u2013858, 2018.1[28]Hao Zhou, Torsten Sattler, and David W Jacobs. Evaluatinglocal features for day-night matching. InEuropean Confer-ence on Computer Vision, pages 724\u2013736. Springer, 2016.1,2,6",
            "ref_ids": [
                "1",
                "2",
                "3",
                "4",
                "5",
                "6",
                "7",
                "8",
                "9",
                "10",
                "11",
                "12",
                "13",
                "14",
                "15",
                "16",
                "17",
                "18",
                "19",
                "20",
                "21",
                "22",
                "23",
                "24",
                "25",
                "26",
                "27",
                "28"
            ],
            "1": "Section2evaluates the matchingquality of our proposed method under rotation and illu-mination variations on the day/night image pairs, with de-tails about the benchmark generation.",
            "2": "Section3shows theresults of realistic downstream task on the IMC2021 [8]dataset.",
            "3": "Section4shows the comparisons of computationaloverhead and the number of parameters.",
            "4": "Section5showsdifferent strategies of multiple descriptor extraction usingdominant orientation candidates.",
            "5": "Section6evaluates theexisting feature matching methods in the Roto-360 dataset.",
            "6": "Section7shows the re-training results of GIFT with cyclicrotation augmentation.",
            "7": "Section8shows the matching re-sults with increasing the number of samples of the Roto-360dataset.",
            "8": "Section9presents additional qualitative results tovisualize the consistency of dominant orientation estima-tion, the similarity maps under in-plane rotations of images,and predicted matches on the HPatches and extreme rota-tion (ER) datasets [1,14].",
            "9": "Group equivarianceA feature extractor\u0000is said to be equivariant to a geo-metric transformationTgif transforming an inputx2XbyTgand then passing it through\u0000gives the same result asfirst passingxthrough\u0000and then transforming the result-ing feature map byT0g.",
            "10": "Formally, the equivariance can beexpressed for transformation groupGand\u0000:X!Yas\u0000[Tg(x)] =T0g[\u0000(x)],(1)whereTgandT0grepresent transformations on each space ofa group actiong2G.",
            "11": "IfTtis a translation group(R2,+),andfis a feature mapping functionZ2!RKgiven convo-lution filter weights 2R2\u21e5K, the translation equivarianceof a convolutional operation can be expressed as follows:[Ttf]\u21e4 (x)=[Tt[f\u21e4 ]](x),(2)where\u21e4indicates the convolution operation.",
            "12": "Recent studies [2\u20134,26,27] propose convolutional neuralnetworks that are equivariant to symmetry groups of trans-lation, rotation, and reflection.",
            "13": "The groupGcan be defined byG\u21e0=(R2,+)oHas thesemidirect product of the translation group(R2,+)with therotation groupH.",
            "14": "Then, the rotation-equivariant convolu-tion on groupGcan be defined as:[Tgf]\u21e4 (g)=[Tg[f\u21e4 ]](g),(3)by replacingt2(R2,+)withg2Gin Eq.",
            "15": "2.",
            "16": "Formally, let\u0000={Li|i2{1,2,3,.",
            "17": "For one layerLi2\u0000, the transformationTgis defined asLi[Tg(g)] =Tg[Li(g)],(4)which indicates that the output is preserved afterLiaboutTg.",
            "18": "(5)2.",
            "19": "Experiments inextremerotated day-nightimage matching (ERDNIM)To show the robustness of our method under both ge-ometric and illumination changes, we evaluate the match-ing performance of our method in theextremerotated Day-Night Image Matching (ERDNIM) dataset, which rotatesthe reference images of the RDNIM dataset [18], which isoriginally from the DNIM dataset [28].",
            "20": "1 SIFT SuperPoint D2Net R2D2KeyNet+HyNetGIFT LISRD ours ours*DayHEstimation 0.",
            "21": "064 0.",
            "22": "073 0.",
            "23": "044 0.",
            "24": "085 0.",
            "25": "108 0.",
            "26": "228 0.",
            "27": "2320.",
            "28": "272MMA 0.",
            "29": "049 0.",
            "30": "082 0.",
            "31": "024 0.",
            "32": "054 0.",
            "33": "068 0.",
            "34": "123 0.",
            "35": "2700.",
            "36": "2450.",
            "37": "277NightHEstimation 0.",
            "38": "108 0.",
            "39": "092 0.",
            "40": "002 0.",
            "41": "062 0.",
            "42": "097 0.",
            "43": "151 0.",
            "44": "291 0.",
            "45": "3160.",
            "46": "364MMA 0.",
            "47": "082 0.",
            "48": "111 0.",
            "49": "033 0.",
            "50": "076 0.",
            "51": "093 0.",
            "52": "177 0.",
            "53": "358 0.",
            "54": "3620.",
            "55": "404Table 1.",
            "56": "We use two evaluation metrics: homography estimation accuracy(HEstimation), and mean matching accuracy (MMA) at 3 pixel thresholds.",
            "57": "We usek=4in this experiment.",
            "58": "2.",
            "59": "Data generationThe source dataset DNIM [28] consists of 1722 imagesfrom 17 sequences of a fixed webcam taking pictures at reg-ular time spans over 48 hours.",
            "60": "Therefore, 1,722 image pairs areobtained for each of the day benchmark and night bench-mark, where the day benchmark is composed of day-dayand day-night image pairs, and the night benchmark is com-posed of night-day and night-night image pairs.",
            "61": "To evalu-ate the robustness under geometric transformation, the RD-NIM [18] dataset is generated by warping the target imageof each pair with homographies as in SuperPoint [5] gen-erated with random translations, rotations, scales, and per-spective distortions.",
            "62": "We randomly rotate the refer-ence images in the range[0\u0000,360\u0000).",
            "63": "The number of imagepairs for evaluation remains the same as RDNIM [18].",
            "64": "Fig-ure2shows some examples of ERDNIM image pairs.",
            "65": "2.",
            "66": "2.",
            "67": "Examples of ERDNIM image pairs2.",
            "68": "3.",
            "69": "Evaluation metricsWe use two evaluation metrics, HEstimation andmean matching accuracy (MMA), following LISRD [18].",
            "70": "We measure the homography estimation score [5] usingRANSAC [7] to fit the homography using the predictedmatches.",
            "71": "The predicted homography is considered to becorrect if the average distance between the four corners isless than a threshold: HEstimation=14P4i=1||\u02c6ci\u0000ci||2\uf8ff\u270f, where we use\u270f=3.",
            "72": "MMA [6,19] is the percentage ofthe correct matches over all the predicted matches, wherewe also use 3 pixels as the threshold to determine the cor-rectness of matches.",
            "73": "2.",
            "74": "4.",
            "75": "We compare the descriptor baselines SIFT [15],SuperPoint [5], D2-Net [6], R2D2 [19], KeyNet+HyNet [9,24], GIFT [14], and LISRD [18].",
            "76": "Our proposed model withthe rotation-equivariant network (ReResNet-18) achievesstate-of-the-art performance in terms of homography esti-mation.",
            "77": "GIFT [14], an existing rotation-invariant descriptor, shows a comparatively lower performance on this extremelyrotated benchmark with varying illumination.",
            "78": "Note thatwe use the same dataset generation scheme with the samesource dataset [13] to GIFT [14].",
            "79": "LISRD [18], which selectsviewpoint and illumination invariance online, demonstratesbetter MMA than ours on theDaybenchmark, but ours*which extracts top-kcandidate descriptors shows the bestMMA and homography estimation on bothDayandNightbenchmarks.",
            "80": "3.",
            "81": "Stereo track# kptsmAA 5\u0000mAA 10\u0000# inliersSuperPoint10240.",
            "82": "259 0.",
            "83": "348 61.",
            "84": "9GIFT10240.",
            "85": "2920.",
            "86": "39470.",
            "87": "8ours10240.",
            "88": "305 0.",
            "89": "404 99.",
            "90": "8SuperPoint20480.",
            "91": "263 0.",
            "92": "358 73.",
            "93": "9GIFT20480.",
            "94": "313 0.",
            "95": "42098.",
            "96": "6ours20480.",
            "97": "2960.",
            "98": "403118.",
            "99": "5Table 2.",
            "100": "Results of the downstream task in IMC2021 [8].",
            "101": "In Table2, we evaluate on IMC 2021 stereo track [8]using the validation set of PhotoTourism and PragueParksto show the results on a realistic downstream task.",
            "102": "Ourdescriptor consistently performs better than SuperPoint [5]descriptors under varying number of keypoints, and obtainscomparable results with GIFT [14] descriptors.",
            "103": "This showsthat our method performs similarly for the general and non-planar transformations, while it significantly outperformsexisting methods on Roto-360 and RDNIM datasets withextreme rotation transformations.",
            "104": "Note that it is also possi-ble to use image pairs with GT annotations of intrinsic andextrinsic parameters byapproximatingthe 2D relative ori-entation for our training1, and we leave this for future.",
            "105": "4.",
            "106": "Computational overhead and the number ofparameters4.",
            "107": "Computational overheadTable3compares an average of the inference time andGPU usage with other descriptor extraction methods above.",
            "108": "1The details of obtaining the rotation from a homography can be foundin Section 2 of \u201cDeeper understanding of the homography decomposition(Malis and Vargas, 2007)\u201d.",
            "109": "methodspeed (ms) GPU usage (GB)ours147.",
            "110": "45.",
            "111": "21 GBours\u2020206.",
            "112": "4 4.",
            "113": "83 GBSuperPoint [5]66.",
            "114": "0 2.",
            "115": "35 GBGIFT [14]198.",
            "116": "8 2.",
            "117": "93 GBLISRD [18]781.",
            "118": "0 2.",
            "119": "85 GBPosFeat [12]208.",
            "120": "8 4.",
            "121": "67 GBTable 3.",
            "122": "(Section 2.",
            "123": "8 of [26])4.",
            "124": "2.",
            "125": "6Mours\u20202.",
            "126": "6MGIFT [14]0.",
            "127": "4MLISRD [18]3.",
            "128": "7MPosFeat [12]21.",
            "129": "1MHardNet [16]9.",
            "130": "0MHyNet [25]1.",
            "131": "3MSuperPoint [5]1.",
            "132": "3MLF-Net [17]2.",
            "133": "6MRF-Net [22]1.",
            "134": "4MD2-Net [6]7.",
            "135": "6MR2D2 [19]0.",
            "136": "5MThe right table shows thenumber of parameters in mil-lions, where the first group(top) are descriptor-only mod-els and the second group(bottom) are joint detectionand description models.",
            "137": "When usingour model with the deeperbackbone denoted denoted byours\u2020, the number of model parameters increases, but itdoes not increase significantly compared to other compar-ison groups, where is still similar to that of LF-Net [17].",
            "138": "5.",
            "139": "Elaboration of multiple descriptor extrac-tionIn this section, we show the results of different configu-rations of the multiple descriptor extraction scheme whichwas mentioned in Section 4.",
            "140": "3, Table 3, Table 4, and Table 6of the main paper.",
            "141": "Table4shows the results with different strategies formultiple descriptor extraction on the Roto-360 dataset.",
            "142": "6 selects mul-tiple candidates dynamically, where the total number ofcandidates is similar to using top-2 candidates, but theMMA@5px is as high as using top-3 candidates which cand.",
            "143": "Roto-360@5px @3pxpred.",
            "144": "top191.",
            "145": "35 90.",
            "146": "18688 1161top292.",
            "147": "31 91.",
            "148": "191315 2322top392.",
            "149": "82 91.",
            "150": "692012 34830.",
            "151": "892.",
            "152": "25 91.",
            "153": "13951 16600.",
            "154": "692.",
            "155": "82 91.",
            "156": "691333 2340Table 4.",
            "157": "Note that this multi-ple descriptor extraction scheme is largely inspired by theclassical method based on an orientation histogram such asSIFT [15].",
            "158": "6.",
            "159": "Comparison with feature matching methodsmethodRoto-360@5px @3pxpred.",
            "160": "ours+NN91.",
            "161": "4 90.",
            "162": "2688.",
            "163": "3SP+SG [5,21]30.",
            "164": "1 29.",
            "165": "8874.",
            "166": "1LoFTR [23]18.",
            "167": "8 15.",
            "168": "9509.",
            "169": "4Table 5.",
            "170": "Comparison with keypoint matching methods on theRoto-360 dataset.",
            "171": "Table5compares the feature matching methods toour descriptors with simple nearest neighbour matching(NN) algorithm.",
            "172": "We evaluate our local feature with near-est neighbour matching (ours+NN) and compare it withSuperGlue [21](i.",
            "173": ", SuperPoint+SuperGlue [5,21]) andLoFTR [23].",
            "174": "The results with the simple matching algo-rithm of ours+NN clearly outperforms the two other meth-ods on the extremely rotated examples of the Roto-360dataset.",
            "175": "Note, however, that both SuperGlue [21] andLoFTR [23] are for featurematchingand thus are not di-rectly comparable to our method for featureextraction.",
            "176": "7.",
            "177": "Changing the rotation range of the GIFTTable6shows that GIFT* does not improve perfor-mance on the Roto-360 dataset because the bilinear pool-ing of GIFT does not guarantee invariance for rotation.",
            "178": "This is because our group aligning computes invariant fea-tures without breaking any equivariance, in contrast toGIFT [14] whose bilinear pooling violates group equivari-ance due to their inter-group interaction from the3\u21e53con-methodRoto-3605px 3pxpred.",
            "179": "ours91.",
            "180": "35 90.",
            "181": "18688.",
            "182": "3GIFT42.",
            "183": "05 41.",
            "184": "59589.",
            "185": "2GIFT*40.",
            "186": "71 40.",
            "187": "27564.",
            "188": "2Table 6.",
            "189": "The result of re-training the GIFT [14] model by re-placing the rotation group with 360-degree cyclic.",
            "190": "GIFT* de-notes a retrained model by extending the rotation sampling intervalfrom -180\u0000to 180\u0000.",
            "191": "8.",
            "192": "The number of sampled images for Roto-360# sample10 100 1KAlign91.",
            "193": "4 80.",
            "194": "0 89.",
            "195": "9Avg82.",
            "196": "1 72.",
            "197": "3 80.",
            "198": "7Max78.",
            "199": "0 69.",
            "200": "3 79.",
            "201": "2None18.",
            "202": "8 16.",
            "203": "4 20.",
            "204": "5Bilinear41.",
            "205": "0 28.",
            "206": "5 43.",
            "207": "7Table 7.",
            "208": "Results on Roto-360 constructed using a differentnumber of source images.",
            "209": "Table7shows the mean matching accuracy (MMA) at 5pixels threshold when increasing the number of source im-ages to 100 images (3,600 pairs) and 1,000 images (36,000pairs).",
            "210": "Therefore, we use 10 samples as they are sufficient to mea-sure the relative rotation robustness of the local features.",
            "211": "9.",
            "212": "Additional qualitative results9.",
            "213": "Visualization of the consistency of orientationestimationWe provide more examples for Figure 5 of the main pa-per, which visualize the consistency of orientation estima-tion.",
            "214": "To visualize Figure3,we create a sequence of480\u21e5640images augmented byrandom in-plane rotation with Gaussian noise sourced byILSVRC2012 [20].",
            "215": "Figure3shows the qualitative com-parison of the estimated orientation consistency.",
            "216": ",the dominant orientation es-timation is consistent if the difference with the ground-truthrotation is within a30\u0000threshold.",
            "217": "Our rotation-equivariantmodel trained with the orientation alignment loss inspiredby [10,11] consistently estimates more correct keypoint ori-entations than LF-Net [17] and RF-Net [22].",
            "218": "9.",
            "219": "2.",
            "220": "Visualization of the similarity maps of a key-point under varying rotationsFigure4shows the similarity maps with respect to a key-point under varying rotations of images with a resolutionof180\u21e5180, with uniform rotation intervals of45\u0000.",
            "221": "We vi-sualize 5 locations with the highest similarity scores withthe query keypoint for better visibility.",
            "222": "Our descriptor lo-calizes the correct keypoint locations more precisely com-pared to GIFT [14] and LF-Net [17].",
            "223": "Specifically, althoughGIFT [14] uses group-equivariant features constructed us-ing rotation augmentation, their descriptor fails to locatethe corresponding keypoints accurately in rotated imageswhich shows that the explicit rotation-equivariant net-works [26] yield better rotation-invariant features than con-structing the group-equivariance features with image aug-mentation [14].",
            "224": "9.",
            "225": "3.",
            "226": "Visualization of the predicted matches on theextreme rotationFigure5visualize the predicted matches on the ERdataset [14].",
            "227": "We extract a maximum of 1,500 keypointsfrom each image and find matches using the mutual near-est neighbor algorithm.",
            "228": "The results show that our methodconsistently finds matches more accurately compared toGIFT [14] and LF-Net [17].",
            "229": "9.",
            "230": "4.",
            "231": "Visualization of the predicted matches on theHPatches viewpointFigure6visualize the predicted matches on theHPatches [1] viewpoint variations We extract a maximumof 1,500 keypoints from each image and find matches usingthe mutual nearest neighbor algorithm.",
            "232": "The results showthat our method consistently finds matches more accuratelycompared to GIFT [14] and LF-Net [17].",
            "233": "ReferenceTargetReferenceTargetFigure 2.",
            "234": "Example of ERDNIM image pairs augmented from [18,28].",
            "235": "The reference image of a pair is augmented with random rotation in the range[0\u0000,360\u0000), and the target image is augmented byhomographies generated with random translation, rotation, scale, perspective distortion.",
            "236": "SourceTargetAligned viewSourceTargetAligned viewLF-NetRF-NetoursLF-NetRF-NetoursFigure 3.",
            "237": "We extract the source keypoints using SuperPoint [5] andobtain the target keypoints using GT homography.",
            "238": "We evaluate the consistency of orientation estimation by comparing the relative angledifference and the ground-truth angle at a threshold of30\u0000.",
            "239": "Our methodpredicts more consistent orientations of keypoints compared to LF-Net [17] and RF-Net [22].",
            "240": "0\u00ba45\u00ba90\u00ba135\u00ba180\u00ba225\u00ba270\u00ba315\u00ba oursGIFTLF-NetInputoursGIFTLF-NetInputoursGIFTLF-NetInputFigure 4.",
            "241": "Forbetter visibility, we visualize the top 5 pixels with the highest similarity to the keypoints.",
            "242": "(a) ours(b) GIFT(c) LF-Net Figure 5.",
            "243": "Visualization of predicted matches in the ER dataset [14].",
            "244": "We use a maximum of 1,500 keypoints for matching by the mutualnearest neighbor algorithm.",
            "245": "(a) ours(b) GIFT(c) LF-Net Figure 6.",
            "246": "We use a maximum of 1,500 keypoints, the mutualnearest neighbor matcher, and a three-pixel threshold for correctness.",
            "247": "In this experiment, we use the rotation-equivariant WideResNet16-8(ReWRN) backbone, which is \u2018ours\u2020\u2019 in table 4 of the main paper.",
            "248": "[1]Vassileios Balntas, Karel Lenc, Andrea Vedaldi, and Krys-tian Mikolajczyk.",
            "249": "InProceed-ings of the IEEE Conference on Computer Vision and PatternRecognition, pages 5173\u20135182, 2017.",
            "250": "1,5[2]Taco Cohen and Max Welling.",
            "251": "InInternational conference on machinelearning, pages 2990\u20132999.",
            "252": "PMLR, 2016.",
            "253": "1[3]Taco S Cohen, Mario Geiger, and Maurice Weiler.",
            "254": "InProceedings of the 33rd International Conference on NeuralInformation Processing Systems, pages 9145\u20139156, 2019.",
            "255": "1[4]Taco S Cohen and Max Welling.",
            "256": "arXivpreprint arXiv:1612.",
            "257": "08498, 2016.",
            "258": "1[5]Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-novich.",
            "259": "InCVPR Deep Learning for Visual SLAMWorkshop, 2018.",
            "260": "2,3,4,7[6]Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Polle-feys, Josef Sivic, Akihiko Torii, and Torsten Sattler.",
            "261": "D2-net: A trainable cnn for joint description and detection oflocal features.",
            "262": "InProceedings of the ieee/cvf conference oncomputer vision and pattern recognition, pages 8092\u20138101,2019.",
            "263": "2,3[7]Martin A Fischler and Robert C Bolles.",
            "264": "Communicationsof the ACM, 24(6):381\u2013395, 1981.",
            "265": "2[8]Yuhe Jin, Dmytro Mishkin, Anastasiia Mishchuk, Jiri Matas,Pascal Fua, Kwang Moo Yi, and Eduard Trulls.",
            "266": "International Journal of Computer Vision, 129(2):517\u2013547,2021.",
            "267": "1,3[9]Axel Barroso Laguna and Krystian Mikolajczyk.",
            "268": "IEEE Transactions on Pattern Analysis and MachineIntelligence, 2022.",
            "269": "2[10]Jongmin Lee, Yoonwoo Jeong, and Minsu Cho.",
            "270": "In31stBritish Machine Vision Conference 2021, BMVC 2021, Vir-tual Event, UK.",
            "271": "BMV A Press, 2021.",
            "272": "5[11]Jongmin Lee, Byungjin Kim, and Minsu Cho.",
            "273": "InProceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 4847\u20134857,2022.",
            "274": "5[12]Kunhong Li, Longguang Wang, Li Liu, Qing Ran, Kai Xu,and Yulan Guo.",
            "275": "InProceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 15838\u201315848, 2022.",
            "276": "3[13]Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,Pietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C LawrenceZitnick.",
            "277": "InEuropean conference on computer vision, pages 740\u2013755.",
            "278": "Springer, 2014.",
            "279": "Advances in NeuralInformation Processing Systems, 32:6992\u20137003, 2019.",
            "280": "1,2,3,4,5,9[15]David G Lowe.",
            "281": "International journal of computer vi-sion, 60(2):91\u2013110, 2004.",
            "282": "2,4[16]Anastasiia Mishchuk, Dmytro Mishkin, Filip Radenovic,and Jiri Matas.",
            "283": "InAdvances in NeuralInformation Processing Systems, pages 4826\u20134837, 2017.",
            "284": "3[17]Yuki Ono, Eduard Trulls, Pascal Fua, and Kwang Moo Yi.",
            "285": "InAdvancesin neural information processing systems, pages 6234\u20136244,2018.",
            "286": "3,5,7[18]R\u00b4emi Pautrat, Viktor Larsson, Martin R Oswald, and MarcPollefeys.",
            "287": "InEuropean Conference on Computer Vision,pages 707\u2013724.",
            "288": "Springer, 2020.",
            "289": "1,2,3,6[19]Jerome Revaud, Cesar De Souza, Martin Humenberger, andPhilippe Weinzaepfel.",
            "290": "R2d2: Reliable and repeatable detec-tor and descriptor.",
            "291": "Advances in neural information process-ing systems, 32:12405\u201312415, 2019.",
            "292": "2,3[20]Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,Aditya Khosla, Michael Bernstein, et al.",
            "293": "International journal ofcomputer vision, 115(3):211\u2013252, 2015.",
            "294": "4[21]Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz,and Andrew Rabinovich.",
            "295": "InProceedings ofthe IEEE/CVF conference on computer vision and patternrecognition, pages 4938\u20134947, 2020.",
            "296": "4[22]Xuelun Shen, Cheng Wang, Xin Li, Zenglei Yu, JonathanLi, Chenglu Wen, Ming Cheng, and Zijian He.",
            "297": "InProceedings of the IEEE Conference on Computer Visionand Pattern Recognition, pages 8132\u20138140, 2019.",
            "298": "InProceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pages8922\u20138931, 2021.",
            "299": "4[24]Yurun Tian, Axel Barroso Laguna, Tony Ng, Vassileios Bal-ntas, and Krystian Mikolajczyk.",
            "300": "Ad-vances in Neural Information Processing Systems, 33:7401\u20137412, 2020.",
            "301": "2[25]Yurun Tian, Axel Barroso Laguna, Tony Ng, Vassileios Bal-ntas, and Krystian Mikolajczyk.",
            "302": "InNeurIPS, 2020.",
            "303": "3[26]Maurice Weiler and Gabriele Cesa.",
            "304": "General e (2)-equivariantsteerable cnns.",
            "305": "Advances in Neural Information ProcessingSystems, 32:14334\u201314345, 2019.",
            "306": "1,3,5[27]Maurice Weiler, Fred A Hamprecht, and Martin Storath.",
            "307": "In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition, pages 849\u2013858, 2018.",
            "308": "1[28]Hao Zhou, Torsten Sattler, and David W Jacobs.",
            "309": "InEuropean Confer-ence on Computer Vision, pages 724\u2013736.",
            "310": "Springer, 2016.",
            "311": "1,2,6\n."
        },
        "A Stricter Constraint Produces Outstanding Matching: Learning More Reliable Image Matching Using a Quadratic Hinge Triplet Loss Network": {
            "authors": [],
            "url": "https://graphicsinterface.org/wp-content/uploads/gi2021-23.pdf",
            "ref_texts": "[15] Y . Liu, Z. Shen, Z. Lin, S. Peng, H. Bao, and X. Zhou. Gift: Learning transformation-invariant dense visual descriptors via group cnns. arXiv preprint arXiv:1911.05932 , 2019.",
            "ref_ids": [
                "15"
            ],
            "1": "Traditionally, detectors and descriptors are separately applied in the pipeline, SIFT [16] (and RootSIFT [3]) and SURF [4] are most popular detectors while some descriptors are followed, in which LogPolar [12] shows better performance than ContextDesc [17] from the relative pose error, while SOSNet [26] and HardNet [18] surpass GIFT [15] in the public validation set.",
            "2": "167\n[15] Y ."
        },
        "Input Image Adaption for Robust Direct SLAM using Deep Learning": {
            "authors": [],
            "url": "https://elib.dlr.de/139103/1/wang20msc.pdf",
            "ref_texts": "[Liu et al., 2019] Liu, Y., Shen, Z., Lin, Z., Peng, S., Bao, H., and Zhou, X. (2019). Gift: Learning transformation-invariant dense visual descriptors via group cnns. InAdvances in Neural Information Processing Systems , pages 6990{7001.",
            "ref_ids": [
                "Liu et al\\., 2019"
            ]
        }
    }
}